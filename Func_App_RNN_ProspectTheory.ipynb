{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as sc_stats\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "onehot_encoder=OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "time_steps = 1\n",
    "inputs = 8\n",
    "outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_releveant_features(task_df):\n",
    "\n",
    "\n",
    "    task_df['PrevOutcome']=task_df['Outcome'].shift(1)\n",
    "    task_df.loc[1,'PrevOutcome']= 0\n",
    "\n",
    "    task_df['PrevChoice']=task_df['Choice'].shift(1)\n",
    "    task_df.loc[1,'PrevChoice']= 0\n",
    "\n",
    "    task_df['PrevSafe']=task_df['Safe'].shift(1)\n",
    "    task_df.loc[1,'PrevSafe']= 0\n",
    "\n",
    "    task_df['PrevBigRisky']=task_df['BigRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevBigRisky']= 0\n",
    "\n",
    "    task_df['PrevSmallRisky']=task_df['SmallRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevSmallRisky']= 0\n",
    "    \n",
    "#     task_df['PrevRT']=task_df['RT'].shift(1)\n",
    "#     task_df.loc[1,'PrevRT']= N\n",
    "    \n",
    "    \n",
    "    \n",
    "    return task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_split_data(data,start_chunk,end_chunk):\n",
    "    \n",
    "    a=[k for k in range(start_chunk,end_chunk)]\n",
    "    out=[]\n",
    "\n",
    "    for d in range(0,data.shape[0],20):\n",
    "\n",
    "        c= [c+d for c in a]\n",
    "        out = out+c\n",
    "\n",
    "    while out[-1]>=data.shape[0]-1:\n",
    "        out.pop()\n",
    "#     return out\n",
    "    return data[out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y): \n",
    "    reset_graph()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50000\n",
    "    batch_size = int(train_X.shape[0]/2)\n",
    "    # batch_size = 100\n",
    "    length = train_X.shape[0]\n",
    "    display = 100\n",
    "    neurons = neurons\n",
    "\n",
    "    num_batches = 100\n",
    "    seq_len = 10\n",
    "\n",
    "    percent_above_PT = 1\n",
    "\n",
    "    train_threshold = 1.5#PT_R2 + percent_above_PT\n",
    "\n",
    "\n",
    "    save_step = 100\n",
    "\n",
    "\n",
    "    best_loss_val = np.infty\n",
    "    checks_since_last_progress = 0\n",
    "    max_checks_without_progress = 1000\n",
    "\n",
    "\n",
    "    # clear graph (if any) before running\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, time_steps, inputs])\n",
    "\n",
    "    y = tf.placeholder(tf.float32, [None, outputs])\n",
    "\n",
    "    # LSTM Cell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=neurons, activation=tf.nn.relu)\n",
    "    cell_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "    # pass into Dense layer\n",
    "    stacked_outputs = tf.reshape(cell_outputs, [-1, neurons])\n",
    "    out = tf.layers.dense(inputs=stacked_outputs, units=outputs)\n",
    "\n",
    "    probability = tf.nn.softmax(out)\n",
    "\n",
    "    # squared error loss or cost function for linear regression\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=y, logits=out))\n",
    "\n",
    "    # optimizer to minimize cost\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels =  tf.argmax(y, 1),\n",
    "                          predictions = tf.argmax(out, 1),\n",
    "                          name = \"accuracy\")\n",
    "    precision = tf.metrics.precision(labels=tf.argmax(y, 1),\n",
    "                                 predictions=tf.argmax(out, 1),\n",
    "                                 name=\"precision\")\n",
    "    recall = tf.metrics.recall(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"recall\")\n",
    "    f1 = 2 * accuracy[1] * recall[1] / ( precision[1] + recall[1] )\n",
    "\n",
    "    acc_up,acc_val = accuracy\n",
    "    auc = tf.metrics.auc(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"auc\")\n",
    "    \n",
    "    valid_store = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #######################\n",
    "#         saver.restore(sess, \"./checkpts/Original_RNN_LSTM_8features_v2.ckpt\")\n",
    "#         saver.restore(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "        \n",
    "        if pretraining == True:\n",
    "\n",
    "            saver.restore(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        #######################\n",
    "        \n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "\n",
    "        # Train the model\n",
    "        for steps in range(epochs):\n",
    "            mini_batch = zip(range(0, length, batch_size),\n",
    "                       range(batch_size, length+1, batch_size))\n",
    "\n",
    "            # train data in mini-batches\n",
    "            for (start, end) in mini_batch:\n",
    "    #             print(start,end)\n",
    "                sess.run(training_op, feed_dict = {X: train_X[start:end,:,:],\n",
    "                                                   y: train_y[start:end,:]}) \n",
    "\n",
    "            ## train data in batches of length subsequence\n",
    "\n",
    "    #         for k in range(num_batches):\n",
    "    #             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "\n",
    "    #             sess.run(training_op, feed_dict = {X:X_seq,y:y_seq}) \n",
    "            loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "            loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "            # print training performance \n",
    "            if (steps+1) % display == 0:\n",
    "                # evaluate loss function on training set\n",
    "\n",
    "\n",
    "                loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining loss: {}'.format((steps+1), loss_fn))\n",
    "\n",
    "                acc_train = acc_val.eval(feed_dict={X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining accuracy: {}'.format((steps+1), acc_train))\n",
    "\n",
    "\n",
    "                acc_test = acc_val.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest accuracy: {}'.format((steps+1), acc_test))\n",
    "\n",
    "                loss_test = loss.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest loss: {}'.format((steps+1), loss_test))\n",
    "\n",
    "                accu_val = acc_val.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "                loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "                print('Step: {}  \\tValid loss: {}'.format((steps+1), loss_val))\n",
    "\n",
    "                valid_store.append(loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (1 + loss_fn/np.log(0.5)) > train_threshold:\n",
    "                    print(\"Threshold achieved, quit training\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            if loss_val < best_loss_val:\n",
    "\n",
    "                        best_loss_val = loss_val\n",
    "                        checks_since_last_progress = 0\n",
    "            else:\n",
    "                            checks_since_last_progress += 1\n",
    "\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if checks_since_last_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "\n",
    "            if (steps+1) % save_step ==0:\n",
    "                                save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#                 save_path = saver.save(sess, \"./checkpts/RNN_Internet_LSTM_model_5features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     evaluate model accuracy\n",
    "        acc, prec, recall, f1, AUC = sess.run([accuracy, precision, recall, f1,auc],\n",
    "                                         feed_dict = {X: train_X, y: train_y})\n",
    "        prob_train = probability.eval(feed_dict = {X: train_X, y: train_y})\n",
    "        prob_test = probability.eval(feed_dict = {X: test_X, y: test_y})\n",
    "        prob_valid = probability.eval(feed_dict = {X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nEvaluation  on training set')\n",
    "        print('Accuracy:', acc[1])\n",
    "        print('Precision:', prec[1])\n",
    "        print('Recall:', recall[1])\n",
    "        print('F1 score:', f1)\n",
    "        print('AUC:', AUC[1])\n",
    "        \n",
    "      \n",
    "    \n",
    "    \n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/LaterDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## APP DATA\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_APPDATA_RNN_LSTM_8features.ckpt\")\n",
    "        save_path = saver.save(sess, \"./checkpts/Later_v2_APPDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "    metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,AUC[1],loss_fn,accu_val,best_loss_val,acc_test,loss_test,neurons,learning_rate,epochs,steps]).reshape(-1,14),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\",\"loss\",\"accuracy_val\",\"loss_val\",\"accuracy_test\",\"loss_test\",\"neurons\",\"learning_rate\",\"n_epochs\",\"steps\"])\n",
    "    return metric_out_df, prob_train, prob_test, prob_valid\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def random_subsequence(X,y,seq_len):\n",
    "    rnd  = random.randint(0,len(X)-seq_len)\n",
    "    X_seq, y_seq = X[rnd:rnd+seq_len,:], y[rnd:rnd+seq_len,:]\n",
    "    return X_seq, y_seq\n",
    "\n",
    "    print(y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ways of cutting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odd plays train, even plays test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\")\n",
    "dir_path =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"\n",
    "\n",
    "subj_files_list =[]; ## list of subject_files fullfilling a criteria\n",
    "\n",
    "dir_files = [i for i in os.listdir(dir_path) if i.startswith('sub')]\n",
    "\n",
    "for subj_file_path in dir_files:\n",
    "\n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "    mypath =file_path\n",
    "    \n",
    "    play_names = [i for i in os.listdir(mypath) if os.path.isfile(os.path.join(mypath,i)) and i.startswith('app')]   \n",
    "    \n",
    "    if len(play_names) >= 50: ## criteria\n",
    "        subj_files_list.append(subj_file_path)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subj_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "num_shuffles=5\n",
    "for num, subj_file_path in enumerate(subj_files_list[]):\n",
    "    print(num)\n",
    "# for subj_file_path in [subj_files_list[0]]:\n",
    "    \n",
    "#     train_data,test_data, val_data = np.empty((0,task_df.columns.shape[0])),  np.empty((0,task_df.columns.shape[0])), np.empty((0,task_df.columns.shape[0]))\n",
    "    train_data,test_data, val_data = np.empty((0,15)),  np.empty((0,15)), np.empty((0,15))\n",
    "\n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "    mypath =file_path\n",
    "        \n",
    "    comp_task_train_df = pd.DataFrame()\n",
    "\n",
    "    play_names = [i for i in os.listdir(mypath) if os.path.isfile(os.path.join(mypath,i)) and i.startswith('app')]    \n",
    "\n",
    "    for randomization_counter in range(0,num_shuffles):\n",
    "            randomized_play_names= random.sample(play_names,len(play_names))\n",
    "            \n",
    "            for play_num, play_name in enumerate(randomized_play_names):\n",
    "#         for play_num,play_name in enumerate(play_names):\n",
    "\n",
    "                file_name = file_path + \"/\" + play_name\n",
    "                task_df = pd.read_csv(file_name)\n",
    "                task_df = add_releveant_features(task_df)\n",
    "\n",
    "                if np.mod(play_num,2)==0: ## odd trials\n",
    "                    train_data = np.append(train_data,task_df[task_df.TrialNum>1].values, axis=0)\n",
    "\n",
    "                else:\n",
    "                    test_data =  np.append(test_data, task_df[task_df.TrialNum>1].values[0:16], axis=0)\n",
    "                    val_data =  np.append(val_data, task_df[task_df.TrialNum>1].values[16:], axis=0)\n",
    "\n",
    "\n",
    "    train_data_df= pd.DataFrame(train_data,columns=task_df.columns)\n",
    "    val_data_df = pd.DataFrame(test_data,columns=task_df.columns)\n",
    "    test_data_df= pd.DataFrame(val_data,columns=task_df.columns)\n",
    "\n",
    "#     file_path = file_path + \"/OddEvenPlays/\"\n",
    "    file_path = file_path + \"/OddEvenPlays/RandomizedPlays10\"\n",
    "\n",
    "#     os.mkdir(file_path)\n",
    "    train_data_df.to_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df.to_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df.to_csv(file_path+\"/val_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_odd_even(train_data_df,test_data_df,val_data_df):\n",
    "\n",
    "#     train_len = 29\n",
    "#     test_len = 14\n",
    "#     val_len = 15\n",
    "\n",
    "    ##----------------- UNCOMMENT BELOW\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "    \n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "####### Prev O + C+ R + CurrO--------------------\n",
    " \n",
    "    train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    \n",
    "    ######## sampling \n",
    "    \n",
    "    \n",
    "#### - Prev RT+C+R+O + Curr O----------------------\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### PRE TRAINING\n",
    "#     stop = int(0.7*len(train_X))\n",
    "#     print(stop)\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int((len(train_X)-stop)/2)], train_X[stop+int((len(train_X)-stop)/2):],train_y[:stop], train_y[stop:stop+int((len(train_X)-stop)/2)], train_y[stop+int((len(train_X)-stop)/2):]\n",
    "    \n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = train_X, test_X, test_X, train_y, test_y, test_y\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(val_X.shape)\n",
    "    print(val_y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "    # # center and scale\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.fit_transform(test_X)\n",
    "    val_X = scaler.fit_transform(val_X)\n",
    "\n",
    "\n",
    "    train_X = train_X[:,None,:]\n",
    "    val_X = val_X[:,None,:]\n",
    "    test_X = test_X[:,None,:]\n",
    "\n",
    "\n",
    "    # # one-hot encode the outputs\n",
    "\n",
    "    onehot_encoder = OneHotEncoder()\n",
    "    encode_categorical = train_y.reshape(len(train_y), 1)\n",
    "    encode_categorical_test = test_y.reshape(len(test_y), 1)\n",
    "    encode_categorical_val = val_y.reshape(len(val_y),1)\n",
    "\n",
    "\n",
    "    train_y = onehot_encoder.fit_transform(encode_categorical).toarray()\n",
    "    test_y = onehot_encoder.fit_transform(encode_categorical_test).toarray()\n",
    "    val_y = onehot_encoder.fit_transform(encode_categorical_val).toarray()\n",
    "\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, val_X,val_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(5945, 8)\n",
      "(5945, 1)\n",
      "(3200, 8)\n",
      "(3200, 1)\n",
      "(2600, 8)\n",
      "(2600, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-45e26c98b632>:36: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-45e26c98b632>:37: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/ritwik7/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-45e26c98b632>:41: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/ritwik7/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/ritwik7/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Step: 100  \tTraining loss: 0.354576051235199\n",
      "Step: 100  \tTraining accuracy: 0.8576955199241638\n",
      "Step: 100  \tValid loss: 0.34452301263809204\n",
      "Step: 200  \tTraining loss: 0.30858197808265686\n",
      "Step: 200  \tTraining accuracy: 0.8671000599861145\n",
      "Step: 200  \tValid loss: 0.3039250373840332\n",
      "Step: 300  \tTraining loss: 0.3038502037525177\n",
      "Step: 300  \tTraining accuracy: 0.8742992877960205\n",
      "Step: 300  \tValid loss: 0.2980923354625702\n",
      "Step: 400  \tTraining loss: 0.3012202978134155\n",
      "Step: 400  \tTraining accuracy: 0.878363311290741\n",
      "Step: 400  \tValid loss: 0.29423993825912476\n",
      "Step: 500  \tTraining loss: 0.29967963695526123\n",
      "Step: 500  \tTraining accuracy: 0.8812092542648315\n",
      "Step: 500  \tValid loss: 0.2916617691516876\n",
      "Step: 600  \tTraining loss: 0.2988569438457489\n",
      "Step: 600  \tTraining accuracy: 0.8832534551620483\n",
      "Step: 600  \tValid loss: 0.2901608943939209\n",
      "Step: 700  \tTraining loss: 0.2983165979385376\n",
      "Step: 700  \tTraining accuracy: 0.8847346901893616\n",
      "Step: 700  \tValid loss: 0.28924426436424255\n",
      "Step: 800  \tTraining loss: 0.2978316843509674\n",
      "Step: 800  \tTraining accuracy: 0.8858325481414795\n",
      "Step: 800  \tValid loss: 0.2884589433670044\n",
      "Step: 900  \tTraining loss: 0.29740196466445923\n",
      "Step: 900  \tTraining accuracy: 0.8867223858833313\n",
      "Step: 900  \tValid loss: 0.28785955905914307\n",
      "Step: 1000  \tTraining loss: 0.29698824882507324\n",
      "Step: 1000  \tTraining accuracy: 0.8875414133071899\n",
      "Step: 1000  \tValid loss: 0.2873373329639435\n",
      "Step: 1100  \tTraining loss: 0.2965342104434967\n",
      "Step: 1100  \tTraining accuracy: 0.8882288336753845\n",
      "Step: 1100  \tValid loss: 0.28680557012557983\n",
      "Step: 1200  \tTraining loss: 0.29592785239219666\n",
      "Step: 1200  \tTraining accuracy: 0.8888115882873535\n",
      "Step: 1200  \tValid loss: 0.2860667407512665\n",
      "Step: 1300  \tTraining loss: 0.2949099540710449\n",
      "Step: 1300  \tTraining accuracy: 0.8894509077072144\n",
      "Step: 1300  \tValid loss: 0.2847048044204712\n",
      "Step: 1400  \tTraining loss: 0.29379698634147644\n",
      "Step: 1400  \tTraining accuracy: 0.8900207877159119\n",
      "Step: 1400  \tValid loss: 0.28305643796920776\n",
      "Step: 1500  \tTraining loss: 0.2926422655582428\n",
      "Step: 1500  \tTraining accuracy: 0.8904710412025452\n",
      "Step: 1500  \tValid loss: 0.2815142869949341\n",
      "Step: 1600  \tTraining loss: 0.2916029393672943\n",
      "Step: 1600  \tTraining accuracy: 0.8908631801605225\n",
      "Step: 1600  \tValid loss: 0.28004124760627747\n",
      "Step: 1700  \tTraining loss: 0.29070615768432617\n",
      "Step: 1700  \tTraining accuracy: 0.8912593722343445\n",
      "Step: 1700  \tValid loss: 0.27892112731933594\n",
      "Step: 1800  \tTraining loss: 0.2898780107498169\n",
      "Step: 1800  \tTraining accuracy: 0.8916686773300171\n",
      "Step: 1800  \tValid loss: 0.2779676020145416\n",
      "Step: 1900  \tTraining loss: 0.2890363335609436\n",
      "Step: 1900  \tTraining accuracy: 0.8921396136283875\n",
      "Step: 1900  \tValid loss: 0.27702251076698303\n",
      "Step: 2000  \tTraining loss: 0.28827664256095886\n",
      "Step: 2000  \tTraining accuracy: 0.8925753235816956\n",
      "Step: 2000  \tValid loss: 0.2761688232421875\n",
      "Step: 2100  \tTraining loss: 0.2874537408351898\n",
      "Step: 2100  \tTraining accuracy: 0.8929768204689026\n",
      "Step: 2100  \tValid loss: 0.2753234803676605\n",
      "Step: 2200  \tTraining loss: 0.28667834401130676\n",
      "Step: 2200  \tTraining accuracy: 0.8933607935905457\n",
      "Step: 2200  \tValid loss: 0.2744874358177185\n",
      "Step: 2300  \tTraining loss: 0.2859298884868622\n",
      "Step: 2300  \tTraining accuracy: 0.8937408924102783\n",
      "Step: 2300  \tValid loss: 0.2736380696296692\n",
      "Step: 2400  \tTraining loss: 0.2851939797401428\n",
      "Step: 2400  \tTraining accuracy: 0.8941394090652466\n",
      "Step: 2400  \tValid loss: 0.2727733254432678\n",
      "Step: 2500  \tTraining loss: 0.28447845578193665\n",
      "Step: 2500  \tTraining accuracy: 0.8945261836051941\n",
      "Step: 2500  \tValid loss: 0.2719362676143646\n",
      "Step: 2600  \tTraining loss: 0.2837883234024048\n",
      "Step: 2600  \tTraining accuracy: 0.8948726654052734\n",
      "Step: 2600  \tValid loss: 0.27113255858421326\n",
      "Step: 2700  \tTraining loss: 0.28313279151916504\n",
      "Step: 2700  \tTraining accuracy: 0.8951961994171143\n",
      "Step: 2700  \tValid loss: 0.2703711986541748\n",
      "Step: 2800  \tTraining loss: 0.28251218795776367\n",
      "Step: 2800  \tTraining accuracy: 0.8955023884773254\n",
      "Step: 2800  \tValid loss: 0.26964429020881653\n",
      "Step: 2900  \tTraining loss: 0.28192752599716187\n",
      "Step: 2900  \tTraining accuracy: 0.8957781195640564\n",
      "Step: 2900  \tValid loss: 0.26894715428352356\n",
      "Step: 3000  \tTraining loss: 0.2813710868358612\n",
      "Step: 3000  \tTraining accuracy: 0.8960784673690796\n",
      "Step: 3000  \tValid loss: 0.26830777525901794\n",
      "Step: 3100  \tTraining loss: 0.28082549571990967\n",
      "Step: 3100  \tTraining accuracy: 0.8963730931282043\n",
      "Step: 3100  \tValid loss: 0.2676564157009125\n",
      "Step: 3200  \tTraining loss: 0.2803216576576233\n",
      "Step: 3200  \tTraining accuracy: 0.8966544270515442\n",
      "Step: 3200  \tValid loss: 0.26706254482269287\n",
      "Step: 3300  \tTraining loss: 0.27985405921936035\n",
      "Step: 3300  \tTraining accuracy: 0.896949827671051\n",
      "Step: 3300  \tValid loss: 0.266520231962204\n",
      "Step: 3400  \tTraining loss: 0.27941983938217163\n",
      "Step: 3400  \tTraining accuracy: 0.8972530961036682\n",
      "Step: 3400  \tValid loss: 0.2660224437713623\n",
      "Step: 3500  \tTraining loss: 0.27902156114578247\n",
      "Step: 3500  \tTraining accuracy: 0.8975387215614319\n",
      "Step: 3500  \tValid loss: 0.2655833959579468\n",
      "Step: 3600  \tTraining loss: 0.278659850358963\n",
      "Step: 3600  \tTraining accuracy: 0.8977986574172974\n",
      "Step: 3600  \tValid loss: 0.26519882678985596\n",
      "Step: 3700  \tTraining loss: 0.2783358693122864\n",
      "Step: 3700  \tTraining accuracy: 0.8980560302734375\n",
      "Step: 3700  \tValid loss: 0.2648576498031616\n",
      "Step: 3800  \tTraining loss: 0.27804529666900635\n",
      "Step: 3800  \tTraining accuracy: 0.8983019590377808\n",
      "Step: 3800  \tValid loss: 0.2645564079284668\n",
      "Step: 3900  \tTraining loss: 0.27775880694389343\n",
      "Step: 3900  \tTraining accuracy: 0.8985439538955688\n",
      "Step: 3900  \tValid loss: 0.2642870247364044\n",
      "Step: 4000  \tTraining loss: 0.2775038182735443\n",
      "Step: 4000  \tTraining accuracy: 0.8987866640090942\n",
      "Step: 4000  \tValid loss: 0.2640244662761688\n",
      "Step: 4100  \tTraining loss: 0.27728071808815\n",
      "Step: 4100  \tTraining accuracy: 0.8990299701690674\n",
      "Step: 4100  \tValid loss: 0.26381903886795044\n",
      "Step: 4200  \tTraining loss: 0.27707841992378235\n",
      "Step: 4200  \tTraining accuracy: 0.8992676734924316\n",
      "Step: 4200  \tValid loss: 0.26363810896873474\n",
      "Step: 4300  \tTraining loss: 0.2768920063972473\n",
      "Step: 4300  \tTraining accuracy: 0.8994942307472229\n",
      "Step: 4300  \tValid loss: 0.26348477602005005\n",
      "Step: 4400  \tTraining loss: 0.27671995759010315\n",
      "Step: 4400  \tTraining accuracy: 0.8997005820274353\n",
      "Step: 4400  \tValid loss: 0.2633552849292755\n",
      "Step: 4500  \tTraining loss: 0.2765604555606842\n",
      "Step: 4500  \tTraining accuracy: 0.8998804092407227\n",
      "Step: 4500  \tValid loss: 0.2632310390472412\n",
      "Step: 4600  \tTraining loss: 0.2764109671115875\n",
      "Step: 4600  \tTraining accuracy: 0.9000411629676819\n",
      "Step: 4600  \tValid loss: 0.2631225287914276\n",
      "Step: 4700  \tTraining loss: 0.2762693166732788\n",
      "Step: 4700  \tTraining accuracy: 0.9001950025558472\n",
      "Step: 4700  \tValid loss: 0.26301905512809753\n",
      "Step: 4800  \tTraining loss: 0.27613502740859985\n",
      "Step: 4800  \tTraining accuracy: 0.9003369212150574\n",
      "Step: 4800  \tValid loss: 0.26291781663894653\n",
      "Step: 4900  \tTraining loss: 0.2760065495967865\n",
      "Step: 4900  \tTraining accuracy: 0.9004660248756409\n",
      "Step: 4900  \tValid loss: 0.2628156840801239\n",
      "Step: 5000  \tTraining loss: 0.2758823037147522\n",
      "Step: 5000  \tTraining accuracy: 0.9005898833274841\n",
      "Step: 5000  \tValid loss: 0.2627217769622803\n",
      "Step: 5100  \tTraining loss: 0.2757621109485626\n",
      "Step: 5100  \tTraining accuracy: 0.900712251663208\n",
      "Step: 5100  \tValid loss: 0.2626330554485321\n",
      "Step: 5200  \tTraining loss: 0.2756458818912506\n",
      "Step: 5200  \tTraining accuracy: 0.9008298516273499\n",
      "Step: 5200  \tValid loss: 0.26255089044570923\n",
      "Step: 5300  \tTraining loss: 0.27553287148475647\n",
      "Step: 5300  \tTraining accuracy: 0.9009429216384888\n",
      "Step: 5300  \tValid loss: 0.2624691128730774\n",
      "Step: 5400  \tTraining loss: 0.2754230797290802\n",
      "Step: 5400  \tTraining accuracy: 0.9010502099990845\n",
      "Step: 5400  \tValid loss: 0.26239538192749023\n",
      "Step: 5500  \tTraining loss: 0.27531710267066956\n",
      "Step: 5500  \tTraining accuracy: 0.9011504650115967\n",
      "Step: 5500  \tValid loss: 0.2623221278190613\n",
      "Step: 5600  \tTraining loss: 0.27521318197250366\n",
      "Step: 5600  \tTraining accuracy: 0.9012670516967773\n",
      "Step: 5600  \tValid loss: 0.2622486650943756\n",
      "Step: 5700  \tTraining loss: 0.27511066198349\n",
      "Step: 5700  \tTraining accuracy: 0.9013885259628296\n",
      "Step: 5700  \tValid loss: 0.2621755301952362\n",
      "Step: 5800  \tTraining loss: 0.27500927448272705\n",
      "Step: 5800  \tTraining accuracy: 0.9015116691589355\n",
      "Step: 5800  \tValid loss: 0.2621041238307953\n",
      "Step: 5900  \tTraining loss: 0.27490901947021484\n",
      "Step: 5900  \tTraining accuracy: 0.9016306400299072\n",
      "Step: 5900  \tValid loss: 0.2620429992675781\n",
      "Step: 6000  \tTraining loss: 0.274809330701828\n",
      "Step: 6000  \tTraining accuracy: 0.901745617389679\n",
      "Step: 6000  \tValid loss: 0.2619852125644684\n",
      "Step: 6100  \tTraining loss: 0.274709016084671\n",
      "Step: 6100  \tTraining accuracy: 0.9018553495407104\n",
      "Step: 6100  \tValid loss: 0.2619255483150482\n",
      "Step: 6200  \tTraining loss: 0.2746078073978424\n",
      "Step: 6200  \tTraining accuracy: 0.9019560217857361\n",
      "Step: 6200  \tValid loss: 0.26186639070510864\n",
      "Step: 6300  \tTraining loss: 0.27450552582740784\n",
      "Step: 6300  \tTraining accuracy: 0.9020534157752991\n",
      "Step: 6300  \tValid loss: 0.2618149220943451\n",
      "Step: 6400  \tTraining loss: 0.2744014859199524\n",
      "Step: 6400  \tTraining accuracy: 0.9021450877189636\n",
      "Step: 6400  \tValid loss: 0.261765718460083\n",
      "Step: 6500  \tTraining loss: 0.27429625391960144\n",
      "Step: 6500  \tTraining accuracy: 0.9022405743598938\n",
      "Step: 6500  \tValid loss: 0.2617201805114746\n",
      "Step: 6600  \tTraining loss: 0.27418091893196106\n",
      "Step: 6600  \tTraining accuracy: 0.9023252725601196\n",
      "Step: 6600  \tValid loss: 0.26169031858444214\n",
      "Step: 6700  \tTraining loss: 0.27406901121139526\n",
      "Step: 6700  \tTraining accuracy: 0.9023984670639038\n",
      "Step: 6700  \tValid loss: 0.26164138317108154\n",
      "Step: 6800  \tTraining loss: 0.2739574909210205\n",
      "Step: 6800  \tTraining accuracy: 0.9024670124053955\n",
      "Step: 6800  \tValid loss: 0.2615821659564972\n",
      "Step: 6900  \tTraining loss: 0.27384626865386963\n",
      "Step: 6900  \tTraining accuracy: 0.9025298357009888\n",
      "Step: 6900  \tValid loss: 0.2615209221839905\n",
      "Step: 7000  \tTraining loss: 0.2737356126308441\n",
      "Step: 7000  \tTraining accuracy: 0.9025883674621582\n",
      "Step: 7000  \tValid loss: 0.2614591121673584\n",
      "Step: 7100  \tTraining loss: 0.273625910282135\n",
      "Step: 7100  \tTraining accuracy: 0.9026500582695007\n",
      "Step: 7100  \tValid loss: 0.26140540838241577\n",
      "Step: 7200  \tTraining loss: 0.27351731061935425\n",
      "Step: 7200  \tTraining accuracy: 0.9027040600776672\n",
      "Step: 7200  \tValid loss: 0.261349618434906\n",
      "Step: 7300  \tTraining loss: 0.27340731024742126\n",
      "Step: 7300  \tTraining accuracy: 0.9027578234672546\n",
      "Step: 7300  \tValid loss: 0.2612614035606384\n",
      "Step: 7400  \tTraining loss: 0.27329373359680176\n",
      "Step: 7400  \tTraining accuracy: 0.9028123617172241\n",
      "Step: 7400  \tValid loss: 0.2611628770828247\n",
      "Step: 7500  \tTraining loss: 0.273181289434433\n",
      "Step: 7500  \tTraining accuracy: 0.902868926525116\n",
      "Step: 7500  \tValid loss: 0.2610836327075958\n",
      "Step: 7600  \tTraining loss: 0.27306973934173584\n",
      "Step: 7600  \tTraining accuracy: 0.9029262065887451\n",
      "Step: 7600  \tValid loss: 0.2610245645046234\n",
      "Step: 7700  \tTraining loss: 0.27295851707458496\n",
      "Step: 7700  \tTraining accuracy: 0.9029819965362549\n",
      "Step: 7700  \tValid loss: 0.2609614133834839\n",
      "Step: 7800  \tTraining loss: 0.27284765243530273\n",
      "Step: 7800  \tTraining accuracy: 0.9030352234840393\n",
      "Step: 7800  \tValid loss: 0.2609040141105652\n",
      "Step: 7900  \tTraining loss: 0.27273669838905334\n",
      "Step: 7900  \tTraining accuracy: 0.9030827879905701\n",
      "Step: 7900  \tValid loss: 0.26083919405937195\n",
      "Step: 8000  \tTraining loss: 0.27262574434280396\n",
      "Step: 8000  \tTraining accuracy: 0.9031313061714172\n",
      "Step: 8000  \tValid loss: 0.2607785165309906\n",
      "Step: 8100  \tTraining loss: 0.272514671087265\n",
      "Step: 8100  \tTraining accuracy: 0.903181791305542\n",
      "Step: 8100  \tValid loss: 0.26071688532829285\n",
      "Step: 8200  \tTraining loss: 0.27240344882011414\n",
      "Step: 8200  \tTraining accuracy: 0.9032310247421265\n",
      "Step: 8200  \tValid loss: 0.26065415143966675\n",
      "Step: 8300  \tTraining loss: 0.27229180932044983\n",
      "Step: 8300  \tTraining accuracy: 0.90327388048172\n",
      "Step: 8300  \tValid loss: 0.26058417558670044\n",
      "Step: 8400  \tTraining loss: 0.2721792757511139\n",
      "Step: 8400  \tTraining accuracy: 0.9033106565475464\n",
      "Step: 8400  \tValid loss: 0.26051196455955505\n",
      "Step: 8500  \tTraining loss: 0.2720666527748108\n",
      "Step: 8500  \tTraining accuracy: 0.9033465385437012\n",
      "Step: 8500  \tValid loss: 0.2604426443576813\n",
      "Step: 8600  \tTraining loss: 0.2719530463218689\n",
      "Step: 8600  \tTraining accuracy: 0.9033815860748291\n",
      "Step: 8600  \tValid loss: 0.260367751121521\n",
      "Step: 8700  \tTraining loss: 0.27183860540390015\n",
      "Step: 8700  \tTraining accuracy: 0.9034138321876526\n",
      "Step: 8700  \tValid loss: 0.26028749346733093\n",
      "Step: 8800  \tTraining loss: 0.2717230021953583\n",
      "Step: 8800  \tTraining accuracy: 0.9034424424171448\n",
      "Step: 8800  \tValid loss: 0.26020318269729614\n",
      "Step: 8900  \tTraining loss: 0.271605908870697\n",
      "Step: 8900  \tTraining accuracy: 0.9034703969955444\n",
      "Step: 8900  \tValid loss: 0.2601149380207062\n",
      "Step: 9000  \tTraining loss: 0.2714873254299164\n",
      "Step: 9000  \tTraining accuracy: 0.9034977555274963\n",
      "Step: 9000  \tValid loss: 0.2600231170654297\n",
      "Step: 9100  \tTraining loss: 0.27136683464050293\n",
      "Step: 9100  \tTraining accuracy: 0.9035226106643677\n",
      "Step: 9100  \tValid loss: 0.2599294185638428\n",
      "Step: 9200  \tTraining loss: 0.27124419808387756\n",
      "Step: 9200  \tTraining accuracy: 0.9035441279411316\n",
      "Step: 9200  \tValid loss: 0.2598329186439514\n",
      "Step: 9300  \tTraining loss: 0.2711188495159149\n",
      "Step: 9300  \tTraining accuracy: 0.9035651683807373\n",
      "Step: 9300  \tValid loss: 0.2597304582595825\n",
      "Step: 9400  \tTraining loss: 0.2709909677505493\n",
      "Step: 9400  \tTraining accuracy: 0.9035857915878296\n",
      "Step: 9400  \tValid loss: 0.25962206721305847\n",
      "Step: 9500  \tTraining loss: 0.2708599269390106\n",
      "Step: 9500  \tTraining accuracy: 0.9036059379577637\n",
      "Step: 9500  \tValid loss: 0.2595066726207733\n",
      "Step: 9600  \tTraining loss: 0.2707253396511078\n",
      "Step: 9600  \tTraining accuracy: 0.9036256670951843\n",
      "Step: 9600  \tValid loss: 0.2593858540058136\n",
      "Step: 9700  \tTraining loss: 0.2705870270729065\n",
      "Step: 9700  \tTraining accuracy: 0.9036450386047363\n",
      "Step: 9700  \tValid loss: 0.2592600882053375\n",
      "Step: 9800  \tTraining loss: 0.2704451084136963\n",
      "Step: 9800  \tTraining accuracy: 0.9036639332771301\n",
      "Step: 9800  \tValid loss: 0.2591385543346405\n",
      "Step: 9900  \tTraining loss: 0.27029934525489807\n",
      "Step: 9900  \tTraining accuracy: 0.9036825299263\n",
      "Step: 9900  \tValid loss: 0.25901028513908386\n",
      "Step: 10000  \tTraining loss: 0.2701491415500641\n",
      "Step: 10000  \tTraining accuracy: 0.9036998152732849\n",
      "Step: 10000  \tValid loss: 0.25887590646743774\n",
      "Step: 10100  \tTraining loss: 0.2699947655200958\n",
      "Step: 10100  \tTraining accuracy: 0.9037176370620728\n",
      "Step: 10100  \tValid loss: 0.2587399482727051\n",
      "Step: 10200  \tTraining loss: 0.26983657479286194\n",
      "Step: 10200  \tTraining accuracy: 0.9037351608276367\n",
      "Step: 10200  \tValid loss: 0.25860244035720825\n",
      "Step: 10300  \tTraining loss: 0.269674688577652\n",
      "Step: 10300  \tTraining accuracy: 0.9037522673606873\n",
      "Step: 10300  \tValid loss: 0.2584642469882965\n",
      "Step: 10400  \tTraining loss: 0.2695094645023346\n",
      "Step: 10400  \tTraining accuracy: 0.9037690758705139\n",
      "Step: 10400  \tValid loss: 0.2583286166191101\n",
      "Step: 10500  \tTraining loss: 0.2693415582180023\n",
      "Step: 10500  \tTraining accuracy: 0.9037855863571167\n",
      "Step: 10500  \tValid loss: 0.2581997811794281\n",
      "Step: 10600  \tTraining loss: 0.26916182041168213\n",
      "Step: 10600  \tTraining accuracy: 0.9038017392158508\n",
      "Step: 10600  \tValid loss: 0.25810107588768005\n",
      "Step: 10700  \tTraining loss: 0.2689836621284485\n",
      "Step: 10700  \tTraining accuracy: 0.9038152098655701\n",
      "Step: 10700  \tValid loss: 0.25799891352653503\n",
      "Step: 10800  \tTraining loss: 0.26880785822868347\n",
      "Step: 10800  \tTraining accuracy: 0.9038244485855103\n",
      "Step: 10800  \tValid loss: 0.25788044929504395\n",
      "Step: 10900  \tTraining loss: 0.2686325013637543\n",
      "Step: 10900  \tTraining accuracy: 0.903831958770752\n",
      "Step: 10900  \tValid loss: 0.2577681243419647\n",
      "Step: 11000  \tTraining loss: 0.26845794916152954\n",
      "Step: 11000  \tTraining accuracy: 0.9038393497467041\n",
      "Step: 11000  \tValid loss: 0.2576594054698944\n",
      "Step: 11100  \tTraining loss: 0.2682848274707794\n",
      "Step: 11100  \tTraining accuracy: 0.9038466215133667\n",
      "Step: 11100  \tValid loss: 0.2575559616088867\n",
      "Step: 11200  \tTraining loss: 0.26811325550079346\n",
      "Step: 11200  \tTraining accuracy: 0.9038552641868591\n",
      "Step: 11200  \tValid loss: 0.25745558738708496\n",
      "Step: 11300  \tTraining loss: 0.26794350147247314\n",
      "Step: 11300  \tTraining accuracy: 0.9038659930229187\n",
      "Step: 11300  \tValid loss: 0.2573641240596771\n",
      "Step: 11400  \tTraining loss: 0.2677604556083679\n",
      "Step: 11400  \tTraining accuracy: 0.9038758277893066\n",
      "Step: 11400  \tValid loss: 0.2572561204433441\n",
      "Step: 11500  \tTraining loss: 0.2675945460796356\n",
      "Step: 11500  \tTraining accuracy: 0.9038810133934021\n",
      "Step: 11500  \tValid loss: 0.25717049837112427\n",
      "Step: 11600  \tTraining loss: 0.26743316650390625\n",
      "Step: 11600  \tTraining accuracy: 0.9038868546485901\n",
      "Step: 11600  \tValid loss: 0.2570948600769043\n",
      "Step: 11700  \tTraining loss: 0.2672744393348694\n",
      "Step: 11700  \tTraining accuracy: 0.9038896560668945\n",
      "Step: 11700  \tValid loss: 0.25702258944511414\n",
      "Step: 11800  \tTraining loss: 0.26711753010749817\n",
      "Step: 11800  \tTraining accuracy: 0.9038909673690796\n",
      "Step: 11800  \tValid loss: 0.2569509446620941\n",
      "Step: 11900  \tTraining loss: 0.2669637203216553\n",
      "Step: 11900  \tTraining accuracy: 0.903892993927002\n",
      "Step: 11900  \tValid loss: 0.2568909823894501\n",
      "Step: 12000  \tTraining loss: 0.26681411266326904\n",
      "Step: 12000  \tTraining accuracy: 0.9038956761360168\n",
      "Step: 12000  \tValid loss: 0.25683751702308655\n",
      "Step: 12100  \tTraining loss: 0.26666703820228577\n",
      "Step: 12100  \tTraining accuracy: 0.9038976430892944\n",
      "Step: 12100  \tValid loss: 0.25678569078445435\n",
      "Step: 12200  \tTraining loss: 0.26652368903160095\n",
      "Step: 12200  \tTraining accuracy: 0.9038988351821899\n",
      "Step: 12200  \tValid loss: 0.25673922896385193\n",
      "Step: 12300  \tTraining loss: 0.2663799822330475\n",
      "Step: 12300  \tTraining accuracy: 0.9039013981819153\n",
      "Step: 12300  \tValid loss: 0.2566725015640259\n",
      "Step: 12400  \tTraining loss: 0.26624032855033875\n",
      "Step: 12400  \tTraining accuracy: 0.9039053320884705\n",
      "Step: 12400  \tValid loss: 0.2566220164299011\n",
      "Step: 12500  \tTraining loss: 0.2661040127277374\n",
      "Step: 12500  \tTraining accuracy: 0.9039112329483032\n",
      "Step: 12500  \tValid loss: 0.25658175349235535\n",
      "Step: 12600  \tTraining loss: 0.26597046852111816\n",
      "Step: 12600  \tTraining accuracy: 0.9039191007614136\n",
      "Step: 12600  \tValid loss: 0.2565479576587677\n",
      "Step: 12700  \tTraining loss: 0.2658262848854065\n",
      "Step: 12700  \tTraining accuracy: 0.9039281606674194\n",
      "Step: 12700  \tValid loss: 0.25648969411849976\n",
      "Step: 12800  \tTraining loss: 0.2656930387020111\n",
      "Step: 12800  \tTraining accuracy: 0.9039350748062134\n",
      "Step: 12800  \tValid loss: 0.25645574927330017\n",
      "Step: 12900  \tTraining loss: 0.26556462049484253\n",
      "Step: 12900  \tTraining accuracy: 0.9039405584335327\n",
      "Step: 12900  \tValid loss: 0.25643911957740784\n",
      "Step: 13000  \tTraining loss: 0.26543954014778137\n",
      "Step: 13000  \tTraining accuracy: 0.9039459824562073\n",
      "Step: 13000  \tValid loss: 0.25642159581184387\n",
      "Step: 13100  \tTraining loss: 0.2653176188468933\n",
      "Step: 13100  \tTraining accuracy: 0.9039512872695923\n",
      "Step: 13100  \tValid loss: 0.2564053237438202\n",
      "Step: 13200  \tTraining loss: 0.2651987075805664\n",
      "Step: 13200  \tTraining accuracy: 0.9039565324783325\n",
      "Step: 13200  \tValid loss: 0.2563896179199219\n",
      "Step: 13300  \tTraining loss: 0.2650834023952484\n",
      "Step: 13300  \tTraining accuracy: 0.9039616584777832\n",
      "Step: 13300  \tValid loss: 0.25637978315353394\n",
      "Step: 13400  \tTraining loss: 0.26497212052345276\n",
      "Step: 13400  \tTraining accuracy: 0.9039667844772339\n",
      "Step: 13400  \tValid loss: 0.25637829303741455\n",
      "Step: 13500  \tTraining loss: 0.2648634910583496\n",
      "Step: 13500  \tTraining accuracy: 0.9039686322212219\n",
      "Step: 13500  \tValid loss: 0.2563757002353668\n",
      "Step: 13600  \tTraining loss: 0.26475751399993896\n",
      "Step: 13600  \tTraining accuracy: 0.9039704203605652\n",
      "Step: 13600  \tValid loss: 0.25637373328208923\n",
      "Step: 13700  \tTraining loss: 0.26465386152267456\n",
      "Step: 13700  \tTraining accuracy: 0.903974711894989\n",
      "Step: 13700  \tValid loss: 0.25637298822402954\n",
      "Step: 13800  \tTraining loss: 0.264552503824234\n",
      "Step: 13800  \tTraining accuracy: 0.9039795398712158\n",
      "Step: 13800  \tValid loss: 0.2563735544681549\n",
      "Step: 13900  \tTraining loss: 0.26445329189300537\n",
      "Step: 13900  \tTraining accuracy: 0.9039843082427979\n",
      "Step: 13900  \tValid loss: 0.2563745379447937\n",
      "Step: 14000  \tTraining loss: 0.26435625553131104\n",
      "Step: 14000  \tTraining accuracy: 0.9039890170097351\n",
      "Step: 14000  \tValid loss: 0.25637662410736084\n",
      "Step: 14100  \tTraining loss: 0.2642611861228943\n",
      "Step: 14100  \tTraining accuracy: 0.9039936661720276\n",
      "Step: 14100  \tValid loss: 0.25637927651405334\n",
      "Step: 14200  \tTraining loss: 0.26416799426078796\n",
      "Step: 14200  \tTraining accuracy: 0.9039981961250305\n",
      "Step: 14200  \tValid loss: 0.25638362765312195\n",
      "Step: 14300  \tTraining loss: 0.26407647132873535\n",
      "Step: 14300  \tTraining accuracy: 0.9040027260780334\n",
      "Step: 14300  \tValid loss: 0.25638943910598755\n",
      "Step: 14400  \tTraining loss: 0.2639867663383484\n",
      "Step: 14400  \tTraining accuracy: 0.9040071964263916\n",
      "Step: 14400  \tValid loss: 0.25639626383781433\n",
      "Step: 14500  \tTraining loss: 0.2638987600803375\n",
      "Step: 14500  \tTraining accuracy: 0.9040133357048035\n",
      "Step: 14500  \tValid loss: 0.25640395283699036\n",
      "Step: 14600  \tTraining loss: 0.2638121545314789\n",
      "Step: 14600  \tTraining accuracy: 0.9040205478668213\n",
      "Step: 14600  \tValid loss: 0.25641247630119324\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.9040277\n",
      "Precision: 0.91903436\n",
      "Recall: 0.9705825\n",
      "F1 score: 0.9286893\n",
      "AUC: 0.727608\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.904028   0.919034  0.970582  0.928689  0.727608  0.263754      0.904037   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.256373        0.90403    0.23591      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  14667.0  \n",
      "1\n",
      "(21315, 8)\n",
      "(21315, 1)\n",
      "(11760, 8)\n",
      "(11760, 1)\n",
      "(9555, 8)\n",
      "(9555, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5308483839035034\n",
      "Step: 100  \tTraining accuracy: 0.7456251382827759\n",
      "Step: 100  \tValid loss: 0.5323190689086914\n",
      "Step: 200  \tTraining loss: 0.5080155730247498\n",
      "Step: 200  \tTraining accuracy: 0.7477988600730896\n",
      "Step: 200  \tValid loss: 0.5122761130332947\n",
      "Step: 300  \tTraining loss: 0.4790612757205963\n",
      "Step: 300  \tTraining accuracy: 0.7522120475769043\n",
      "Step: 300  \tValid loss: 0.48429223895072937\n",
      "Step: 400  \tTraining loss: 0.42741942405700684\n",
      "Step: 400  \tTraining accuracy: 0.7610669732093811\n",
      "Step: 400  \tValid loss: 0.4308953881263733\n",
      "Step: 500  \tTraining loss: 0.4028255343437195\n",
      "Step: 500  \tTraining accuracy: 0.770401656627655\n",
      "Step: 500  \tValid loss: 0.40474647283554077\n",
      "Step: 600  \tTraining loss: 0.3972308039665222\n",
      "Step: 600  \tTraining accuracy: 0.7781886458396912\n",
      "Step: 600  \tValid loss: 0.3983350694179535\n",
      "Step: 700  \tTraining loss: 0.39569175243377686\n",
      "Step: 700  \tTraining accuracy: 0.7838538885116577\n",
      "Step: 700  \tValid loss: 0.3965577781200409\n",
      "Step: 800  \tTraining loss: 0.39351972937583923\n",
      "Step: 800  \tTraining accuracy: 0.7880709767341614\n",
      "Step: 800  \tValid loss: 0.3952887952327728\n",
      "Step: 900  \tTraining loss: 0.391937792301178\n",
      "Step: 900  \tTraining accuracy: 0.791309654712677\n",
      "Step: 900  \tValid loss: 0.3935883939266205\n",
      "Step: 1000  \tTraining loss: 0.3901033401489258\n",
      "Step: 1000  \tTraining accuracy: 0.7939232587814331\n",
      "Step: 1000  \tValid loss: 0.3914259374141693\n",
      "Step: 1100  \tTraining loss: 0.38797447085380554\n",
      "Step: 1100  \tTraining accuracy: 0.7961395382881165\n",
      "Step: 1100  \tValid loss: 0.3888727128505707\n",
      "Step: 1200  \tTraining loss: 0.38579633831977844\n",
      "Step: 1200  \tTraining accuracy: 0.7979989647865295\n",
      "Step: 1200  \tValid loss: 0.38644590973854065\n",
      "Step: 1300  \tTraining loss: 0.3837737739086151\n",
      "Step: 1300  \tTraining accuracy: 0.7996078133583069\n",
      "Step: 1300  \tValid loss: 0.3842507600784302\n",
      "Step: 1400  \tTraining loss: 0.3819548189640045\n",
      "Step: 1400  \tTraining accuracy: 0.8010599613189697\n",
      "Step: 1400  \tValid loss: 0.3823641538619995\n",
      "Step: 1500  \tTraining loss: 0.3805761933326721\n",
      "Step: 1500  \tTraining accuracy: 0.8023441433906555\n",
      "Step: 1500  \tValid loss: 0.38122814893722534\n",
      "Step: 1600  \tTraining loss: 0.3794313073158264\n",
      "Step: 1600  \tTraining accuracy: 0.8035004734992981\n",
      "Step: 1600  \tValid loss: 0.3803040683269501\n",
      "Step: 1700  \tTraining loss: 0.37845054268836975\n",
      "Step: 1700  \tTraining accuracy: 0.8045607209205627\n",
      "Step: 1700  \tValid loss: 0.3794531524181366\n",
      "Step: 1800  \tTraining loss: 0.37757226824760437\n",
      "Step: 1800  \tTraining accuracy: 0.8055011630058289\n",
      "Step: 1800  \tValid loss: 0.37867048382759094\n",
      "Step: 1900  \tTraining loss: 0.3767958879470825\n",
      "Step: 1900  \tTraining accuracy: 0.8063741326332092\n",
      "Step: 1900  \tValid loss: 0.3780250549316406\n",
      "Step: 2000  \tTraining loss: 0.37605762481689453\n",
      "Step: 2000  \tTraining accuracy: 0.8072441816329956\n",
      "Step: 2000  \tValid loss: 0.37750348448753357\n",
      "Step: 2100  \tTraining loss: 0.37540560960769653\n",
      "Step: 2100  \tTraining accuracy: 0.8080706000328064\n",
      "Step: 2100  \tValid loss: 0.3771399259567261\n",
      "Step: 2200  \tTraining loss: 0.37481391429901123\n",
      "Step: 2200  \tTraining accuracy: 0.8088331818580627\n",
      "Step: 2200  \tValid loss: 0.3768315017223358\n",
      "Step: 2300  \tTraining loss: 0.37423574924468994\n",
      "Step: 2300  \tTraining accuracy: 0.8095686435699463\n",
      "Step: 2300  \tValid loss: 0.3765449821949005\n",
      "Step: 2400  \tTraining loss: 0.3736492991447449\n",
      "Step: 2400  \tTraining accuracy: 0.8102824687957764\n",
      "Step: 2400  \tValid loss: 0.3762584924697876\n",
      "Step: 2500  \tTraining loss: 0.37306860089302063\n",
      "Step: 2500  \tTraining accuracy: 0.810957133769989\n",
      "Step: 2500  \tValid loss: 0.37597960233688354\n",
      "Step: 2600  \tTraining loss: 0.37248751521110535\n",
      "Step: 2600  \tTraining accuracy: 0.8115898966789246\n",
      "Step: 2600  \tValid loss: 0.3757288157939911\n",
      "Step: 2700  \tTraining loss: 0.3718990385532379\n",
      "Step: 2700  \tTraining accuracy: 0.8121864795684814\n",
      "Step: 2700  \tValid loss: 0.3754446804523468\n",
      "Step: 2800  \tTraining loss: 0.37131115794181824\n",
      "Step: 2800  \tTraining accuracy: 0.8127464652061462\n",
      "Step: 2800  \tValid loss: 0.3751155436038971\n",
      "Step: 2900  \tTraining loss: 0.3707505762577057\n",
      "Step: 2900  \tTraining accuracy: 0.8132976293563843\n",
      "Step: 2900  \tValid loss: 0.3748745322227478\n",
      "Step: 3000  \tTraining loss: 0.37022894620895386\n",
      "Step: 3000  \tTraining accuracy: 0.8138114213943481\n",
      "Step: 3000  \tValid loss: 0.37462979555130005\n",
      "Step: 3100  \tTraining loss: 0.3696907162666321\n",
      "Step: 3100  \tTraining accuracy: 0.8142845630645752\n",
      "Step: 3100  \tValid loss: 0.37439411878585815\n",
      "Step: 3200  \tTraining loss: 0.3690641522407532\n",
      "Step: 3200  \tTraining accuracy: 0.8147269487380981\n",
      "Step: 3200  \tValid loss: 0.37401247024536133\n",
      "Step: 3300  \tTraining loss: 0.36832839250564575\n",
      "Step: 3300  \tTraining accuracy: 0.8151420950889587\n",
      "Step: 3300  \tValid loss: 0.37352484464645386\n",
      "Step: 3400  \tTraining loss: 0.3675311803817749\n",
      "Step: 3400  \tTraining accuracy: 0.8155401945114136\n",
      "Step: 3400  \tValid loss: 0.3729635775089264\n",
      "Step: 3500  \tTraining loss: 0.3667217493057251\n",
      "Step: 3500  \tTraining accuracy: 0.8159117698669434\n",
      "Step: 3500  \tValid loss: 0.37228670716285706\n",
      "Step: 3600  \tTraining loss: 0.36593687534332275\n",
      "Step: 3600  \tTraining accuracy: 0.8162670731544495\n",
      "Step: 3600  \tValid loss: 0.3716122508049011\n",
      "Step: 3700  \tTraining loss: 0.3651908338069916\n",
      "Step: 3700  \tTraining accuracy: 0.8166054487228394\n",
      "Step: 3700  \tValid loss: 0.3709295094013214\n",
      "Step: 3800  \tTraining loss: 0.36448934674263\n",
      "Step: 3800  \tTraining accuracy: 0.8169145584106445\n",
      "Step: 3800  \tValid loss: 0.37031295895576477\n",
      "Step: 3900  \tTraining loss: 0.3638348877429962\n",
      "Step: 3900  \tTraining accuracy: 0.8172130584716797\n",
      "Step: 3900  \tValid loss: 0.3697390556335449\n",
      "Step: 4000  \tTraining loss: 0.36325782537460327\n",
      "Step: 4000  \tTraining accuracy: 0.8175065517425537\n",
      "Step: 4000  \tValid loss: 0.3692530691623688\n",
      "Step: 4100  \tTraining loss: 0.36273276805877686\n",
      "Step: 4100  \tTraining accuracy: 0.8177774548530579\n",
      "Step: 4100  \tValid loss: 0.3688429594039917\n",
      "Step: 4200  \tTraining loss: 0.36224564909935\n",
      "Step: 4200  \tTraining accuracy: 0.8180460333824158\n",
      "Step: 4200  \tValid loss: 0.368463397026062\n",
      "Step: 4300  \tTraining loss: 0.36178120970726013\n",
      "Step: 4300  \tTraining accuracy: 0.8183157444000244\n",
      "Step: 4300  \tValid loss: 0.36811044812202454\n",
      "Step: 4400  \tTraining loss: 0.36134907603263855\n",
      "Step: 4400  \tTraining accuracy: 0.8185854554176331\n",
      "Step: 4400  \tValid loss: 0.36779478192329407\n",
      "Step: 4500  \tTraining loss: 0.3609490394592285\n",
      "Step: 4500  \tTraining accuracy: 0.8188441395759583\n",
      "Step: 4500  \tValid loss: 0.3675103783607483\n",
      "Step: 4600  \tTraining loss: 0.3605840802192688\n",
      "Step: 4600  \tTraining accuracy: 0.8191141486167908\n",
      "Step: 4600  \tValid loss: 0.3673117756843567\n",
      "Step: 4700  \tTraining loss: 0.3602514863014221\n",
      "Step: 4700  \tTraining accuracy: 0.8193780183792114\n",
      "Step: 4700  \tValid loss: 0.36712899804115295\n",
      "Step: 4800  \tTraining loss: 0.3599494695663452\n",
      "Step: 4800  \tTraining accuracy: 0.8196229338645935\n",
      "Step: 4800  \tValid loss: 0.36690565943717957\n",
      "Step: 4900  \tTraining loss: 0.35967618227005005\n",
      "Step: 4900  \tTraining accuracy: 0.8198519349098206\n",
      "Step: 4900  \tValid loss: 0.366715669631958\n",
      "Step: 5000  \tTraining loss: 0.35943126678466797\n",
      "Step: 5000  \tTraining accuracy: 0.8200740814208984\n",
      "Step: 5000  \tValid loss: 0.36655595898628235\n",
      "Step: 5100  \tTraining loss: 0.35921531915664673\n",
      "Step: 5100  \tTraining accuracy: 0.8203008770942688\n",
      "Step: 5100  \tValid loss: 0.3663782775402069\n",
      "Step: 5200  \tTraining loss: 0.3590204417705536\n",
      "Step: 5200  \tTraining accuracy: 0.8205257058143616\n",
      "Step: 5200  \tValid loss: 0.3662223219871521\n",
      "Step: 5300  \tTraining loss: 0.358835905790329\n",
      "Step: 5300  \tTraining accuracy: 0.8207432627677917\n",
      "Step: 5300  \tValid loss: 0.36608877778053284\n",
      "Step: 5400  \tTraining loss: 0.3586604595184326\n",
      "Step: 5400  \tTraining accuracy: 0.820959746837616\n",
      "Step: 5400  \tValid loss: 0.36595508456230164\n",
      "Step: 5500  \tTraining loss: 0.3584868311882019\n",
      "Step: 5500  \tTraining accuracy: 0.8211734294891357\n",
      "Step: 5500  \tValid loss: 0.36584144830703735\n",
      "Step: 5600  \tTraining loss: 0.3583151698112488\n",
      "Step: 5600  \tTraining accuracy: 0.8213802576065063\n",
      "Step: 5600  \tValid loss: 0.36570826172828674\n",
      "Step: 5700  \tTraining loss: 0.35813677310943604\n",
      "Step: 5700  \tTraining accuracy: 0.8215926885604858\n",
      "Step: 5700  \tValid loss: 0.36559349298477173\n",
      "Step: 5800  \tTraining loss: 0.3579539358615875\n",
      "Step: 5800  \tTraining accuracy: 0.8218038082122803\n",
      "Step: 5800  \tValid loss: 0.36550259590148926\n",
      "Step: 5900  \tTraining loss: 0.35776904225349426\n",
      "Step: 5900  \tTraining accuracy: 0.822023332118988\n",
      "Step: 5900  \tValid loss: 0.3653389513492584\n",
      "Step: 6000  \tTraining loss: 0.35759198665618896\n",
      "Step: 6000  \tTraining accuracy: 0.822245717048645\n",
      "Step: 6000  \tValid loss: 0.36525392532348633\n",
      "Step: 6100  \tTraining loss: 0.3574182689189911\n",
      "Step: 6100  \tTraining accuracy: 0.822466254234314\n",
      "Step: 6100  \tValid loss: 0.3651335537433624\n",
      "Step: 6200  \tTraining loss: 0.35725149512290955\n",
      "Step: 6200  \tTraining accuracy: 0.8226814866065979\n",
      "Step: 6200  \tValid loss: 0.36503124237060547\n",
      "Step: 6300  \tTraining loss: 0.3570888042449951\n",
      "Step: 6300  \tTraining accuracy: 0.8228883147239685\n",
      "Step: 6300  \tValid loss: 0.36494630575180054\n",
      "Step: 6400  \tTraining loss: 0.3569270670413971\n",
      "Step: 6400  \tTraining accuracy: 0.8230831027030945\n",
      "Step: 6400  \tValid loss: 0.36487799882888794\n",
      "Step: 6500  \tTraining loss: 0.35677382349967957\n",
      "Step: 6500  \tTraining accuracy: 0.8232740163803101\n",
      "Step: 6500  \tValid loss: 0.3648112118244171\n",
      "Step: 6600  \tTraining loss: 0.35662007331848145\n",
      "Step: 6600  \tTraining accuracy: 0.8234698176383972\n",
      "Step: 6600  \tValid loss: 0.3647882640361786\n",
      "Step: 6700  \tTraining loss: 0.3564750850200653\n",
      "Step: 6700  \tTraining accuracy: 0.8236636519432068\n",
      "Step: 6700  \tValid loss: 0.36462485790252686\n",
      "Step: 6800  \tTraining loss: 0.35633793473243713\n",
      "Step: 6800  \tTraining accuracy: 0.8238531351089478\n",
      "Step: 6800  \tValid loss: 0.3645443022251129\n",
      "Step: 6900  \tTraining loss: 0.3561772108078003\n",
      "Step: 6900  \tTraining accuracy: 0.8240360617637634\n",
      "Step: 6900  \tValid loss: 0.36445969343185425\n",
      "Step: 7000  \tTraining loss: 0.3559611439704895\n",
      "Step: 7000  \tTraining accuracy: 0.8242079615592957\n",
      "Step: 7000  \tValid loss: 0.3644050359725952\n",
      "Step: 7100  \tTraining loss: 0.3558129072189331\n",
      "Step: 7100  \tTraining accuracy: 0.8243726491928101\n",
      "Step: 7100  \tValid loss: 0.36428549885749817\n",
      "Step: 7200  \tTraining loss: 0.35567331314086914\n",
      "Step: 7200  \tTraining accuracy: 0.8245373964309692\n",
      "Step: 7200  \tValid loss: 0.3641877770423889\n",
      "Step: 7300  \tTraining loss: 0.3554929792881012\n",
      "Step: 7300  \tTraining accuracy: 0.8247014284133911\n",
      "Step: 7300  \tValid loss: 0.36413076519966125\n",
      "Step: 7400  \tTraining loss: 0.3553408682346344\n",
      "Step: 7400  \tTraining accuracy: 0.8248628973960876\n",
      "Step: 7400  \tValid loss: 0.3639802932739258\n",
      "Step: 7500  \tTraining loss: 0.3551892638206482\n",
      "Step: 7500  \tTraining accuracy: 0.8250285387039185\n",
      "Step: 7500  \tValid loss: 0.36384817957878113\n",
      "Step: 7600  \tTraining loss: 0.3550620377063751\n",
      "Step: 7600  \tTraining accuracy: 0.825189471244812\n",
      "Step: 7600  \tValid loss: 0.36371567845344543\n",
      "Step: 7700  \tTraining loss: 0.35495537519454956\n",
      "Step: 7700  \tTraining accuracy: 0.8253465294837952\n",
      "Step: 7700  \tValid loss: 0.36364904046058655\n",
      "Step: 7800  \tTraining loss: 0.3548530340194702\n",
      "Step: 7800  \tTraining accuracy: 0.8254961967468262\n",
      "Step: 7800  \tValid loss: 0.36350661516189575\n",
      "Step: 7900  \tTraining loss: 0.35475432872772217\n",
      "Step: 7900  \tTraining accuracy: 0.8256465196609497\n",
      "Step: 7900  \tValid loss: 0.36340123414993286\n",
      "Step: 8000  \tTraining loss: 0.3546581268310547\n",
      "Step: 8000  \tTraining accuracy: 0.8257966637611389\n",
      "Step: 8000  \tValid loss: 0.3633122146129608\n",
      "Step: 8100  \tTraining loss: 0.3545731008052826\n",
      "Step: 8100  \tTraining accuracy: 0.8259479403495789\n",
      "Step: 8100  \tValid loss: 0.3632108271121979\n",
      "Step: 8200  \tTraining loss: 0.35448455810546875\n",
      "Step: 8200  \tTraining accuracy: 0.8260952830314636\n",
      "Step: 8200  \tValid loss: 0.36317670345306396\n",
      "Step: 8300  \tTraining loss: 0.354402631521225\n",
      "Step: 8300  \tTraining accuracy: 0.8262376189231873\n",
      "Step: 8300  \tValid loss: 0.3630932867527008\n",
      "Step: 8400  \tTraining loss: 0.35432273149490356\n",
      "Step: 8400  \tTraining accuracy: 0.8263751268386841\n",
      "Step: 8400  \tValid loss: 0.36303678154945374\n",
      "Step: 8500  \tTraining loss: 0.3542449176311493\n",
      "Step: 8500  \tTraining accuracy: 0.8265124559402466\n",
      "Step: 8500  \tValid loss: 0.3629980683326721\n",
      "Step: 8600  \tTraining loss: 0.35417047142982483\n",
      "Step: 8600  \tTraining accuracy: 0.8266503810882568\n",
      "Step: 8600  \tValid loss: 0.3628990948200226\n",
      "Step: 8700  \tTraining loss: 0.3540952503681183\n",
      "Step: 8700  \tTraining accuracy: 0.8267862200737\n",
      "Step: 8700  \tValid loss: 0.36285898089408875\n",
      "Step: 8800  \tTraining loss: 0.35402217507362366\n",
      "Step: 8800  \tTraining accuracy: 0.826913058757782\n",
      "Step: 8800  \tValid loss: 0.3628264367580414\n",
      "Step: 8900  \tTraining loss: 0.3539506494998932\n",
      "Step: 8900  \tTraining accuracy: 0.8270401954650879\n",
      "Step: 8900  \tValid loss: 0.3627511262893677\n",
      "Step: 9000  \tTraining loss: 0.35388123989105225\n",
      "Step: 9000  \tTraining accuracy: 0.8271650075912476\n",
      "Step: 9000  \tValid loss: 0.3626960515975952\n",
      "Step: 9100  \tTraining loss: 0.3538139760494232\n",
      "Step: 9100  \tTraining accuracy: 0.8272883892059326\n",
      "Step: 9100  \tValid loss: 0.36264050006866455\n",
      "Step: 9200  \tTraining loss: 0.3537505567073822\n",
      "Step: 9200  \tTraining accuracy: 0.8274105787277222\n",
      "Step: 9200  \tValid loss: 0.3625583052635193\n",
      "Step: 9300  \tTraining loss: 0.35368528962135315\n",
      "Step: 9300  \tTraining accuracy: 0.8275304436683655\n",
      "Step: 9300  \tValid loss: 0.36250972747802734\n",
      "Step: 9400  \tTraining loss: 0.35361891984939575\n",
      "Step: 9400  \tTraining accuracy: 0.8276466727256775\n",
      "Step: 9400  \tValid loss: 0.362501323223114\n",
      "Step: 9500  \tTraining loss: 0.3535563051700592\n",
      "Step: 9500  \tTraining accuracy: 0.8277597427368164\n",
      "Step: 9500  \tValid loss: 0.3624670207500458\n",
      "Step: 9600  \tTraining loss: 0.3534972071647644\n",
      "Step: 9600  \tTraining accuracy: 0.827872097492218\n",
      "Step: 9600  \tValid loss: 0.3623705804347992\n",
      "Step: 9700  \tTraining loss: 0.3534344732761383\n",
      "Step: 9700  \tTraining accuracy: 0.8279809951782227\n",
      "Step: 9700  \tValid loss: 0.36236488819122314\n",
      "Step: 9800  \tTraining loss: 0.3533753752708435\n",
      "Step: 9800  \tTraining accuracy: 0.8280883431434631\n",
      "Step: 9800  \tValid loss: 0.36231207847595215\n",
      "Step: 9900  \tTraining loss: 0.3533165454864502\n",
      "Step: 9900  \tTraining accuracy: 0.8281925320625305\n",
      "Step: 9900  \tValid loss: 0.3622919023036957\n",
      "Step: 10000  \tTraining loss: 0.35325878858566284\n",
      "Step: 10000  \tTraining accuracy: 0.8282913565635681\n",
      "Step: 10000  \tValid loss: 0.36225736141204834\n",
      "Step: 10100  \tTraining loss: 0.35320109128952026\n",
      "Step: 10100  \tTraining accuracy: 0.8283898234367371\n",
      "Step: 10100  \tValid loss: 0.3621754050254822\n",
      "Step: 10200  \tTraining loss: 0.3531435430049896\n",
      "Step: 10200  \tTraining accuracy: 0.8284875154495239\n",
      "Step: 10200  \tValid loss: 0.3621492385864258\n",
      "Step: 10300  \tTraining loss: 0.35308778285980225\n",
      "Step: 10300  \tTraining accuracy: 0.8285819292068481\n",
      "Step: 10300  \tValid loss: 0.3621520698070526\n",
      "Step: 10400  \tTraining loss: 0.3530316948890686\n",
      "Step: 10400  \tTraining accuracy: 0.8286729454994202\n",
      "Step: 10400  \tValid loss: 0.36207112669944763\n",
      "Step: 10500  \tTraining loss: 0.35297656059265137\n",
      "Step: 10500  \tTraining accuracy: 0.8287602066993713\n",
      "Step: 10500  \tValid loss: 0.3620779812335968\n",
      "Step: 10600  \tTraining loss: 0.35292214155197144\n",
      "Step: 10600  \tTraining accuracy: 0.8288422226905823\n",
      "Step: 10600  \tValid loss: 0.362074077129364\n",
      "Step: 10700  \tTraining loss: 0.35286787152290344\n",
      "Step: 10700  \tTraining accuracy: 0.8289214372634888\n",
      "Step: 10700  \tValid loss: 0.3620498478412628\n",
      "Step: 10800  \tTraining loss: 0.35281190276145935\n",
      "Step: 10800  \tTraining accuracy: 0.8289991021156311\n",
      "Step: 10800  \tValid loss: 0.3619626462459564\n",
      "Step: 10900  \tTraining loss: 0.3527592122554779\n",
      "Step: 10900  \tTraining accuracy: 0.8290740847587585\n",
      "Step: 10900  \tValid loss: 0.361920028924942\n",
      "Step: 11000  \tTraining loss: 0.3527056872844696\n",
      "Step: 11000  \tTraining accuracy: 0.8291468620300293\n",
      "Step: 11000  \tValid loss: 0.36190006136894226\n",
      "Step: 11100  \tTraining loss: 0.3526533246040344\n",
      "Step: 11100  \tTraining accuracy: 0.8292174339294434\n",
      "Step: 11100  \tValid loss: 0.36192548274993896\n",
      "Step: 11200  \tTraining loss: 0.3526003658771515\n",
      "Step: 11200  \tTraining accuracy: 0.829287588596344\n",
      "Step: 11200  \tValid loss: 0.36185139417648315\n",
      "Step: 11300  \tTraining loss: 0.3525474965572357\n",
      "Step: 11300  \tTraining accuracy: 0.8293527364730835\n",
      "Step: 11300  \tValid loss: 0.36183875799179077\n",
      "Step: 11400  \tTraining loss: 0.35249561071395874\n",
      "Step: 11400  \tTraining accuracy: 0.82941734790802\n",
      "Step: 11400  \tValid loss: 0.3618125021457672\n",
      "Step: 11500  \tTraining loss: 0.3524402976036072\n",
      "Step: 11500  \tTraining accuracy: 0.829483687877655\n",
      "Step: 11500  \tValid loss: 0.36183032393455505\n",
      "Step: 11600  \tTraining loss: 0.3523857295513153\n",
      "Step: 11600  \tTraining accuracy: 0.8295507431030273\n",
      "Step: 11600  \tValid loss: 0.3617860674858093\n",
      "Step: 11700  \tTraining loss: 0.3523311913013458\n",
      "Step: 11700  \tTraining accuracy: 0.8296146392822266\n",
      "Step: 11700  \tValid loss: 0.36172762513160706\n",
      "Step: 11800  \tTraining loss: 0.3522709310054779\n",
      "Step: 11800  \tTraining accuracy: 0.8296760320663452\n",
      "Step: 11800  \tValid loss: 0.36175620555877686\n",
      "Step: 11900  \tTraining loss: 0.35221531987190247\n",
      "Step: 11900  \tTraining accuracy: 0.8297399282455444\n",
      "Step: 11900  \tValid loss: 0.3617527186870575\n",
      "Step: 12000  \tTraining loss: 0.35215890407562256\n",
      "Step: 12000  \tTraining accuracy: 0.8298022150993347\n",
      "Step: 12000  \tValid loss: 0.3617108166217804\n",
      "Step: 12100  \tTraining loss: 0.35208776593208313\n",
      "Step: 12100  \tTraining accuracy: 0.8298647999763489\n",
      "Step: 12100  \tValid loss: 0.3616524338722229\n",
      "Step: 12200  \tTraining loss: 0.3520191013813019\n",
      "Step: 12200  \tTraining accuracy: 0.829927921295166\n",
      "Step: 12200  \tValid loss: 0.3616301119327545\n",
      "Step: 12300  \tTraining loss: 0.35188981890678406\n",
      "Step: 12300  \tTraining accuracy: 0.8299890160560608\n",
      "Step: 12300  \tValid loss: 0.36150091886520386\n",
      "Step: 12400  \tTraining loss: 0.35179346799850464\n",
      "Step: 12400  \tTraining accuracy: 0.8300472497940063\n",
      "Step: 12400  \tValid loss: 0.36143723130226135\n",
      "Step: 12500  \tTraining loss: 0.3517136573791504\n",
      "Step: 12500  \tTraining accuracy: 0.8301045894622803\n",
      "Step: 12500  \tValid loss: 0.3613542914390564\n",
      "Step: 12600  \tTraining loss: 0.35164403915405273\n",
      "Step: 12600  \tTraining accuracy: 0.8301615118980408\n",
      "Step: 12600  \tValid loss: 0.3613279461860657\n",
      "Step: 12700  \tTraining loss: 0.35157808661460876\n",
      "Step: 12700  \tTraining accuracy: 0.8302210569381714\n",
      "Step: 12700  \tValid loss: 0.36123594641685486\n",
      "Step: 12800  \tTraining loss: 0.3515177369117737\n",
      "Step: 12800  \tTraining accuracy: 0.8302785754203796\n",
      "Step: 12800  \tValid loss: 0.3612169623374939\n",
      "Step: 12900  \tTraining loss: 0.35146066546440125\n",
      "Step: 12900  \tTraining accuracy: 0.8303337693214417\n",
      "Step: 12900  \tValid loss: 0.3612114489078522\n",
      "Step: 13000  \tTraining loss: 0.3514048457145691\n",
      "Step: 13000  \tTraining accuracy: 0.8303897380828857\n",
      "Step: 13000  \tValid loss: 0.3611549735069275\n",
      "Step: 13100  \tTraining loss: 0.35135164856910706\n",
      "Step: 13100  \tTraining accuracy: 0.8304453492164612\n",
      "Step: 13100  \tValid loss: 0.3611307740211487\n",
      "Step: 13200  \tTraining loss: 0.35129785537719727\n",
      "Step: 13200  \tTraining accuracy: 0.8304990530014038\n",
      "Step: 13200  \tValid loss: 0.36110690236091614\n",
      "Step: 13300  \tTraining loss: 0.35124671459198\n",
      "Step: 13300  \tTraining accuracy: 0.8305521607398987\n",
      "Step: 13300  \tValid loss: 0.36111247539520264\n",
      "Step: 13400  \tTraining loss: 0.35119667649269104\n",
      "Step: 13400  \tTraining accuracy: 0.8306030035018921\n",
      "Step: 13400  \tValid loss: 0.3611052334308624\n",
      "Step: 13500  \tTraining loss: 0.35115063190460205\n",
      "Step: 13500  \tTraining accuracy: 0.8306536674499512\n",
      "Step: 13500  \tValid loss: 0.36114227771759033\n",
      "Step: 13600  \tTraining loss: 0.3511020839214325\n",
      "Step: 13600  \tTraining accuracy: 0.830702543258667\n",
      "Step: 13600  \tValid loss: 0.3610590398311615\n",
      "Step: 13700  \tTraining loss: 0.351054310798645\n",
      "Step: 13700  \tTraining accuracy: 0.830751895904541\n",
      "Step: 13700  \tValid loss: 0.3611025810241699\n",
      "Step: 13800  \tTraining loss: 0.3510103225708008\n",
      "Step: 13800  \tTraining accuracy: 0.8307991623878479\n",
      "Step: 13800  \tValid loss: 0.3610496520996094\n",
      "Step: 13900  \tTraining loss: 0.3509652316570282\n",
      "Step: 13900  \tTraining accuracy: 0.830846905708313\n",
      "Step: 13900  \tValid loss: 0.36108705401420593\n",
      "Step: 14000  \tTraining loss: 0.35091447830200195\n",
      "Step: 14000  \tTraining accuracy: 0.8308950066566467\n",
      "Step: 14000  \tValid loss: 0.36103010177612305\n",
      "Step: 14100  \tTraining loss: 0.3508675992488861\n",
      "Step: 14100  \tTraining accuracy: 0.8309400677680969\n",
      "Step: 14100  \tValid loss: 0.3609894812107086\n",
      "Step: 14200  \tTraining loss: 0.35082265734672546\n",
      "Step: 14200  \tTraining accuracy: 0.8309850096702576\n",
      "Step: 14200  \tValid loss: 0.36094290018081665\n",
      "Step: 14300  \tTraining loss: 0.3507785201072693\n",
      "Step: 14300  \tTraining accuracy: 0.8310297727584839\n",
      "Step: 14300  \tValid loss: 0.360973984003067\n",
      "Step: 14400  \tTraining loss: 0.3507362902164459\n",
      "Step: 14400  \tTraining accuracy: 0.8310762643814087\n",
      "Step: 14400  \tValid loss: 0.36091700196266174\n",
      "Step: 14500  \tTraining loss: 0.3506929278373718\n",
      "Step: 14500  \tTraining accuracy: 0.8311219215393066\n",
      "Step: 14500  \tValid loss: 0.3609391748905182\n",
      "Step: 14600  \tTraining loss: 0.35064926743507385\n",
      "Step: 14600  \tTraining accuracy: 0.8311672210693359\n",
      "Step: 14600  \tValid loss: 0.3608701527118683\n",
      "Step: 14700  \tTraining loss: 0.3506074845790863\n",
      "Step: 14700  \tTraining accuracy: 0.8312126398086548\n",
      "Step: 14700  \tValid loss: 0.3608672618865967\n",
      "Step: 14800  \tTraining loss: 0.3505675494670868\n",
      "Step: 14800  \tTraining accuracy: 0.8312572240829468\n",
      "Step: 14800  \tValid loss: 0.3608458936214447\n",
      "Step: 14900  \tTraining loss: 0.35052984952926636\n",
      "Step: 14900  \tTraining accuracy: 0.8313019871711731\n",
      "Step: 14900  \tValid loss: 0.36087778210639954\n",
      "Step: 15000  \tTraining loss: 0.3504912257194519\n",
      "Step: 15000  \tTraining accuracy: 0.8313461542129517\n",
      "Step: 15000  \tValid loss: 0.36085009574890137\n",
      "Step: 15100  \tTraining loss: 0.3504534065723419\n",
      "Step: 15100  \tTraining accuracy: 0.8313900828361511\n",
      "Step: 15100  \tValid loss: 0.36083394289016724\n",
      "Step: 15200  \tTraining loss: 0.3504156768321991\n",
      "Step: 15200  \tTraining accuracy: 0.8314343690872192\n",
      "Step: 15200  \tValid loss: 0.3608088791370392\n",
      "Step: 15300  \tTraining loss: 0.35038089752197266\n",
      "Step: 15300  \tTraining accuracy: 0.831479549407959\n",
      "Step: 15300  \tValid loss: 0.36077383160591125\n",
      "Step: 15400  \tTraining loss: 0.3503512144088745\n",
      "Step: 15400  \tTraining accuracy: 0.8315226435661316\n",
      "Step: 15400  \tValid loss: 0.3608454763889313\n",
      "Step: 15500  \tTraining loss: 0.35031187534332275\n",
      "Step: 15500  \tTraining accuracy: 0.8315659761428833\n",
      "Step: 15500  \tValid loss: 0.3607656955718994\n",
      "Step: 15600  \tTraining loss: 0.3502788543701172\n",
      "Step: 15600  \tTraining accuracy: 0.8316085338592529\n",
      "Step: 15600  \tValid loss: 0.36072707176208496\n",
      "Step: 15700  \tTraining loss: 0.3502381443977356\n",
      "Step: 15700  \tTraining accuracy: 0.8316510319709778\n",
      "Step: 15700  \tValid loss: 0.3607572317123413\n",
      "Step: 15800  \tTraining loss: 0.3501967787742615\n",
      "Step: 15800  \tTraining accuracy: 0.8316937685012817\n",
      "Step: 15800  \tValid loss: 0.3607126772403717\n",
      "Step: 15900  \tTraining loss: 0.3501499891281128\n",
      "Step: 15900  \tTraining accuracy: 0.8317351937294006\n",
      "Step: 15900  \tValid loss: 0.36070412397384644\n",
      "Step: 16000  \tTraining loss: 0.35011065006256104\n",
      "Step: 16000  \tTraining accuracy: 0.8317760825157166\n",
      "Step: 16000  \tValid loss: 0.3607565462589264\n",
      "Step: 16100  \tTraining loss: 0.35006988048553467\n",
      "Step: 16100  \tTraining accuracy: 0.8318164944648743\n",
      "Step: 16100  \tValid loss: 0.36071059107780457\n",
      "Step: 16200  \tTraining loss: 0.3500213921070099\n",
      "Step: 16200  \tTraining accuracy: 0.8318559527397156\n",
      "Step: 16200  \tValid loss: 0.3606937825679779\n",
      "Step: 16300  \tTraining loss: 0.3499731421470642\n",
      "Step: 16300  \tTraining accuracy: 0.8318954706192017\n",
      "Step: 16300  \tValid loss: 0.36068639159202576\n",
      "Step: 16400  \tTraining loss: 0.34987926483154297\n",
      "Step: 16400  \tTraining accuracy: 0.8319331407546997\n",
      "Step: 16400  \tValid loss: 0.3607608675956726\n",
      "Step: 16500  \tTraining loss: 0.34982576966285706\n",
      "Step: 16500  \tTraining accuracy: 0.8319693207740784\n",
      "Step: 16500  \tValid loss: 0.3608076274394989\n",
      "Step: 16600  \tTraining loss: 0.3497844338417053\n",
      "Step: 16600  \tTraining accuracy: 0.8320046067237854\n",
      "Step: 16600  \tValid loss: 0.36080002784729004\n",
      "Step: 16700  \tTraining loss: 0.3497471511363983\n",
      "Step: 16700  \tTraining accuracy: 0.8320392370223999\n",
      "Step: 16700  \tValid loss: 0.3608294725418091\n",
      "Step: 16800  \tTraining loss: 0.3497087359428406\n",
      "Step: 16800  \tTraining accuracy: 0.8320731520652771\n",
      "Step: 16800  \tValid loss: 0.3608522117137909\n",
      "Step: 16900  \tTraining loss: 0.3496745228767395\n",
      "Step: 16900  \tTraining accuracy: 0.8321074843406677\n",
      "Step: 16900  \tValid loss: 0.36078736186027527\n",
      "Step: 17000  \tTraining loss: 0.34963929653167725\n",
      "Step: 17000  \tTraining accuracy: 0.8321425318717957\n",
      "Step: 17000  \tValid loss: 0.3608117401599884\n",
      "Step: 17100  \tTraining loss: 0.3496076464653015\n",
      "Step: 17100  \tTraining accuracy: 0.8321767449378967\n",
      "Step: 17100  \tValid loss: 0.3607757091522217\n",
      "Step: 17200  \tTraining loss: 0.3495774269104004\n",
      "Step: 17200  \tTraining accuracy: 0.8322111368179321\n",
      "Step: 17200  \tValid loss: 0.36075231432914734\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.83224535\n",
      "Precision: 0.8708775\n",
      "Recall: 0.92046815\n",
      "F1 score: 0.8552848\n",
      "AUC: 0.7602155\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.832245   0.870878  0.920468  0.855285  0.760216  0.349557      0.832226   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.360654       0.832218   0.340769      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  17248.0  \n",
      "2\n",
      "(3770, 8)\n",
      "(3770, 1)\n",
      "(2080, 8)\n",
      "(2080, 1)\n",
      "(1690, 8)\n",
      "(1690, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5979021787643433\n",
      "Step: 100  \tTraining accuracy: 0.66366046667099\n",
      "Step: 100  \tValid loss: 0.5855364203453064\n",
      "Step: 200  \tTraining loss: 0.578339695930481\n",
      "Step: 200  \tTraining accuracy: 0.6680813431739807\n",
      "Step: 200  \tValid loss: 0.5642643570899963\n",
      "Step: 300  \tTraining loss: 0.5619208216667175\n",
      "Step: 300  \tTraining accuracy: 0.6778249144554138\n",
      "Step: 300  \tValid loss: 0.5463973879814148\n",
      "Step: 400  \tTraining loss: 0.5540898442268372\n",
      "Step: 400  \tTraining accuracy: 0.6863206028938293\n",
      "Step: 400  \tValid loss: 0.5390844345092773\n",
      "Step: 500  \tTraining loss: 0.5499165058135986\n",
      "Step: 500  \tTraining accuracy: 0.691895067691803\n",
      "Step: 500  \tValid loss: 0.5347586870193481\n",
      "Step: 600  \tTraining loss: 0.5469805002212524\n",
      "Step: 600  \tTraining accuracy: 0.6959971189498901\n",
      "Step: 600  \tValid loss: 0.5325190424919128\n",
      "Step: 700  \tTraining loss: 0.5441792607307434\n",
      "Step: 700  \tTraining accuracy: 0.6991634368896484\n",
      "Step: 700  \tValid loss: 0.530225396156311\n",
      "Step: 800  \tTraining loss: 0.5418705940246582\n",
      "Step: 800  \tTraining accuracy: 0.7015738487243652\n",
      "Step: 800  \tValid loss: 0.5277726650238037\n",
      "Step: 900  \tTraining loss: 0.539789080619812\n",
      "Step: 900  \tTraining accuracy: 0.7033390402793884\n",
      "Step: 900  \tValid loss: 0.5253341197967529\n",
      "Step: 1000  \tTraining loss: 0.5378400683403015\n",
      "Step: 1000  \tTraining accuracy: 0.7046489119529724\n",
      "Step: 1000  \tValid loss: 0.5230484008789062\n",
      "Step: 1100  \tTraining loss: 0.5358409285545349\n",
      "Step: 1100  \tTraining accuracy: 0.706403911113739\n",
      "Step: 1100  \tValid loss: 0.5212449431419373\n",
      "Step: 1200  \tTraining loss: 0.5336657762527466\n",
      "Step: 1200  \tTraining accuracy: 0.7080959677696228\n",
      "Step: 1200  \tValid loss: 0.518798828125\n",
      "Step: 1300  \tTraining loss: 0.5314403772354126\n",
      "Step: 1300  \tTraining accuracy: 0.7094111442565918\n",
      "Step: 1300  \tValid loss: 0.5160771012306213\n",
      "Step: 1400  \tTraining loss: 0.5292994379997253\n",
      "Step: 1400  \tTraining accuracy: 0.7108065485954285\n",
      "Step: 1400  \tValid loss: 0.5134057998657227\n",
      "Step: 1500  \tTraining loss: 0.5273820161819458\n",
      "Step: 1500  \tTraining accuracy: 0.7120186686515808\n",
      "Step: 1500  \tValid loss: 0.5113304257392883\n",
      "Step: 1600  \tTraining loss: 0.5257860422134399\n",
      "Step: 1600  \tTraining accuracy: 0.7130658030509949\n",
      "Step: 1600  \tValid loss: 0.5099310874938965\n",
      "Step: 1700  \tTraining loss: 0.5245062708854675\n",
      "Step: 1700  \tTraining accuracy: 0.7142753601074219\n",
      "Step: 1700  \tValid loss: 0.5088092684745789\n",
      "Step: 1800  \tTraining loss: 0.5234630703926086\n",
      "Step: 1800  \tTraining accuracy: 0.7153618931770325\n",
      "Step: 1800  \tValid loss: 0.5079157948493958\n",
      "Step: 1900  \tTraining loss: 0.5225757956504822\n",
      "Step: 1900  \tTraining accuracy: 0.7162879109382629\n",
      "Step: 1900  \tValid loss: 0.5071729421615601\n",
      "Step: 2000  \tTraining loss: 0.521756112575531\n",
      "Step: 2000  \tTraining accuracy: 0.7172345519065857\n",
      "Step: 2000  \tValid loss: 0.5066143274307251\n",
      "Step: 2100  \tTraining loss: 0.520916759967804\n",
      "Step: 2100  \tTraining accuracy: 0.7181147933006287\n",
      "Step: 2100  \tValid loss: 0.5065430998802185\n",
      "Step: 2200  \tTraining loss: 0.5201300978660583\n",
      "Step: 2200  \tTraining accuracy: 0.718925416469574\n",
      "Step: 2200  \tValid loss: 0.5061373114585876\n",
      "Step: 2300  \tTraining loss: 0.519412100315094\n",
      "Step: 2300  \tTraining accuracy: 0.7197701334953308\n",
      "Step: 2300  \tValid loss: 0.5056890249252319\n",
      "Step: 2400  \tTraining loss: 0.518663227558136\n",
      "Step: 2400  \tTraining accuracy: 0.7206388711929321\n",
      "Step: 2400  \tValid loss: 0.5053887367248535\n",
      "Step: 2500  \tTraining loss: 0.5179175138473511\n",
      "Step: 2500  \tTraining accuracy: 0.7215179204940796\n",
      "Step: 2500  \tValid loss: 0.5050491094589233\n",
      "Step: 2600  \tTraining loss: 0.5172064900398254\n",
      "Step: 2600  \tTraining accuracy: 0.7223331928253174\n",
      "Step: 2600  \tValid loss: 0.5045215487480164\n",
      "Step: 2700  \tTraining loss: 0.5165261626243591\n",
      "Step: 2700  \tTraining accuracy: 0.7231770157814026\n",
      "Step: 2700  \tValid loss: 0.5040462017059326\n",
      "Step: 2800  \tTraining loss: 0.5156873464584351\n",
      "Step: 2800  \tTraining accuracy: 0.7239691615104675\n",
      "Step: 2800  \tValid loss: 0.5030788779258728\n",
      "Step: 2900  \tTraining loss: 0.5150041580200195\n",
      "Step: 2900  \tTraining accuracy: 0.7247196435928345\n",
      "Step: 2900  \tValid loss: 0.5026780366897583\n",
      "Step: 3000  \tTraining loss: 0.5143737196922302\n",
      "Step: 3000  \tTraining accuracy: 0.7254192233085632\n",
      "Step: 3000  \tValid loss: 0.502161979675293\n",
      "Step: 3100  \tTraining loss: 0.5137778520584106\n",
      "Step: 3100  \tTraining accuracy: 0.7260294556617737\n",
      "Step: 3100  \tValid loss: 0.5018623471260071\n",
      "Step: 3200  \tTraining loss: 0.513219952583313\n",
      "Step: 3200  \tTraining accuracy: 0.7265715003013611\n",
      "Step: 3200  \tValid loss: 0.5015225410461426\n",
      "Step: 3300  \tTraining loss: 0.5126876831054688\n",
      "Step: 3300  \tTraining accuracy: 0.7270638942718506\n",
      "Step: 3300  \tValid loss: 0.5011430978775024\n",
      "Step: 3400  \tTraining loss: 0.512167751789093\n",
      "Step: 3400  \tTraining accuracy: 0.7275109887123108\n",
      "Step: 3400  \tValid loss: 0.5006887316703796\n",
      "Step: 3500  \tTraining loss: 0.5116663575172424\n",
      "Step: 3500  \tTraining accuracy: 0.7279629707336426\n",
      "Step: 3500  \tValid loss: 0.5004181265830994\n",
      "Step: 3600  \tTraining loss: 0.5112051963806152\n",
      "Step: 3600  \tTraining accuracy: 0.7283782362937927\n",
      "Step: 3600  \tValid loss: 0.500140368938446\n",
      "Step: 3700  \tTraining loss: 0.5107346773147583\n",
      "Step: 3700  \tTraining accuracy: 0.7287889122962952\n",
      "Step: 3700  \tValid loss: 0.4999825060367584\n",
      "Step: 3800  \tTraining loss: 0.5102996230125427\n",
      "Step: 3800  \tTraining accuracy: 0.7292060256004333\n",
      "Step: 3800  \tValid loss: 0.49980592727661133\n",
      "Step: 3900  \tTraining loss: 0.5098954439163208\n",
      "Step: 3900  \tTraining accuracy: 0.7296393513679504\n",
      "Step: 3900  \tValid loss: 0.4996037483215332\n",
      "Step: 4000  \tTraining loss: 0.5094994902610779\n",
      "Step: 4000  \tTraining accuracy: 0.7300540804862976\n",
      "Step: 4000  \tValid loss: 0.4994450509548187\n",
      "Step: 4100  \tTraining loss: 0.5091280937194824\n",
      "Step: 4100  \tTraining accuracy: 0.7304417490959167\n",
      "Step: 4100  \tValid loss: 0.49924221634864807\n",
      "Step: 4200  \tTraining loss: 0.5087729096412659\n",
      "Step: 4200  \tTraining accuracy: 0.7307692170143127\n",
      "Step: 4200  \tValid loss: 0.49900418519973755\n",
      "Step: 4300  \tTraining loss: 0.5083585381507874\n",
      "Step: 4300  \tTraining accuracy: 0.7310625910758972\n",
      "Step: 4300  \tValid loss: 0.4988064169883728\n",
      "Step: 4400  \tTraining loss: 0.5080059170722961\n",
      "Step: 4400  \tTraining accuracy: 0.7313515543937683\n",
      "Step: 4400  \tValid loss: 0.49852657318115234\n",
      "Step: 4500  \tTraining loss: 0.5076804161071777\n",
      "Step: 4500  \tTraining accuracy: 0.7316156625747681\n",
      "Step: 4500  \tValid loss: 0.4983634948730469\n",
      "Step: 4600  \tTraining loss: 0.5072625875473022\n",
      "Step: 4600  \tTraining accuracy: 0.7318768501281738\n",
      "Step: 4600  \tValid loss: 0.4977697432041168\n",
      "Step: 4700  \tTraining loss: 0.5068775415420532\n",
      "Step: 4700  \tTraining accuracy: 0.7321325540542603\n",
      "Step: 4700  \tValid loss: 0.4972406327724457\n",
      "Step: 4800  \tTraining loss: 0.5065575242042542\n",
      "Step: 4800  \tTraining accuracy: 0.7323942184448242\n",
      "Step: 4800  \tValid loss: 0.49693596363067627\n",
      "Step: 4900  \tTraining loss: 0.5062569975852966\n",
      "Step: 4900  \tTraining accuracy: 0.7326341867446899\n",
      "Step: 4900  \tValid loss: 0.4967063367366791\n",
      "Step: 5000  \tTraining loss: 0.5059401392936707\n",
      "Step: 5000  \tTraining accuracy: 0.7328617572784424\n",
      "Step: 5000  \tValid loss: 0.49644845724105835\n",
      "Step: 5100  \tTraining loss: 0.5056097507476807\n",
      "Step: 5100  \tTraining accuracy: 0.7330803275108337\n",
      "Step: 5100  \tValid loss: 0.49617379903793335\n",
      "Step: 5200  \tTraining loss: 0.5052747130393982\n",
      "Step: 5200  \tTraining accuracy: 0.7332775592803955\n",
      "Step: 5200  \tValid loss: 0.49593958258628845\n",
      "Step: 5300  \tTraining loss: 0.504950225353241\n",
      "Step: 5300  \tTraining accuracy: 0.7334445118904114\n",
      "Step: 5300  \tValid loss: 0.49570193886756897\n",
      "Step: 5400  \tTraining loss: 0.504626989364624\n",
      "Step: 5400  \tTraining accuracy: 0.7336225509643555\n",
      "Step: 5400  \tValid loss: 0.4954768121242523\n",
      "Step: 5500  \tTraining loss: 0.5042446851730347\n",
      "Step: 5500  \tTraining accuracy: 0.7337892055511475\n",
      "Step: 5500  \tValid loss: 0.49536561965942383\n",
      "Step: 5600  \tTraining loss: 0.503883421421051\n",
      "Step: 5600  \tTraining accuracy: 0.733971357345581\n",
      "Step: 5600  \tValid loss: 0.4952297508716583\n",
      "Step: 5700  \tTraining loss: 0.503511369228363\n",
      "Step: 5700  \tTraining accuracy: 0.7341400384902954\n",
      "Step: 5700  \tValid loss: 0.49500027298927307\n",
      "Step: 5800  \tTraining loss: 0.5031452178955078\n",
      "Step: 5800  \tTraining accuracy: 0.7342636585235596\n",
      "Step: 5800  \tValid loss: 0.49481648206710815\n",
      "Step: 5900  \tTraining loss: 0.5027977824211121\n",
      "Step: 5900  \tTraining accuracy: 0.734378457069397\n",
      "Step: 5900  \tValid loss: 0.4945922791957855\n",
      "Step: 6000  \tTraining loss: 0.5024685263633728\n",
      "Step: 6000  \tTraining accuracy: 0.7344872355461121\n",
      "Step: 6000  \tValid loss: 0.4944034218788147\n",
      "Step: 6100  \tTraining loss: 0.5021618008613586\n",
      "Step: 6100  \tTraining accuracy: 0.7345857620239258\n",
      "Step: 6100  \tValid loss: 0.494255006313324\n",
      "Step: 6200  \tTraining loss: 0.501876175403595\n",
      "Step: 6200  \tTraining accuracy: 0.7346660494804382\n",
      "Step: 6200  \tValid loss: 0.49408668279647827\n",
      "Step: 6300  \tTraining loss: 0.5016045570373535\n",
      "Step: 6300  \tTraining accuracy: 0.7347501516342163\n",
      "Step: 6300  \tValid loss: 0.49391210079193115\n",
      "Step: 6400  \tTraining loss: 0.5013394355773926\n",
      "Step: 6400  \tTraining accuracy: 0.7348482608795166\n",
      "Step: 6400  \tValid loss: 0.49375611543655396\n",
      "Step: 6500  \tTraining loss: 0.5010820627212524\n",
      "Step: 6500  \tTraining accuracy: 0.734941303730011\n",
      "Step: 6500  \tValid loss: 0.4935868978500366\n",
      "Step: 6600  \tTraining loss: 0.5008191466331482\n",
      "Step: 6600  \tTraining accuracy: 0.7350355386734009\n",
      "Step: 6600  \tValid loss: 0.4934271574020386\n",
      "Step: 6700  \tTraining loss: 0.5005677342414856\n",
      "Step: 6700  \tTraining accuracy: 0.7351369261741638\n",
      "Step: 6700  \tValid loss: 0.4932838976383209\n",
      "Step: 6800  \tTraining loss: 0.5003247261047363\n",
      "Step: 6800  \tTraining accuracy: 0.7352490425109863\n",
      "Step: 6800  \tValid loss: 0.4931340217590332\n",
      "Step: 6900  \tTraining loss: 0.5000928640365601\n",
      "Step: 6900  \tTraining accuracy: 0.7353637218475342\n",
      "Step: 6900  \tValid loss: 0.49297332763671875\n",
      "Step: 7000  \tTraining loss: 0.499866783618927\n",
      "Step: 7000  \tTraining accuracy: 0.7354846000671387\n",
      "Step: 7000  \tValid loss: 0.4928779602050781\n",
      "Step: 7100  \tTraining loss: 0.49964696168899536\n",
      "Step: 7100  \tTraining accuracy: 0.7356133460998535\n",
      "Step: 7100  \tValid loss: 0.4927810728549957\n",
      "Step: 7200  \tTraining loss: 0.49943307042121887\n",
      "Step: 7200  \tTraining accuracy: 0.7357366681098938\n",
      "Step: 7200  \tValid loss: 0.49267590045928955\n",
      "Step: 7300  \tTraining loss: 0.4991902709007263\n",
      "Step: 7300  \tTraining accuracy: 0.7358474135398865\n",
      "Step: 7300  \tValid loss: 0.4925122857093811\n",
      "Step: 7400  \tTraining loss: 0.49896249175071716\n",
      "Step: 7400  \tTraining accuracy: 0.7359353303909302\n",
      "Step: 7400  \tValid loss: 0.4923373758792877\n",
      "Step: 7500  \tTraining loss: 0.49874866008758545\n",
      "Step: 7500  \tTraining accuracy: 0.736040472984314\n",
      "Step: 7500  \tValid loss: 0.4921712577342987\n",
      "Step: 7600  \tTraining loss: 0.498544305562973\n",
      "Step: 7600  \tTraining accuracy: 0.7361375093460083\n",
      "Step: 7600  \tValid loss: 0.4920007884502411\n",
      "Step: 7700  \tTraining loss: 0.4983459413051605\n",
      "Step: 7700  \tTraining accuracy: 0.7362441420555115\n",
      "Step: 7700  \tValid loss: 0.49186617136001587\n",
      "Step: 7800  \tTraining loss: 0.498151570558548\n",
      "Step: 7800  \tTraining accuracy: 0.7363549470901489\n",
      "Step: 7800  \tValid loss: 0.49173253774642944\n",
      "Step: 7900  \tTraining loss: 0.49794191122055054\n",
      "Step: 7900  \tTraining accuracy: 0.7364526987075806\n",
      "Step: 7900  \tValid loss: 0.49151429533958435\n",
      "Step: 8000  \tTraining loss: 0.4977469742298126\n",
      "Step: 8000  \tTraining accuracy: 0.7365313768386841\n",
      "Step: 8000  \tValid loss: 0.4913262724876404\n",
      "Step: 8100  \tTraining loss: 0.4975472390651703\n",
      "Step: 8100  \tTraining accuracy: 0.7366047501564026\n",
      "Step: 8100  \tValid loss: 0.49113523960113525\n",
      "Step: 8200  \tTraining loss: 0.4972984492778778\n",
      "Step: 8200  \tTraining accuracy: 0.7366698384284973\n",
      "Step: 8200  \tValid loss: 0.4908561110496521\n",
      "Step: 8300  \tTraining loss: 0.49710461497306824\n",
      "Step: 8300  \tTraining accuracy: 0.7367349863052368\n",
      "Step: 8300  \tValid loss: 0.4906197488307953\n",
      "Step: 8400  \tTraining loss: 0.49691441655158997\n",
      "Step: 8400  \tTraining accuracy: 0.7368017435073853\n",
      "Step: 8400  \tValid loss: 0.4904029965400696\n",
      "Step: 8500  \tTraining loss: 0.49672573804855347\n",
      "Step: 8500  \tTraining accuracy: 0.7368731498718262\n",
      "Step: 8500  \tValid loss: 0.4902391731739044\n",
      "Step: 8600  \tTraining loss: 0.4965420365333557\n",
      "Step: 8600  \tTraining accuracy: 0.7369460463523865\n",
      "Step: 8600  \tValid loss: 0.4900631010532379\n",
      "Step: 8700  \tTraining loss: 0.49636462330818176\n",
      "Step: 8700  \tTraining accuracy: 0.7370095252990723\n",
      "Step: 8700  \tValid loss: 0.4898611605167389\n",
      "Step: 8800  \tTraining loss: 0.4961841106414795\n",
      "Step: 8800  \tTraining accuracy: 0.7370700836181641\n",
      "Step: 8800  \tValid loss: 0.4897156357765198\n",
      "Step: 8900  \tTraining loss: 0.4960016906261444\n",
      "Step: 8900  \tTraining accuracy: 0.73714280128479\n",
      "Step: 8900  \tValid loss: 0.4895564913749695\n",
      "Step: 9000  \tTraining loss: 0.495816707611084\n",
      "Step: 9000  \tTraining accuracy: 0.7372123599052429\n",
      "Step: 9000  \tValid loss: 0.48941826820373535\n",
      "Step: 9100  \tTraining loss: 0.49563854932785034\n",
      "Step: 9100  \tTraining accuracy: 0.7372774481773376\n",
      "Step: 9100  \tValid loss: 0.4892171323299408\n",
      "Step: 9200  \tTraining loss: 0.495443195104599\n",
      "Step: 9200  \tTraining accuracy: 0.7373440265655518\n",
      "Step: 9200  \tValid loss: 0.4890788793563843\n",
      "Step: 9300  \tTraining loss: 0.49522677063941956\n",
      "Step: 9300  \tTraining accuracy: 0.7374091148376465\n",
      "Step: 9300  \tValid loss: 0.4889085292816162\n",
      "Step: 9400  \tTraining loss: 0.49504750967025757\n",
      "Step: 9400  \tTraining accuracy: 0.7374728918075562\n",
      "Step: 9400  \tValid loss: 0.4887234568595886\n",
      "Step: 9500  \tTraining loss: 0.4948712885379791\n",
      "Step: 9500  \tTraining accuracy: 0.7375324368476868\n",
      "Step: 9500  \tValid loss: 0.48859578371047974\n",
      "Step: 9600  \tTraining loss: 0.4946984052658081\n",
      "Step: 9600  \tTraining accuracy: 0.7375810742378235\n",
      "Step: 9600  \tValid loss: 0.48846468329429626\n",
      "Step: 9700  \tTraining loss: 0.4945248067378998\n",
      "Step: 9700  \tTraining accuracy: 0.7376382946968079\n",
      "Step: 9700  \tValid loss: 0.4882916808128357\n",
      "Step: 9800  \tTraining loss: 0.4943508505821228\n",
      "Step: 9800  \tTraining accuracy: 0.7376970648765564\n",
      "Step: 9800  \tValid loss: 0.48813846707344055\n",
      "Step: 9900  \tTraining loss: 0.49417853355407715\n",
      "Step: 9900  \tTraining accuracy: 0.7377519607543945\n",
      "Step: 9900  \tValid loss: 0.4879889488220215\n",
      "Step: 10000  \tTraining loss: 0.4939981997013092\n",
      "Step: 10000  \tTraining accuracy: 0.7378084063529968\n",
      "Step: 10000  \tValid loss: 0.4878242313861847\n",
      "Step: 10100  \tTraining loss: 0.49381911754608154\n",
      "Step: 10100  \tTraining accuracy: 0.7378729581832886\n",
      "Step: 10100  \tValid loss: 0.4876795709133148\n",
      "Step: 10200  \tTraining loss: 0.4936375319957733\n",
      "Step: 10200  \tTraining accuracy: 0.7379466891288757\n",
      "Step: 10200  \tValid loss: 0.4875069856643677\n",
      "Step: 10300  \tTraining loss: 0.4934542775154114\n",
      "Step: 10300  \tTraining accuracy: 0.7380229234695435\n",
      "Step: 10300  \tValid loss: 0.48735523223876953\n",
      "Step: 10400  \tTraining loss: 0.49326783418655396\n",
      "Step: 10400  \tTraining accuracy: 0.7381066083908081\n",
      "Step: 10400  \tValid loss: 0.487190306186676\n",
      "Step: 10500  \tTraining loss: 0.493080198764801\n",
      "Step: 10500  \tTraining accuracy: 0.7381886839866638\n",
      "Step: 10500  \tValid loss: 0.48704206943511963\n",
      "Step: 10600  \tTraining loss: 0.49288836121559143\n",
      "Step: 10600  \tTraining accuracy: 0.7382628917694092\n",
      "Step: 10600  \tValid loss: 0.4868775010108948\n",
      "Step: 10700  \tTraining loss: 0.4926741421222687\n",
      "Step: 10700  \tTraining accuracy: 0.7383432388305664\n",
      "Step: 10700  \tValid loss: 0.486571729183197\n",
      "Step: 10800  \tTraining loss: 0.49247241020202637\n",
      "Step: 10800  \tTraining accuracy: 0.7384257316589355\n",
      "Step: 10800  \tValid loss: 0.48637863993644714\n",
      "Step: 10900  \tTraining loss: 0.4922676682472229\n",
      "Step: 10900  \tTraining accuracy: 0.7385079860687256\n",
      "Step: 10900  \tValid loss: 0.48620250821113586\n",
      "Step: 11000  \tTraining loss: 0.4920498728752136\n",
      "Step: 11000  \tTraining accuracy: 0.7385959625244141\n",
      "Step: 11000  \tValid loss: 0.4859231114387512\n",
      "Step: 11100  \tTraining loss: 0.49182912707328796\n",
      "Step: 11100  \tTraining accuracy: 0.7386896014213562\n",
      "Step: 11100  \tValid loss: 0.48566392064094543\n",
      "Step: 11200  \tTraining loss: 0.4916055202484131\n",
      "Step: 11200  \tTraining accuracy: 0.7387815117835999\n",
      "Step: 11200  \tValid loss: 0.48543208837509155\n",
      "Step: 11300  \tTraining loss: 0.4913787841796875\n",
      "Step: 11300  \tTraining accuracy: 0.7388647198677063\n",
      "Step: 11300  \tValid loss: 0.4852108359336853\n",
      "Step: 11400  \tTraining loss: 0.49114909768104553\n",
      "Step: 11400  \tTraining accuracy: 0.7389464974403381\n",
      "Step: 11400  \tValid loss: 0.4850017726421356\n",
      "Step: 11500  \tTraining loss: 0.49091577529907227\n",
      "Step: 11500  \tTraining accuracy: 0.7390279769897461\n",
      "Step: 11500  \tValid loss: 0.48480549454689026\n",
      "Step: 11600  \tTraining loss: 0.4906691610813141\n",
      "Step: 11600  \tTraining accuracy: 0.7391091585159302\n",
      "Step: 11600  \tValid loss: 0.48451581597328186\n",
      "Step: 11700  \tTraining loss: 0.4904089868068695\n",
      "Step: 11700  \tTraining accuracy: 0.7391958236694336\n",
      "Step: 11700  \tValid loss: 0.48425909876823425\n",
      "Step: 11800  \tTraining loss: 0.4901474416255951\n",
      "Step: 11800  \tTraining accuracy: 0.7392866611480713\n",
      "Step: 11800  \tValid loss: 0.4839927554130554\n",
      "Step: 11900  \tTraining loss: 0.4898817837238312\n",
      "Step: 11900  \tTraining accuracy: 0.7393893599510193\n",
      "Step: 11900  \tValid loss: 0.4837317168712616\n",
      "Step: 12000  \tTraining loss: 0.48961037397384644\n",
      "Step: 12000  \tTraining accuracy: 0.7395025491714478\n",
      "Step: 12000  \tValid loss: 0.48341891169548035\n",
      "Step: 12100  \tTraining loss: 0.48933011293411255\n",
      "Step: 12100  \tTraining accuracy: 0.7396094799041748\n",
      "Step: 12100  \tValid loss: 0.4831295311450958\n",
      "Step: 12200  \tTraining loss: 0.4890088438987732\n",
      "Step: 12200  \tTraining accuracy: 0.7397201061248779\n",
      "Step: 12200  \tValid loss: 0.4826776385307312\n",
      "Step: 12300  \tTraining loss: 0.48870646953582764\n",
      "Step: 12300  \tTraining accuracy: 0.7398256659507751\n",
      "Step: 12300  \tValid loss: 0.48232772946357727\n",
      "Step: 12400  \tTraining loss: 0.4884050786495209\n",
      "Step: 12400  \tTraining accuracy: 0.7399381399154663\n",
      "Step: 12400  \tValid loss: 0.48194196820259094\n",
      "Step: 12500  \tTraining loss: 0.4881037473678589\n",
      "Step: 12500  \tTraining accuracy: 0.7400519847869873\n",
      "Step: 12500  \tValid loss: 0.48158639669418335\n",
      "Step: 12600  \tTraining loss: 0.4878060221672058\n",
      "Step: 12600  \tTraining accuracy: 0.740164041519165\n",
      "Step: 12600  \tValid loss: 0.4812573194503784\n",
      "Step: 12700  \tTraining loss: 0.4875130355358124\n",
      "Step: 12700  \tTraining accuracy: 0.7402763366699219\n",
      "Step: 12700  \tValid loss: 0.4809240698814392\n",
      "Step: 12800  \tTraining loss: 0.4872238337993622\n",
      "Step: 12800  \tTraining accuracy: 0.7403827905654907\n",
      "Step: 12800  \tValid loss: 0.48058202862739563\n",
      "Step: 12900  \tTraining loss: 0.48693713545799255\n",
      "Step: 12900  \tTraining accuracy: 0.7404792904853821\n",
      "Step: 12900  \tValid loss: 0.480238139629364\n",
      "Step: 13000  \tTraining loss: 0.4866569936275482\n",
      "Step: 13000  \tTraining accuracy: 0.7405773997306824\n",
      "Step: 13000  \tValid loss: 0.4799104630947113\n",
      "Step: 13100  \tTraining loss: 0.4863796532154083\n",
      "Step: 13100  \tTraining accuracy: 0.7406790852546692\n",
      "Step: 13100  \tValid loss: 0.4795595705509186\n",
      "Step: 13200  \tTraining loss: 0.48611003160476685\n",
      "Step: 13200  \tTraining accuracy: 0.7407852411270142\n",
      "Step: 13200  \tValid loss: 0.4792126715183258\n",
      "Step: 13300  \tTraining loss: 0.4858432710170746\n",
      "Step: 13300  \tTraining accuracy: 0.740889847278595\n",
      "Step: 13300  \tValid loss: 0.4788980782032013\n",
      "Step: 13400  \tTraining loss: 0.4855799376964569\n",
      "Step: 13400  \tTraining accuracy: 0.7409898638725281\n",
      "Step: 13400  \tValid loss: 0.4785543382167816\n",
      "Step: 13500  \tTraining loss: 0.4853231906890869\n",
      "Step: 13500  \tTraining accuracy: 0.7410864233970642\n",
      "Step: 13500  \tValid loss: 0.47819432616233826\n",
      "Step: 13600  \tTraining loss: 0.485073983669281\n",
      "Step: 13600  \tTraining accuracy: 0.7411816120147705\n",
      "Step: 13600  \tValid loss: 0.47788190841674805\n",
      "Step: 13700  \tTraining loss: 0.4848277270793915\n",
      "Step: 13700  \tTraining accuracy: 0.741283118724823\n",
      "Step: 13700  \tValid loss: 0.477573037147522\n",
      "Step: 13800  \tTraining loss: 0.48458775877952576\n",
      "Step: 13800  \tTraining accuracy: 0.7413812279701233\n",
      "Step: 13800  \tValid loss: 0.4772551357746124\n",
      "Step: 13900  \tTraining loss: 0.48435354232788086\n",
      "Step: 13900  \tTraining accuracy: 0.7414759993553162\n",
      "Step: 13900  \tValid loss: 0.47695523500442505\n",
      "Step: 14000  \tTraining loss: 0.48411834239959717\n",
      "Step: 14000  \tTraining accuracy: 0.7415665984153748\n",
      "Step: 14000  \tValid loss: 0.4766604006290436\n",
      "Step: 14100  \tTraining loss: 0.48388397693634033\n",
      "Step: 14100  \tTraining accuracy: 0.7416539788246155\n",
      "Step: 14100  \tValid loss: 0.4763341248035431\n",
      "Step: 14200  \tTraining loss: 0.483656644821167\n",
      "Step: 14200  \tTraining accuracy: 0.7417411208152771\n",
      "Step: 14200  \tValid loss: 0.47604700922966003\n",
      "Step: 14300  \tTraining loss: 0.4834355115890503\n",
      "Step: 14300  \tTraining accuracy: 0.7418288588523865\n",
      "Step: 14300  \tValid loss: 0.47577327489852905\n",
      "Step: 14400  \tTraining loss: 0.4832197427749634\n",
      "Step: 14400  \tTraining accuracy: 0.7419134974479675\n",
      "Step: 14400  \tValid loss: 0.4755175709724426\n",
      "Step: 14500  \tTraining loss: 0.48301011323928833\n",
      "Step: 14500  \tTraining accuracy: 0.7419924139976501\n",
      "Step: 14500  \tValid loss: 0.4752708375453949\n",
      "Step: 14600  \tTraining loss: 0.48280516266822815\n",
      "Step: 14600  \tTraining accuracy: 0.7420720458030701\n",
      "Step: 14600  \tValid loss: 0.47504860162734985\n",
      "Step: 14700  \tTraining loss: 0.4826032221317291\n",
      "Step: 14700  \tTraining accuracy: 0.742150604724884\n",
      "Step: 14700  \tValid loss: 0.4748246371746063\n",
      "Step: 14800  \tTraining loss: 0.48240506649017334\n",
      "Step: 14800  \tTraining accuracy: 0.742228090763092\n",
      "Step: 14800  \tValid loss: 0.4746086001396179\n",
      "Step: 14900  \tTraining loss: 0.4822111427783966\n",
      "Step: 14900  \tTraining accuracy: 0.7423036694526672\n",
      "Step: 14900  \tValid loss: 0.4744095802307129\n",
      "Step: 15000  \tTraining loss: 0.4820239543914795\n",
      "Step: 15000  \tTraining accuracy: 0.7423746585845947\n",
      "Step: 15000  \tValid loss: 0.4741818606853485\n",
      "Step: 15100  \tTraining loss: 0.48183733224868774\n",
      "Step: 15100  \tTraining accuracy: 0.7424482703208923\n",
      "Step: 15100  \tValid loss: 0.47401365637779236\n",
      "Step: 15200  \tTraining loss: 0.48165163397789\n",
      "Step: 15200  \tTraining accuracy: 0.7425234913825989\n",
      "Step: 15200  \tValid loss: 0.47382330894470215\n",
      "Step: 15300  \tTraining loss: 0.48147067427635193\n",
      "Step: 15300  \tTraining accuracy: 0.7425916194915771\n",
      "Step: 15300  \tValid loss: 0.4736661911010742\n",
      "Step: 15400  \tTraining loss: 0.48129069805145264\n",
      "Step: 15400  \tTraining accuracy: 0.7426571846008301\n",
      "Step: 15400  \tValid loss: 0.47348982095718384\n",
      "Step: 15500  \tTraining loss: 0.48111364245414734\n",
      "Step: 15500  \tTraining accuracy: 0.7427150011062622\n",
      "Step: 15500  \tValid loss: 0.4733292758464813\n",
      "Step: 15600  \tTraining loss: 0.48093926906585693\n",
      "Step: 15600  \tTraining accuracy: 0.7427695393562317\n",
      "Step: 15600  \tValid loss: 0.4731625020503998\n",
      "Step: 15700  \tTraining loss: 0.48076605796813965\n",
      "Step: 15700  \tTraining accuracy: 0.7428216934204102\n",
      "Step: 15700  \tValid loss: 0.47303107380867004\n",
      "Step: 15800  \tTraining loss: 0.480593740940094\n",
      "Step: 15800  \tTraining accuracy: 0.7428680658340454\n",
      "Step: 15800  \tValid loss: 0.4728751480579376\n",
      "Step: 15900  \tTraining loss: 0.48042500019073486\n",
      "Step: 15900  \tTraining accuracy: 0.7429139018058777\n",
      "Step: 15900  \tValid loss: 0.4726806879043579\n",
      "Step: 16000  \tTraining loss: 0.4802548289299011\n",
      "Step: 16000  \tTraining accuracy: 0.7429550290107727\n",
      "Step: 16000  \tValid loss: 0.4725550711154938\n",
      "Step: 16100  \tTraining loss: 0.4800867736339569\n",
      "Step: 16100  \tTraining accuracy: 0.7429890036582947\n",
      "Step: 16100  \tValid loss: 0.4723949134349823\n",
      "Step: 16200  \tTraining loss: 0.4799206554889679\n",
      "Step: 16200  \tTraining accuracy: 0.743025004863739\n",
      "Step: 16200  \tValid loss: 0.47223353385925293\n",
      "Step: 16300  \tTraining loss: 0.4797538220882416\n",
      "Step: 16300  \tTraining accuracy: 0.7430622577667236\n",
      "Step: 16300  \tValid loss: 0.47209322452545166\n",
      "Step: 16400  \tTraining loss: 0.4795868694782257\n",
      "Step: 16400  \tTraining accuracy: 0.7431014180183411\n",
      "Step: 16400  \tValid loss: 0.47192591428756714\n",
      "Step: 16500  \tTraining loss: 0.4794204533100128\n",
      "Step: 16500  \tTraining accuracy: 0.743141770362854\n",
      "Step: 16500  \tValid loss: 0.4717758297920227\n",
      "Step: 16600  \tTraining loss: 0.47925570607185364\n",
      "Step: 16600  \tTraining accuracy: 0.743183970451355\n",
      "Step: 16600  \tValid loss: 0.47162967920303345\n",
      "Step: 16700  \tTraining loss: 0.4790888726711273\n",
      "Step: 16700  \tTraining accuracy: 0.7432296872138977\n",
      "Step: 16700  \tValid loss: 0.47148168087005615\n",
      "Step: 16800  \tTraining loss: 0.4789239764213562\n",
      "Step: 16800  \tTraining accuracy: 0.7432764768600464\n",
      "Step: 16800  \tValid loss: 0.4713597893714905\n",
      "Step: 16900  \tTraining loss: 0.47875526547431946\n",
      "Step: 16900  \tTraining accuracy: 0.7433249950408936\n",
      "Step: 16900  \tValid loss: 0.4712570905685425\n",
      "Step: 17000  \tTraining loss: 0.47858476638793945\n",
      "Step: 17000  \tTraining accuracy: 0.7433745861053467\n",
      "Step: 17000  \tValid loss: 0.4711456000804901\n",
      "Step: 17100  \tTraining loss: 0.4784143567085266\n",
      "Step: 17100  \tTraining accuracy: 0.7434235215187073\n",
      "Step: 17100  \tValid loss: 0.4710347652435303\n",
      "Step: 17200  \tTraining loss: 0.4782407581806183\n",
      "Step: 17200  \tTraining accuracy: 0.743472695350647\n",
      "Step: 17200  \tValid loss: 0.47091758251190186\n",
      "Step: 17300  \tTraining loss: 0.47806280851364136\n",
      "Step: 17300  \tTraining accuracy: 0.7435174584388733\n",
      "Step: 17300  \tValid loss: 0.4707847535610199\n",
      "Step: 17400  \tTraining loss: 0.47787120938301086\n",
      "Step: 17400  \tTraining accuracy: 0.7435609698295593\n",
      "Step: 17400  \tValid loss: 0.47067955136299133\n",
      "Step: 17500  \tTraining loss: 0.4776773452758789\n",
      "Step: 17500  \tTraining accuracy: 0.7436062097549438\n",
      "Step: 17500  \tValid loss: 0.47055935859680176\n",
      "Step: 17600  \tTraining loss: 0.4774887263774872\n",
      "Step: 17600  \tTraining accuracy: 0.7436516880989075\n",
      "Step: 17600  \tValid loss: 0.4703994691371918\n",
      "Step: 17700  \tTraining loss: 0.4773015081882477\n",
      "Step: 17700  \tTraining accuracy: 0.7436959743499756\n",
      "Step: 17700  \tValid loss: 0.4702790379524231\n",
      "Step: 17800  \tTraining loss: 0.47711533308029175\n",
      "Step: 17800  \tTraining accuracy: 0.7437344193458557\n",
      "Step: 17800  \tValid loss: 0.4701157808303833\n",
      "Step: 17900  \tTraining loss: 0.4769160747528076\n",
      "Step: 17900  \tTraining accuracy: 0.7437710165977478\n",
      "Step: 17900  \tValid loss: 0.46987369656562805\n",
      "Step: 18000  \tTraining loss: 0.476726770401001\n",
      "Step: 18000  \tTraining accuracy: 0.7438071966171265\n",
      "Step: 18000  \tValid loss: 0.4697078466415405\n",
      "Step: 18100  \tTraining loss: 0.476539671421051\n",
      "Step: 18100  \tTraining accuracy: 0.7438386082649231\n",
      "Step: 18100  \tValid loss: 0.46956712007522583\n",
      "Step: 18200  \tTraining loss: 0.4763520359992981\n",
      "Step: 18200  \tTraining accuracy: 0.7438673973083496\n",
      "Step: 18200  \tValid loss: 0.4694342613220215\n",
      "Step: 18300  \tTraining loss: 0.47616469860076904\n",
      "Step: 18300  \tTraining accuracy: 0.743895947933197\n",
      "Step: 18300  \tValid loss: 0.4693068265914917\n",
      "Step: 18400  \tTraining loss: 0.4759786128997803\n",
      "Step: 18400  \tTraining accuracy: 0.7439248561859131\n",
      "Step: 18400  \tValid loss: 0.46917638182640076\n",
      "Step: 18500  \tTraining loss: 0.4757553040981293\n",
      "Step: 18500  \tTraining accuracy: 0.7439498901367188\n",
      "Step: 18500  \tValid loss: 0.46904370188713074\n",
      "Step: 18600  \tTraining loss: 0.4755235016345978\n",
      "Step: 18600  \tTraining accuracy: 0.7439767718315125\n",
      "Step: 18600  \tValid loss: 0.4688609838485718\n",
      "Step: 18700  \tTraining loss: 0.4753226935863495\n",
      "Step: 18700  \tTraining accuracy: 0.7440140247344971\n",
      "Step: 18700  \tValid loss: 0.4688081741333008\n",
      "Step: 18800  \tTraining loss: 0.47514569759368896\n",
      "Step: 18800  \tTraining accuracy: 0.744053065776825\n",
      "Step: 18800  \tValid loss: 0.4687548875808716\n",
      "Step: 18900  \tTraining loss: 0.4749850034713745\n",
      "Step: 18900  \tTraining accuracy: 0.7440916299819946\n",
      "Step: 18900  \tValid loss: 0.46865829825401306\n",
      "Step: 19000  \tTraining loss: 0.4748299717903137\n",
      "Step: 19000  \tTraining accuracy: 0.7441298365592957\n",
      "Step: 19000  \tValid loss: 0.46856459975242615\n",
      "Step: 19100  \tTraining loss: 0.47467944025993347\n",
      "Step: 19100  \tTraining accuracy: 0.744166910648346\n",
      "Step: 19100  \tValid loss: 0.4684655964374542\n",
      "Step: 19200  \tTraining loss: 0.4745326340198517\n",
      "Step: 19200  \tTraining accuracy: 0.7442091107368469\n",
      "Step: 19200  \tValid loss: 0.4683772623538971\n",
      "Step: 19300  \tTraining loss: 0.47439128160476685\n",
      "Step: 19300  \tTraining accuracy: 0.7442516088485718\n",
      "Step: 19300  \tValid loss: 0.4682594835758209\n",
      "Step: 19400  \tTraining loss: 0.4742482602596283\n",
      "Step: 19400  \tTraining accuracy: 0.7442902326583862\n",
      "Step: 19400  \tValid loss: 0.46820148825645447\n",
      "Step: 19500  \tTraining loss: 0.47410038113594055\n",
      "Step: 19500  \tTraining accuracy: 0.7443284392356873\n",
      "Step: 19500  \tValid loss: 0.46805381774902344\n",
      "Step: 19600  \tTraining loss: 0.4739525020122528\n",
      "Step: 19600  \tTraining accuracy: 0.7443696856498718\n",
      "Step: 19600  \tValid loss: 0.46799159049987793\n",
      "Step: 19700  \tTraining loss: 0.4738113284111023\n",
      "Step: 19700  \tTraining accuracy: 0.7444138526916504\n",
      "Step: 19700  \tValid loss: 0.4679360091686249\n",
      "Step: 19800  \tTraining loss: 0.4736740291118622\n",
      "Step: 19800  \tTraining accuracy: 0.744462251663208\n",
      "Step: 19800  \tValid loss: 0.46787571907043457\n",
      "Step: 19900  \tTraining loss: 0.4735388159751892\n",
      "Step: 19900  \tTraining accuracy: 0.7445115447044373\n",
      "Step: 19900  \tValid loss: 0.4678233563899994\n",
      "Step: 20000  \tTraining loss: 0.4734056293964386\n",
      "Step: 20000  \tTraining accuracy: 0.7445563673973083\n",
      "Step: 20000  \tValid loss: 0.46777021884918213\n",
      "Step: 20100  \tTraining loss: 0.47327694296836853\n",
      "Step: 20100  \tTraining accuracy: 0.7445954084396362\n",
      "Step: 20100  \tValid loss: 0.4677213430404663\n",
      "Step: 20200  \tTraining loss: 0.473145991563797\n",
      "Step: 20200  \tTraining accuracy: 0.7446354031562805\n",
      "Step: 20200  \tValid loss: 0.46768349409103394\n",
      "Step: 20300  \tTraining loss: 0.47301626205444336\n",
      "Step: 20300  \tTraining accuracy: 0.7446749806404114\n",
      "Step: 20300  \tValid loss: 0.4676499366760254\n",
      "Step: 20400  \tTraining loss: 0.47289130091667175\n",
      "Step: 20400  \tTraining accuracy: 0.7447167634963989\n",
      "Step: 20400  \tValid loss: 0.46759331226348877\n",
      "Step: 20500  \tTraining loss: 0.47276434302330017\n",
      "Step: 20500  \tTraining accuracy: 0.7447595000267029\n",
      "Step: 20500  \tValid loss: 0.4675588011741638\n",
      "Step: 20600  \tTraining loss: 0.4726286232471466\n",
      "Step: 20600  \tTraining accuracy: 0.7448024153709412\n",
      "Step: 20600  \tValid loss: 0.467587411403656\n",
      "Step: 20700  \tTraining loss: 0.4724830090999603\n",
      "Step: 20700  \tTraining accuracy: 0.7448423504829407\n",
      "Step: 20700  \tValid loss: 0.4676019847393036\n",
      "Step: 20800  \tTraining loss: 0.47234347462654114\n",
      "Step: 20800  \tTraining accuracy: 0.7448800206184387\n",
      "Step: 20800  \tValid loss: 0.4675827622413635\n",
      "Step: 20900  \tTraining loss: 0.472209095954895\n",
      "Step: 20900  \tTraining accuracy: 0.7449191808700562\n",
      "Step: 20900  \tValid loss: 0.46758079528808594\n",
      "Step: 21000  \tTraining loss: 0.47207796573638916\n",
      "Step: 21000  \tTraining accuracy: 0.744961142539978\n",
      "Step: 21000  \tValid loss: 0.4675714373588562\n",
      "Step: 21100  \tTraining loss: 0.4719502925872803\n",
      "Step: 21100  \tTraining accuracy: 0.7450071573257446\n",
      "Step: 21100  \tValid loss: 0.4675566554069519\n",
      "Step: 21200  \tTraining loss: 0.47182655334472656\n",
      "Step: 21200  \tTraining accuracy: 0.7450539469718933\n",
      "Step: 21200  \tValid loss: 0.46752792596817017\n",
      "Step: 21300  \tTraining loss: 0.47171148657798767\n",
      "Step: 21300  \tTraining accuracy: 0.7450990676879883\n",
      "Step: 21300  \tValid loss: 0.46747472882270813\n",
      "Step: 21400  \tTraining loss: 0.4715961217880249\n",
      "Step: 21400  \tTraining accuracy: 0.7451450228691101\n",
      "Step: 21400  \tValid loss: 0.4674489498138428\n",
      "Step: 21500  \tTraining loss: 0.4714817702770233\n",
      "Step: 21500  \tTraining accuracy: 0.7451886534690857\n",
      "Step: 21500  \tValid loss: 0.46743112802505493\n",
      "Step: 21600  \tTraining loss: 0.47136974334716797\n",
      "Step: 21600  \tTraining accuracy: 0.7452306747436523\n",
      "Step: 21600  \tValid loss: 0.46741577982902527\n",
      "Step: 21700  \tTraining loss: 0.47125884890556335\n",
      "Step: 21700  \tTraining accuracy: 0.7452772259712219\n",
      "Step: 21700  \tValid loss: 0.4673973619937897\n",
      "Step: 21800  \tTraining loss: 0.4711506962776184\n",
      "Step: 21800  \tTraining accuracy: 0.7453209161758423\n",
      "Step: 21800  \tValid loss: 0.46736037731170654\n",
      "Step: 21900  \tTraining loss: 0.47103700041770935\n",
      "Step: 21900  \tTraining accuracy: 0.7453641891479492\n",
      "Step: 21900  \tValid loss: 0.4673801064491272\n",
      "Step: 22000  \tTraining loss: 0.4709298312664032\n",
      "Step: 22000  \tTraining accuracy: 0.745406448841095\n",
      "Step: 22000  \tValid loss: 0.4673633277416229\n",
      "Step: 22100  \tTraining loss: 0.47082072496414185\n",
      "Step: 22100  \tTraining accuracy: 0.7454537153244019\n",
      "Step: 22100  \tValid loss: 0.46734580397605896\n",
      "Step: 22200  \tTraining loss: 0.4707147181034088\n",
      "Step: 22200  \tTraining accuracy: 0.7455047965049744\n",
      "Step: 22200  \tValid loss: 0.46730858087539673\n",
      "Step: 22300  \tTraining loss: 0.47060832381248474\n",
      "Step: 22300  \tTraining accuracy: 0.7455577850341797\n",
      "Step: 22300  \tValid loss: 0.4673101305961609\n",
      "Step: 22400  \tTraining loss: 0.4704989790916443\n",
      "Step: 22400  \tTraining accuracy: 0.7456102967262268\n",
      "Step: 22400  \tValid loss: 0.4672819972038269\n",
      "Step: 22500  \tTraining loss: 0.47038882970809937\n",
      "Step: 22500  \tTraining accuracy: 0.7456623315811157\n",
      "Step: 22500  \tValid loss: 0.467227578163147\n",
      "Step: 22600  \tTraining loss: 0.47028061747550964\n",
      "Step: 22600  \tTraining accuracy: 0.7457138895988464\n",
      "Step: 22600  \tValid loss: 0.46720176935195923\n",
      "Step: 22700  \tTraining loss: 0.4701756536960602\n",
      "Step: 22700  \tTraining accuracy: 0.7457650303840637\n",
      "Step: 22700  \tValid loss: 0.46719154715538025\n",
      "Step: 22800  \tTraining loss: 0.47007158398628235\n",
      "Step: 22800  \tTraining accuracy: 0.7458156943321228\n",
      "Step: 22800  \tValid loss: 0.4671816825866699\n",
      "Step: 22900  \tTraining loss: 0.4699650704860687\n",
      "Step: 22900  \tTraining accuracy: 0.7458659410476685\n",
      "Step: 22900  \tValid loss: 0.46718883514404297\n",
      "Step: 23000  \tTraining loss: 0.4698629379272461\n",
      "Step: 23000  \tTraining accuracy: 0.7459140419960022\n",
      "Step: 23000  \tValid loss: 0.4671977460384369\n",
      "Step: 23100  \tTraining loss: 0.46975836157798767\n",
      "Step: 23100  \tTraining accuracy: 0.7459599375724792\n",
      "Step: 23100  \tValid loss: 0.46721044182777405\n",
      "Step: 23200  \tTraining loss: 0.46965718269348145\n",
      "Step: 23200  \tTraining accuracy: 0.7460089325904846\n",
      "Step: 23200  \tValid loss: 0.4672050178050995\n",
      "Step: 23300  \tTraining loss: 0.46955612301826477\n",
      "Step: 23300  \tTraining accuracy: 0.746060311794281\n",
      "Step: 23300  \tValid loss: 0.46723228693008423\n",
      "Step: 23400  \tTraining loss: 0.4694550633430481\n",
      "Step: 23400  \tTraining accuracy: 0.7461118102073669\n",
      "Step: 23400  \tValid loss: 0.4672333896160126\n",
      "Step: 23500  \tTraining loss: 0.4693553149700165\n",
      "Step: 23500  \tTraining accuracy: 0.7461651563644409\n",
      "Step: 23500  \tValid loss: 0.467250257730484\n",
      "Step: 23600  \tTraining loss: 0.4692549705505371\n",
      "Step: 23600  \tTraining accuracy: 0.7462180256843567\n",
      "Step: 23600  \tValid loss: 0.4672504663467407\n",
      "Step: 23700  \tTraining loss: 0.46915528178215027\n",
      "Step: 23700  \tTraining accuracy: 0.746270477771759\n",
      "Step: 23700  \tValid loss: 0.4672473967075348\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7463225\n",
      "Precision: 0.78507674\n",
      "Recall: 0.8856222\n",
      "F1 score: 0.7912375\n",
      "AUC: 0.70893383\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.746323   0.785077  0.885622  0.791237  0.708934  0.469117      0.746283   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.467167        0.74625   0.532734      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  23739.0  \n",
      "3\n",
      "(3915, 8)\n",
      "(3915, 1)\n",
      "(2080, 8)\n",
      "(2080, 1)\n",
      "(1690, 8)\n",
      "(1690, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5443854331970215\n",
      "Step: 100  \tTraining accuracy: 0.7402299046516418\n",
      "Step: 100  \tValid loss: 0.5622454285621643\n",
      "Step: 200  \tTraining loss: 0.516231894493103\n",
      "Step: 200  \tTraining accuracy: 0.7402586340904236\n",
      "Step: 200  \tValid loss: 0.5391093492507935\n",
      "Step: 300  \tTraining loss: 0.4887220561504364\n",
      "Step: 300  \tTraining accuracy: 0.7442053556442261\n",
      "Step: 300  \tValid loss: 0.5161009430885315\n",
      "Step: 400  \tTraining loss: 0.4505140781402588\n",
      "Step: 400  \tTraining accuracy: 0.7519466280937195\n",
      "Step: 400  \tValid loss: 0.4840161204338074\n",
      "Step: 500  \tTraining loss: 0.4101460874080658\n",
      "Step: 500  \tTraining accuracy: 0.7598326206207275\n",
      "Step: 500  \tValid loss: 0.4504571855068207\n",
      "Step: 600  \tTraining loss: 0.3893895745277405\n",
      "Step: 600  \tTraining accuracy: 0.7677373886108398\n",
      "Step: 600  \tValid loss: 0.4347098469734192\n",
      "Step: 700  \tTraining loss: 0.3811158835887909\n",
      "Step: 700  \tTraining accuracy: 0.7746726870536804\n",
      "Step: 700  \tValid loss: 0.42928314208984375\n",
      "Step: 800  \tTraining loss: 0.3775855004787445\n",
      "Step: 800  \tTraining accuracy: 0.7803500294685364\n",
      "Step: 800  \tValid loss: 0.4270687401294708\n",
      "Step: 900  \tTraining loss: 0.3756044805049896\n",
      "Step: 900  \tTraining accuracy: 0.7848153710365295\n",
      "Step: 900  \tValid loss: 0.4255731403827667\n",
      "Step: 1000  \tTraining loss: 0.37417545914649963\n",
      "Step: 1000  \tTraining accuracy: 0.7883825898170471\n",
      "Step: 1000  \tValid loss: 0.4241933226585388\n",
      "Step: 1100  \tTraining loss: 0.3729790449142456\n",
      "Step: 1100  \tTraining accuracy: 0.7913947701454163\n",
      "Step: 1100  \tValid loss: 0.4228370785713196\n",
      "Step: 1200  \tTraining loss: 0.37189629673957825\n",
      "Step: 1200  \tTraining accuracy: 0.7938269972801208\n",
      "Step: 1200  \tValid loss: 0.4214938282966614\n",
      "Step: 1300  \tTraining loss: 0.37088045477867126\n",
      "Step: 1300  \tTraining accuracy: 0.7959120273590088\n",
      "Step: 1300  \tValid loss: 0.42018115520477295\n",
      "Step: 1400  \tTraining loss: 0.3699110448360443\n",
      "Step: 1400  \tTraining accuracy: 0.7977461218833923\n",
      "Step: 1400  \tValid loss: 0.4189116358757019\n",
      "Step: 1500  \tTraining loss: 0.36898159980773926\n",
      "Step: 1500  \tTraining accuracy: 0.7993273735046387\n",
      "Step: 1500  \tValid loss: 0.41770458221435547\n",
      "Step: 1600  \tTraining loss: 0.3680887520313263\n",
      "Step: 1600  \tTraining accuracy: 0.8006879687309265\n",
      "Step: 1600  \tValid loss: 0.41657161712646484\n",
      "Step: 1700  \tTraining loss: 0.36723271012306213\n",
      "Step: 1700  \tTraining accuracy: 0.8018600940704346\n",
      "Step: 1700  \tValid loss: 0.41551846265792847\n",
      "Step: 1800  \tTraining loss: 0.366416871547699\n",
      "Step: 1800  \tTraining accuracy: 0.8028760552406311\n",
      "Step: 1800  \tValid loss: 0.4145525395870209\n",
      "Step: 1900  \tTraining loss: 0.36564216017723083\n",
      "Step: 1900  \tTraining accuracy: 0.8038033246994019\n",
      "Step: 1900  \tValid loss: 0.413677841424942\n",
      "Step: 2000  \tTraining loss: 0.36490726470947266\n",
      "Step: 2000  \tTraining accuracy: 0.8047622442245483\n",
      "Step: 2000  \tValid loss: 0.4128935933113098\n",
      "Step: 2100  \tTraining loss: 0.3642096519470215\n",
      "Step: 2100  \tTraining accuracy: 0.8056974411010742\n",
      "Step: 2100  \tValid loss: 0.41219523549079895\n",
      "Step: 2200  \tTraining loss: 0.3635381758213043\n",
      "Step: 2200  \tTraining accuracy: 0.8066121935844421\n",
      "Step: 2200  \tValid loss: 0.41156890988349915\n",
      "Step: 2300  \tTraining loss: 0.3628826141357422\n",
      "Step: 2300  \tTraining accuracy: 0.8074514865875244\n",
      "Step: 2300  \tValid loss: 0.4110001027584076\n",
      "Step: 2400  \tTraining loss: 0.3622472882270813\n",
      "Step: 2400  \tTraining accuracy: 0.8082138895988464\n",
      "Step: 2400  \tValid loss: 0.4104786813259125\n",
      "Step: 2500  \tTraining loss: 0.3616432547569275\n",
      "Step: 2500  \tTraining accuracy: 0.8089246153831482\n",
      "Step: 2500  \tValid loss: 0.41001424193382263\n",
      "Step: 2600  \tTraining loss: 0.3610771894454956\n",
      "Step: 2600  \tTraining accuracy: 0.8095694780349731\n",
      "Step: 2600  \tValid loss: 0.40961092710494995\n",
      "Step: 2700  \tTraining loss: 0.36054959893226624\n",
      "Step: 2700  \tTraining accuracy: 0.810200035572052\n",
      "Step: 2700  \tValid loss: 0.40927553176879883\n",
      "Step: 2800  \tTraining loss: 0.36005455255508423\n",
      "Step: 2800  \tTraining accuracy: 0.8107941746711731\n",
      "Step: 2800  \tValid loss: 0.408988356590271\n",
      "Step: 2900  \tTraining loss: 0.3595810830593109\n",
      "Step: 2900  \tTraining accuracy: 0.8113694787025452\n",
      "Step: 2900  \tValid loss: 0.4087374806404114\n",
      "Step: 3000  \tTraining loss: 0.35911765694618225\n",
      "Step: 3000  \tTraining accuracy: 0.8118969798088074\n",
      "Step: 3000  \tValid loss: 0.4085049629211426\n",
      "Step: 3100  \tTraining loss: 0.3586505055427551\n",
      "Step: 3100  \tTraining accuracy: 0.8123728632926941\n",
      "Step: 3100  \tValid loss: 0.4082660973072052\n",
      "Step: 3200  \tTraining loss: 0.3581658899784088\n",
      "Step: 3200  \tTraining accuracy: 0.8128143548965454\n",
      "Step: 3200  \tValid loss: 0.4080020785331726\n",
      "Step: 3300  \tTraining loss: 0.3576432466506958\n",
      "Step: 3300  \tTraining accuracy: 0.8132287263870239\n",
      "Step: 3300  \tValid loss: 0.407690167427063\n",
      "Step: 3400  \tTraining loss: 0.3570774793624878\n",
      "Step: 3400  \tTraining accuracy: 0.8136261105537415\n",
      "Step: 3400  \tValid loss: 0.40728941559791565\n",
      "Step: 3500  \tTraining loss: 0.3564802408218384\n",
      "Step: 3500  \tTraining accuracy: 0.8139740824699402\n",
      "Step: 3500  \tValid loss: 0.40681958198547363\n",
      "Step: 3600  \tTraining loss: 0.3558635115623474\n",
      "Step: 3600  \tTraining accuracy: 0.8142988085746765\n",
      "Step: 3600  \tValid loss: 0.40632444620132446\n",
      "Step: 3700  \tTraining loss: 0.3552459478378296\n",
      "Step: 3700  \tTraining accuracy: 0.8146235346794128\n",
      "Step: 3700  \tValid loss: 0.4058292806148529\n",
      "Step: 3800  \tTraining loss: 0.3546305298805237\n",
      "Step: 3800  \tTraining accuracy: 0.8149552345275879\n",
      "Step: 3800  \tValid loss: 0.40533778071403503\n",
      "Step: 3900  \tTraining loss: 0.3540198802947998\n",
      "Step: 3900  \tTraining accuracy: 0.8152562379837036\n",
      "Step: 3900  \tValid loss: 0.4048207402229309\n",
      "Step: 4000  \tTraining loss: 0.35341760516166687\n",
      "Step: 4000  \tTraining accuracy: 0.815525472164154\n",
      "Step: 4000  \tValid loss: 0.40436437726020813\n",
      "Step: 4100  \tTraining loss: 0.3528296947479248\n",
      "Step: 4100  \tTraining accuracy: 0.8157878518104553\n",
      "Step: 4100  \tValid loss: 0.4039435386657715\n",
      "Step: 4200  \tTraining loss: 0.35225218534469604\n",
      "Step: 4200  \tTraining accuracy: 0.8160407543182373\n",
      "Step: 4200  \tValid loss: 0.4035545885562897\n",
      "Step: 4300  \tTraining loss: 0.35168927907943726\n",
      "Step: 4300  \tTraining accuracy: 0.8162817358970642\n",
      "Step: 4300  \tValid loss: 0.4031633734703064\n",
      "Step: 4400  \tTraining loss: 0.35111451148986816\n",
      "Step: 4400  \tTraining accuracy: 0.816520631313324\n",
      "Step: 4400  \tValid loss: 0.4029185175895691\n",
      "Step: 4500  \tTraining loss: 0.35052451491355896\n",
      "Step: 4500  \tTraining accuracy: 0.8167546391487122\n",
      "Step: 4500  \tValid loss: 0.40272441506385803\n",
      "Step: 4600  \tTraining loss: 0.34996718168258667\n",
      "Step: 4600  \tTraining accuracy: 0.8169783353805542\n",
      "Step: 4600  \tValid loss: 0.4023961126804352\n",
      "Step: 4700  \tTraining loss: 0.3494281768798828\n",
      "Step: 4700  \tTraining accuracy: 0.8171980381011963\n",
      "Step: 4700  \tValid loss: 0.4020065367221832\n",
      "Step: 4800  \tTraining loss: 0.34890463948249817\n",
      "Step: 4800  \tTraining accuracy: 0.8174029588699341\n",
      "Step: 4800  \tValid loss: 0.40158432722091675\n",
      "Step: 4900  \tTraining loss: 0.3483940362930298\n",
      "Step: 4900  \tTraining accuracy: 0.8175994753837585\n",
      "Step: 4900  \tValid loss: 0.40116068720817566\n",
      "Step: 5000  \tTraining loss: 0.34789708256721497\n",
      "Step: 5000  \tTraining accuracy: 0.8177985548973083\n",
      "Step: 5000  \tValid loss: 0.40071427822113037\n",
      "Step: 5100  \tTraining loss: 0.34741079807281494\n",
      "Step: 5100  \tTraining accuracy: 0.8179923295974731\n",
      "Step: 5100  \tValid loss: 0.4003041088581085\n",
      "Step: 5200  \tTraining loss: 0.3468473255634308\n",
      "Step: 5200  \tTraining accuracy: 0.8181735277175903\n",
      "Step: 5200  \tValid loss: 0.3994458019733429\n",
      "Step: 5300  \tTraining loss: 0.3463311791419983\n",
      "Step: 5300  \tTraining accuracy: 0.8183577656745911\n",
      "Step: 5300  \tValid loss: 0.3991895914077759\n",
      "Step: 5400  \tTraining loss: 0.34580448269844055\n",
      "Step: 5400  \tTraining accuracy: 0.8185375332832336\n",
      "Step: 5400  \tValid loss: 0.39853718876838684\n",
      "Step: 5500  \tTraining loss: 0.34527191519737244\n",
      "Step: 5500  \tTraining accuracy: 0.8187154531478882\n",
      "Step: 5500  \tValid loss: 0.397899866104126\n",
      "Step: 5600  \tTraining loss: 0.3447655439376831\n",
      "Step: 5600  \tTraining accuracy: 0.8188682198524475\n",
      "Step: 5600  \tValid loss: 0.3973901867866516\n",
      "Step: 5700  \tTraining loss: 0.3442719578742981\n",
      "Step: 5700  \tTraining accuracy: 0.8190132975578308\n",
      "Step: 5700  \tValid loss: 0.3969210088253021\n",
      "Step: 5800  \tTraining loss: 0.34378981590270996\n",
      "Step: 5800  \tTraining accuracy: 0.8191623687744141\n",
      "Step: 5800  \tValid loss: 0.3964685797691345\n",
      "Step: 5900  \tTraining loss: 0.34331706166267395\n",
      "Step: 5900  \tTraining accuracy: 0.8193063139915466\n",
      "Step: 5900  \tValid loss: 0.39601781964302063\n",
      "Step: 6000  \tTraining loss: 0.34284523129463196\n",
      "Step: 6000  \tTraining accuracy: 0.8194454908370972\n",
      "Step: 6000  \tValid loss: 0.39554083347320557\n",
      "Step: 6100  \tTraining loss: 0.34237971901893616\n",
      "Step: 6100  \tTraining accuracy: 0.819586455821991\n",
      "Step: 6100  \tValid loss: 0.395079106092453\n",
      "Step: 6200  \tTraining loss: 0.34191790223121643\n",
      "Step: 6200  \tTraining accuracy: 0.8197080492973328\n",
      "Step: 6200  \tValid loss: 0.39462825655937195\n",
      "Step: 6300  \tTraining loss: 0.34145891666412354\n",
      "Step: 6300  \tTraining accuracy: 0.8198341131210327\n",
      "Step: 6300  \tValid loss: 0.3942022919654846\n",
      "Step: 6400  \tTraining loss: 0.34100309014320374\n",
      "Step: 6400  \tTraining accuracy: 0.8199623227119446\n",
      "Step: 6400  \tValid loss: 0.3937855064868927\n",
      "Step: 6500  \tTraining loss: 0.34055066108703613\n",
      "Step: 6500  \tTraining accuracy: 0.8200945854187012\n",
      "Step: 6500  \tValid loss: 0.39337411522865295\n",
      "Step: 6600  \tTraining loss: 0.34010136127471924\n",
      "Step: 6600  \tTraining accuracy: 0.8202268481254578\n",
      "Step: 6600  \tValid loss: 0.392995148897171\n",
      "Step: 6700  \tTraining loss: 0.33965879678726196\n",
      "Step: 6700  \tTraining accuracy: 0.8203609585762024\n",
      "Step: 6700  \tValid loss: 0.39262911677360535\n",
      "Step: 6800  \tTraining loss: 0.3392154276371002\n",
      "Step: 6800  \tTraining accuracy: 0.820483386516571\n",
      "Step: 6800  \tValid loss: 0.39227405190467834\n",
      "Step: 6900  \tTraining loss: 0.3387793004512787\n",
      "Step: 6900  \tTraining accuracy: 0.820602297782898\n",
      "Step: 6900  \tValid loss: 0.39192289113998413\n",
      "Step: 7000  \tTraining loss: 0.3383471369743347\n",
      "Step: 7000  \tTraining accuracy: 0.8207083940505981\n",
      "Step: 7000  \tValid loss: 0.3915860652923584\n",
      "Step: 7100  \tTraining loss: 0.33792781829833984\n",
      "Step: 7100  \tTraining accuracy: 0.8208059072494507\n",
      "Step: 7100  \tValid loss: 0.39126086235046387\n",
      "Step: 7200  \tTraining loss: 0.3375014364719391\n",
      "Step: 7200  \tTraining accuracy: 0.8209043741226196\n",
      "Step: 7200  \tValid loss: 0.3909205496311188\n",
      "Step: 7300  \tTraining loss: 0.3370867371559143\n",
      "Step: 7300  \tTraining accuracy: 0.8210055232048035\n",
      "Step: 7300  \tValid loss: 0.39061906933784485\n",
      "Step: 7400  \tTraining loss: 0.3366723954677582\n",
      "Step: 7400  \tTraining accuracy: 0.8211003541946411\n",
      "Step: 7400  \tValid loss: 0.39028307795524597\n",
      "Step: 7500  \tTraining loss: 0.3358460068702698\n",
      "Step: 7500  \tTraining accuracy: 0.8212048411369324\n",
      "Step: 7500  \tValid loss: 0.3895648419857025\n",
      "Step: 7600  \tTraining loss: 0.3353131413459778\n",
      "Step: 7600  \tTraining accuracy: 0.821330726146698\n",
      "Step: 7600  \tValid loss: 0.38898327946662903\n",
      "Step: 7700  \tTraining loss: 0.3347899913787842\n",
      "Step: 7700  \tTraining accuracy: 0.8214566707611084\n",
      "Step: 7700  \tValid loss: 0.3885607421398163\n",
      "Step: 7800  \tTraining loss: 0.3342800736427307\n",
      "Step: 7800  \tTraining accuracy: 0.8215659856796265\n",
      "Step: 7800  \tValid loss: 0.3881892263889313\n",
      "Step: 7900  \tTraining loss: 0.3338056802749634\n",
      "Step: 7900  \tTraining accuracy: 0.8216542601585388\n",
      "Step: 7900  \tValid loss: 0.38784894347190857\n",
      "Step: 8000  \tTraining loss: 0.33336302638053894\n",
      "Step: 8000  \tTraining accuracy: 0.8217534422874451\n",
      "Step: 8000  \tValid loss: 0.38758620619773865\n",
      "Step: 8100  \tTraining loss: 0.3329433798789978\n",
      "Step: 8100  \tTraining accuracy: 0.8218469023704529\n",
      "Step: 8100  \tValid loss: 0.38731008768081665\n",
      "Step: 8200  \tTraining loss: 0.3325384855270386\n",
      "Step: 8200  \tTraining accuracy: 0.8219316601753235\n",
      "Step: 8200  \tValid loss: 0.3870738744735718\n",
      "Step: 8300  \tTraining loss: 0.3321459889411926\n",
      "Step: 8300  \tTraining accuracy: 0.8220160007476807\n",
      "Step: 8300  \tValid loss: 0.38686853647232056\n",
      "Step: 8400  \tTraining loss: 0.3317701518535614\n",
      "Step: 8400  \tTraining accuracy: 0.8221029043197632\n",
      "Step: 8400  \tValid loss: 0.3866428732872009\n",
      "Step: 8500  \tTraining loss: 0.3314090669155121\n",
      "Step: 8500  \tTraining accuracy: 0.8221847414970398\n",
      "Step: 8500  \tValid loss: 0.38644173741340637\n",
      "Step: 8600  \tTraining loss: 0.33105483651161194\n",
      "Step: 8600  \tTraining accuracy: 0.8222707509994507\n",
      "Step: 8600  \tValid loss: 0.38622352480888367\n",
      "Step: 8700  \tTraining loss: 0.33071449398994446\n",
      "Step: 8700  \tTraining accuracy: 0.8223577737808228\n",
      "Step: 8700  \tValid loss: 0.386075884103775\n",
      "Step: 8800  \tTraining loss: 0.3303762674331665\n",
      "Step: 8800  \tTraining accuracy: 0.8224472403526306\n",
      "Step: 8800  \tValid loss: 0.3859129846096039\n",
      "Step: 8900  \tTraining loss: 0.3300477862358093\n",
      "Step: 8900  \tTraining accuracy: 0.8225435614585876\n",
      "Step: 8900  \tValid loss: 0.38574719429016113\n",
      "Step: 9000  \tTraining loss: 0.3297252357006073\n",
      "Step: 9000  \tTraining accuracy: 0.822640597820282\n",
      "Step: 9000  \tValid loss: 0.385648638010025\n",
      "Step: 9100  \tTraining loss: 0.3294094204902649\n",
      "Step: 9100  \tTraining accuracy: 0.8227340579032898\n",
      "Step: 9100  \tValid loss: 0.3855492174625397\n",
      "Step: 9200  \tTraining loss: 0.3290995657444\n",
      "Step: 9200  \tTraining accuracy: 0.8228311538696289\n",
      "Step: 9200  \tValid loss: 0.38543859124183655\n",
      "Step: 9300  \tTraining loss: 0.3287948966026306\n",
      "Step: 9300  \tTraining accuracy: 0.8229387998580933\n",
      "Step: 9300  \tValid loss: 0.3853498697280884\n",
      "Step: 9400  \tTraining loss: 0.3284958600997925\n",
      "Step: 9400  \tTraining accuracy: 0.8230441808700562\n",
      "Step: 9400  \tValid loss: 0.385263592004776\n",
      "Step: 9500  \tTraining loss: 0.3282020092010498\n",
      "Step: 9500  \tTraining accuracy: 0.8231472969055176\n",
      "Step: 9500  \tValid loss: 0.3852120041847229\n",
      "Step: 9600  \tTraining loss: 0.3279126286506653\n",
      "Step: 9600  \tTraining accuracy: 0.8232578039169312\n",
      "Step: 9600  \tValid loss: 0.3851250112056732\n",
      "Step: 9700  \tTraining loss: 0.32762810587882996\n",
      "Step: 9700  \tTraining accuracy: 0.823371410369873\n",
      "Step: 9700  \tValid loss: 0.38508498668670654\n",
      "Step: 9800  \tTraining loss: 0.32734864950180054\n",
      "Step: 9800  \tTraining accuracy: 0.8234880566596985\n",
      "Step: 9800  \tValid loss: 0.3850329518318176\n",
      "Step: 9900  \tTraining loss: 0.32707399129867554\n",
      "Step: 9900  \tTraining accuracy: 0.8236023187637329\n",
      "Step: 9900  \tValid loss: 0.3849901258945465\n",
      "Step: 10000  \tTraining loss: 0.32680296897888184\n",
      "Step: 10000  \tTraining accuracy: 0.8237181901931763\n",
      "Step: 10000  \tValid loss: 0.3849537670612335\n",
      "Step: 10100  \tTraining loss: 0.32653677463531494\n",
      "Step: 10100  \tTraining accuracy: 0.8238343596458435\n",
      "Step: 10100  \tValid loss: 0.38492149114608765\n",
      "Step: 10200  \tTraining loss: 0.3262752294540405\n",
      "Step: 10200  \tTraining accuracy: 0.8239443898200989\n",
      "Step: 10200  \tValid loss: 0.38489818572998047\n",
      "Step: 10300  \tTraining loss: 0.3260173201560974\n",
      "Step: 10300  \tTraining accuracy: 0.824049711227417\n",
      "Step: 10300  \tValid loss: 0.38488808274269104\n",
      "Step: 10400  \tTraining loss: 0.32576271891593933\n",
      "Step: 10400  \tTraining accuracy: 0.8241530060768127\n",
      "Step: 10400  \tValid loss: 0.38491082191467285\n",
      "Step: 10500  \tTraining loss: 0.32551389932632446\n",
      "Step: 10500  \tTraining accuracy: 0.8242518305778503\n",
      "Step: 10500  \tValid loss: 0.38487064838409424\n",
      "Step: 10600  \tTraining loss: 0.3252660036087036\n",
      "Step: 10600  \tTraining accuracy: 0.824342668056488\n",
      "Step: 10600  \tValid loss: 0.3848561942577362\n",
      "Step: 10700  \tTraining loss: 0.32502448558807373\n",
      "Step: 10700  \tTraining accuracy: 0.8244280815124512\n",
      "Step: 10700  \tValid loss: 0.3848455250263214\n",
      "Step: 10800  \tTraining loss: 0.32478442788124084\n",
      "Step: 10800  \tTraining accuracy: 0.8245179653167725\n",
      "Step: 10800  \tValid loss: 0.38482755422592163\n",
      "Step: 10900  \tTraining loss: 0.32454901933670044\n",
      "Step: 10900  \tTraining accuracy: 0.8246074318885803\n",
      "Step: 10900  \tValid loss: 0.38478803634643555\n",
      "Step: 11000  \tTraining loss: 0.32432112097740173\n",
      "Step: 11000  \tTraining accuracy: 0.82469642162323\n",
      "Step: 11000  \tValid loss: 0.38480326533317566\n",
      "Step: 11100  \tTraining loss: 0.3240867257118225\n",
      "Step: 11100  \tTraining accuracy: 0.8247908353805542\n",
      "Step: 11100  \tValid loss: 0.38478416204452515\n",
      "Step: 11200  \tTraining loss: 0.3238604664802551\n",
      "Step: 11200  \tTraining accuracy: 0.8248777389526367\n",
      "Step: 11200  \tValid loss: 0.3847880959510803\n",
      "Step: 11300  \tTraining loss: 0.323635458946228\n",
      "Step: 11300  \tTraining accuracy: 0.8249492645263672\n",
      "Step: 11300  \tValid loss: 0.3848150372505188\n",
      "Step: 11400  \tTraining loss: 0.3234129250049591\n",
      "Step: 11400  \tTraining accuracy: 0.8250194787979126\n",
      "Step: 11400  \tValid loss: 0.38477107882499695\n",
      "Step: 11500  \tTraining loss: 0.323192298412323\n",
      "Step: 11500  \tTraining accuracy: 0.8250873684883118\n",
      "Step: 11500  \tValid loss: 0.3847697675228119\n",
      "Step: 11600  \tTraining loss: 0.3229762017726898\n",
      "Step: 11600  \tTraining accuracy: 0.8251540660858154\n",
      "Step: 11600  \tValid loss: 0.3847775459289551\n",
      "Step: 11700  \tTraining loss: 0.3227577209472656\n",
      "Step: 11700  \tTraining accuracy: 0.8252195715904236\n",
      "Step: 11700  \tValid loss: 0.3847368359565735\n",
      "Step: 11800  \tTraining loss: 0.32254353165626526\n",
      "Step: 11800  \tTraining accuracy: 0.8252840638160706\n",
      "Step: 11800  \tValid loss: 0.3847495913505554\n",
      "Step: 11900  \tTraining loss: 0.3223291039466858\n",
      "Step: 11900  \tTraining accuracy: 0.8253440856933594\n",
      "Step: 11900  \tValid loss: 0.3847416043281555\n",
      "Step: 12000  \tTraining loss: 0.3221159279346466\n",
      "Step: 12000  \tTraining accuracy: 0.8254075050354004\n",
      "Step: 12000  \tValid loss: 0.38474419713020325\n",
      "Step: 12100  \tTraining loss: 0.32190462946891785\n",
      "Step: 12100  \tTraining accuracy: 0.8254687786102295\n",
      "Step: 12100  \tValid loss: 0.38470736145973206\n",
      "Step: 12200  \tTraining loss: 0.32170024514198303\n",
      "Step: 12200  \tTraining accuracy: 0.8255193829536438\n",
      "Step: 12200  \tValid loss: 0.38476091623306274\n",
      "Step: 12300  \tTraining loss: 0.321489542722702\n",
      "Step: 12300  \tTraining accuracy: 0.8255628347396851\n",
      "Step: 12300  \tValid loss: 0.38471606373786926\n",
      "Step: 12400  \tTraining loss: 0.3212828040122986\n",
      "Step: 12400  \tTraining accuracy: 0.8256108164787292\n",
      "Step: 12400  \tValid loss: 0.38467535376548767\n",
      "Step: 12500  \tTraining loss: 0.3210776150226593\n",
      "Step: 12500  \tTraining accuracy: 0.8256590366363525\n",
      "Step: 12500  \tValid loss: 0.3846632242202759\n",
      "Step: 12600  \tTraining loss: 0.3208731710910797\n",
      "Step: 12600  \tTraining accuracy: 0.8257055282592773\n",
      "Step: 12600  \tValid loss: 0.38459959626197815\n",
      "Step: 12700  \tTraining loss: 0.32067182660102844\n",
      "Step: 12700  \tTraining accuracy: 0.8257502317428589\n",
      "Step: 12700  \tValid loss: 0.3845897316932678\n",
      "Step: 12800  \tTraining loss: 0.32047027349472046\n",
      "Step: 12800  \tTraining accuracy: 0.8257962465286255\n",
      "Step: 12800  \tValid loss: 0.3845628798007965\n",
      "Step: 12900  \tTraining loss: 0.3202727735042572\n",
      "Step: 12900  \tTraining accuracy: 0.8258435726165771\n",
      "Step: 12900  \tValid loss: 0.38450610637664795\n",
      "Step: 13000  \tTraining loss: 0.3200761675834656\n",
      "Step: 13000  \tTraining accuracy: 0.8258882164955139\n",
      "Step: 13000  \tValid loss: 0.3844428062438965\n",
      "Step: 13100  \tTraining loss: 0.31988340616226196\n",
      "Step: 13100  \tTraining accuracy: 0.8259351253509521\n",
      "Step: 13100  \tValid loss: 0.38442733883857727\n",
      "Step: 13200  \tTraining loss: 0.3196929097175598\n",
      "Step: 13200  \tTraining accuracy: 0.8259872198104858\n",
      "Step: 13200  \tValid loss: 0.38438573479652405\n",
      "Step: 13300  \tTraining loss: 0.31950631737709045\n",
      "Step: 13300  \tTraining accuracy: 0.8260385990142822\n",
      "Step: 13300  \tValid loss: 0.38441208004951477\n",
      "Step: 13400  \tTraining loss: 0.3193223774433136\n",
      "Step: 13400  \tTraining accuracy: 0.8260920643806458\n",
      "Step: 13400  \tValid loss: 0.38436204195022583\n",
      "Step: 13500  \tTraining loss: 0.3191417157649994\n",
      "Step: 13500  \tTraining accuracy: 0.8261438012123108\n",
      "Step: 13500  \tValid loss: 0.3843514025211334\n",
      "Step: 13600  \tTraining loss: 0.31896382570266724\n",
      "Step: 13600  \tTraining accuracy: 0.8261928558349609\n",
      "Step: 13600  \tValid loss: 0.3843218982219696\n",
      "Step: 13700  \tTraining loss: 0.3187890946865082\n",
      "Step: 13700  \tTraining accuracy: 0.8262345194816589\n",
      "Step: 13700  \tValid loss: 0.38428255915641785\n",
      "Step: 13800  \tTraining loss: 0.31861644983291626\n",
      "Step: 13800  \tTraining accuracy: 0.826277494430542\n",
      "Step: 13800  \tValid loss: 0.3842792212963104\n",
      "Step: 13900  \tTraining loss: 0.31844663619995117\n",
      "Step: 13900  \tTraining accuracy: 0.8263235688209534\n",
      "Step: 13900  \tValid loss: 0.38427478075027466\n",
      "Step: 14000  \tTraining loss: 0.3182791769504547\n",
      "Step: 14000  \tTraining accuracy: 0.8263745903968811\n",
      "Step: 14000  \tValid loss: 0.38426142930984497\n",
      "Step: 14100  \tTraining loss: 0.3181140720844269\n",
      "Step: 14100  \tTraining accuracy: 0.8264248967170715\n",
      "Step: 14100  \tValid loss: 0.38424426317214966\n",
      "Step: 14200  \tTraining loss: 0.3179514706134796\n",
      "Step: 14200  \tTraining accuracy: 0.8264781832695007\n",
      "Step: 14200  \tValid loss: 0.384238600730896\n",
      "Step: 14300  \tTraining loss: 0.31779128313064575\n",
      "Step: 14300  \tTraining accuracy: 0.8265324831008911\n",
      "Step: 14300  \tValid loss: 0.384238600730896\n",
      "Step: 14400  \tTraining loss: 0.31763240694999695\n",
      "Step: 14400  \tTraining accuracy: 0.8265969753265381\n",
      "Step: 14400  \tValid loss: 0.3842187821865082\n",
      "Step: 14500  \tTraining loss: 0.3174760341644287\n",
      "Step: 14500  \tTraining accuracy: 0.8266578316688538\n",
      "Step: 14500  \tValid loss: 0.38421767950057983\n",
      "Step: 14600  \tTraining loss: 0.3173218071460724\n",
      "Step: 14600  \tTraining accuracy: 0.8267205357551575\n",
      "Step: 14600  \tValid loss: 0.384210467338562\n",
      "Step: 14700  \tTraining loss: 0.3171694278717041\n",
      "Step: 14700  \tTraining accuracy: 0.8267797827720642\n",
      "Step: 14700  \tValid loss: 0.38420772552490234\n",
      "Step: 14800  \tTraining loss: 0.3170188367366791\n",
      "Step: 14800  \tTraining accuracy: 0.8268355131149292\n",
      "Step: 14800  \tValid loss: 0.3841991424560547\n",
      "Step: 14900  \tTraining loss: 0.31687068939208984\n",
      "Step: 14900  \tTraining accuracy: 0.8268870115280151\n",
      "Step: 14900  \tValid loss: 0.38419201970100403\n",
      "Step: 15000  \tTraining loss: 0.3167246878147125\n",
      "Step: 15000  \tTraining accuracy: 0.8269351720809937\n",
      "Step: 15000  \tValid loss: 0.3842066526412964\n",
      "Step: 15100  \tTraining loss: 0.31658005714416504\n",
      "Step: 15100  \tTraining accuracy: 0.8269827365875244\n",
      "Step: 15100  \tValid loss: 0.3842235207557678\n",
      "Step: 15200  \tTraining loss: 0.31643736362457275\n",
      "Step: 15200  \tTraining accuracy: 0.8270296454429626\n",
      "Step: 15200  \tValid loss: 0.3842374384403229\n",
      "Step: 15300  \tTraining loss: 0.3162955343723297\n",
      "Step: 15300  \tTraining accuracy: 0.8270759582519531\n",
      "Step: 15300  \tValid loss: 0.38422295451164246\n",
      "Step: 15400  \tTraining loss: 0.31615644693374634\n",
      "Step: 15400  \tTraining accuracy: 0.8271191716194153\n",
      "Step: 15400  \tValid loss: 0.3842449188232422\n",
      "Step: 15500  \tTraining loss: 0.31601688265800476\n",
      "Step: 15500  \tTraining accuracy: 0.8271600604057312\n",
      "Step: 15500  \tValid loss: 0.38421785831451416\n",
      "Step: 15600  \tTraining loss: 0.3158790171146393\n",
      "Step: 15600  \tTraining accuracy: 0.8271996378898621\n",
      "Step: 15600  \tValid loss: 0.3842134475708008\n",
      "Step: 15700  \tTraining loss: 0.31574299931526184\n",
      "Step: 15700  \tTraining accuracy: 0.8272353410720825\n",
      "Step: 15700  \tValid loss: 0.3842141926288605\n",
      "Step: 15800  \tTraining loss: 0.3156079053878784\n",
      "Step: 15800  \tTraining accuracy: 0.8272706270217896\n",
      "Step: 15800  \tValid loss: 0.3842165470123291\n",
      "Step: 15900  \tTraining loss: 0.3154737055301666\n",
      "Step: 15900  \tTraining accuracy: 0.8273054361343384\n",
      "Step: 15900  \tValid loss: 0.3841668963432312\n",
      "Step: 16000  \tTraining loss: 0.31534022092819214\n",
      "Step: 16000  \tTraining accuracy: 0.8273398280143738\n",
      "Step: 16000  \tValid loss: 0.38417595624923706\n",
      "Step: 16100  \tTraining loss: 0.3152095377445221\n",
      "Step: 16100  \tTraining accuracy: 0.8273802995681763\n",
      "Step: 16100  \tValid loss: 0.38417938351631165\n",
      "Step: 16200  \tTraining loss: 0.3150777220726013\n",
      "Step: 16200  \tTraining accuracy: 0.8274202346801758\n",
      "Step: 16200  \tValid loss: 0.38415244221687317\n",
      "Step: 16300  \tTraining loss: 0.3149467408657074\n",
      "Step: 16300  \tTraining accuracy: 0.8274669051170349\n",
      "Step: 16300  \tValid loss: 0.38412144780158997\n",
      "Step: 16400  \tTraining loss: 0.31481799483299255\n",
      "Step: 16400  \tTraining accuracy: 0.8275193572044373\n",
      "Step: 16400  \tValid loss: 0.3841139078140259\n",
      "Step: 16500  \tTraining loss: 0.31469014286994934\n",
      "Step: 16500  \tTraining accuracy: 0.8275711536407471\n",
      "Step: 16500  \tValid loss: 0.3840875029563904\n",
      "Step: 16600  \tTraining loss: 0.31456276774406433\n",
      "Step: 16600  \tTraining accuracy: 0.8276223540306091\n",
      "Step: 16600  \tValid loss: 0.3840516209602356\n",
      "Step: 16700  \tTraining loss: 0.31443652510643005\n",
      "Step: 16700  \tTraining accuracy: 0.8276729583740234\n",
      "Step: 16700  \tValid loss: 0.3840350806713104\n",
      "Step: 16800  \tTraining loss: 0.3143097460269928\n",
      "Step: 16800  \tTraining accuracy: 0.8277252316474915\n",
      "Step: 16800  \tValid loss: 0.38399866223335266\n",
      "Step: 16900  \tTraining loss: 0.3141838312149048\n",
      "Step: 16900  \tTraining accuracy: 0.8277815580368042\n",
      "Step: 16900  \tValid loss: 0.3839796781539917\n",
      "Step: 17000  \tTraining loss: 0.3140594959259033\n",
      "Step: 17000  \tTraining accuracy: 0.8278380036354065\n",
      "Step: 17000  \tValid loss: 0.3839488625526428\n",
      "Step: 17100  \tTraining loss: 0.31393635272979736\n",
      "Step: 17100  \tTraining accuracy: 0.8278937339782715\n",
      "Step: 17100  \tValid loss: 0.3838872015476227\n",
      "Step: 17200  \tTraining loss: 0.3138129413127899\n",
      "Step: 17200  \tTraining accuracy: 0.8279488682746887\n",
      "Step: 17200  \tValid loss: 0.3838455080986023\n",
      "Step: 17300  \tTraining loss: 0.3136901557445526\n",
      "Step: 17300  \tTraining accuracy: 0.8280063271522522\n",
      "Step: 17300  \tValid loss: 0.38382843136787415\n",
      "Step: 17400  \tTraining loss: 0.31356820464134216\n",
      "Step: 17400  \tTraining accuracy: 0.8280661702156067\n",
      "Step: 17400  \tValid loss: 0.38378220796585083\n",
      "Step: 17500  \tTraining loss: 0.31344664096832275\n",
      "Step: 17500  \tTraining accuracy: 0.8281267881393433\n",
      "Step: 17500  \tValid loss: 0.38370317220687866\n",
      "Step: 17600  \tTraining loss: 0.3133258819580078\n",
      "Step: 17600  \tTraining accuracy: 0.8281904458999634\n",
      "Step: 17600  \tValid loss: 0.3836614489555359\n",
      "Step: 17700  \tTraining loss: 0.3132052719593048\n",
      "Step: 17700  \tTraining accuracy: 0.8282533884048462\n",
      "Step: 17700  \tValid loss: 0.38360339403152466\n",
      "Step: 17800  \tTraining loss: 0.31308531761169434\n",
      "Step: 17800  \tTraining accuracy: 0.8283156156539917\n",
      "Step: 17800  \tValid loss: 0.38355734944343567\n",
      "Step: 17900  \tTraining loss: 0.3129662573337555\n",
      "Step: 17900  \tTraining accuracy: 0.8283742070198059\n",
      "Step: 17900  \tValid loss: 0.3834924101829529\n",
      "Step: 18000  \tTraining loss: 0.3128467798233032\n",
      "Step: 18000  \tTraining accuracy: 0.8284270763397217\n",
      "Step: 18000  \tValid loss: 0.3834354877471924\n",
      "Step: 18100  \tTraining loss: 0.3127293586730957\n",
      "Step: 18100  \tTraining accuracy: 0.8284743428230286\n",
      "Step: 18100  \tValid loss: 0.383360892534256\n",
      "Step: 18200  \tTraining loss: 0.31261077523231506\n",
      "Step: 18200  \tTraining accuracy: 0.8285195827484131\n",
      "Step: 18200  \tValid loss: 0.38331764936447144\n",
      "Step: 18300  \tTraining loss: 0.3124934434890747\n",
      "Step: 18300  \tTraining accuracy: 0.8285644054412842\n",
      "Step: 18300  \tValid loss: 0.3832709789276123\n",
      "Step: 18400  \tTraining loss: 0.31237661838531494\n",
      "Step: 18400  \tTraining accuracy: 0.8286086916923523\n",
      "Step: 18400  \tValid loss: 0.383199542760849\n",
      "Step: 18500  \tTraining loss: 0.31226032972335815\n",
      "Step: 18500  \tTraining accuracy: 0.8286504149436951\n",
      "Step: 18500  \tValid loss: 0.38315537571907043\n",
      "Step: 18600  \tTraining loss: 0.31214359402656555\n",
      "Step: 18600  \tTraining accuracy: 0.8286916613578796\n",
      "Step: 18600  \tValid loss: 0.3830626606941223\n",
      "Step: 18700  \tTraining loss: 0.3120284676551819\n",
      "Step: 18700  \tTraining accuracy: 0.8287345767021179\n",
      "Step: 18700  \tValid loss: 0.3829856812953949\n",
      "Step: 18800  \tTraining loss: 0.31191372871398926\n",
      "Step: 18800  \tTraining accuracy: 0.828777015209198\n",
      "Step: 18800  \tValid loss: 0.38293352723121643\n",
      "Step: 18900  \tTraining loss: 0.311799019575119\n",
      "Step: 18900  \tTraining accuracy: 0.8288190364837646\n",
      "Step: 18900  \tValid loss: 0.3828642666339874\n",
      "Step: 19000  \tTraining loss: 0.3116563856601715\n",
      "Step: 19000  \tTraining accuracy: 0.8288633227348328\n",
      "Step: 19000  \tValid loss: 0.38267239928245544\n",
      "Step: 19100  \tTraining loss: 0.3115307688713074\n",
      "Step: 19100  \tTraining accuracy: 0.8289058208465576\n",
      "Step: 19100  \tValid loss: 0.382488489151001\n",
      "Step: 19200  \tTraining loss: 0.31141045689582825\n",
      "Step: 19200  \tTraining accuracy: 0.8289478421211243\n",
      "Step: 19200  \tValid loss: 0.3823416531085968\n",
      "Step: 19300  \tTraining loss: 0.3112891912460327\n",
      "Step: 19300  \tTraining accuracy: 0.828988790512085\n",
      "Step: 19300  \tValid loss: 0.3821786642074585\n",
      "Step: 19400  \tTraining loss: 0.31116557121276855\n",
      "Step: 19400  \tTraining accuracy: 0.8290286064147949\n",
      "Step: 19400  \tValid loss: 0.38208943605422974\n",
      "Step: 19500  \tTraining loss: 0.31104031205177307\n",
      "Step: 19500  \tTraining accuracy: 0.8290680050849915\n",
      "Step: 19500  \tValid loss: 0.38193416595458984\n",
      "Step: 19600  \tTraining loss: 0.31091856956481934\n",
      "Step: 19600  \tTraining accuracy: 0.829105019569397\n",
      "Step: 19600  \tValid loss: 0.3817775547504425\n",
      "Step: 19700  \tTraining loss: 0.3107972741127014\n",
      "Step: 19700  \tTraining accuracy: 0.829140305519104\n",
      "Step: 19700  \tValid loss: 0.3816491365432739\n",
      "Step: 19800  \tTraining loss: 0.3106769323348999\n",
      "Step: 19800  \tTraining accuracy: 0.8291752934455872\n",
      "Step: 19800  \tValid loss: 0.38152411580085754\n",
      "Step: 19900  \tTraining loss: 0.31055688858032227\n",
      "Step: 19900  \tTraining accuracy: 0.8292098641395569\n",
      "Step: 19900  \tValid loss: 0.3814103901386261\n",
      "Step: 20000  \tTraining loss: 0.3104383647441864\n",
      "Step: 20000  \tTraining accuracy: 0.8292441368103027\n",
      "Step: 20000  \tValid loss: 0.3812812268733978\n",
      "Step: 20100  \tTraining loss: 0.31031736731529236\n",
      "Step: 20100  \tTraining accuracy: 0.8292780518531799\n",
      "Step: 20100  \tValid loss: 0.3811437487602234\n",
      "Step: 20200  \tTraining loss: 0.3102012574672699\n",
      "Step: 20200  \tTraining accuracy: 0.8293135762214661\n",
      "Step: 20200  \tValid loss: 0.38099536299705505\n",
      "Step: 20300  \tTraining loss: 0.3100847005844116\n",
      "Step: 20300  \tTraining accuracy: 0.8293532133102417\n",
      "Step: 20300  \tValid loss: 0.38093140721321106\n",
      "Step: 20400  \tTraining loss: 0.30996638536453247\n",
      "Step: 20400  \tTraining accuracy: 0.8293924927711487\n",
      "Step: 20400  \tValid loss: 0.3808087408542633\n",
      "Step: 20500  \tTraining loss: 0.30985015630722046\n",
      "Step: 20500  \tTraining accuracy: 0.829431414604187\n",
      "Step: 20500  \tValid loss: 0.38068804144859314\n",
      "Step: 20600  \tTraining loss: 0.3097369372844696\n",
      "Step: 20600  \tTraining accuracy: 0.8294699192047119\n",
      "Step: 20600  \tValid loss: 0.38061442971229553\n",
      "Step: 20700  \tTraining loss: 0.30962181091308594\n",
      "Step: 20700  \tTraining accuracy: 0.8295080661773682\n",
      "Step: 20700  \tValid loss: 0.3804817199707031\n",
      "Step: 20800  \tTraining loss: 0.3095073103904724\n",
      "Step: 20800  \tTraining accuracy: 0.829545795917511\n",
      "Step: 20800  \tValid loss: 0.38039684295654297\n",
      "Step: 20900  \tTraining loss: 0.3093911409378052\n",
      "Step: 20900  \tTraining accuracy: 0.8295807242393494\n",
      "Step: 20900  \tValid loss: 0.38029512763023376\n",
      "Step: 21000  \tTraining loss: 0.30927881598472595\n",
      "Step: 21000  \tTraining accuracy: 0.8296146988868713\n",
      "Step: 21000  \tValid loss: 0.3801783621311188\n",
      "Step: 21100  \tTraining loss: 0.3091646134853363\n",
      "Step: 21100  \tTraining accuracy: 0.8296501636505127\n",
      "Step: 21100  \tValid loss: 0.3801191449165344\n",
      "Step: 21200  \tTraining loss: 0.3090527355670929\n",
      "Step: 21200  \tTraining accuracy: 0.8296890258789062\n",
      "Step: 21200  \tValid loss: 0.38003331422805786\n",
      "Step: 21300  \tTraining loss: 0.3089420199394226\n",
      "Step: 21300  \tTraining accuracy: 0.8297299146652222\n",
      "Step: 21300  \tValid loss: 0.3799712657928467\n",
      "Step: 21400  \tTraining loss: 0.30883046984672546\n",
      "Step: 21400  \tTraining accuracy: 0.8297705054283142\n",
      "Step: 21400  \tValid loss: 0.37986791133880615\n",
      "Step: 21500  \tTraining loss: 0.30871865153312683\n",
      "Step: 21500  \tTraining accuracy: 0.8298088312149048\n",
      "Step: 21500  \tValid loss: 0.3797850012779236\n",
      "Step: 21600  \tTraining loss: 0.30861032009124756\n",
      "Step: 21600  \tTraining accuracy: 0.8298467993736267\n",
      "Step: 21600  \tValid loss: 0.37973299622535706\n",
      "Step: 21700  \tTraining loss: 0.3084988296031952\n",
      "Step: 21700  \tTraining accuracy: 0.8298844695091248\n",
      "Step: 21700  \tValid loss: 0.37962767481803894\n",
      "Step: 21800  \tTraining loss: 0.3083893954753876\n",
      "Step: 21800  \tTraining accuracy: 0.8299241662025452\n",
      "Step: 21800  \tValid loss: 0.37953218817710876\n",
      "Step: 21900  \tTraining loss: 0.3082781732082367\n",
      "Step: 21900  \tTraining accuracy: 0.8299658298492432\n",
      "Step: 21900  \tValid loss: 0.37947094440460205\n",
      "Step: 22000  \tTraining loss: 0.30816781520843506\n",
      "Step: 22000  \tTraining accuracy: 0.8300083875656128\n",
      "Step: 22000  \tValid loss: 0.3793739378452301\n",
      "Step: 22100  \tTraining loss: 0.3080584704875946\n",
      "Step: 22100  \tTraining accuracy: 0.8300504684448242\n",
      "Step: 22100  \tValid loss: 0.3792853057384491\n",
      "Step: 22200  \tTraining loss: 0.30795037746429443\n",
      "Step: 22200  \tTraining accuracy: 0.8300922513008118\n",
      "Step: 22200  \tValid loss: 0.3792110085487366\n",
      "Step: 22300  \tTraining loss: 0.30784109234809875\n",
      "Step: 22300  \tTraining accuracy: 0.8301336169242859\n",
      "Step: 22300  \tValid loss: 0.37916818261146545\n",
      "Step: 22400  \tTraining loss: 0.3077335059642792\n",
      "Step: 22400  \tTraining accuracy: 0.8301734328269958\n",
      "Step: 22400  \tValid loss: 0.379083514213562\n",
      "Step: 22500  \tTraining loss: 0.30762946605682373\n",
      "Step: 22500  \tTraining accuracy: 0.8302111625671387\n",
      "Step: 22500  \tValid loss: 0.3789995610713959\n",
      "Step: 22600  \tTraining loss: 0.3075222671031952\n",
      "Step: 22600  \tTraining accuracy: 0.8302485942840576\n",
      "Step: 22600  \tValid loss: 0.3789447844028473\n",
      "Step: 22700  \tTraining loss: 0.3074157238006592\n",
      "Step: 22700  \tTraining accuracy: 0.8302839398384094\n",
      "Step: 22700  \tValid loss: 0.37889719009399414\n",
      "Step: 22800  \tTraining loss: 0.3073098957538605\n",
      "Step: 22800  \tTraining accuracy: 0.8303178548812866\n",
      "Step: 22800  \tValid loss: 0.3788453936576843\n",
      "Step: 22900  \tTraining loss: 0.3072008788585663\n",
      "Step: 22900  \tTraining accuracy: 0.830348014831543\n",
      "Step: 22900  \tValid loss: 0.37879058718681335\n",
      "Step: 23000  \tTraining loss: 0.3070976138114929\n",
      "Step: 23000  \tTraining accuracy: 0.8303756713867188\n",
      "Step: 23000  \tValid loss: 0.37870705127716064\n",
      "Step: 23100  \tTraining loss: 0.30699044466018677\n",
      "Step: 23100  \tTraining accuracy: 0.8304014205932617\n",
      "Step: 23100  \tValid loss: 0.3786400854587555\n",
      "Step: 23200  \tTraining loss: 0.30688372254371643\n",
      "Step: 23200  \tTraining accuracy: 0.8304257392883301\n",
      "Step: 23200  \tValid loss: 0.37856388092041016\n",
      "Step: 23300  \tTraining loss: 0.3067843019962311\n",
      "Step: 23300  \tTraining accuracy: 0.8304516077041626\n",
      "Step: 23300  \tValid loss: 0.378531277179718\n",
      "Step: 23400  \tTraining loss: 0.30667608976364136\n",
      "Step: 23400  \tTraining accuracy: 0.8304800391197205\n",
      "Step: 23400  \tValid loss: 0.3784063458442688\n",
      "Step: 23500  \tTraining loss: 0.3065708875656128\n",
      "Step: 23500  \tTraining accuracy: 0.8305093050003052\n",
      "Step: 23500  \tValid loss: 0.37835168838500977\n",
      "Step: 23600  \tTraining loss: 0.3064633309841156\n",
      "Step: 23600  \tTraining accuracy: 0.8305383324623108\n",
      "Step: 23600  \tValid loss: 0.37828925251960754\n",
      "Step: 23700  \tTraining loss: 0.30636289715766907\n",
      "Step: 23700  \tTraining accuracy: 0.8305671215057373\n",
      "Step: 23700  \tValid loss: 0.3781908452510834\n",
      "Step: 23800  \tTraining loss: 0.3062556982040405\n",
      "Step: 23800  \tTraining accuracy: 0.830593466758728\n",
      "Step: 23800  \tValid loss: 0.3781663477420807\n",
      "Step: 23900  \tTraining loss: 0.30615076422691345\n",
      "Step: 23900  \tTraining accuracy: 0.8306190371513367\n",
      "Step: 23900  \tValid loss: 0.37809091806411743\n",
      "Step: 24000  \tTraining loss: 0.3060474097728729\n",
      "Step: 24000  \tTraining accuracy: 0.830644428730011\n",
      "Step: 24000  \tValid loss: 0.3780059814453125\n",
      "Step: 24100  \tTraining loss: 0.3059437572956085\n",
      "Step: 24100  \tTraining accuracy: 0.8306711912155151\n",
      "Step: 24100  \tValid loss: 0.37796157598495483\n",
      "Step: 24200  \tTraining loss: 0.30583661794662476\n",
      "Step: 24200  \tTraining accuracy: 0.8306987881660461\n",
      "Step: 24200  \tValid loss: 0.37789130210876465\n",
      "Step: 24300  \tTraining loss: 0.30573299527168274\n",
      "Step: 24300  \tTraining accuracy: 0.8307262063026428\n",
      "Step: 24300  \tValid loss: 0.377817302942276\n",
      "Step: 24400  \tTraining loss: 0.3056296706199646\n",
      "Step: 24400  \tTraining accuracy: 0.8307517766952515\n",
      "Step: 24400  \tValid loss: 0.37775611877441406\n",
      "Step: 24500  \tTraining loss: 0.30552610754966736\n",
      "Step: 24500  \tTraining accuracy: 0.8307760953903198\n",
      "Step: 24500  \tValid loss: 0.3776688873767853\n",
      "Step: 24600  \tTraining loss: 0.30542394518852234\n",
      "Step: 24600  \tTraining accuracy: 0.8308002352714539\n",
      "Step: 24600  \tValid loss: 0.37760859727859497\n",
      "Step: 24700  \tTraining loss: 0.30532053112983704\n",
      "Step: 24700  \tTraining accuracy: 0.8308267593383789\n",
      "Step: 24700  \tValid loss: 0.37751635909080505\n",
      "Step: 24800  \tTraining loss: 0.3052213788032532\n",
      "Step: 24800  \tTraining accuracy: 0.8308557271957397\n",
      "Step: 24800  \tValid loss: 0.37750086188316345\n",
      "Step: 24900  \tTraining loss: 0.30511438846588135\n",
      "Step: 24900  \tTraining accuracy: 0.8308844566345215\n",
      "Step: 24900  \tValid loss: 0.37736940383911133\n",
      "Step: 25000  \tTraining loss: 0.30501309037208557\n",
      "Step: 25000  \tTraining accuracy: 0.8309129476547241\n",
      "Step: 25000  \tValid loss: 0.3772881031036377\n",
      "Step: 25100  \tTraining loss: 0.3049089014530182\n",
      "Step: 25100  \tTraining accuracy: 0.8309412598609924\n",
      "Step: 25100  \tValid loss: 0.377236008644104\n",
      "Step: 25200  \tTraining loss: 0.3048076331615448\n",
      "Step: 25200  \tTraining accuracy: 0.8309692740440369\n",
      "Step: 25200  \tValid loss: 0.3771587610244751\n",
      "Step: 25300  \tTraining loss: 0.3047054409980774\n",
      "Step: 25300  \tTraining accuracy: 0.830997109413147\n",
      "Step: 25300  \tValid loss: 0.3770703673362732\n",
      "Step: 25400  \tTraining loss: 0.30460456013679504\n",
      "Step: 25400  \tTraining accuracy: 0.831024706363678\n",
      "Step: 25400  \tValid loss: 0.37699878215789795\n",
      "Step: 25500  \tTraining loss: 0.30450350046157837\n",
      "Step: 25500  \tTraining accuracy: 0.8310521245002747\n",
      "Step: 25500  \tValid loss: 0.376898854970932\n",
      "Step: 25600  \tTraining loss: 0.3044019341468811\n",
      "Step: 25600  \tTraining accuracy: 0.8310793042182922\n",
      "Step: 25600  \tValid loss: 0.37681785225868225\n",
      "Step: 25700  \tTraining loss: 0.3043017089366913\n",
      "Step: 25700  \tTraining accuracy: 0.8311092853546143\n",
      "Step: 25700  \tValid loss: 0.37673264741897583\n",
      "Step: 25800  \tTraining loss: 0.3042023181915283\n",
      "Step: 25800  \tTraining accuracy: 0.8311411142349243\n",
      "Step: 25800  \tValid loss: 0.3766607940196991\n",
      "Step: 25900  \tTraining loss: 0.3041013777256012\n",
      "Step: 25900  \tTraining accuracy: 0.8311726450920105\n",
      "Step: 25900  \tValid loss: 0.3765772879123688\n",
      "Step: 26000  \tTraining loss: 0.30400219559669495\n",
      "Step: 26000  \tTraining accuracy: 0.8312039375305176\n",
      "Step: 26000  \tValid loss: 0.3764839172363281\n",
      "Step: 26100  \tTraining loss: 0.3039035499095917\n",
      "Step: 26100  \tTraining accuracy: 0.8312350511550903\n",
      "Step: 26100  \tValid loss: 0.37640005350112915\n",
      "Step: 26200  \tTraining loss: 0.3038051128387451\n",
      "Step: 26200  \tTraining accuracy: 0.8312658667564392\n",
      "Step: 26200  \tValid loss: 0.3762855529785156\n",
      "Step: 26300  \tTraining loss: 0.30370768904685974\n",
      "Step: 26300  \tTraining accuracy: 0.831296443939209\n",
      "Step: 26300  \tValid loss: 0.3762156665325165\n",
      "Step: 26400  \tTraining loss: 0.3036099672317505\n",
      "Step: 26400  \tTraining accuracy: 0.8313268423080444\n",
      "Step: 26400  \tValid loss: 0.3761250078678131\n",
      "Step: 26500  \tTraining loss: 0.3035133481025696\n",
      "Step: 26500  \tTraining accuracy: 0.8313579559326172\n",
      "Step: 26500  \tValid loss: 0.37604326009750366\n",
      "Step: 26600  \tTraining loss: 0.3034142255783081\n",
      "Step: 26600  \tTraining accuracy: 0.8313902616500854\n",
      "Step: 26600  \tValid loss: 0.3759390115737915\n",
      "Step: 26700  \tTraining loss: 0.3033178448677063\n",
      "Step: 26700  \tTraining accuracy: 0.8314223885536194\n",
      "Step: 26700  \tValid loss: 0.3758411705493927\n",
      "Step: 26800  \tTraining loss: 0.30322161316871643\n",
      "Step: 26800  \tTraining accuracy: 0.8314542770385742\n",
      "Step: 26800  \tValid loss: 0.3757565915584564\n",
      "Step: 26900  \tTraining loss: 0.3031262159347534\n",
      "Step: 26900  \tTraining accuracy: 0.8314839601516724\n",
      "Step: 26900  \tValid loss: 0.37565141916275024\n",
      "Step: 27000  \tTraining loss: 0.3030308187007904\n",
      "Step: 27000  \tTraining accuracy: 0.8315129280090332\n",
      "Step: 27000  \tValid loss: 0.3755568265914917\n",
      "Step: 27100  \tTraining loss: 0.30293506383895874\n",
      "Step: 27100  \tTraining accuracy: 0.8315417170524597\n",
      "Step: 27100  \tValid loss: 0.3754580616950989\n",
      "Step: 27200  \tTraining loss: 0.3028407394886017\n",
      "Step: 27200  \tTraining accuracy: 0.8315708041191101\n",
      "Step: 27200  \tValid loss: 0.3753482401371002\n",
      "Step: 27300  \tTraining loss: 0.3027458190917969\n",
      "Step: 27300  \tTraining accuracy: 0.8316024541854858\n",
      "Step: 27300  \tValid loss: 0.3752731680870056\n",
      "Step: 27400  \tTraining loss: 0.3026546835899353\n",
      "Step: 27400  \tTraining accuracy: 0.8316386938095093\n",
      "Step: 27400  \tValid loss: 0.3751829266548157\n",
      "Step: 27500  \tTraining loss: 0.30255982279777527\n",
      "Step: 27500  \tTraining accuracy: 0.8316760659217834\n",
      "Step: 27500  \tValid loss: 0.3750765919685364\n",
      "Step: 27600  \tTraining loss: 0.3024698495864868\n",
      "Step: 27600  \tTraining accuracy: 0.8317131996154785\n",
      "Step: 27600  \tValid loss: 0.3749947249889374\n",
      "Step: 27700  \tTraining loss: 0.3023742735385895\n",
      "Step: 27700  \tTraining accuracy: 0.8317500352859497\n",
      "Step: 27700  \tValid loss: 0.3748782277107239\n",
      "Step: 27800  \tTraining loss: 0.30228373408317566\n",
      "Step: 27800  \tTraining accuracy: 0.8317894339561462\n",
      "Step: 27800  \tValid loss: 0.3747822642326355\n",
      "Step: 27900  \tTraining loss: 0.3021913766860962\n",
      "Step: 27900  \tTraining accuracy: 0.8318294286727905\n",
      "Step: 27900  \tValid loss: 0.374648779630661\n",
      "Step: 28000  \tTraining loss: 0.30209922790527344\n",
      "Step: 28000  \tTraining accuracy: 0.8318668603897095\n",
      "Step: 28000  \tValid loss: 0.3745828866958618\n",
      "Step: 28100  \tTraining loss: 0.3020084798336029\n",
      "Step: 28100  \tTraining accuracy: 0.8319044709205627\n",
      "Step: 28100  \tValid loss: 0.3744669556617737\n",
      "Step: 28200  \tTraining loss: 0.3019184470176697\n",
      "Step: 28200  \tTraining accuracy: 0.8319423198699951\n",
      "Step: 28200  \tValid loss: 0.3743635416030884\n",
      "Step: 28300  \tTraining loss: 0.30182820558547974\n",
      "Step: 28300  \tTraining accuracy: 0.8319798707962036\n",
      "Step: 28300  \tValid loss: 0.3742745518684387\n",
      "Step: 28400  \tTraining loss: 0.3017388880252838\n",
      "Step: 28400  \tTraining accuracy: 0.8320184946060181\n",
      "Step: 28400  \tValid loss: 0.3741642236709595\n",
      "Step: 28500  \tTraining loss: 0.3016510605812073\n",
      "Step: 28500  \tTraining accuracy: 0.8320578336715698\n",
      "Step: 28500  \tValid loss: 0.37406206130981445\n",
      "Step: 28600  \tTraining loss: 0.3015621602535248\n",
      "Step: 28600  \tTraining accuracy: 0.8320977687835693\n",
      "Step: 28600  \tValid loss: 0.3739517331123352\n",
      "Step: 28700  \tTraining loss: 0.3014737069606781\n",
      "Step: 28700  \tTraining accuracy: 0.8321342468261719\n",
      "Step: 28700  \tValid loss: 0.3738445043563843\n",
      "Step: 28800  \tTraining loss: 0.3013858497142792\n",
      "Step: 28800  \tTraining accuracy: 0.8321704864501953\n",
      "Step: 28800  \tValid loss: 0.37374454736709595\n",
      "Step: 28900  \tTraining loss: 0.30129778385162354\n",
      "Step: 28900  \tTraining accuracy: 0.8322064876556396\n",
      "Step: 28900  \tValid loss: 0.37364742159843445\n",
      "Step: 29000  \tTraining loss: 0.3012107312679291\n",
      "Step: 29000  \tTraining accuracy: 0.8322439789772034\n",
      "Step: 29000  \tValid loss: 0.37353116273880005\n",
      "Step: 29100  \tTraining loss: 0.301123708486557\n",
      "Step: 29100  \tTraining accuracy: 0.8322834968566895\n",
      "Step: 29100  \tValid loss: 0.3734463155269623\n",
      "Step: 29200  \tTraining loss: 0.3010382354259491\n",
      "Step: 29200  \tTraining accuracy: 0.8323231935501099\n",
      "Step: 29200  \tValid loss: 0.3733704388141632\n",
      "Step: 29300  \tTraining loss: 0.3009502589702606\n",
      "Step: 29300  \tTraining accuracy: 0.8323647975921631\n",
      "Step: 29300  \tValid loss: 0.3732529580593109\n",
      "Step: 29400  \tTraining loss: 0.3008643090724945\n",
      "Step: 29400  \tTraining accuracy: 0.8324061632156372\n",
      "Step: 29400  \tValid loss: 0.3731549084186554\n",
      "Step: 29500  \tTraining loss: 0.3007785975933075\n",
      "Step: 29500  \tTraining accuracy: 0.8324472308158875\n",
      "Step: 29500  \tValid loss: 0.37304845452308655\n",
      "Step: 29600  \tTraining loss: 0.3006943464279175\n",
      "Step: 29600  \tTraining accuracy: 0.8324880599975586\n",
      "Step: 29600  \tValid loss: 0.3729613125324249\n",
      "Step: 29700  \tTraining loss: 0.30060797929763794\n",
      "Step: 29700  \tTraining accuracy: 0.8325285315513611\n",
      "Step: 29700  \tValid loss: 0.3728654384613037\n",
      "Step: 29800  \tTraining loss: 0.3005220592021942\n",
      "Step: 29800  \tTraining accuracy: 0.8325696587562561\n",
      "Step: 29800  \tValid loss: 0.37276434898376465\n",
      "Step: 29900  \tTraining loss: 0.30043932795524597\n",
      "Step: 29900  \tTraining accuracy: 0.8326117992401123\n",
      "Step: 29900  \tValid loss: 0.37267979979515076\n",
      "Step: 30000  \tTraining loss: 0.3003532290458679\n",
      "Step: 30000  \tTraining accuracy: 0.8326537013053894\n",
      "Step: 30000  \tValid loss: 0.37254956364631653\n",
      "Step: 30100  \tTraining loss: 0.3002655804157257\n",
      "Step: 30100  \tTraining accuracy: 0.8326953053474426\n",
      "Step: 30100  \tValid loss: 0.3724512457847595\n",
      "Step: 30200  \tTraining loss: 0.3001776933670044\n",
      "Step: 30200  \tTraining accuracy: 0.832736611366272\n",
      "Step: 30200  \tValid loss: 0.3723645508289337\n",
      "Step: 30300  \tTraining loss: 0.2996690273284912\n",
      "Step: 30300  \tTraining accuracy: 0.832780659198761\n",
      "Step: 30300  \tValid loss: 0.3720107972621918\n",
      "Step: 30400  \tTraining loss: 0.2994508147239685\n",
      "Step: 30400  \tTraining accuracy: 0.8328312635421753\n",
      "Step: 30400  \tValid loss: 0.3721911311149597\n",
      "Step: 30500  \tTraining loss: 0.29929572343826294\n",
      "Step: 30500  \tTraining accuracy: 0.8328807353973389\n",
      "Step: 30500  \tValid loss: 0.3722643256187439\n",
      "Step: 30600  \tTraining loss: 0.29915884137153625\n",
      "Step: 30600  \tTraining accuracy: 0.8329285383224487\n",
      "Step: 30600  \tValid loss: 0.3722105026245117\n",
      "Step: 30700  \tTraining loss: 0.29903504252433777\n",
      "Step: 30700  \tTraining accuracy: 0.8329769372940063\n",
      "Step: 30700  \tValid loss: 0.37214043736457825\n",
      "Step: 30800  \tTraining loss: 0.29891836643218994\n",
      "Step: 30800  \tTraining accuracy: 0.8330245614051819\n",
      "Step: 30800  \tValid loss: 0.37208542227745056\n",
      "Step: 30900  \tTraining loss: 0.2988058030605316\n",
      "Step: 30900  \tTraining accuracy: 0.8330702185630798\n",
      "Step: 30900  \tValid loss: 0.37198787927627563\n",
      "Step: 31000  \tTraining loss: 0.2986984848976135\n",
      "Step: 31000  \tTraining accuracy: 0.8331155776977539\n",
      "Step: 31000  \tValid loss: 0.37191107869148254\n",
      "Step: 31100  \tTraining loss: 0.2985924482345581\n",
      "Step: 31100  \tTraining accuracy: 0.8331605792045593\n",
      "Step: 31100  \tValid loss: 0.371808260679245\n",
      "Step: 31200  \tTraining loss: 0.298488587141037\n",
      "Step: 31200  \tTraining accuracy: 0.8332053422927856\n",
      "Step: 31200  \tValid loss: 0.3717104196548462\n",
      "Step: 31300  \tTraining loss: 0.2983892261981964\n",
      "Step: 31300  \tTraining accuracy: 0.8332498669624329\n",
      "Step: 31300  \tValid loss: 0.37162482738494873\n",
      "Step: 31400  \tTraining loss: 0.29828834533691406\n",
      "Step: 31400  \tTraining accuracy: 0.8332940340042114\n",
      "Step: 31400  \tValid loss: 0.3715238571166992\n",
      "Step: 31500  \tTraining loss: 0.2981853485107422\n",
      "Step: 31500  \tTraining accuracy: 0.833341658115387\n",
      "Step: 31500  \tValid loss: 0.3714047968387604\n",
      "Step: 31600  \tTraining loss: 0.29808729887008667\n",
      "Step: 31600  \tTraining accuracy: 0.833392322063446\n",
      "Step: 31600  \tValid loss: 0.3713191747665405\n",
      "Step: 31700  \tTraining loss: 0.29798758029937744\n",
      "Step: 31700  \tTraining accuracy: 0.8334434628486633\n",
      "Step: 31700  \tValid loss: 0.3712072968482971\n",
      "Step: 31800  \tTraining loss: 0.2978880703449249\n",
      "Step: 31800  \tTraining accuracy: 0.8334954977035522\n",
      "Step: 31800  \tValid loss: 0.37109559774398804\n",
      "Step: 31900  \tTraining loss: 0.2977895438671112\n",
      "Step: 31900  \tTraining accuracy: 0.8335443735122681\n",
      "Step: 31900  \tValid loss: 0.3709966838359833\n",
      "Step: 32000  \tTraining loss: 0.2976905107498169\n",
      "Step: 32000  \tTraining accuracy: 0.8335928916931152\n",
      "Step: 32000  \tValid loss: 0.3709038197994232\n",
      "Step: 32100  \tTraining loss: 0.2975940406322479\n",
      "Step: 32100  \tTraining accuracy: 0.8336423635482788\n",
      "Step: 32100  \tValid loss: 0.3707876205444336\n",
      "Step: 32200  \tTraining loss: 0.29749542474746704\n",
      "Step: 32200  \tTraining accuracy: 0.8336923122406006\n",
      "Step: 32200  \tValid loss: 0.3706967830657959\n",
      "Step: 32300  \tTraining loss: 0.2973979413509369\n",
      "Step: 32300  \tTraining accuracy: 0.8337427973747253\n",
      "Step: 32300  \tValid loss: 0.37060311436653137\n",
      "Step: 32400  \tTraining loss: 0.29730308055877686\n",
      "Step: 32400  \tTraining accuracy: 0.8337913155555725\n",
      "Step: 32400  \tValid loss: 0.3704856336116791\n"
     ]
    }
   ],
   "source": [
    "neurons = 8\n",
    "\n",
    "\n",
    "for num, subj_file_path in enumerate(subj_files_list):\n",
    "    print(num)\n",
    "# for subj_file_path in [subj_files_list[0]]:\n",
    "    \n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "                \n",
    "#     file_path = file_path + \"/OddEvenPlays\"\n",
    "    file_path = file_path + \"/OddEvenPlays/RandomizedPlays10\"\n",
    "\n",
    "    train_data_df= pd.read_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df = pd.read_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df = pd.read_csv(file_path+\"/val_data.csv\")\n",
    "\n",
    " \n",
    "    train_X, train_y, test_X, test_y,val_X,val_y = data_split_odd_even(train_data_df,test_data_df,val_data_df)\n",
    "\n",
    "    pretraining = False;\n",
    "    metric_out_df, prob_train, prob_test, prob_val = train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y)\n",
    "    \n",
    "    print(metric_out_df)\n",
    "    \n",
    "    metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    " \n",
    "    prob_train_df = pd.DataFrame(prob_train.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_test_df = pd.DataFrame(prob_test.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_val_df = pd.DataFrame(prob_val.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "\n",
    "\n",
    "# ################################\n",
    "    prob_train_df.to_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_test_df.to_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_val_df.to_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "# #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
