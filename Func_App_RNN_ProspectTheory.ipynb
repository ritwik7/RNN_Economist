{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as sc_stats\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "onehot_encoder=OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "time_steps = 1\n",
    "inputs = 8\n",
    "outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_releveant_features(task_df):\n",
    "\n",
    "\n",
    "    task_df['PrevOutcome']=task_df['Outcome'].shift(1)\n",
    "    task_df.loc[1,'PrevOutcome']= 0\n",
    "\n",
    "    task_df['PrevChoice']=task_df['Choice'].shift(1)\n",
    "    task_df.loc[1,'PrevChoice']= 0\n",
    "\n",
    "    task_df['PrevSafe']=task_df['Safe'].shift(1)\n",
    "    task_df.loc[1,'PrevSafe']= 0\n",
    "\n",
    "    task_df['PrevBigRisky']=task_df['BigRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevBigRisky']= 0\n",
    "\n",
    "    task_df['PrevSmallRisky']=task_df['SmallRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevSmallRisky']= 0\n",
    "    \n",
    "#     task_df['PrevRT']=task_df['RT'].shift(1)\n",
    "#     task_df.loc[1,'PrevRT']= N\n",
    "    \n",
    "    \n",
    "    \n",
    "    return task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_split_data(data,start_chunk,end_chunk):\n",
    "    \n",
    "    a=[k for k in range(start_chunk,end_chunk)]\n",
    "    out=[]\n",
    "\n",
    "    for d in range(0,data.shape[0],20):\n",
    "\n",
    "        c= [c+d for c in a]\n",
    "        out = out+c\n",
    "\n",
    "    while out[-1]>=data.shape[0]-1:\n",
    "        out.pop()\n",
    "#     return out\n",
    "    return data[out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y): \n",
    "    reset_graph()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50000\n",
    "    batch_size = int(train_X.shape[0]/2)\n",
    "    # batch_size = 100\n",
    "    length = train_X.shape[0]\n",
    "    display = 100\n",
    "    neurons = neurons\n",
    "\n",
    "    num_batches = 100\n",
    "    seq_len = 10\n",
    "\n",
    "    percent_above_PT = 1\n",
    "\n",
    "    train_threshold = 1.5#PT_R2 + percent_above_PT\n",
    "\n",
    "\n",
    "    save_step = 100\n",
    "\n",
    "\n",
    "    best_loss_val = np.infty\n",
    "    checks_since_last_progress = 0\n",
    "    max_checks_without_progress = 1000\n",
    "\n",
    "\n",
    "    # clear graph (if any) before running\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, time_steps, inputs])\n",
    "\n",
    "    y = tf.placeholder(tf.float32, [None, outputs])\n",
    "\n",
    "    # LSTM Cell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=neurons, activation=tf.nn.relu)\n",
    "    cell_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "    # pass into Dense layer\n",
    "    stacked_outputs = tf.reshape(cell_outputs, [-1, neurons])\n",
    "    out = tf.layers.dense(inputs=stacked_outputs, units=outputs)\n",
    "\n",
    "    probability = tf.nn.softmax(out)\n",
    "\n",
    "    # squared error loss or cost function for linear regression\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=y, logits=out))\n",
    "\n",
    "    # optimizer to minimize cost\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels =  tf.argmax(y, 1),\n",
    "                          predictions = tf.argmax(out, 1),\n",
    "                          name = \"accuracy\")\n",
    "    precision = tf.metrics.precision(labels=tf.argmax(y, 1),\n",
    "                                 predictions=tf.argmax(out, 1),\n",
    "                                 name=\"precision\")\n",
    "    recall = tf.metrics.recall(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"recall\")\n",
    "    f1 = 2 * accuracy[1] * recall[1] / ( precision[1] + recall[1] )\n",
    "\n",
    "    acc_up,acc_val = accuracy\n",
    "    auc = tf.metrics.auc(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"auc\")\n",
    "    \n",
    "    valid_store = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #######################\n",
    "#         saver.restore(sess, \"./checkpts/Original_RNN_LSTM_8features_v2.ckpt\")\n",
    "#         saver.restore(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "        \n",
    "        if pretraining == True:\n",
    "\n",
    "            saver.restore(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        #######################\n",
    "        \n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "\n",
    "        # Train the model\n",
    "        for steps in range(epochs):\n",
    "            mini_batch = zip(range(0, length, batch_size),\n",
    "                       range(batch_size, length+1, batch_size))\n",
    "\n",
    "            # train data in mini-batches\n",
    "            for (start, end) in mini_batch:\n",
    "    #             print(start,end)\n",
    "                sess.run(training_op, feed_dict = {X: train_X[start:end,:,:],\n",
    "                                                   y: train_y[start:end,:]}) \n",
    "\n",
    "            ## train data in batches of length subsequence\n",
    "\n",
    "    #         for k in range(num_batches):\n",
    "    #             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "\n",
    "    #             sess.run(training_op, feed_dict = {X:X_seq,y:y_seq}) \n",
    "            loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "            loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "            # print training performance \n",
    "            if (steps+1) % display == 0:\n",
    "                # evaluate loss function on training set\n",
    "\n",
    "\n",
    "                loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining loss: {}'.format((steps+1), loss_fn))\n",
    "\n",
    "                acc_train = acc_val.eval(feed_dict={X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining accuracy: {}'.format((steps+1), acc_train))\n",
    "\n",
    "\n",
    "                acc_test = acc_val.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest accuracy: {}'.format((steps+1), acc_test))\n",
    "\n",
    "                loss_test = loss.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest loss: {}'.format((steps+1), loss_test))\n",
    "\n",
    "                accu_val = acc_val.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "                loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "                print('Step: {}  \\tValid loss: {}'.format((steps+1), loss_val))\n",
    "\n",
    "                valid_store.append(loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (1 + loss_fn/np.log(0.5)) > train_threshold:\n",
    "                    print(\"Threshold achieved, quit training\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            if loss_val < best_loss_val:\n",
    "\n",
    "                        best_loss_val = loss_val\n",
    "                        checks_since_last_progress = 0\n",
    "            else:\n",
    "                            checks_since_last_progress += 1\n",
    "\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if checks_since_last_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "\n",
    "            if (steps+1) % save_step ==0:\n",
    "                                save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#                 save_path = saver.save(sess, \"./checkpts/RNN_Internet_LSTM_model_5features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     evaluate model accuracy\n",
    "        acc, prec, recall, f1, AUC = sess.run([accuracy, precision, recall, f1,auc],\n",
    "                                         feed_dict = {X: train_X, y: train_y})\n",
    "        prob_train = probability.eval(feed_dict = {X: train_X, y: train_y})\n",
    "        prob_test = probability.eval(feed_dict = {X: test_X, y: test_y})\n",
    "        prob_valid = probability.eval(feed_dict = {X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nEvaluation  on training set')\n",
    "        print('Accuracy:', acc[1])\n",
    "        print('Precision:', prec[1])\n",
    "        print('Recall:', recall[1])\n",
    "        print('F1 score:', f1)\n",
    "        print('AUC:', AUC[1])\n",
    "        \n",
    "      \n",
    "    \n",
    "    \n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/LaterDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## APP DATA\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_APPDATA_RNN_LSTM_8features.ckpt\")\n",
    "        save_path = saver.save(sess, \"./checkpts/Later_v2_APPDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "    metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,AUC[1],loss_fn,accu_val,best_loss_val,acc_test,loss_test,neurons,learning_rate,epochs,steps]).reshape(-1,14),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\",\"loss\",\"accuracy_val\",\"loss_val\",\"accuracy_test\",\"loss_test\",\"neurons\",\"learning_rate\",\"n_epochs\",\"steps\"])\n",
    "    return metric_out_df, prob_train, prob_test, prob_valid\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def random_subsequence(X,y,seq_len):\n",
    "    rnd  = random.randint(0,len(X)-seq_len)\n",
    "    X_seq, y_seq = X[rnd:rnd+seq_len,:], y[rnd:rnd+seq_len,:]\n",
    "    return X_seq, y_seq\n",
    "\n",
    "    print(y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ways of cutting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odd plays train, even plays test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\")\n",
    "dir_path =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"\n",
    "\n",
    "subj_files_list =[]; ## list of subject_files fullfilling a criteria\n",
    "\n",
    "dir_files = [i for i in os.listdir(dir_path) if i.startswith('sub')]\n",
    "\n",
    "for subj_file_path in dir_files:\n",
    "\n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "    mypath =file_path\n",
    "    \n",
    "    play_names = [i for i in os.listdir(mypath) if os.path.isfile(os.path.join(mypath,i)) and i.startswith('app')]   \n",
    "    \n",
    "    if len(play_names) >= 50: ## criteria\n",
    "        subj_files_list.append(subj_file_path)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subj_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "num_shuffles=5\n",
    "for num, subj_file_path in enumerate(subj_files_list[]):\n",
    "    print(num)\n",
    "# for subj_file_path in [subj_files_list[0]]:\n",
    "    \n",
    "#     train_data,test_data, val_data = np.empty((0,task_df.columns.shape[0])),  np.empty((0,task_df.columns.shape[0])), np.empty((0,task_df.columns.shape[0]))\n",
    "    train_data,test_data, val_data = np.empty((0,15)),  np.empty((0,15)), np.empty((0,15))\n",
    "\n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "    mypath =file_path\n",
    "        \n",
    "    comp_task_train_df = pd.DataFrame()\n",
    "\n",
    "    play_names = [i for i in os.listdir(mypath) if os.path.isfile(os.path.join(mypath,i)) and i.startswith('app')]    \n",
    "\n",
    "    for randomization_counter in range(0,num_shuffles):\n",
    "            randomized_play_names= random.sample(play_names,len(play_names))\n",
    "            \n",
    "            for play_num, play_name in enumerate(randomized_play_names):\n",
    "#         for play_num,play_name in enumerate(play_names):\n",
    "\n",
    "                file_name = file_path + \"/\" + play_name\n",
    "                task_df = pd.read_csv(file_name)\n",
    "                task_df = add_releveant_features(task_df)\n",
    "\n",
    "                if np.mod(play_num,2)==0: ## odd trials\n",
    "                    train_data = np.append(train_data,task_df[task_df.TrialNum>1].values, axis=0)\n",
    "\n",
    "                else:\n",
    "                    test_data =  np.append(test_data, task_df[task_df.TrialNum>1].values[0:16], axis=0)\n",
    "                    val_data =  np.append(val_data, task_df[task_df.TrialNum>1].values[16:], axis=0)\n",
    "\n",
    "\n",
    "    train_data_df= pd.DataFrame(train_data,columns=task_df.columns)\n",
    "    val_data_df = pd.DataFrame(test_data,columns=task_df.columns)\n",
    "    test_data_df= pd.DataFrame(val_data,columns=task_df.columns)\n",
    "\n",
    "#     file_path = file_path + \"/OddEvenPlays/\"\n",
    "    file_path = file_path + \"/OddEvenPlays/RandomizedPlays10\"\n",
    "\n",
    "#     os.mkdir(file_path)\n",
    "    train_data_df.to_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df.to_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df.to_csv(file_path+\"/val_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_odd_even(train_data_df,test_data_df,val_data_df):\n",
    "\n",
    "#     train_len = 29\n",
    "#     test_len = 14\n",
    "#     val_len = 15\n",
    "\n",
    "    ##----------------- UNCOMMENT BELOW\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "    \n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "####### Prev O + C+ R + CurrO--------------------\n",
    " \n",
    "    train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    \n",
    "    ######## sampling \n",
    "    \n",
    "    \n",
    "#### - Prev RT+C+R+O + Curr O----------------------\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### PRE TRAINING\n",
    "#     stop = int(0.7*len(train_X))\n",
    "#     print(stop)\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int((len(train_X)-stop)/2)], train_X[stop+int((len(train_X)-stop)/2):],train_y[:stop], train_y[stop:stop+int((len(train_X)-stop)/2)], train_y[stop+int((len(train_X)-stop)/2):]\n",
    "    \n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = train_X, test_X, test_X, train_y, test_y, test_y\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(val_X.shape)\n",
    "    print(val_y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "    # # center and scale\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.fit_transform(test_X)\n",
    "    val_X = scaler.fit_transform(val_X)\n",
    "\n",
    "\n",
    "    train_X = train_X[:,None,:]\n",
    "    val_X = val_X[:,None,:]\n",
    "    test_X = test_X[:,None,:]\n",
    "\n",
    "\n",
    "    # # one-hot encode the outputs\n",
    "\n",
    "    onehot_encoder = OneHotEncoder()\n",
    "    encode_categorical = train_y.reshape(len(train_y), 1)\n",
    "    encode_categorical_test = test_y.reshape(len(test_y), 1)\n",
    "    encode_categorical_val = val_y.reshape(len(val_y),1)\n",
    "\n",
    "\n",
    "    train_y = onehot_encoder.fit_transform(encode_categorical).toarray()\n",
    "    test_y = onehot_encoder.fit_transform(encode_categorical_test).toarray()\n",
    "    val_y = onehot_encoder.fit_transform(encode_categorical_val).toarray()\n",
    "\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, val_X,val_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(5945, 8)\n",
      "(5945, 1)\n",
      "(3200, 8)\n",
      "(3200, 1)\n",
      "(2600, 8)\n",
      "(2600, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-45e26c98b632>:36: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-45e26c98b632>:37: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/ritwik7/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-45e26c98b632>:41: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/ritwik7/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/ritwik7/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Step: 100  \tTraining loss: 0.354576051235199\n",
      "Step: 100  \tTraining accuracy: 0.8576955199241638\n",
      "Step: 100  \tValid loss: 0.34452301263809204\n",
      "Step: 200  \tTraining loss: 0.30858197808265686\n",
      "Step: 200  \tTraining accuracy: 0.8671000599861145\n",
      "Step: 200  \tValid loss: 0.3039250373840332\n",
      "Step: 300  \tTraining loss: 0.3038502037525177\n",
      "Step: 300  \tTraining accuracy: 0.8742992877960205\n",
      "Step: 300  \tValid loss: 0.2980923354625702\n",
      "Step: 400  \tTraining loss: 0.3012202978134155\n",
      "Step: 400  \tTraining accuracy: 0.878363311290741\n",
      "Step: 400  \tValid loss: 0.29423993825912476\n",
      "Step: 500  \tTraining loss: 0.29967963695526123\n",
      "Step: 500  \tTraining accuracy: 0.8812092542648315\n",
      "Step: 500  \tValid loss: 0.2916617691516876\n",
      "Step: 600  \tTraining loss: 0.2988569438457489\n",
      "Step: 600  \tTraining accuracy: 0.8832534551620483\n",
      "Step: 600  \tValid loss: 0.2901608943939209\n",
      "Step: 700  \tTraining loss: 0.2983165979385376\n",
      "Step: 700  \tTraining accuracy: 0.8847346901893616\n",
      "Step: 700  \tValid loss: 0.28924426436424255\n",
      "Step: 800  \tTraining loss: 0.2978316843509674\n",
      "Step: 800  \tTraining accuracy: 0.8858325481414795\n",
      "Step: 800  \tValid loss: 0.2884589433670044\n",
      "Step: 900  \tTraining loss: 0.29740196466445923\n",
      "Step: 900  \tTraining accuracy: 0.8867223858833313\n",
      "Step: 900  \tValid loss: 0.28785955905914307\n",
      "Step: 1000  \tTraining loss: 0.29698824882507324\n",
      "Step: 1000  \tTraining accuracy: 0.8875414133071899\n",
      "Step: 1000  \tValid loss: 0.2873373329639435\n",
      "Step: 1100  \tTraining loss: 0.2965342104434967\n",
      "Step: 1100  \tTraining accuracy: 0.8882288336753845\n",
      "Step: 1100  \tValid loss: 0.28680557012557983\n",
      "Step: 1200  \tTraining loss: 0.29592785239219666\n",
      "Step: 1200  \tTraining accuracy: 0.8888115882873535\n",
      "Step: 1200  \tValid loss: 0.2860667407512665\n",
      "Step: 1300  \tTraining loss: 0.2949099540710449\n",
      "Step: 1300  \tTraining accuracy: 0.8894509077072144\n",
      "Step: 1300  \tValid loss: 0.2847048044204712\n",
      "Step: 1400  \tTraining loss: 0.29379698634147644\n",
      "Step: 1400  \tTraining accuracy: 0.8900207877159119\n",
      "Step: 1400  \tValid loss: 0.28305643796920776\n",
      "Step: 1500  \tTraining loss: 0.2926422655582428\n",
      "Step: 1500  \tTraining accuracy: 0.8904710412025452\n",
      "Step: 1500  \tValid loss: 0.2815142869949341\n",
      "Step: 1600  \tTraining loss: 0.2916029393672943\n",
      "Step: 1600  \tTraining accuracy: 0.8908631801605225\n",
      "Step: 1600  \tValid loss: 0.28004124760627747\n",
      "Step: 1700  \tTraining loss: 0.29070615768432617\n",
      "Step: 1700  \tTraining accuracy: 0.8912593722343445\n",
      "Step: 1700  \tValid loss: 0.27892112731933594\n",
      "Step: 1800  \tTraining loss: 0.2898780107498169\n",
      "Step: 1800  \tTraining accuracy: 0.8916686773300171\n",
      "Step: 1800  \tValid loss: 0.2779676020145416\n",
      "Step: 1900  \tTraining loss: 0.2890363335609436\n",
      "Step: 1900  \tTraining accuracy: 0.8921396136283875\n",
      "Step: 1900  \tValid loss: 0.27702251076698303\n",
      "Step: 2000  \tTraining loss: 0.28827664256095886\n",
      "Step: 2000  \tTraining accuracy: 0.8925753235816956\n",
      "Step: 2000  \tValid loss: 0.2761688232421875\n",
      "Step: 2100  \tTraining loss: 0.2874537408351898\n",
      "Step: 2100  \tTraining accuracy: 0.8929768204689026\n",
      "Step: 2100  \tValid loss: 0.2753234803676605\n",
      "Step: 2200  \tTraining loss: 0.28667834401130676\n",
      "Step: 2200  \tTraining accuracy: 0.8933607935905457\n",
      "Step: 2200  \tValid loss: 0.2744874358177185\n",
      "Step: 2300  \tTraining loss: 0.2859298884868622\n",
      "Step: 2300  \tTraining accuracy: 0.8937408924102783\n",
      "Step: 2300  \tValid loss: 0.2736380696296692\n",
      "Step: 2400  \tTraining loss: 0.2851939797401428\n",
      "Step: 2400  \tTraining accuracy: 0.8941394090652466\n",
      "Step: 2400  \tValid loss: 0.2727733254432678\n",
      "Step: 2500  \tTraining loss: 0.28447845578193665\n",
      "Step: 2500  \tTraining accuracy: 0.8945261836051941\n",
      "Step: 2500  \tValid loss: 0.2719362676143646\n",
      "Step: 2600  \tTraining loss: 0.2837883234024048\n",
      "Step: 2600  \tTraining accuracy: 0.8948726654052734\n",
      "Step: 2600  \tValid loss: 0.27113255858421326\n",
      "Step: 2700  \tTraining loss: 0.28313279151916504\n",
      "Step: 2700  \tTraining accuracy: 0.8951961994171143\n",
      "Step: 2700  \tValid loss: 0.2703711986541748\n",
      "Step: 2800  \tTraining loss: 0.28251218795776367\n",
      "Step: 2800  \tTraining accuracy: 0.8955023884773254\n",
      "Step: 2800  \tValid loss: 0.26964429020881653\n",
      "Step: 2900  \tTraining loss: 0.28192752599716187\n",
      "Step: 2900  \tTraining accuracy: 0.8957781195640564\n",
      "Step: 2900  \tValid loss: 0.26894715428352356\n",
      "Step: 3000  \tTraining loss: 0.2813710868358612\n",
      "Step: 3000  \tTraining accuracy: 0.8960784673690796\n",
      "Step: 3000  \tValid loss: 0.26830777525901794\n",
      "Step: 3100  \tTraining loss: 0.28082549571990967\n",
      "Step: 3100  \tTraining accuracy: 0.8963730931282043\n",
      "Step: 3100  \tValid loss: 0.2676564157009125\n",
      "Step: 3200  \tTraining loss: 0.2803216576576233\n",
      "Step: 3200  \tTraining accuracy: 0.8966544270515442\n",
      "Step: 3200  \tValid loss: 0.26706254482269287\n",
      "Step: 3300  \tTraining loss: 0.27985405921936035\n",
      "Step: 3300  \tTraining accuracy: 0.896949827671051\n",
      "Step: 3300  \tValid loss: 0.266520231962204\n",
      "Step: 3400  \tTraining loss: 0.27941983938217163\n",
      "Step: 3400  \tTraining accuracy: 0.8972530961036682\n",
      "Step: 3400  \tValid loss: 0.2660224437713623\n",
      "Step: 3500  \tTraining loss: 0.27902156114578247\n",
      "Step: 3500  \tTraining accuracy: 0.8975387215614319\n",
      "Step: 3500  \tValid loss: 0.2655833959579468\n",
      "Step: 3600  \tTraining loss: 0.278659850358963\n",
      "Step: 3600  \tTraining accuracy: 0.8977986574172974\n",
      "Step: 3600  \tValid loss: 0.26519882678985596\n",
      "Step: 3700  \tTraining loss: 0.2783358693122864\n",
      "Step: 3700  \tTraining accuracy: 0.8980560302734375\n",
      "Step: 3700  \tValid loss: 0.2648576498031616\n",
      "Step: 3800  \tTraining loss: 0.27804529666900635\n",
      "Step: 3800  \tTraining accuracy: 0.8983019590377808\n",
      "Step: 3800  \tValid loss: 0.2645564079284668\n",
      "Step: 3900  \tTraining loss: 0.27775880694389343\n",
      "Step: 3900  \tTraining accuracy: 0.8985439538955688\n",
      "Step: 3900  \tValid loss: 0.2642870247364044\n",
      "Step: 4000  \tTraining loss: 0.2775038182735443\n",
      "Step: 4000  \tTraining accuracy: 0.8987866640090942\n",
      "Step: 4000  \tValid loss: 0.2640244662761688\n",
      "Step: 4100  \tTraining loss: 0.27728071808815\n",
      "Step: 4100  \tTraining accuracy: 0.8990299701690674\n",
      "Step: 4100  \tValid loss: 0.26381903886795044\n",
      "Step: 4200  \tTraining loss: 0.27707841992378235\n",
      "Step: 4200  \tTraining accuracy: 0.8992676734924316\n",
      "Step: 4200  \tValid loss: 0.26363810896873474\n",
      "Step: 4300  \tTraining loss: 0.2768920063972473\n",
      "Step: 4300  \tTraining accuracy: 0.8994942307472229\n",
      "Step: 4300  \tValid loss: 0.26348477602005005\n",
      "Step: 4400  \tTraining loss: 0.27671995759010315\n",
      "Step: 4400  \tTraining accuracy: 0.8997005820274353\n",
      "Step: 4400  \tValid loss: 0.2633552849292755\n",
      "Step: 4500  \tTraining loss: 0.2765604555606842\n",
      "Step: 4500  \tTraining accuracy: 0.8998804092407227\n",
      "Step: 4500  \tValid loss: 0.2632310390472412\n",
      "Step: 4600  \tTraining loss: 0.2764109671115875\n",
      "Step: 4600  \tTraining accuracy: 0.9000411629676819\n",
      "Step: 4600  \tValid loss: 0.2631225287914276\n",
      "Step: 4700  \tTraining loss: 0.2762693166732788\n",
      "Step: 4700  \tTraining accuracy: 0.9001950025558472\n",
      "Step: 4700  \tValid loss: 0.26301905512809753\n",
      "Step: 4800  \tTraining loss: 0.27613502740859985\n",
      "Step: 4800  \tTraining accuracy: 0.9003369212150574\n",
      "Step: 4800  \tValid loss: 0.26291781663894653\n",
      "Step: 4900  \tTraining loss: 0.2760065495967865\n",
      "Step: 4900  \tTraining accuracy: 0.9004660248756409\n",
      "Step: 4900  \tValid loss: 0.2628156840801239\n",
      "Step: 5000  \tTraining loss: 0.2758823037147522\n",
      "Step: 5000  \tTraining accuracy: 0.9005898833274841\n",
      "Step: 5000  \tValid loss: 0.2627217769622803\n",
      "Step: 5100  \tTraining loss: 0.2757621109485626\n",
      "Step: 5100  \tTraining accuracy: 0.900712251663208\n",
      "Step: 5100  \tValid loss: 0.2626330554485321\n",
      "Step: 5200  \tTraining loss: 0.2756458818912506\n",
      "Step: 5200  \tTraining accuracy: 0.9008298516273499\n",
      "Step: 5200  \tValid loss: 0.26255089044570923\n",
      "Step: 5300  \tTraining loss: 0.27553287148475647\n",
      "Step: 5300  \tTraining accuracy: 0.9009429216384888\n",
      "Step: 5300  \tValid loss: 0.2624691128730774\n",
      "Step: 5400  \tTraining loss: 0.2754230797290802\n",
      "Step: 5400  \tTraining accuracy: 0.9010502099990845\n",
      "Step: 5400  \tValid loss: 0.26239538192749023\n",
      "Step: 5500  \tTraining loss: 0.27531710267066956\n",
      "Step: 5500  \tTraining accuracy: 0.9011504650115967\n",
      "Step: 5500  \tValid loss: 0.2623221278190613\n",
      "Step: 5600  \tTraining loss: 0.27521318197250366\n",
      "Step: 5600  \tTraining accuracy: 0.9012670516967773\n",
      "Step: 5600  \tValid loss: 0.2622486650943756\n",
      "Step: 5700  \tTraining loss: 0.27511066198349\n",
      "Step: 5700  \tTraining accuracy: 0.9013885259628296\n",
      "Step: 5700  \tValid loss: 0.2621755301952362\n",
      "Step: 5800  \tTraining loss: 0.27500927448272705\n",
      "Step: 5800  \tTraining accuracy: 0.9015116691589355\n",
      "Step: 5800  \tValid loss: 0.2621041238307953\n",
      "Step: 5900  \tTraining loss: 0.27490901947021484\n",
      "Step: 5900  \tTraining accuracy: 0.9016306400299072\n",
      "Step: 5900  \tValid loss: 0.2620429992675781\n",
      "Step: 6000  \tTraining loss: 0.274809330701828\n",
      "Step: 6000  \tTraining accuracy: 0.901745617389679\n",
      "Step: 6000  \tValid loss: 0.2619852125644684\n",
      "Step: 6100  \tTraining loss: 0.274709016084671\n",
      "Step: 6100  \tTraining accuracy: 0.9018553495407104\n",
      "Step: 6100  \tValid loss: 0.2619255483150482\n",
      "Step: 6200  \tTraining loss: 0.2746078073978424\n",
      "Step: 6200  \tTraining accuracy: 0.9019560217857361\n",
      "Step: 6200  \tValid loss: 0.26186639070510864\n",
      "Step: 6300  \tTraining loss: 0.27450552582740784\n",
      "Step: 6300  \tTraining accuracy: 0.9020534157752991\n",
      "Step: 6300  \tValid loss: 0.2618149220943451\n",
      "Step: 6400  \tTraining loss: 0.2744014859199524\n",
      "Step: 6400  \tTraining accuracy: 0.9021450877189636\n",
      "Step: 6400  \tValid loss: 0.261765718460083\n",
      "Step: 6500  \tTraining loss: 0.27429625391960144\n",
      "Step: 6500  \tTraining accuracy: 0.9022405743598938\n",
      "Step: 6500  \tValid loss: 0.2617201805114746\n",
      "Step: 6600  \tTraining loss: 0.27418091893196106\n",
      "Step: 6600  \tTraining accuracy: 0.9023252725601196\n",
      "Step: 6600  \tValid loss: 0.26169031858444214\n",
      "Step: 6700  \tTraining loss: 0.27406901121139526\n",
      "Step: 6700  \tTraining accuracy: 0.9023984670639038\n",
      "Step: 6700  \tValid loss: 0.26164138317108154\n",
      "Step: 6800  \tTraining loss: 0.2739574909210205\n",
      "Step: 6800  \tTraining accuracy: 0.9024670124053955\n",
      "Step: 6800  \tValid loss: 0.2615821659564972\n",
      "Step: 6900  \tTraining loss: 0.27384626865386963\n",
      "Step: 6900  \tTraining accuracy: 0.9025298357009888\n",
      "Step: 6900  \tValid loss: 0.2615209221839905\n",
      "Step: 7000  \tTraining loss: 0.2737356126308441\n",
      "Step: 7000  \tTraining accuracy: 0.9025883674621582\n",
      "Step: 7000  \tValid loss: 0.2614591121673584\n",
      "Step: 7100  \tTraining loss: 0.273625910282135\n",
      "Step: 7100  \tTraining accuracy: 0.9026500582695007\n",
      "Step: 7100  \tValid loss: 0.26140540838241577\n",
      "Step: 7200  \tTraining loss: 0.27351731061935425\n",
      "Step: 7200  \tTraining accuracy: 0.9027040600776672\n",
      "Step: 7200  \tValid loss: 0.261349618434906\n",
      "Step: 7300  \tTraining loss: 0.27340731024742126\n",
      "Step: 7300  \tTraining accuracy: 0.9027578234672546\n",
      "Step: 7300  \tValid loss: 0.2612614035606384\n",
      "Step: 7400  \tTraining loss: 0.27329373359680176\n",
      "Step: 7400  \tTraining accuracy: 0.9028123617172241\n",
      "Step: 7400  \tValid loss: 0.2611628770828247\n",
      "Step: 7500  \tTraining loss: 0.273181289434433\n",
      "Step: 7500  \tTraining accuracy: 0.902868926525116\n",
      "Step: 7500  \tValid loss: 0.2610836327075958\n",
      "Step: 7600  \tTraining loss: 0.27306973934173584\n",
      "Step: 7600  \tTraining accuracy: 0.9029262065887451\n",
      "Step: 7600  \tValid loss: 0.2610245645046234\n",
      "Step: 7700  \tTraining loss: 0.27295851707458496\n",
      "Step: 7700  \tTraining accuracy: 0.9029819965362549\n",
      "Step: 7700  \tValid loss: 0.2609614133834839\n",
      "Step: 7800  \tTraining loss: 0.27284765243530273\n",
      "Step: 7800  \tTraining accuracy: 0.9030352234840393\n",
      "Step: 7800  \tValid loss: 0.2609040141105652\n",
      "Step: 7900  \tTraining loss: 0.27273669838905334\n",
      "Step: 7900  \tTraining accuracy: 0.9030827879905701\n",
      "Step: 7900  \tValid loss: 0.26083919405937195\n",
      "Step: 8000  \tTraining loss: 0.27262574434280396\n",
      "Step: 8000  \tTraining accuracy: 0.9031313061714172\n",
      "Step: 8000  \tValid loss: 0.2607785165309906\n",
      "Step: 8100  \tTraining loss: 0.272514671087265\n",
      "Step: 8100  \tTraining accuracy: 0.903181791305542\n",
      "Step: 8100  \tValid loss: 0.26071688532829285\n",
      "Step: 8200  \tTraining loss: 0.27240344882011414\n",
      "Step: 8200  \tTraining accuracy: 0.9032310247421265\n",
      "Step: 8200  \tValid loss: 0.26065415143966675\n",
      "Step: 8300  \tTraining loss: 0.27229180932044983\n",
      "Step: 8300  \tTraining accuracy: 0.90327388048172\n",
      "Step: 8300  \tValid loss: 0.26058417558670044\n",
      "Step: 8400  \tTraining loss: 0.2721792757511139\n",
      "Step: 8400  \tTraining accuracy: 0.9033106565475464\n",
      "Step: 8400  \tValid loss: 0.26051196455955505\n",
      "Step: 8500  \tTraining loss: 0.2720666527748108\n",
      "Step: 8500  \tTraining accuracy: 0.9033465385437012\n",
      "Step: 8500  \tValid loss: 0.2604426443576813\n",
      "Step: 8600  \tTraining loss: 0.2719530463218689\n",
      "Step: 8600  \tTraining accuracy: 0.9033815860748291\n",
      "Step: 8600  \tValid loss: 0.260367751121521\n",
      "Step: 8700  \tTraining loss: 0.27183860540390015\n",
      "Step: 8700  \tTraining accuracy: 0.9034138321876526\n",
      "Step: 8700  \tValid loss: 0.26028749346733093\n",
      "Step: 8800  \tTraining loss: 0.2717230021953583\n",
      "Step: 8800  \tTraining accuracy: 0.9034424424171448\n",
      "Step: 8800  \tValid loss: 0.26020318269729614\n",
      "Step: 8900  \tTraining loss: 0.271605908870697\n",
      "Step: 8900  \tTraining accuracy: 0.9034703969955444\n",
      "Step: 8900  \tValid loss: 0.2601149380207062\n",
      "Step: 9000  \tTraining loss: 0.2714873254299164\n",
      "Step: 9000  \tTraining accuracy: 0.9034977555274963\n",
      "Step: 9000  \tValid loss: 0.2600231170654297\n",
      "Step: 9100  \tTraining loss: 0.27136683464050293\n",
      "Step: 9100  \tTraining accuracy: 0.9035226106643677\n",
      "Step: 9100  \tValid loss: 0.2599294185638428\n",
      "Step: 9200  \tTraining loss: 0.27124419808387756\n",
      "Step: 9200  \tTraining accuracy: 0.9035441279411316\n",
      "Step: 9200  \tValid loss: 0.2598329186439514\n",
      "Step: 9300  \tTraining loss: 0.2711188495159149\n",
      "Step: 9300  \tTraining accuracy: 0.9035651683807373\n",
      "Step: 9300  \tValid loss: 0.2597304582595825\n",
      "Step: 9400  \tTraining loss: 0.2709909677505493\n",
      "Step: 9400  \tTraining accuracy: 0.9035857915878296\n",
      "Step: 9400  \tValid loss: 0.25962206721305847\n",
      "Step: 9500  \tTraining loss: 0.2708599269390106\n",
      "Step: 9500  \tTraining accuracy: 0.9036059379577637\n",
      "Step: 9500  \tValid loss: 0.2595066726207733\n",
      "Step: 9600  \tTraining loss: 0.2707253396511078\n",
      "Step: 9600  \tTraining accuracy: 0.9036256670951843\n",
      "Step: 9600  \tValid loss: 0.2593858540058136\n",
      "Step: 9700  \tTraining loss: 0.2705870270729065\n",
      "Step: 9700  \tTraining accuracy: 0.9036450386047363\n",
      "Step: 9700  \tValid loss: 0.2592600882053375\n",
      "Step: 9800  \tTraining loss: 0.2704451084136963\n",
      "Step: 9800  \tTraining accuracy: 0.9036639332771301\n",
      "Step: 9800  \tValid loss: 0.2591385543346405\n",
      "Step: 9900  \tTraining loss: 0.27029934525489807\n",
      "Step: 9900  \tTraining accuracy: 0.9036825299263\n",
      "Step: 9900  \tValid loss: 0.25901028513908386\n",
      "Step: 10000  \tTraining loss: 0.2701491415500641\n",
      "Step: 10000  \tTraining accuracy: 0.9036998152732849\n",
      "Step: 10000  \tValid loss: 0.25887590646743774\n",
      "Step: 10100  \tTraining loss: 0.2699947655200958\n",
      "Step: 10100  \tTraining accuracy: 0.9037176370620728\n",
      "Step: 10100  \tValid loss: 0.2587399482727051\n",
      "Step: 10200  \tTraining loss: 0.26983657479286194\n",
      "Step: 10200  \tTraining accuracy: 0.9037351608276367\n",
      "Step: 10200  \tValid loss: 0.25860244035720825\n",
      "Step: 10300  \tTraining loss: 0.269674688577652\n",
      "Step: 10300  \tTraining accuracy: 0.9037522673606873\n",
      "Step: 10300  \tValid loss: 0.2584642469882965\n",
      "Step: 10400  \tTraining loss: 0.2695094645023346\n",
      "Step: 10400  \tTraining accuracy: 0.9037690758705139\n",
      "Step: 10400  \tValid loss: 0.2583286166191101\n",
      "Step: 10500  \tTraining loss: 0.2693415582180023\n",
      "Step: 10500  \tTraining accuracy: 0.9037855863571167\n",
      "Step: 10500  \tValid loss: 0.2581997811794281\n",
      "Step: 10600  \tTraining loss: 0.26916182041168213\n",
      "Step: 10600  \tTraining accuracy: 0.9038017392158508\n",
      "Step: 10600  \tValid loss: 0.25810107588768005\n",
      "Step: 10700  \tTraining loss: 0.2689836621284485\n",
      "Step: 10700  \tTraining accuracy: 0.9038152098655701\n",
      "Step: 10700  \tValid loss: 0.25799891352653503\n",
      "Step: 10800  \tTraining loss: 0.26880785822868347\n",
      "Step: 10800  \tTraining accuracy: 0.9038244485855103\n",
      "Step: 10800  \tValid loss: 0.25788044929504395\n",
      "Step: 10900  \tTraining loss: 0.2686325013637543\n",
      "Step: 10900  \tTraining accuracy: 0.903831958770752\n",
      "Step: 10900  \tValid loss: 0.2577681243419647\n",
      "Step: 11000  \tTraining loss: 0.26845794916152954\n",
      "Step: 11000  \tTraining accuracy: 0.9038393497467041\n",
      "Step: 11000  \tValid loss: 0.2576594054698944\n",
      "Step: 11100  \tTraining loss: 0.2682848274707794\n",
      "Step: 11100  \tTraining accuracy: 0.9038466215133667\n",
      "Step: 11100  \tValid loss: 0.2575559616088867\n",
      "Step: 11200  \tTraining loss: 0.26811325550079346\n",
      "Step: 11200  \tTraining accuracy: 0.9038552641868591\n",
      "Step: 11200  \tValid loss: 0.25745558738708496\n",
      "Step: 11300  \tTraining loss: 0.26794350147247314\n",
      "Step: 11300  \tTraining accuracy: 0.9038659930229187\n",
      "Step: 11300  \tValid loss: 0.2573641240596771\n",
      "Step: 11400  \tTraining loss: 0.2677604556083679\n",
      "Step: 11400  \tTraining accuracy: 0.9038758277893066\n",
      "Step: 11400  \tValid loss: 0.2572561204433441\n",
      "Step: 11500  \tTraining loss: 0.2675945460796356\n",
      "Step: 11500  \tTraining accuracy: 0.9038810133934021\n",
      "Step: 11500  \tValid loss: 0.25717049837112427\n",
      "Step: 11600  \tTraining loss: 0.26743316650390625\n",
      "Step: 11600  \tTraining accuracy: 0.9038868546485901\n",
      "Step: 11600  \tValid loss: 0.2570948600769043\n",
      "Step: 11700  \tTraining loss: 0.2672744393348694\n",
      "Step: 11700  \tTraining accuracy: 0.9038896560668945\n",
      "Step: 11700  \tValid loss: 0.25702258944511414\n",
      "Step: 11800  \tTraining loss: 0.26711753010749817\n",
      "Step: 11800  \tTraining accuracy: 0.9038909673690796\n",
      "Step: 11800  \tValid loss: 0.2569509446620941\n",
      "Step: 11900  \tTraining loss: 0.2669637203216553\n",
      "Step: 11900  \tTraining accuracy: 0.903892993927002\n",
      "Step: 11900  \tValid loss: 0.2568909823894501\n",
      "Step: 12000  \tTraining loss: 0.26681411266326904\n",
      "Step: 12000  \tTraining accuracy: 0.9038956761360168\n",
      "Step: 12000  \tValid loss: 0.25683751702308655\n",
      "Step: 12100  \tTraining loss: 0.26666703820228577\n",
      "Step: 12100  \tTraining accuracy: 0.9038976430892944\n",
      "Step: 12100  \tValid loss: 0.25678569078445435\n",
      "Step: 12200  \tTraining loss: 0.26652368903160095\n",
      "Step: 12200  \tTraining accuracy: 0.9038988351821899\n",
      "Step: 12200  \tValid loss: 0.25673922896385193\n",
      "Step: 12300  \tTraining loss: 0.2663799822330475\n",
      "Step: 12300  \tTraining accuracy: 0.9039013981819153\n",
      "Step: 12300  \tValid loss: 0.2566725015640259\n",
      "Step: 12400  \tTraining loss: 0.26624032855033875\n",
      "Step: 12400  \tTraining accuracy: 0.9039053320884705\n",
      "Step: 12400  \tValid loss: 0.2566220164299011\n",
      "Step: 12500  \tTraining loss: 0.2661040127277374\n",
      "Step: 12500  \tTraining accuracy: 0.9039112329483032\n",
      "Step: 12500  \tValid loss: 0.25658175349235535\n",
      "Step: 12600  \tTraining loss: 0.26597046852111816\n",
      "Step: 12600  \tTraining accuracy: 0.9039191007614136\n",
      "Step: 12600  \tValid loss: 0.2565479576587677\n",
      "Step: 12700  \tTraining loss: 0.2658262848854065\n",
      "Step: 12700  \tTraining accuracy: 0.9039281606674194\n",
      "Step: 12700  \tValid loss: 0.25648969411849976\n",
      "Step: 12800  \tTraining loss: 0.2656930387020111\n",
      "Step: 12800  \tTraining accuracy: 0.9039350748062134\n",
      "Step: 12800  \tValid loss: 0.25645574927330017\n",
      "Step: 12900  \tTraining loss: 0.26556462049484253\n",
      "Step: 12900  \tTraining accuracy: 0.9039405584335327\n",
      "Step: 12900  \tValid loss: 0.25643911957740784\n",
      "Step: 13000  \tTraining loss: 0.26543954014778137\n",
      "Step: 13000  \tTraining accuracy: 0.9039459824562073\n",
      "Step: 13000  \tValid loss: 0.25642159581184387\n",
      "Step: 13100  \tTraining loss: 0.2653176188468933\n",
      "Step: 13100  \tTraining accuracy: 0.9039512872695923\n",
      "Step: 13100  \tValid loss: 0.2564053237438202\n",
      "Step: 13200  \tTraining loss: 0.2651987075805664\n",
      "Step: 13200  \tTraining accuracy: 0.9039565324783325\n",
      "Step: 13200  \tValid loss: 0.2563896179199219\n",
      "Step: 13300  \tTraining loss: 0.2650834023952484\n",
      "Step: 13300  \tTraining accuracy: 0.9039616584777832\n",
      "Step: 13300  \tValid loss: 0.25637978315353394\n",
      "Step: 13400  \tTraining loss: 0.26497212052345276\n",
      "Step: 13400  \tTraining accuracy: 0.9039667844772339\n",
      "Step: 13400  \tValid loss: 0.25637829303741455\n",
      "Step: 13500  \tTraining loss: 0.2648634910583496\n",
      "Step: 13500  \tTraining accuracy: 0.9039686322212219\n",
      "Step: 13500  \tValid loss: 0.2563757002353668\n",
      "Step: 13600  \tTraining loss: 0.26475751399993896\n",
      "Step: 13600  \tTraining accuracy: 0.9039704203605652\n",
      "Step: 13600  \tValid loss: 0.25637373328208923\n",
      "Step: 13700  \tTraining loss: 0.26465386152267456\n",
      "Step: 13700  \tTraining accuracy: 0.903974711894989\n",
      "Step: 13700  \tValid loss: 0.25637298822402954\n",
      "Step: 13800  \tTraining loss: 0.264552503824234\n",
      "Step: 13800  \tTraining accuracy: 0.9039795398712158\n",
      "Step: 13800  \tValid loss: 0.2563735544681549\n",
      "Step: 13900  \tTraining loss: 0.26445329189300537\n",
      "Step: 13900  \tTraining accuracy: 0.9039843082427979\n",
      "Step: 13900  \tValid loss: 0.2563745379447937\n",
      "Step: 14000  \tTraining loss: 0.26435625553131104\n",
      "Step: 14000  \tTraining accuracy: 0.9039890170097351\n",
      "Step: 14000  \tValid loss: 0.25637662410736084\n",
      "Step: 14100  \tTraining loss: 0.2642611861228943\n",
      "Step: 14100  \tTraining accuracy: 0.9039936661720276\n",
      "Step: 14100  \tValid loss: 0.25637927651405334\n",
      "Step: 14200  \tTraining loss: 0.26416799426078796\n",
      "Step: 14200  \tTraining accuracy: 0.9039981961250305\n",
      "Step: 14200  \tValid loss: 0.25638362765312195\n",
      "Step: 14300  \tTraining loss: 0.26407647132873535\n",
      "Step: 14300  \tTraining accuracy: 0.9040027260780334\n",
      "Step: 14300  \tValid loss: 0.25638943910598755\n",
      "Step: 14400  \tTraining loss: 0.2639867663383484\n",
      "Step: 14400  \tTraining accuracy: 0.9040071964263916\n",
      "Step: 14400  \tValid loss: 0.25639626383781433\n",
      "Step: 14500  \tTraining loss: 0.2638987600803375\n",
      "Step: 14500  \tTraining accuracy: 0.9040133357048035\n",
      "Step: 14500  \tValid loss: 0.25640395283699036\n",
      "Step: 14600  \tTraining loss: 0.2638121545314789\n",
      "Step: 14600  \tTraining accuracy: 0.9040205478668213\n",
      "Step: 14600  \tValid loss: 0.25641247630119324\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.9040277\n",
      "Precision: 0.91903436\n",
      "Recall: 0.9705825\n",
      "F1 score: 0.9286893\n",
      "AUC: 0.727608\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.904028   0.919034  0.970582  0.928689  0.727608  0.263754      0.904037   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.256373        0.90403    0.23591      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  14667.0  \n",
      "1\n",
      "(21315, 8)\n",
      "(21315, 1)\n",
      "(11760, 8)\n",
      "(11760, 1)\n",
      "(9555, 8)\n",
      "(9555, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5308483839035034\n",
      "Step: 100  \tTraining accuracy: 0.7456251382827759\n",
      "Step: 100  \tValid loss: 0.5323190689086914\n",
      "Step: 200  \tTraining loss: 0.5080155730247498\n",
      "Step: 200  \tTraining accuracy: 0.7477988600730896\n",
      "Step: 200  \tValid loss: 0.5122761130332947\n",
      "Step: 300  \tTraining loss: 0.4790612757205963\n",
      "Step: 300  \tTraining accuracy: 0.7522120475769043\n",
      "Step: 300  \tValid loss: 0.48429223895072937\n",
      "Step: 400  \tTraining loss: 0.42741942405700684\n",
      "Step: 400  \tTraining accuracy: 0.7610669732093811\n",
      "Step: 400  \tValid loss: 0.4308953881263733\n",
      "Step: 500  \tTraining loss: 0.4028255343437195\n",
      "Step: 500  \tTraining accuracy: 0.770401656627655\n",
      "Step: 500  \tValid loss: 0.40474647283554077\n",
      "Step: 600  \tTraining loss: 0.3972308039665222\n",
      "Step: 600  \tTraining accuracy: 0.7781886458396912\n",
      "Step: 600  \tValid loss: 0.3983350694179535\n",
      "Step: 700  \tTraining loss: 0.39569175243377686\n",
      "Step: 700  \tTraining accuracy: 0.7838538885116577\n",
      "Step: 700  \tValid loss: 0.3965577781200409\n",
      "Step: 800  \tTraining loss: 0.39351972937583923\n",
      "Step: 800  \tTraining accuracy: 0.7880709767341614\n",
      "Step: 800  \tValid loss: 0.3952887952327728\n",
      "Step: 900  \tTraining loss: 0.391937792301178\n",
      "Step: 900  \tTraining accuracy: 0.791309654712677\n",
      "Step: 900  \tValid loss: 0.3935883939266205\n",
      "Step: 1000  \tTraining loss: 0.3901033401489258\n",
      "Step: 1000  \tTraining accuracy: 0.7939232587814331\n",
      "Step: 1000  \tValid loss: 0.3914259374141693\n",
      "Step: 1100  \tTraining loss: 0.38797447085380554\n",
      "Step: 1100  \tTraining accuracy: 0.7961395382881165\n",
      "Step: 1100  \tValid loss: 0.3888727128505707\n",
      "Step: 1200  \tTraining loss: 0.38579633831977844\n",
      "Step: 1200  \tTraining accuracy: 0.7979989647865295\n",
      "Step: 1200  \tValid loss: 0.38644590973854065\n",
      "Step: 1300  \tTraining loss: 0.3837737739086151\n",
      "Step: 1300  \tTraining accuracy: 0.7996078133583069\n",
      "Step: 1300  \tValid loss: 0.3842507600784302\n",
      "Step: 1400  \tTraining loss: 0.3819548189640045\n",
      "Step: 1400  \tTraining accuracy: 0.8010599613189697\n",
      "Step: 1400  \tValid loss: 0.3823641538619995\n",
      "Step: 1500  \tTraining loss: 0.3805761933326721\n",
      "Step: 1500  \tTraining accuracy: 0.8023441433906555\n",
      "Step: 1500  \tValid loss: 0.38122814893722534\n",
      "Step: 1600  \tTraining loss: 0.3794313073158264\n",
      "Step: 1600  \tTraining accuracy: 0.8035004734992981\n",
      "Step: 1600  \tValid loss: 0.3803040683269501\n",
      "Step: 1700  \tTraining loss: 0.37845054268836975\n",
      "Step: 1700  \tTraining accuracy: 0.8045607209205627\n",
      "Step: 1700  \tValid loss: 0.3794531524181366\n",
      "Step: 1800  \tTraining loss: 0.37757226824760437\n",
      "Step: 1800  \tTraining accuracy: 0.8055011630058289\n",
      "Step: 1800  \tValid loss: 0.37867048382759094\n",
      "Step: 1900  \tTraining loss: 0.3767958879470825\n",
      "Step: 1900  \tTraining accuracy: 0.8063741326332092\n",
      "Step: 1900  \tValid loss: 0.3780250549316406\n",
      "Step: 2000  \tTraining loss: 0.37605762481689453\n",
      "Step: 2000  \tTraining accuracy: 0.8072441816329956\n",
      "Step: 2000  \tValid loss: 0.37750348448753357\n",
      "Step: 2100  \tTraining loss: 0.37540560960769653\n",
      "Step: 2100  \tTraining accuracy: 0.8080706000328064\n",
      "Step: 2100  \tValid loss: 0.3771399259567261\n",
      "Step: 2200  \tTraining loss: 0.37481391429901123\n",
      "Step: 2200  \tTraining accuracy: 0.8088331818580627\n",
      "Step: 2200  \tValid loss: 0.3768315017223358\n",
      "Step: 2300  \tTraining loss: 0.37423574924468994\n",
      "Step: 2300  \tTraining accuracy: 0.8095686435699463\n",
      "Step: 2300  \tValid loss: 0.3765449821949005\n",
      "Step: 2400  \tTraining loss: 0.3736492991447449\n",
      "Step: 2400  \tTraining accuracy: 0.8102824687957764\n",
      "Step: 2400  \tValid loss: 0.3762584924697876\n",
      "Step: 2500  \tTraining loss: 0.37306860089302063\n",
      "Step: 2500  \tTraining accuracy: 0.810957133769989\n",
      "Step: 2500  \tValid loss: 0.37597960233688354\n",
      "Step: 2600  \tTraining loss: 0.37248751521110535\n",
      "Step: 2600  \tTraining accuracy: 0.8115898966789246\n",
      "Step: 2600  \tValid loss: 0.3757288157939911\n",
      "Step: 2700  \tTraining loss: 0.3718990385532379\n",
      "Step: 2700  \tTraining accuracy: 0.8121864795684814\n",
      "Step: 2700  \tValid loss: 0.3754446804523468\n",
      "Step: 2800  \tTraining loss: 0.37131115794181824\n",
      "Step: 2800  \tTraining accuracy: 0.8127464652061462\n",
      "Step: 2800  \tValid loss: 0.3751155436038971\n",
      "Step: 2900  \tTraining loss: 0.3707505762577057\n",
      "Step: 2900  \tTraining accuracy: 0.8132976293563843\n",
      "Step: 2900  \tValid loss: 0.3748745322227478\n",
      "Step: 3000  \tTraining loss: 0.37022894620895386\n",
      "Step: 3000  \tTraining accuracy: 0.8138114213943481\n",
      "Step: 3000  \tValid loss: 0.37462979555130005\n",
      "Step: 3100  \tTraining loss: 0.3696907162666321\n",
      "Step: 3100  \tTraining accuracy: 0.8142845630645752\n",
      "Step: 3100  \tValid loss: 0.37439411878585815\n",
      "Step: 3200  \tTraining loss: 0.3690641522407532\n",
      "Step: 3200  \tTraining accuracy: 0.8147269487380981\n",
      "Step: 3200  \tValid loss: 0.37401247024536133\n",
      "Step: 3300  \tTraining loss: 0.36832839250564575\n",
      "Step: 3300  \tTraining accuracy: 0.8151420950889587\n",
      "Step: 3300  \tValid loss: 0.37352484464645386\n",
      "Step: 3400  \tTraining loss: 0.3675311803817749\n",
      "Step: 3400  \tTraining accuracy: 0.8155401945114136\n",
      "Step: 3400  \tValid loss: 0.3729635775089264\n",
      "Step: 3500  \tTraining loss: 0.3667217493057251\n",
      "Step: 3500  \tTraining accuracy: 0.8159117698669434\n",
      "Step: 3500  \tValid loss: 0.37228670716285706\n",
      "Step: 3600  \tTraining loss: 0.36593687534332275\n",
      "Step: 3600  \tTraining accuracy: 0.8162670731544495\n",
      "Step: 3600  \tValid loss: 0.3716122508049011\n",
      "Step: 3700  \tTraining loss: 0.3651908338069916\n",
      "Step: 3700  \tTraining accuracy: 0.8166054487228394\n",
      "Step: 3700  \tValid loss: 0.3709295094013214\n",
      "Step: 3800  \tTraining loss: 0.36448934674263\n",
      "Step: 3800  \tTraining accuracy: 0.8169145584106445\n",
      "Step: 3800  \tValid loss: 0.37031295895576477\n",
      "Step: 3900  \tTraining loss: 0.3638348877429962\n",
      "Step: 3900  \tTraining accuracy: 0.8172130584716797\n",
      "Step: 3900  \tValid loss: 0.3697390556335449\n",
      "Step: 4000  \tTraining loss: 0.36325782537460327\n",
      "Step: 4000  \tTraining accuracy: 0.8175065517425537\n",
      "Step: 4000  \tValid loss: 0.3692530691623688\n",
      "Step: 4100  \tTraining loss: 0.36273276805877686\n",
      "Step: 4100  \tTraining accuracy: 0.8177774548530579\n",
      "Step: 4100  \tValid loss: 0.3688429594039917\n",
      "Step: 4200  \tTraining loss: 0.36224564909935\n",
      "Step: 4200  \tTraining accuracy: 0.8180460333824158\n",
      "Step: 4200  \tValid loss: 0.368463397026062\n",
      "Step: 4300  \tTraining loss: 0.36178120970726013\n",
      "Step: 4300  \tTraining accuracy: 0.8183157444000244\n",
      "Step: 4300  \tValid loss: 0.36811044812202454\n",
      "Step: 4400  \tTraining loss: 0.36134907603263855\n",
      "Step: 4400  \tTraining accuracy: 0.8185854554176331\n",
      "Step: 4400  \tValid loss: 0.36779478192329407\n",
      "Step: 4500  \tTraining loss: 0.3609490394592285\n",
      "Step: 4500  \tTraining accuracy: 0.8188441395759583\n",
      "Step: 4500  \tValid loss: 0.3675103783607483\n",
      "Step: 4600  \tTraining loss: 0.3605840802192688\n",
      "Step: 4600  \tTraining accuracy: 0.8191141486167908\n",
      "Step: 4600  \tValid loss: 0.3673117756843567\n",
      "Step: 4700  \tTraining loss: 0.3602514863014221\n",
      "Step: 4700  \tTraining accuracy: 0.8193780183792114\n",
      "Step: 4700  \tValid loss: 0.36712899804115295\n",
      "Step: 4800  \tTraining loss: 0.3599494695663452\n",
      "Step: 4800  \tTraining accuracy: 0.8196229338645935\n",
      "Step: 4800  \tValid loss: 0.36690565943717957\n",
      "Step: 4900  \tTraining loss: 0.35967618227005005\n",
      "Step: 4900  \tTraining accuracy: 0.8198519349098206\n",
      "Step: 4900  \tValid loss: 0.366715669631958\n",
      "Step: 5000  \tTraining loss: 0.35943126678466797\n",
      "Step: 5000  \tTraining accuracy: 0.8200740814208984\n",
      "Step: 5000  \tValid loss: 0.36655595898628235\n",
      "Step: 5100  \tTraining loss: 0.35921531915664673\n",
      "Step: 5100  \tTraining accuracy: 0.8203008770942688\n",
      "Step: 5100  \tValid loss: 0.3663782775402069\n",
      "Step: 5200  \tTraining loss: 0.3590204417705536\n",
      "Step: 5200  \tTraining accuracy: 0.8205257058143616\n",
      "Step: 5200  \tValid loss: 0.3662223219871521\n",
      "Step: 5300  \tTraining loss: 0.358835905790329\n",
      "Step: 5300  \tTraining accuracy: 0.8207432627677917\n",
      "Step: 5300  \tValid loss: 0.36608877778053284\n",
      "Step: 5400  \tTraining loss: 0.3586604595184326\n",
      "Step: 5400  \tTraining accuracy: 0.820959746837616\n",
      "Step: 5400  \tValid loss: 0.36595508456230164\n",
      "Step: 5500  \tTraining loss: 0.3584868311882019\n",
      "Step: 5500  \tTraining accuracy: 0.8211734294891357\n",
      "Step: 5500  \tValid loss: 0.36584144830703735\n",
      "Step: 5600  \tTraining loss: 0.3583151698112488\n",
      "Step: 5600  \tTraining accuracy: 0.8213802576065063\n",
      "Step: 5600  \tValid loss: 0.36570826172828674\n",
      "Step: 5700  \tTraining loss: 0.35813677310943604\n",
      "Step: 5700  \tTraining accuracy: 0.8215926885604858\n",
      "Step: 5700  \tValid loss: 0.36559349298477173\n",
      "Step: 5800  \tTraining loss: 0.3579539358615875\n",
      "Step: 5800  \tTraining accuracy: 0.8218038082122803\n",
      "Step: 5800  \tValid loss: 0.36550259590148926\n",
      "Step: 5900  \tTraining loss: 0.35776904225349426\n",
      "Step: 5900  \tTraining accuracy: 0.822023332118988\n",
      "Step: 5900  \tValid loss: 0.3653389513492584\n",
      "Step: 6000  \tTraining loss: 0.35759198665618896\n",
      "Step: 6000  \tTraining accuracy: 0.822245717048645\n",
      "Step: 6000  \tValid loss: 0.36525392532348633\n",
      "Step: 6100  \tTraining loss: 0.3574182689189911\n",
      "Step: 6100  \tTraining accuracy: 0.822466254234314\n",
      "Step: 6100  \tValid loss: 0.3651335537433624\n",
      "Step: 6200  \tTraining loss: 0.35725149512290955\n",
      "Step: 6200  \tTraining accuracy: 0.8226814866065979\n",
      "Step: 6200  \tValid loss: 0.36503124237060547\n",
      "Step: 6300  \tTraining loss: 0.3570888042449951\n",
      "Step: 6300  \tTraining accuracy: 0.8228883147239685\n",
      "Step: 6300  \tValid loss: 0.36494630575180054\n",
      "Step: 6400  \tTraining loss: 0.3569270670413971\n",
      "Step: 6400  \tTraining accuracy: 0.8230831027030945\n",
      "Step: 6400  \tValid loss: 0.36487799882888794\n",
      "Step: 6500  \tTraining loss: 0.35677382349967957\n",
      "Step: 6500  \tTraining accuracy: 0.8232740163803101\n",
      "Step: 6500  \tValid loss: 0.3648112118244171\n",
      "Step: 6600  \tTraining loss: 0.35662007331848145\n",
      "Step: 6600  \tTraining accuracy: 0.8234698176383972\n",
      "Step: 6600  \tValid loss: 0.3647882640361786\n",
      "Step: 6700  \tTraining loss: 0.3564750850200653\n",
      "Step: 6700  \tTraining accuracy: 0.8236636519432068\n",
      "Step: 6700  \tValid loss: 0.36462485790252686\n",
      "Step: 6800  \tTraining loss: 0.35633793473243713\n",
      "Step: 6800  \tTraining accuracy: 0.8238531351089478\n",
      "Step: 6800  \tValid loss: 0.3645443022251129\n",
      "Step: 6900  \tTraining loss: 0.3561772108078003\n",
      "Step: 6900  \tTraining accuracy: 0.8240360617637634\n",
      "Step: 6900  \tValid loss: 0.36445969343185425\n",
      "Step: 7000  \tTraining loss: 0.3559611439704895\n",
      "Step: 7000  \tTraining accuracy: 0.8242079615592957\n",
      "Step: 7000  \tValid loss: 0.3644050359725952\n",
      "Step: 7100  \tTraining loss: 0.3558129072189331\n",
      "Step: 7100  \tTraining accuracy: 0.8243726491928101\n",
      "Step: 7100  \tValid loss: 0.36428549885749817\n",
      "Step: 7200  \tTraining loss: 0.35567331314086914\n",
      "Step: 7200  \tTraining accuracy: 0.8245373964309692\n",
      "Step: 7200  \tValid loss: 0.3641877770423889\n",
      "Step: 7300  \tTraining loss: 0.3554929792881012\n",
      "Step: 7300  \tTraining accuracy: 0.8247014284133911\n",
      "Step: 7300  \tValid loss: 0.36413076519966125\n",
      "Step: 7400  \tTraining loss: 0.3553408682346344\n",
      "Step: 7400  \tTraining accuracy: 0.8248628973960876\n",
      "Step: 7400  \tValid loss: 0.3639802932739258\n",
      "Step: 7500  \tTraining loss: 0.3551892638206482\n",
      "Step: 7500  \tTraining accuracy: 0.8250285387039185\n",
      "Step: 7500  \tValid loss: 0.36384817957878113\n",
      "Step: 7600  \tTraining loss: 0.3550620377063751\n",
      "Step: 7600  \tTraining accuracy: 0.825189471244812\n",
      "Step: 7600  \tValid loss: 0.36371567845344543\n",
      "Step: 7700  \tTraining loss: 0.35495537519454956\n",
      "Step: 7700  \tTraining accuracy: 0.8253465294837952\n",
      "Step: 7700  \tValid loss: 0.36364904046058655\n",
      "Step: 7800  \tTraining loss: 0.3548530340194702\n",
      "Step: 7800  \tTraining accuracy: 0.8254961967468262\n",
      "Step: 7800  \tValid loss: 0.36350661516189575\n",
      "Step: 7900  \tTraining loss: 0.35475432872772217\n",
      "Step: 7900  \tTraining accuracy: 0.8256465196609497\n",
      "Step: 7900  \tValid loss: 0.36340123414993286\n",
      "Step: 8000  \tTraining loss: 0.3546581268310547\n",
      "Step: 8000  \tTraining accuracy: 0.8257966637611389\n",
      "Step: 8000  \tValid loss: 0.3633122146129608\n",
      "Step: 8100  \tTraining loss: 0.3545731008052826\n",
      "Step: 8100  \tTraining accuracy: 0.8259479403495789\n",
      "Step: 8100  \tValid loss: 0.3632108271121979\n",
      "Step: 8200  \tTraining loss: 0.35448455810546875\n",
      "Step: 8200  \tTraining accuracy: 0.8260952830314636\n",
      "Step: 8200  \tValid loss: 0.36317670345306396\n",
      "Step: 8300  \tTraining loss: 0.354402631521225\n",
      "Step: 8300  \tTraining accuracy: 0.8262376189231873\n",
      "Step: 8300  \tValid loss: 0.3630932867527008\n",
      "Step: 8400  \tTraining loss: 0.35432273149490356\n",
      "Step: 8400  \tTraining accuracy: 0.8263751268386841\n",
      "Step: 8400  \tValid loss: 0.36303678154945374\n",
      "Step: 8500  \tTraining loss: 0.3542449176311493\n",
      "Step: 8500  \tTraining accuracy: 0.8265124559402466\n",
      "Step: 8500  \tValid loss: 0.3629980683326721\n",
      "Step: 8600  \tTraining loss: 0.35417047142982483\n",
      "Step: 8600  \tTraining accuracy: 0.8266503810882568\n",
      "Step: 8600  \tValid loss: 0.3628990948200226\n",
      "Step: 8700  \tTraining loss: 0.3540952503681183\n",
      "Step: 8700  \tTraining accuracy: 0.8267862200737\n",
      "Step: 8700  \tValid loss: 0.36285898089408875\n",
      "Step: 8800  \tTraining loss: 0.35402217507362366\n",
      "Step: 8800  \tTraining accuracy: 0.826913058757782\n",
      "Step: 8800  \tValid loss: 0.3628264367580414\n",
      "Step: 8900  \tTraining loss: 0.3539506494998932\n",
      "Step: 8900  \tTraining accuracy: 0.8270401954650879\n",
      "Step: 8900  \tValid loss: 0.3627511262893677\n",
      "Step: 9000  \tTraining loss: 0.35388123989105225\n",
      "Step: 9000  \tTraining accuracy: 0.8271650075912476\n",
      "Step: 9000  \tValid loss: 0.3626960515975952\n",
      "Step: 9100  \tTraining loss: 0.3538139760494232\n",
      "Step: 9100  \tTraining accuracy: 0.8272883892059326\n",
      "Step: 9100  \tValid loss: 0.36264050006866455\n",
      "Step: 9200  \tTraining loss: 0.3537505567073822\n",
      "Step: 9200  \tTraining accuracy: 0.8274105787277222\n",
      "Step: 9200  \tValid loss: 0.3625583052635193\n",
      "Step: 9300  \tTraining loss: 0.35368528962135315\n",
      "Step: 9300  \tTraining accuracy: 0.8275304436683655\n",
      "Step: 9300  \tValid loss: 0.36250972747802734\n",
      "Step: 9400  \tTraining loss: 0.35361891984939575\n",
      "Step: 9400  \tTraining accuracy: 0.8276466727256775\n",
      "Step: 9400  \tValid loss: 0.362501323223114\n",
      "Step: 9500  \tTraining loss: 0.3535563051700592\n",
      "Step: 9500  \tTraining accuracy: 0.8277597427368164\n",
      "Step: 9500  \tValid loss: 0.3624670207500458\n",
      "Step: 9600  \tTraining loss: 0.3534972071647644\n",
      "Step: 9600  \tTraining accuracy: 0.827872097492218\n",
      "Step: 9600  \tValid loss: 0.3623705804347992\n",
      "Step: 9700  \tTraining loss: 0.3534344732761383\n",
      "Step: 9700  \tTraining accuracy: 0.8279809951782227\n",
      "Step: 9700  \tValid loss: 0.36236488819122314\n",
      "Step: 9800  \tTraining loss: 0.3533753752708435\n",
      "Step: 9800  \tTraining accuracy: 0.8280883431434631\n",
      "Step: 9800  \tValid loss: 0.36231207847595215\n",
      "Step: 9900  \tTraining loss: 0.3533165454864502\n",
      "Step: 9900  \tTraining accuracy: 0.8281925320625305\n",
      "Step: 9900  \tValid loss: 0.3622919023036957\n",
      "Step: 10000  \tTraining loss: 0.35325878858566284\n",
      "Step: 10000  \tTraining accuracy: 0.8282913565635681\n",
      "Step: 10000  \tValid loss: 0.36225736141204834\n",
      "Step: 10100  \tTraining loss: 0.35320109128952026\n",
      "Step: 10100  \tTraining accuracy: 0.8283898234367371\n",
      "Step: 10100  \tValid loss: 0.3621754050254822\n",
      "Step: 10200  \tTraining loss: 0.3531435430049896\n",
      "Step: 10200  \tTraining accuracy: 0.8284875154495239\n",
      "Step: 10200  \tValid loss: 0.3621492385864258\n",
      "Step: 10300  \tTraining loss: 0.35308778285980225\n",
      "Step: 10300  \tTraining accuracy: 0.8285819292068481\n",
      "Step: 10300  \tValid loss: 0.3621520698070526\n",
      "Step: 10400  \tTraining loss: 0.3530316948890686\n",
      "Step: 10400  \tTraining accuracy: 0.8286729454994202\n",
      "Step: 10400  \tValid loss: 0.36207112669944763\n",
      "Step: 10500  \tTraining loss: 0.35297656059265137\n",
      "Step: 10500  \tTraining accuracy: 0.8287602066993713\n",
      "Step: 10500  \tValid loss: 0.3620779812335968\n",
      "Step: 10600  \tTraining loss: 0.35292214155197144\n",
      "Step: 10600  \tTraining accuracy: 0.8288422226905823\n",
      "Step: 10600  \tValid loss: 0.362074077129364\n",
      "Step: 10700  \tTraining loss: 0.35286787152290344\n",
      "Step: 10700  \tTraining accuracy: 0.8289214372634888\n",
      "Step: 10700  \tValid loss: 0.3620498478412628\n",
      "Step: 10800  \tTraining loss: 0.35281190276145935\n",
      "Step: 10800  \tTraining accuracy: 0.8289991021156311\n",
      "Step: 10800  \tValid loss: 0.3619626462459564\n",
      "Step: 10900  \tTraining loss: 0.3527592122554779\n",
      "Step: 10900  \tTraining accuracy: 0.8290740847587585\n",
      "Step: 10900  \tValid loss: 0.361920028924942\n",
      "Step: 11000  \tTraining loss: 0.3527056872844696\n",
      "Step: 11000  \tTraining accuracy: 0.8291468620300293\n",
      "Step: 11000  \tValid loss: 0.36190006136894226\n",
      "Step: 11100  \tTraining loss: 0.3526533246040344\n",
      "Step: 11100  \tTraining accuracy: 0.8292174339294434\n",
      "Step: 11100  \tValid loss: 0.36192548274993896\n",
      "Step: 11200  \tTraining loss: 0.3526003658771515\n",
      "Step: 11200  \tTraining accuracy: 0.829287588596344\n",
      "Step: 11200  \tValid loss: 0.36185139417648315\n",
      "Step: 11300  \tTraining loss: 0.3525474965572357\n",
      "Step: 11300  \tTraining accuracy: 0.8293527364730835\n",
      "Step: 11300  \tValid loss: 0.36183875799179077\n",
      "Step: 11400  \tTraining loss: 0.35249561071395874\n",
      "Step: 11400  \tTraining accuracy: 0.82941734790802\n",
      "Step: 11400  \tValid loss: 0.3618125021457672\n",
      "Step: 11500  \tTraining loss: 0.3524402976036072\n",
      "Step: 11500  \tTraining accuracy: 0.829483687877655\n",
      "Step: 11500  \tValid loss: 0.36183032393455505\n",
      "Step: 11600  \tTraining loss: 0.3523857295513153\n",
      "Step: 11600  \tTraining accuracy: 0.8295507431030273\n",
      "Step: 11600  \tValid loss: 0.3617860674858093\n",
      "Step: 11700  \tTraining loss: 0.3523311913013458\n",
      "Step: 11700  \tTraining accuracy: 0.8296146392822266\n",
      "Step: 11700  \tValid loss: 0.36172762513160706\n",
      "Step: 11800  \tTraining loss: 0.3522709310054779\n",
      "Step: 11800  \tTraining accuracy: 0.8296760320663452\n",
      "Step: 11800  \tValid loss: 0.36175620555877686\n",
      "Step: 11900  \tTraining loss: 0.35221531987190247\n",
      "Step: 11900  \tTraining accuracy: 0.8297399282455444\n",
      "Step: 11900  \tValid loss: 0.3617527186870575\n",
      "Step: 12000  \tTraining loss: 0.35215890407562256\n",
      "Step: 12000  \tTraining accuracy: 0.8298022150993347\n",
      "Step: 12000  \tValid loss: 0.3617108166217804\n",
      "Step: 12100  \tTraining loss: 0.35208776593208313\n",
      "Step: 12100  \tTraining accuracy: 0.8298647999763489\n",
      "Step: 12100  \tValid loss: 0.3616524338722229\n",
      "Step: 12200  \tTraining loss: 0.3520191013813019\n",
      "Step: 12200  \tTraining accuracy: 0.829927921295166\n",
      "Step: 12200  \tValid loss: 0.3616301119327545\n",
      "Step: 12300  \tTraining loss: 0.35188981890678406\n",
      "Step: 12300  \tTraining accuracy: 0.8299890160560608\n",
      "Step: 12300  \tValid loss: 0.36150091886520386\n",
      "Step: 12400  \tTraining loss: 0.35179346799850464\n",
      "Step: 12400  \tTraining accuracy: 0.8300472497940063\n",
      "Step: 12400  \tValid loss: 0.36143723130226135\n",
      "Step: 12500  \tTraining loss: 0.3517136573791504\n",
      "Step: 12500  \tTraining accuracy: 0.8301045894622803\n",
      "Step: 12500  \tValid loss: 0.3613542914390564\n",
      "Step: 12600  \tTraining loss: 0.35164403915405273\n",
      "Step: 12600  \tTraining accuracy: 0.8301615118980408\n",
      "Step: 12600  \tValid loss: 0.3613279461860657\n",
      "Step: 12700  \tTraining loss: 0.35157808661460876\n",
      "Step: 12700  \tTraining accuracy: 0.8302210569381714\n",
      "Step: 12700  \tValid loss: 0.36123594641685486\n",
      "Step: 12800  \tTraining loss: 0.3515177369117737\n",
      "Step: 12800  \tTraining accuracy: 0.8302785754203796\n",
      "Step: 12800  \tValid loss: 0.3612169623374939\n",
      "Step: 12900  \tTraining loss: 0.35146066546440125\n",
      "Step: 12900  \tTraining accuracy: 0.8303337693214417\n",
      "Step: 12900  \tValid loss: 0.3612114489078522\n",
      "Step: 13000  \tTraining loss: 0.3514048457145691\n",
      "Step: 13000  \tTraining accuracy: 0.8303897380828857\n",
      "Step: 13000  \tValid loss: 0.3611549735069275\n",
      "Step: 13100  \tTraining loss: 0.35135164856910706\n",
      "Step: 13100  \tTraining accuracy: 0.8304453492164612\n",
      "Step: 13100  \tValid loss: 0.3611307740211487\n",
      "Step: 13200  \tTraining loss: 0.35129785537719727\n",
      "Step: 13200  \tTraining accuracy: 0.8304990530014038\n",
      "Step: 13200  \tValid loss: 0.36110690236091614\n",
      "Step: 13300  \tTraining loss: 0.35124671459198\n",
      "Step: 13300  \tTraining accuracy: 0.8305521607398987\n",
      "Step: 13300  \tValid loss: 0.36111247539520264\n",
      "Step: 13400  \tTraining loss: 0.35119667649269104\n",
      "Step: 13400  \tTraining accuracy: 0.8306030035018921\n",
      "Step: 13400  \tValid loss: 0.3611052334308624\n",
      "Step: 13500  \tTraining loss: 0.35115063190460205\n",
      "Step: 13500  \tTraining accuracy: 0.8306536674499512\n",
      "Step: 13500  \tValid loss: 0.36114227771759033\n",
      "Step: 13600  \tTraining loss: 0.3511020839214325\n",
      "Step: 13600  \tTraining accuracy: 0.830702543258667\n",
      "Step: 13600  \tValid loss: 0.3610590398311615\n",
      "Step: 13700  \tTraining loss: 0.351054310798645\n",
      "Step: 13700  \tTraining accuracy: 0.830751895904541\n",
      "Step: 13700  \tValid loss: 0.3611025810241699\n",
      "Step: 13800  \tTraining loss: 0.3510103225708008\n",
      "Step: 13800  \tTraining accuracy: 0.8307991623878479\n",
      "Step: 13800  \tValid loss: 0.3610496520996094\n",
      "Step: 13900  \tTraining loss: 0.3509652316570282\n",
      "Step: 13900  \tTraining accuracy: 0.830846905708313\n",
      "Step: 13900  \tValid loss: 0.36108705401420593\n",
      "Step: 14000  \tTraining loss: 0.35091447830200195\n",
      "Step: 14000  \tTraining accuracy: 0.8308950066566467\n",
      "Step: 14000  \tValid loss: 0.36103010177612305\n",
      "Step: 14100  \tTraining loss: 0.3508675992488861\n",
      "Step: 14100  \tTraining accuracy: 0.8309400677680969\n",
      "Step: 14100  \tValid loss: 0.3609894812107086\n",
      "Step: 14200  \tTraining loss: 0.35082265734672546\n",
      "Step: 14200  \tTraining accuracy: 0.8309850096702576\n",
      "Step: 14200  \tValid loss: 0.36094290018081665\n",
      "Step: 14300  \tTraining loss: 0.3507785201072693\n",
      "Step: 14300  \tTraining accuracy: 0.8310297727584839\n",
      "Step: 14300  \tValid loss: 0.360973984003067\n",
      "Step: 14400  \tTraining loss: 0.3507362902164459\n",
      "Step: 14400  \tTraining accuracy: 0.8310762643814087\n",
      "Step: 14400  \tValid loss: 0.36091700196266174\n",
      "Step: 14500  \tTraining loss: 0.3506929278373718\n",
      "Step: 14500  \tTraining accuracy: 0.8311219215393066\n",
      "Step: 14500  \tValid loss: 0.3609391748905182\n",
      "Step: 14600  \tTraining loss: 0.35064926743507385\n",
      "Step: 14600  \tTraining accuracy: 0.8311672210693359\n",
      "Step: 14600  \tValid loss: 0.3608701527118683\n",
      "Step: 14700  \tTraining loss: 0.3506074845790863\n",
      "Step: 14700  \tTraining accuracy: 0.8312126398086548\n",
      "Step: 14700  \tValid loss: 0.3608672618865967\n",
      "Step: 14800  \tTraining loss: 0.3505675494670868\n",
      "Step: 14800  \tTraining accuracy: 0.8312572240829468\n",
      "Step: 14800  \tValid loss: 0.3608458936214447\n",
      "Step: 14900  \tTraining loss: 0.35052984952926636\n",
      "Step: 14900  \tTraining accuracy: 0.8313019871711731\n",
      "Step: 14900  \tValid loss: 0.36087778210639954\n",
      "Step: 15000  \tTraining loss: 0.3504912257194519\n",
      "Step: 15000  \tTraining accuracy: 0.8313461542129517\n",
      "Step: 15000  \tValid loss: 0.36085009574890137\n",
      "Step: 15100  \tTraining loss: 0.3504534065723419\n",
      "Step: 15100  \tTraining accuracy: 0.8313900828361511\n",
      "Step: 15100  \tValid loss: 0.36083394289016724\n",
      "Step: 15200  \tTraining loss: 0.3504156768321991\n",
      "Step: 15200  \tTraining accuracy: 0.8314343690872192\n",
      "Step: 15200  \tValid loss: 0.3608088791370392\n",
      "Step: 15300  \tTraining loss: 0.35038089752197266\n",
      "Step: 15300  \tTraining accuracy: 0.831479549407959\n",
      "Step: 15300  \tValid loss: 0.36077383160591125\n",
      "Step: 15400  \tTraining loss: 0.3503512144088745\n",
      "Step: 15400  \tTraining accuracy: 0.8315226435661316\n",
      "Step: 15400  \tValid loss: 0.3608454763889313\n",
      "Step: 15500  \tTraining loss: 0.35031187534332275\n",
      "Step: 15500  \tTraining accuracy: 0.8315659761428833\n",
      "Step: 15500  \tValid loss: 0.3607656955718994\n",
      "Step: 15600  \tTraining loss: 0.3502788543701172\n",
      "Step: 15600  \tTraining accuracy: 0.8316085338592529\n",
      "Step: 15600  \tValid loss: 0.36072707176208496\n",
      "Step: 15700  \tTraining loss: 0.3502381443977356\n",
      "Step: 15700  \tTraining accuracy: 0.8316510319709778\n",
      "Step: 15700  \tValid loss: 0.3607572317123413\n",
      "Step: 15800  \tTraining loss: 0.3501967787742615\n",
      "Step: 15800  \tTraining accuracy: 0.8316937685012817\n",
      "Step: 15800  \tValid loss: 0.3607126772403717\n",
      "Step: 15900  \tTraining loss: 0.3501499891281128\n",
      "Step: 15900  \tTraining accuracy: 0.8317351937294006\n",
      "Step: 15900  \tValid loss: 0.36070412397384644\n",
      "Step: 16000  \tTraining loss: 0.35011065006256104\n",
      "Step: 16000  \tTraining accuracy: 0.8317760825157166\n",
      "Step: 16000  \tValid loss: 0.3607565462589264\n",
      "Step: 16100  \tTraining loss: 0.35006988048553467\n",
      "Step: 16100  \tTraining accuracy: 0.8318164944648743\n",
      "Step: 16100  \tValid loss: 0.36071059107780457\n",
      "Step: 16200  \tTraining loss: 0.3500213921070099\n",
      "Step: 16200  \tTraining accuracy: 0.8318559527397156\n",
      "Step: 16200  \tValid loss: 0.3606937825679779\n",
      "Step: 16300  \tTraining loss: 0.3499731421470642\n",
      "Step: 16300  \tTraining accuracy: 0.8318954706192017\n",
      "Step: 16300  \tValid loss: 0.36068639159202576\n",
      "Step: 16400  \tTraining loss: 0.34987926483154297\n",
      "Step: 16400  \tTraining accuracy: 0.8319331407546997\n",
      "Step: 16400  \tValid loss: 0.3607608675956726\n",
      "Step: 16500  \tTraining loss: 0.34982576966285706\n",
      "Step: 16500  \tTraining accuracy: 0.8319693207740784\n",
      "Step: 16500  \tValid loss: 0.3608076274394989\n",
      "Step: 16600  \tTraining loss: 0.3497844338417053\n",
      "Step: 16600  \tTraining accuracy: 0.8320046067237854\n",
      "Step: 16600  \tValid loss: 0.36080002784729004\n",
      "Step: 16700  \tTraining loss: 0.3497471511363983\n",
      "Step: 16700  \tTraining accuracy: 0.8320392370223999\n",
      "Step: 16700  \tValid loss: 0.3608294725418091\n",
      "Step: 16800  \tTraining loss: 0.3497087359428406\n",
      "Step: 16800  \tTraining accuracy: 0.8320731520652771\n",
      "Step: 16800  \tValid loss: 0.3608522117137909\n",
      "Step: 16900  \tTraining loss: 0.3496745228767395\n",
      "Step: 16900  \tTraining accuracy: 0.8321074843406677\n",
      "Step: 16900  \tValid loss: 0.36078736186027527\n",
      "Step: 17000  \tTraining loss: 0.34963929653167725\n",
      "Step: 17000  \tTraining accuracy: 0.8321425318717957\n",
      "Step: 17000  \tValid loss: 0.3608117401599884\n",
      "Step: 17100  \tTraining loss: 0.3496076464653015\n",
      "Step: 17100  \tTraining accuracy: 0.8321767449378967\n",
      "Step: 17100  \tValid loss: 0.3607757091522217\n",
      "Step: 17200  \tTraining loss: 0.3495774269104004\n",
      "Step: 17200  \tTraining accuracy: 0.8322111368179321\n",
      "Step: 17200  \tValid loss: 0.36075231432914734\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.83224535\n",
      "Precision: 0.8708775\n",
      "Recall: 0.92046815\n",
      "F1 score: 0.8552848\n",
      "AUC: 0.7602155\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.832245   0.870878  0.920468  0.855285  0.760216  0.349557      0.832226   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.360654       0.832218   0.340769      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  17248.0  \n",
      "2\n",
      "(3770, 8)\n",
      "(3770, 1)\n",
      "(2080, 8)\n",
      "(2080, 1)\n",
      "(1690, 8)\n",
      "(1690, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5979021787643433\n",
      "Step: 100  \tTraining accuracy: 0.66366046667099\n",
      "Step: 100  \tValid loss: 0.5855364203453064\n",
      "Step: 200  \tTraining loss: 0.578339695930481\n",
      "Step: 200  \tTraining accuracy: 0.6680813431739807\n",
      "Step: 200  \tValid loss: 0.5642643570899963\n",
      "Step: 300  \tTraining loss: 0.5619208216667175\n",
      "Step: 300  \tTraining accuracy: 0.6778249144554138\n",
      "Step: 300  \tValid loss: 0.5463973879814148\n",
      "Step: 400  \tTraining loss: 0.5540898442268372\n",
      "Step: 400  \tTraining accuracy: 0.6863206028938293\n",
      "Step: 400  \tValid loss: 0.5390844345092773\n",
      "Step: 500  \tTraining loss: 0.5499165058135986\n",
      "Step: 500  \tTraining accuracy: 0.691895067691803\n",
      "Step: 500  \tValid loss: 0.5347586870193481\n",
      "Step: 600  \tTraining loss: 0.5469805002212524\n",
      "Step: 600  \tTraining accuracy: 0.6959971189498901\n",
      "Step: 600  \tValid loss: 0.5325190424919128\n",
      "Step: 700  \tTraining loss: 0.5441792607307434\n",
      "Step: 700  \tTraining accuracy: 0.6991634368896484\n",
      "Step: 700  \tValid loss: 0.530225396156311\n",
      "Step: 800  \tTraining loss: 0.5418705940246582\n",
      "Step: 800  \tTraining accuracy: 0.7015738487243652\n",
      "Step: 800  \tValid loss: 0.5277726650238037\n",
      "Step: 900  \tTraining loss: 0.539789080619812\n",
      "Step: 900  \tTraining accuracy: 0.7033390402793884\n",
      "Step: 900  \tValid loss: 0.5253341197967529\n",
      "Step: 1000  \tTraining loss: 0.5378400683403015\n",
      "Step: 1000  \tTraining accuracy: 0.7046489119529724\n",
      "Step: 1000  \tValid loss: 0.5230484008789062\n",
      "Step: 1100  \tTraining loss: 0.5358409285545349\n",
      "Step: 1100  \tTraining accuracy: 0.706403911113739\n",
      "Step: 1100  \tValid loss: 0.5212449431419373\n",
      "Step: 1200  \tTraining loss: 0.5336657762527466\n",
      "Step: 1200  \tTraining accuracy: 0.7080959677696228\n",
      "Step: 1200  \tValid loss: 0.518798828125\n",
      "Step: 1300  \tTraining loss: 0.5314403772354126\n",
      "Step: 1300  \tTraining accuracy: 0.7094111442565918\n",
      "Step: 1300  \tValid loss: 0.5160771012306213\n",
      "Step: 1400  \tTraining loss: 0.5292994379997253\n",
      "Step: 1400  \tTraining accuracy: 0.7108065485954285\n",
      "Step: 1400  \tValid loss: 0.5134057998657227\n",
      "Step: 1500  \tTraining loss: 0.5273820161819458\n",
      "Step: 1500  \tTraining accuracy: 0.7120186686515808\n",
      "Step: 1500  \tValid loss: 0.5113304257392883\n",
      "Step: 1600  \tTraining loss: 0.5257860422134399\n",
      "Step: 1600  \tTraining accuracy: 0.7130658030509949\n",
      "Step: 1600  \tValid loss: 0.5099310874938965\n",
      "Step: 1700  \tTraining loss: 0.5245062708854675\n",
      "Step: 1700  \tTraining accuracy: 0.7142753601074219\n",
      "Step: 1700  \tValid loss: 0.5088092684745789\n",
      "Step: 1800  \tTraining loss: 0.5234630703926086\n",
      "Step: 1800  \tTraining accuracy: 0.7153618931770325\n",
      "Step: 1800  \tValid loss: 0.5079157948493958\n",
      "Step: 1900  \tTraining loss: 0.5225757956504822\n",
      "Step: 1900  \tTraining accuracy: 0.7162879109382629\n",
      "Step: 1900  \tValid loss: 0.5071729421615601\n",
      "Step: 2000  \tTraining loss: 0.521756112575531\n",
      "Step: 2000  \tTraining accuracy: 0.7172345519065857\n",
      "Step: 2000  \tValid loss: 0.5066143274307251\n",
      "Step: 2100  \tTraining loss: 0.520916759967804\n",
      "Step: 2100  \tTraining accuracy: 0.7181147933006287\n",
      "Step: 2100  \tValid loss: 0.5065430998802185\n",
      "Step: 2200  \tTraining loss: 0.5201300978660583\n",
      "Step: 2200  \tTraining accuracy: 0.718925416469574\n",
      "Step: 2200  \tValid loss: 0.5061373114585876\n",
      "Step: 2300  \tTraining loss: 0.519412100315094\n",
      "Step: 2300  \tTraining accuracy: 0.7197701334953308\n",
      "Step: 2300  \tValid loss: 0.5056890249252319\n",
      "Step: 2400  \tTraining loss: 0.518663227558136\n",
      "Step: 2400  \tTraining accuracy: 0.7206388711929321\n",
      "Step: 2400  \tValid loss: 0.5053887367248535\n",
      "Step: 2500  \tTraining loss: 0.5179175138473511\n",
      "Step: 2500  \tTraining accuracy: 0.7215179204940796\n",
      "Step: 2500  \tValid loss: 0.5050491094589233\n",
      "Step: 2600  \tTraining loss: 0.5172064900398254\n",
      "Step: 2600  \tTraining accuracy: 0.7223331928253174\n",
      "Step: 2600  \tValid loss: 0.5045215487480164\n",
      "Step: 2700  \tTraining loss: 0.5165261626243591\n",
      "Step: 2700  \tTraining accuracy: 0.7231770157814026\n",
      "Step: 2700  \tValid loss: 0.5040462017059326\n",
      "Step: 2800  \tTraining loss: 0.5156873464584351\n",
      "Step: 2800  \tTraining accuracy: 0.7239691615104675\n",
      "Step: 2800  \tValid loss: 0.5030788779258728\n",
      "Step: 2900  \tTraining loss: 0.5150041580200195\n",
      "Step: 2900  \tTraining accuracy: 0.7247196435928345\n",
      "Step: 2900  \tValid loss: 0.5026780366897583\n",
      "Step: 3000  \tTraining loss: 0.5143737196922302\n",
      "Step: 3000  \tTraining accuracy: 0.7254192233085632\n",
      "Step: 3000  \tValid loss: 0.502161979675293\n",
      "Step: 3100  \tTraining loss: 0.5137778520584106\n",
      "Step: 3100  \tTraining accuracy: 0.7260294556617737\n",
      "Step: 3100  \tValid loss: 0.5018623471260071\n",
      "Step: 3200  \tTraining loss: 0.513219952583313\n",
      "Step: 3200  \tTraining accuracy: 0.7265715003013611\n",
      "Step: 3200  \tValid loss: 0.5015225410461426\n",
      "Step: 3300  \tTraining loss: 0.5126876831054688\n",
      "Step: 3300  \tTraining accuracy: 0.7270638942718506\n",
      "Step: 3300  \tValid loss: 0.5011430978775024\n",
      "Step: 3400  \tTraining loss: 0.512167751789093\n",
      "Step: 3400  \tTraining accuracy: 0.7275109887123108\n",
      "Step: 3400  \tValid loss: 0.5006887316703796\n",
      "Step: 3500  \tTraining loss: 0.5116663575172424\n",
      "Step: 3500  \tTraining accuracy: 0.7279629707336426\n",
      "Step: 3500  \tValid loss: 0.5004181265830994\n",
      "Step: 3600  \tTraining loss: 0.5112051963806152\n",
      "Step: 3600  \tTraining accuracy: 0.7283782362937927\n",
      "Step: 3600  \tValid loss: 0.500140368938446\n",
      "Step: 3700  \tTraining loss: 0.5107346773147583\n",
      "Step: 3700  \tTraining accuracy: 0.7287889122962952\n",
      "Step: 3700  \tValid loss: 0.4999825060367584\n",
      "Step: 3800  \tTraining loss: 0.5102996230125427\n",
      "Step: 3800  \tTraining accuracy: 0.7292060256004333\n",
      "Step: 3800  \tValid loss: 0.49980592727661133\n",
      "Step: 3900  \tTraining loss: 0.5098954439163208\n",
      "Step: 3900  \tTraining accuracy: 0.7296393513679504\n",
      "Step: 3900  \tValid loss: 0.4996037483215332\n",
      "Step: 4000  \tTraining loss: 0.5094994902610779\n",
      "Step: 4000  \tTraining accuracy: 0.7300540804862976\n",
      "Step: 4000  \tValid loss: 0.4994450509548187\n",
      "Step: 4100  \tTraining loss: 0.5091280937194824\n",
      "Step: 4100  \tTraining accuracy: 0.7304417490959167\n",
      "Step: 4100  \tValid loss: 0.49924221634864807\n",
      "Step: 4200  \tTraining loss: 0.5087729096412659\n",
      "Step: 4200  \tTraining accuracy: 0.7307692170143127\n",
      "Step: 4200  \tValid loss: 0.49900418519973755\n",
      "Step: 4300  \tTraining loss: 0.5083585381507874\n",
      "Step: 4300  \tTraining accuracy: 0.7310625910758972\n",
      "Step: 4300  \tValid loss: 0.4988064169883728\n",
      "Step: 4400  \tTraining loss: 0.5080059170722961\n",
      "Step: 4400  \tTraining accuracy: 0.7313515543937683\n",
      "Step: 4400  \tValid loss: 0.49852657318115234\n",
      "Step: 4500  \tTraining loss: 0.5076804161071777\n",
      "Step: 4500  \tTraining accuracy: 0.7316156625747681\n",
      "Step: 4500  \tValid loss: 0.4983634948730469\n",
      "Step: 4600  \tTraining loss: 0.5072625875473022\n",
      "Step: 4600  \tTraining accuracy: 0.7318768501281738\n",
      "Step: 4600  \tValid loss: 0.4977697432041168\n",
      "Step: 4700  \tTraining loss: 0.5068775415420532\n",
      "Step: 4700  \tTraining accuracy: 0.7321325540542603\n",
      "Step: 4700  \tValid loss: 0.4972406327724457\n",
      "Step: 4800  \tTraining loss: 0.5065575242042542\n",
      "Step: 4800  \tTraining accuracy: 0.7323942184448242\n",
      "Step: 4800  \tValid loss: 0.49693596363067627\n",
      "Step: 4900  \tTraining loss: 0.5062569975852966\n",
      "Step: 4900  \tTraining accuracy: 0.7326341867446899\n",
      "Step: 4900  \tValid loss: 0.4967063367366791\n",
      "Step: 5000  \tTraining loss: 0.5059401392936707\n",
      "Step: 5000  \tTraining accuracy: 0.7328617572784424\n",
      "Step: 5000  \tValid loss: 0.49644845724105835\n",
      "Step: 5100  \tTraining loss: 0.5056097507476807\n",
      "Step: 5100  \tTraining accuracy: 0.7330803275108337\n",
      "Step: 5100  \tValid loss: 0.49617379903793335\n",
      "Step: 5200  \tTraining loss: 0.5052747130393982\n",
      "Step: 5200  \tTraining accuracy: 0.7332775592803955\n",
      "Step: 5200  \tValid loss: 0.49593958258628845\n",
      "Step: 5300  \tTraining loss: 0.504950225353241\n",
      "Step: 5300  \tTraining accuracy: 0.7334445118904114\n",
      "Step: 5300  \tValid loss: 0.49570193886756897\n",
      "Step: 5400  \tTraining loss: 0.504626989364624\n",
      "Step: 5400  \tTraining accuracy: 0.7336225509643555\n",
      "Step: 5400  \tValid loss: 0.4954768121242523\n",
      "Step: 5500  \tTraining loss: 0.5042446851730347\n",
      "Step: 5500  \tTraining accuracy: 0.7337892055511475\n",
      "Step: 5500  \tValid loss: 0.49536561965942383\n",
      "Step: 5600  \tTraining loss: 0.503883421421051\n",
      "Step: 5600  \tTraining accuracy: 0.733971357345581\n",
      "Step: 5600  \tValid loss: 0.4952297508716583\n",
      "Step: 5700  \tTraining loss: 0.503511369228363\n",
      "Step: 5700  \tTraining accuracy: 0.7341400384902954\n",
      "Step: 5700  \tValid loss: 0.49500027298927307\n",
      "Step: 5800  \tTraining loss: 0.5031452178955078\n",
      "Step: 5800  \tTraining accuracy: 0.7342636585235596\n",
      "Step: 5800  \tValid loss: 0.49481648206710815\n",
      "Step: 5900  \tTraining loss: 0.5027977824211121\n",
      "Step: 5900  \tTraining accuracy: 0.734378457069397\n",
      "Step: 5900  \tValid loss: 0.4945922791957855\n",
      "Step: 6000  \tTraining loss: 0.5024685263633728\n",
      "Step: 6000  \tTraining accuracy: 0.7344872355461121\n",
      "Step: 6000  \tValid loss: 0.4944034218788147\n",
      "Step: 6100  \tTraining loss: 0.5021618008613586\n",
      "Step: 6100  \tTraining accuracy: 0.7345857620239258\n",
      "Step: 6100  \tValid loss: 0.494255006313324\n",
      "Step: 6200  \tTraining loss: 0.501876175403595\n",
      "Step: 6200  \tTraining accuracy: 0.7346660494804382\n",
      "Step: 6200  \tValid loss: 0.49408668279647827\n",
      "Step: 6300  \tTraining loss: 0.5016045570373535\n",
      "Step: 6300  \tTraining accuracy: 0.7347501516342163\n",
      "Step: 6300  \tValid loss: 0.49391210079193115\n",
      "Step: 6400  \tTraining loss: 0.5013394355773926\n",
      "Step: 6400  \tTraining accuracy: 0.7348482608795166\n",
      "Step: 6400  \tValid loss: 0.49375611543655396\n",
      "Step: 6500  \tTraining loss: 0.5010820627212524\n",
      "Step: 6500  \tTraining accuracy: 0.734941303730011\n",
      "Step: 6500  \tValid loss: 0.4935868978500366\n",
      "Step: 6600  \tTraining loss: 0.5008191466331482\n",
      "Step: 6600  \tTraining accuracy: 0.7350355386734009\n",
      "Step: 6600  \tValid loss: 0.4934271574020386\n",
      "Step: 6700  \tTraining loss: 0.5005677342414856\n",
      "Step: 6700  \tTraining accuracy: 0.7351369261741638\n",
      "Step: 6700  \tValid loss: 0.4932838976383209\n",
      "Step: 6800  \tTraining loss: 0.5003247261047363\n",
      "Step: 6800  \tTraining accuracy: 0.7352490425109863\n",
      "Step: 6800  \tValid loss: 0.4931340217590332\n",
      "Step: 6900  \tTraining loss: 0.5000928640365601\n",
      "Step: 6900  \tTraining accuracy: 0.7353637218475342\n",
      "Step: 6900  \tValid loss: 0.49297332763671875\n",
      "Step: 7000  \tTraining loss: 0.499866783618927\n",
      "Step: 7000  \tTraining accuracy: 0.7354846000671387\n",
      "Step: 7000  \tValid loss: 0.4928779602050781\n",
      "Step: 7100  \tTraining loss: 0.49964696168899536\n",
      "Step: 7100  \tTraining accuracy: 0.7356133460998535\n",
      "Step: 7100  \tValid loss: 0.4927810728549957\n",
      "Step: 7200  \tTraining loss: 0.49943307042121887\n",
      "Step: 7200  \tTraining accuracy: 0.7357366681098938\n",
      "Step: 7200  \tValid loss: 0.49267590045928955\n",
      "Step: 7300  \tTraining loss: 0.4991902709007263\n",
      "Step: 7300  \tTraining accuracy: 0.7358474135398865\n",
      "Step: 7300  \tValid loss: 0.4925122857093811\n",
      "Step: 7400  \tTraining loss: 0.49896249175071716\n",
      "Step: 7400  \tTraining accuracy: 0.7359353303909302\n",
      "Step: 7400  \tValid loss: 0.4923373758792877\n",
      "Step: 7500  \tTraining loss: 0.49874866008758545\n",
      "Step: 7500  \tTraining accuracy: 0.736040472984314\n",
      "Step: 7500  \tValid loss: 0.4921712577342987\n",
      "Step: 7600  \tTraining loss: 0.498544305562973\n",
      "Step: 7600  \tTraining accuracy: 0.7361375093460083\n",
      "Step: 7600  \tValid loss: 0.4920007884502411\n",
      "Step: 7700  \tTraining loss: 0.4983459413051605\n",
      "Step: 7700  \tTraining accuracy: 0.7362441420555115\n",
      "Step: 7700  \tValid loss: 0.49186617136001587\n",
      "Step: 7800  \tTraining loss: 0.498151570558548\n",
      "Step: 7800  \tTraining accuracy: 0.7363549470901489\n",
      "Step: 7800  \tValid loss: 0.49173253774642944\n",
      "Step: 7900  \tTraining loss: 0.49794191122055054\n",
      "Step: 7900  \tTraining accuracy: 0.7364526987075806\n",
      "Step: 7900  \tValid loss: 0.49151429533958435\n",
      "Step: 8000  \tTraining loss: 0.4977469742298126\n",
      "Step: 8000  \tTraining accuracy: 0.7365313768386841\n",
      "Step: 8000  \tValid loss: 0.4913262724876404\n",
      "Step: 8100  \tTraining loss: 0.4975472390651703\n",
      "Step: 8100  \tTraining accuracy: 0.7366047501564026\n",
      "Step: 8100  \tValid loss: 0.49113523960113525\n",
      "Step: 8200  \tTraining loss: 0.4972984492778778\n",
      "Step: 8200  \tTraining accuracy: 0.7366698384284973\n",
      "Step: 8200  \tValid loss: 0.4908561110496521\n",
      "Step: 8300  \tTraining loss: 0.49710461497306824\n",
      "Step: 8300  \tTraining accuracy: 0.7367349863052368\n",
      "Step: 8300  \tValid loss: 0.4906197488307953\n",
      "Step: 8400  \tTraining loss: 0.49691441655158997\n",
      "Step: 8400  \tTraining accuracy: 0.7368017435073853\n",
      "Step: 8400  \tValid loss: 0.4904029965400696\n",
      "Step: 8500  \tTraining loss: 0.49672573804855347\n",
      "Step: 8500  \tTraining accuracy: 0.7368731498718262\n",
      "Step: 8500  \tValid loss: 0.4902391731739044\n",
      "Step: 8600  \tTraining loss: 0.4965420365333557\n",
      "Step: 8600  \tTraining accuracy: 0.7369460463523865\n",
      "Step: 8600  \tValid loss: 0.4900631010532379\n",
      "Step: 8700  \tTraining loss: 0.49636462330818176\n",
      "Step: 8700  \tTraining accuracy: 0.7370095252990723\n",
      "Step: 8700  \tValid loss: 0.4898611605167389\n",
      "Step: 8800  \tTraining loss: 0.4961841106414795\n",
      "Step: 8800  \tTraining accuracy: 0.7370700836181641\n",
      "Step: 8800  \tValid loss: 0.4897156357765198\n",
      "Step: 8900  \tTraining loss: 0.4960016906261444\n",
      "Step: 8900  \tTraining accuracy: 0.73714280128479\n",
      "Step: 8900  \tValid loss: 0.4895564913749695\n",
      "Step: 9000  \tTraining loss: 0.495816707611084\n",
      "Step: 9000  \tTraining accuracy: 0.7372123599052429\n",
      "Step: 9000  \tValid loss: 0.48941826820373535\n",
      "Step: 9100  \tTraining loss: 0.49563854932785034\n",
      "Step: 9100  \tTraining accuracy: 0.7372774481773376\n",
      "Step: 9100  \tValid loss: 0.4892171323299408\n",
      "Step: 9200  \tTraining loss: 0.495443195104599\n",
      "Step: 9200  \tTraining accuracy: 0.7373440265655518\n",
      "Step: 9200  \tValid loss: 0.4890788793563843\n",
      "Step: 9300  \tTraining loss: 0.49522677063941956\n",
      "Step: 9300  \tTraining accuracy: 0.7374091148376465\n",
      "Step: 9300  \tValid loss: 0.4889085292816162\n",
      "Step: 9400  \tTraining loss: 0.49504750967025757\n",
      "Step: 9400  \tTraining accuracy: 0.7374728918075562\n",
      "Step: 9400  \tValid loss: 0.4887234568595886\n",
      "Step: 9500  \tTraining loss: 0.4948712885379791\n",
      "Step: 9500  \tTraining accuracy: 0.7375324368476868\n",
      "Step: 9500  \tValid loss: 0.48859578371047974\n",
      "Step: 9600  \tTraining loss: 0.4946984052658081\n",
      "Step: 9600  \tTraining accuracy: 0.7375810742378235\n",
      "Step: 9600  \tValid loss: 0.48846468329429626\n",
      "Step: 9700  \tTraining loss: 0.4945248067378998\n",
      "Step: 9700  \tTraining accuracy: 0.7376382946968079\n",
      "Step: 9700  \tValid loss: 0.4882916808128357\n",
      "Step: 9800  \tTraining loss: 0.4943508505821228\n",
      "Step: 9800  \tTraining accuracy: 0.7376970648765564\n",
      "Step: 9800  \tValid loss: 0.48813846707344055\n",
      "Step: 9900  \tTraining loss: 0.49417853355407715\n",
      "Step: 9900  \tTraining accuracy: 0.7377519607543945\n",
      "Step: 9900  \tValid loss: 0.4879889488220215\n",
      "Step: 10000  \tTraining loss: 0.4939981997013092\n",
      "Step: 10000  \tTraining accuracy: 0.7378084063529968\n",
      "Step: 10000  \tValid loss: 0.4878242313861847\n",
      "Step: 10100  \tTraining loss: 0.49381911754608154\n",
      "Step: 10100  \tTraining accuracy: 0.7378729581832886\n",
      "Step: 10100  \tValid loss: 0.4876795709133148\n",
      "Step: 10200  \tTraining loss: 0.4936375319957733\n",
      "Step: 10200  \tTraining accuracy: 0.7379466891288757\n",
      "Step: 10200  \tValid loss: 0.4875069856643677\n",
      "Step: 10300  \tTraining loss: 0.4934542775154114\n",
      "Step: 10300  \tTraining accuracy: 0.7380229234695435\n",
      "Step: 10300  \tValid loss: 0.48735523223876953\n",
      "Step: 10400  \tTraining loss: 0.49326783418655396\n",
      "Step: 10400  \tTraining accuracy: 0.7381066083908081\n",
      "Step: 10400  \tValid loss: 0.487190306186676\n",
      "Step: 10500  \tTraining loss: 0.493080198764801\n",
      "Step: 10500  \tTraining accuracy: 0.7381886839866638\n",
      "Step: 10500  \tValid loss: 0.48704206943511963\n",
      "Step: 10600  \tTraining loss: 0.49288836121559143\n",
      "Step: 10600  \tTraining accuracy: 0.7382628917694092\n",
      "Step: 10600  \tValid loss: 0.4868775010108948\n",
      "Step: 10700  \tTraining loss: 0.4926741421222687\n",
      "Step: 10700  \tTraining accuracy: 0.7383432388305664\n",
      "Step: 10700  \tValid loss: 0.486571729183197\n",
      "Step: 10800  \tTraining loss: 0.49247241020202637\n",
      "Step: 10800  \tTraining accuracy: 0.7384257316589355\n",
      "Step: 10800  \tValid loss: 0.48637863993644714\n",
      "Step: 10900  \tTraining loss: 0.4922676682472229\n",
      "Step: 10900  \tTraining accuracy: 0.7385079860687256\n",
      "Step: 10900  \tValid loss: 0.48620250821113586\n",
      "Step: 11000  \tTraining loss: 0.4920498728752136\n",
      "Step: 11000  \tTraining accuracy: 0.7385959625244141\n",
      "Step: 11000  \tValid loss: 0.4859231114387512\n",
      "Step: 11100  \tTraining loss: 0.49182912707328796\n",
      "Step: 11100  \tTraining accuracy: 0.7386896014213562\n",
      "Step: 11100  \tValid loss: 0.48566392064094543\n",
      "Step: 11200  \tTraining loss: 0.4916055202484131\n",
      "Step: 11200  \tTraining accuracy: 0.7387815117835999\n",
      "Step: 11200  \tValid loss: 0.48543208837509155\n",
      "Step: 11300  \tTraining loss: 0.4913787841796875\n",
      "Step: 11300  \tTraining accuracy: 0.7388647198677063\n",
      "Step: 11300  \tValid loss: 0.4852108359336853\n",
      "Step: 11400  \tTraining loss: 0.49114909768104553\n",
      "Step: 11400  \tTraining accuracy: 0.7389464974403381\n",
      "Step: 11400  \tValid loss: 0.4850017726421356\n",
      "Step: 11500  \tTraining loss: 0.49091577529907227\n",
      "Step: 11500  \tTraining accuracy: 0.7390279769897461\n",
      "Step: 11500  \tValid loss: 0.48480549454689026\n",
      "Step: 11600  \tTraining loss: 0.4906691610813141\n",
      "Step: 11600  \tTraining accuracy: 0.7391091585159302\n",
      "Step: 11600  \tValid loss: 0.48451581597328186\n",
      "Step: 11700  \tTraining loss: 0.4904089868068695\n",
      "Step: 11700  \tTraining accuracy: 0.7391958236694336\n",
      "Step: 11700  \tValid loss: 0.48425909876823425\n",
      "Step: 11800  \tTraining loss: 0.4901474416255951\n",
      "Step: 11800  \tTraining accuracy: 0.7392866611480713\n",
      "Step: 11800  \tValid loss: 0.4839927554130554\n",
      "Step: 11900  \tTraining loss: 0.4898817837238312\n",
      "Step: 11900  \tTraining accuracy: 0.7393893599510193\n",
      "Step: 11900  \tValid loss: 0.4837317168712616\n",
      "Step: 12000  \tTraining loss: 0.48961037397384644\n",
      "Step: 12000  \tTraining accuracy: 0.7395025491714478\n",
      "Step: 12000  \tValid loss: 0.48341891169548035\n",
      "Step: 12100  \tTraining loss: 0.48933011293411255\n",
      "Step: 12100  \tTraining accuracy: 0.7396094799041748\n",
      "Step: 12100  \tValid loss: 0.4831295311450958\n",
      "Step: 12200  \tTraining loss: 0.4890088438987732\n",
      "Step: 12200  \tTraining accuracy: 0.7397201061248779\n",
      "Step: 12200  \tValid loss: 0.4826776385307312\n",
      "Step: 12300  \tTraining loss: 0.48870646953582764\n",
      "Step: 12300  \tTraining accuracy: 0.7398256659507751\n",
      "Step: 12300  \tValid loss: 0.48232772946357727\n",
      "Step: 12400  \tTraining loss: 0.4884050786495209\n",
      "Step: 12400  \tTraining accuracy: 0.7399381399154663\n",
      "Step: 12400  \tValid loss: 0.48194196820259094\n",
      "Step: 12500  \tTraining loss: 0.4881037473678589\n",
      "Step: 12500  \tTraining accuracy: 0.7400519847869873\n",
      "Step: 12500  \tValid loss: 0.48158639669418335\n",
      "Step: 12600  \tTraining loss: 0.4878060221672058\n",
      "Step: 12600  \tTraining accuracy: 0.740164041519165\n",
      "Step: 12600  \tValid loss: 0.4812573194503784\n",
      "Step: 12700  \tTraining loss: 0.4875130355358124\n",
      "Step: 12700  \tTraining accuracy: 0.7402763366699219\n",
      "Step: 12700  \tValid loss: 0.4809240698814392\n",
      "Step: 12800  \tTraining loss: 0.4872238337993622\n",
      "Step: 12800  \tTraining accuracy: 0.7403827905654907\n",
      "Step: 12800  \tValid loss: 0.48058202862739563\n",
      "Step: 12900  \tTraining loss: 0.48693713545799255\n",
      "Step: 12900  \tTraining accuracy: 0.7404792904853821\n",
      "Step: 12900  \tValid loss: 0.480238139629364\n",
      "Step: 13000  \tTraining loss: 0.4866569936275482\n",
      "Step: 13000  \tTraining accuracy: 0.7405773997306824\n",
      "Step: 13000  \tValid loss: 0.4799104630947113\n",
      "Step: 13100  \tTraining loss: 0.4863796532154083\n",
      "Step: 13100  \tTraining accuracy: 0.7406790852546692\n",
      "Step: 13100  \tValid loss: 0.4795595705509186\n",
      "Step: 13200  \tTraining loss: 0.48611003160476685\n",
      "Step: 13200  \tTraining accuracy: 0.7407852411270142\n",
      "Step: 13200  \tValid loss: 0.4792126715183258\n",
      "Step: 13300  \tTraining loss: 0.4858432710170746\n",
      "Step: 13300  \tTraining accuracy: 0.740889847278595\n",
      "Step: 13300  \tValid loss: 0.4788980782032013\n",
      "Step: 13400  \tTraining loss: 0.4855799376964569\n",
      "Step: 13400  \tTraining accuracy: 0.7409898638725281\n",
      "Step: 13400  \tValid loss: 0.4785543382167816\n",
      "Step: 13500  \tTraining loss: 0.4853231906890869\n",
      "Step: 13500  \tTraining accuracy: 0.7410864233970642\n",
      "Step: 13500  \tValid loss: 0.47819432616233826\n",
      "Step: 13600  \tTraining loss: 0.485073983669281\n",
      "Step: 13600  \tTraining accuracy: 0.7411816120147705\n",
      "Step: 13600  \tValid loss: 0.47788190841674805\n",
      "Step: 13700  \tTraining loss: 0.4848277270793915\n",
      "Step: 13700  \tTraining accuracy: 0.741283118724823\n",
      "Step: 13700  \tValid loss: 0.477573037147522\n",
      "Step: 13800  \tTraining loss: 0.48458775877952576\n",
      "Step: 13800  \tTraining accuracy: 0.7413812279701233\n",
      "Step: 13800  \tValid loss: 0.4772551357746124\n",
      "Step: 13900  \tTraining loss: 0.48435354232788086\n",
      "Step: 13900  \tTraining accuracy: 0.7414759993553162\n",
      "Step: 13900  \tValid loss: 0.47695523500442505\n",
      "Step: 14000  \tTraining loss: 0.48411834239959717\n",
      "Step: 14000  \tTraining accuracy: 0.7415665984153748\n",
      "Step: 14000  \tValid loss: 0.4766604006290436\n",
      "Step: 14100  \tTraining loss: 0.48388397693634033\n",
      "Step: 14100  \tTraining accuracy: 0.7416539788246155\n",
      "Step: 14100  \tValid loss: 0.4763341248035431\n",
      "Step: 14200  \tTraining loss: 0.483656644821167\n",
      "Step: 14200  \tTraining accuracy: 0.7417411208152771\n",
      "Step: 14200  \tValid loss: 0.47604700922966003\n",
      "Step: 14300  \tTraining loss: 0.4834355115890503\n",
      "Step: 14300  \tTraining accuracy: 0.7418288588523865\n",
      "Step: 14300  \tValid loss: 0.47577327489852905\n",
      "Step: 14400  \tTraining loss: 0.4832197427749634\n",
      "Step: 14400  \tTraining accuracy: 0.7419134974479675\n",
      "Step: 14400  \tValid loss: 0.4755175709724426\n",
      "Step: 14500  \tTraining loss: 0.48301011323928833\n",
      "Step: 14500  \tTraining accuracy: 0.7419924139976501\n",
      "Step: 14500  \tValid loss: 0.4752708375453949\n",
      "Step: 14600  \tTraining loss: 0.48280516266822815\n",
      "Step: 14600  \tTraining accuracy: 0.7420720458030701\n",
      "Step: 14600  \tValid loss: 0.47504860162734985\n",
      "Step: 14700  \tTraining loss: 0.4826032221317291\n",
      "Step: 14700  \tTraining accuracy: 0.742150604724884\n",
      "Step: 14700  \tValid loss: 0.4748246371746063\n",
      "Step: 14800  \tTraining loss: 0.48240506649017334\n",
      "Step: 14800  \tTraining accuracy: 0.742228090763092\n",
      "Step: 14800  \tValid loss: 0.4746086001396179\n",
      "Step: 14900  \tTraining loss: 0.4822111427783966\n",
      "Step: 14900  \tTraining accuracy: 0.7423036694526672\n",
      "Step: 14900  \tValid loss: 0.4744095802307129\n",
      "Step: 15000  \tTraining loss: 0.4820239543914795\n",
      "Step: 15000  \tTraining accuracy: 0.7423746585845947\n",
      "Step: 15000  \tValid loss: 0.4741818606853485\n",
      "Step: 15100  \tTraining loss: 0.48183733224868774\n",
      "Step: 15100  \tTraining accuracy: 0.7424482703208923\n",
      "Step: 15100  \tValid loss: 0.47401365637779236\n",
      "Step: 15200  \tTraining loss: 0.48165163397789\n",
      "Step: 15200  \tTraining accuracy: 0.7425234913825989\n",
      "Step: 15200  \tValid loss: 0.47382330894470215\n",
      "Step: 15300  \tTraining loss: 0.48147067427635193\n",
      "Step: 15300  \tTraining accuracy: 0.7425916194915771\n",
      "Step: 15300  \tValid loss: 0.4736661911010742\n",
      "Step: 15400  \tTraining loss: 0.48129069805145264\n",
      "Step: 15400  \tTraining accuracy: 0.7426571846008301\n",
      "Step: 15400  \tValid loss: 0.47348982095718384\n",
      "Step: 15500  \tTraining loss: 0.48111364245414734\n",
      "Step: 15500  \tTraining accuracy: 0.7427150011062622\n",
      "Step: 15500  \tValid loss: 0.4733292758464813\n",
      "Step: 15600  \tTraining loss: 0.48093926906585693\n",
      "Step: 15600  \tTraining accuracy: 0.7427695393562317\n",
      "Step: 15600  \tValid loss: 0.4731625020503998\n",
      "Step: 15700  \tTraining loss: 0.48076605796813965\n",
      "Step: 15700  \tTraining accuracy: 0.7428216934204102\n",
      "Step: 15700  \tValid loss: 0.47303107380867004\n",
      "Step: 15800  \tTraining loss: 0.480593740940094\n",
      "Step: 15800  \tTraining accuracy: 0.7428680658340454\n",
      "Step: 15800  \tValid loss: 0.4728751480579376\n",
      "Step: 15900  \tTraining loss: 0.48042500019073486\n",
      "Step: 15900  \tTraining accuracy: 0.7429139018058777\n",
      "Step: 15900  \tValid loss: 0.4726806879043579\n",
      "Step: 16000  \tTraining loss: 0.4802548289299011\n",
      "Step: 16000  \tTraining accuracy: 0.7429550290107727\n",
      "Step: 16000  \tValid loss: 0.4725550711154938\n",
      "Step: 16100  \tTraining loss: 0.4800867736339569\n",
      "Step: 16100  \tTraining accuracy: 0.7429890036582947\n",
      "Step: 16100  \tValid loss: 0.4723949134349823\n",
      "Step: 16200  \tTraining loss: 0.4799206554889679\n",
      "Step: 16200  \tTraining accuracy: 0.743025004863739\n",
      "Step: 16200  \tValid loss: 0.47223353385925293\n",
      "Step: 16300  \tTraining loss: 0.4797538220882416\n",
      "Step: 16300  \tTraining accuracy: 0.7430622577667236\n",
      "Step: 16300  \tValid loss: 0.47209322452545166\n",
      "Step: 16400  \tTraining loss: 0.4795868694782257\n",
      "Step: 16400  \tTraining accuracy: 0.7431014180183411\n",
      "Step: 16400  \tValid loss: 0.47192591428756714\n",
      "Step: 16500  \tTraining loss: 0.4794204533100128\n",
      "Step: 16500  \tTraining accuracy: 0.743141770362854\n",
      "Step: 16500  \tValid loss: 0.4717758297920227\n",
      "Step: 16600  \tTraining loss: 0.47925570607185364\n",
      "Step: 16600  \tTraining accuracy: 0.743183970451355\n",
      "Step: 16600  \tValid loss: 0.47162967920303345\n",
      "Step: 16700  \tTraining loss: 0.4790888726711273\n",
      "Step: 16700  \tTraining accuracy: 0.7432296872138977\n",
      "Step: 16700  \tValid loss: 0.47148168087005615\n",
      "Step: 16800  \tTraining loss: 0.4789239764213562\n",
      "Step: 16800  \tTraining accuracy: 0.7432764768600464\n",
      "Step: 16800  \tValid loss: 0.4713597893714905\n",
      "Step: 16900  \tTraining loss: 0.47875526547431946\n",
      "Step: 16900  \tTraining accuracy: 0.7433249950408936\n",
      "Step: 16900  \tValid loss: 0.4712570905685425\n",
      "Step: 17000  \tTraining loss: 0.47858476638793945\n",
      "Step: 17000  \tTraining accuracy: 0.7433745861053467\n",
      "Step: 17000  \tValid loss: 0.4711456000804901\n",
      "Step: 17100  \tTraining loss: 0.4784143567085266\n",
      "Step: 17100  \tTraining accuracy: 0.7434235215187073\n",
      "Step: 17100  \tValid loss: 0.4710347652435303\n",
      "Step: 17200  \tTraining loss: 0.4782407581806183\n",
      "Step: 17200  \tTraining accuracy: 0.743472695350647\n",
      "Step: 17200  \tValid loss: 0.47091758251190186\n",
      "Step: 17300  \tTraining loss: 0.47806280851364136\n",
      "Step: 17300  \tTraining accuracy: 0.7435174584388733\n",
      "Step: 17300  \tValid loss: 0.4707847535610199\n",
      "Step: 17400  \tTraining loss: 0.47787120938301086\n",
      "Step: 17400  \tTraining accuracy: 0.7435609698295593\n",
      "Step: 17400  \tValid loss: 0.47067955136299133\n",
      "Step: 17500  \tTraining loss: 0.4776773452758789\n",
      "Step: 17500  \tTraining accuracy: 0.7436062097549438\n",
      "Step: 17500  \tValid loss: 0.47055935859680176\n",
      "Step: 17600  \tTraining loss: 0.4774887263774872\n",
      "Step: 17600  \tTraining accuracy: 0.7436516880989075\n",
      "Step: 17600  \tValid loss: 0.4703994691371918\n",
      "Step: 17700  \tTraining loss: 0.4773015081882477\n",
      "Step: 17700  \tTraining accuracy: 0.7436959743499756\n",
      "Step: 17700  \tValid loss: 0.4702790379524231\n",
      "Step: 17800  \tTraining loss: 0.47711533308029175\n",
      "Step: 17800  \tTraining accuracy: 0.7437344193458557\n",
      "Step: 17800  \tValid loss: 0.4701157808303833\n",
      "Step: 17900  \tTraining loss: 0.4769160747528076\n",
      "Step: 17900  \tTraining accuracy: 0.7437710165977478\n",
      "Step: 17900  \tValid loss: 0.46987369656562805\n",
      "Step: 18000  \tTraining loss: 0.476726770401001\n",
      "Step: 18000  \tTraining accuracy: 0.7438071966171265\n",
      "Step: 18000  \tValid loss: 0.4697078466415405\n",
      "Step: 18100  \tTraining loss: 0.476539671421051\n",
      "Step: 18100  \tTraining accuracy: 0.7438386082649231\n",
      "Step: 18100  \tValid loss: 0.46956712007522583\n",
      "Step: 18200  \tTraining loss: 0.4763520359992981\n",
      "Step: 18200  \tTraining accuracy: 0.7438673973083496\n",
      "Step: 18200  \tValid loss: 0.4694342613220215\n",
      "Step: 18300  \tTraining loss: 0.47616469860076904\n",
      "Step: 18300  \tTraining accuracy: 0.743895947933197\n",
      "Step: 18300  \tValid loss: 0.4693068265914917\n",
      "Step: 18400  \tTraining loss: 0.4759786128997803\n",
      "Step: 18400  \tTraining accuracy: 0.7439248561859131\n",
      "Step: 18400  \tValid loss: 0.46917638182640076\n",
      "Step: 18500  \tTraining loss: 0.4757553040981293\n",
      "Step: 18500  \tTraining accuracy: 0.7439498901367188\n",
      "Step: 18500  \tValid loss: 0.46904370188713074\n",
      "Step: 18600  \tTraining loss: 0.4755235016345978\n",
      "Step: 18600  \tTraining accuracy: 0.7439767718315125\n",
      "Step: 18600  \tValid loss: 0.4688609838485718\n",
      "Step: 18700  \tTraining loss: 0.4753226935863495\n",
      "Step: 18700  \tTraining accuracy: 0.7440140247344971\n",
      "Step: 18700  \tValid loss: 0.4688081741333008\n",
      "Step: 18800  \tTraining loss: 0.47514569759368896\n",
      "Step: 18800  \tTraining accuracy: 0.744053065776825\n",
      "Step: 18800  \tValid loss: 0.4687548875808716\n",
      "Step: 18900  \tTraining loss: 0.4749850034713745\n",
      "Step: 18900  \tTraining accuracy: 0.7440916299819946\n",
      "Step: 18900  \tValid loss: 0.46865829825401306\n",
      "Step: 19000  \tTraining loss: 0.4748299717903137\n",
      "Step: 19000  \tTraining accuracy: 0.7441298365592957\n",
      "Step: 19000  \tValid loss: 0.46856459975242615\n",
      "Step: 19100  \tTraining loss: 0.47467944025993347\n",
      "Step: 19100  \tTraining accuracy: 0.744166910648346\n",
      "Step: 19100  \tValid loss: 0.4684655964374542\n",
      "Step: 19200  \tTraining loss: 0.4745326340198517\n",
      "Step: 19200  \tTraining accuracy: 0.7442091107368469\n",
      "Step: 19200  \tValid loss: 0.4683772623538971\n",
      "Step: 19300  \tTraining loss: 0.47439128160476685\n",
      "Step: 19300  \tTraining accuracy: 0.7442516088485718\n",
      "Step: 19300  \tValid loss: 0.4682594835758209\n",
      "Step: 19400  \tTraining loss: 0.4742482602596283\n",
      "Step: 19400  \tTraining accuracy: 0.7442902326583862\n",
      "Step: 19400  \tValid loss: 0.46820148825645447\n",
      "Step: 19500  \tTraining loss: 0.47410038113594055\n",
      "Step: 19500  \tTraining accuracy: 0.7443284392356873\n",
      "Step: 19500  \tValid loss: 0.46805381774902344\n",
      "Step: 19600  \tTraining loss: 0.4739525020122528\n",
      "Step: 19600  \tTraining accuracy: 0.7443696856498718\n",
      "Step: 19600  \tValid loss: 0.46799159049987793\n",
      "Step: 19700  \tTraining loss: 0.4738113284111023\n",
      "Step: 19700  \tTraining accuracy: 0.7444138526916504\n",
      "Step: 19700  \tValid loss: 0.4679360091686249\n",
      "Step: 19800  \tTraining loss: 0.4736740291118622\n",
      "Step: 19800  \tTraining accuracy: 0.744462251663208\n",
      "Step: 19800  \tValid loss: 0.46787571907043457\n",
      "Step: 19900  \tTraining loss: 0.4735388159751892\n",
      "Step: 19900  \tTraining accuracy: 0.7445115447044373\n",
      "Step: 19900  \tValid loss: 0.4678233563899994\n",
      "Step: 20000  \tTraining loss: 0.4734056293964386\n",
      "Step: 20000  \tTraining accuracy: 0.7445563673973083\n",
      "Step: 20000  \tValid loss: 0.46777021884918213\n",
      "Step: 20100  \tTraining loss: 0.47327694296836853\n",
      "Step: 20100  \tTraining accuracy: 0.7445954084396362\n",
      "Step: 20100  \tValid loss: 0.4677213430404663\n",
      "Step: 20200  \tTraining loss: 0.473145991563797\n",
      "Step: 20200  \tTraining accuracy: 0.7446354031562805\n",
      "Step: 20200  \tValid loss: 0.46768349409103394\n",
      "Step: 20300  \tTraining loss: 0.47301626205444336\n",
      "Step: 20300  \tTraining accuracy: 0.7446749806404114\n",
      "Step: 20300  \tValid loss: 0.4676499366760254\n",
      "Step: 20400  \tTraining loss: 0.47289130091667175\n",
      "Step: 20400  \tTraining accuracy: 0.7447167634963989\n",
      "Step: 20400  \tValid loss: 0.46759331226348877\n",
      "Step: 20500  \tTraining loss: 0.47276434302330017\n",
      "Step: 20500  \tTraining accuracy: 0.7447595000267029\n",
      "Step: 20500  \tValid loss: 0.4675588011741638\n",
      "Step: 20600  \tTraining loss: 0.4726286232471466\n",
      "Step: 20600  \tTraining accuracy: 0.7448024153709412\n",
      "Step: 20600  \tValid loss: 0.467587411403656\n",
      "Step: 20700  \tTraining loss: 0.4724830090999603\n",
      "Step: 20700  \tTraining accuracy: 0.7448423504829407\n",
      "Step: 20700  \tValid loss: 0.4676019847393036\n",
      "Step: 20800  \tTraining loss: 0.47234347462654114\n",
      "Step: 20800  \tTraining accuracy: 0.7448800206184387\n",
      "Step: 20800  \tValid loss: 0.4675827622413635\n",
      "Step: 20900  \tTraining loss: 0.472209095954895\n",
      "Step: 20900  \tTraining accuracy: 0.7449191808700562\n",
      "Step: 20900  \tValid loss: 0.46758079528808594\n",
      "Step: 21000  \tTraining loss: 0.47207796573638916\n",
      "Step: 21000  \tTraining accuracy: 0.744961142539978\n",
      "Step: 21000  \tValid loss: 0.4675714373588562\n",
      "Step: 21100  \tTraining loss: 0.4719502925872803\n",
      "Step: 21100  \tTraining accuracy: 0.7450071573257446\n",
      "Step: 21100  \tValid loss: 0.4675566554069519\n",
      "Step: 21200  \tTraining loss: 0.47182655334472656\n",
      "Step: 21200  \tTraining accuracy: 0.7450539469718933\n",
      "Step: 21200  \tValid loss: 0.46752792596817017\n",
      "Step: 21300  \tTraining loss: 0.47171148657798767\n",
      "Step: 21300  \tTraining accuracy: 0.7450990676879883\n",
      "Step: 21300  \tValid loss: 0.46747472882270813\n",
      "Step: 21400  \tTraining loss: 0.4715961217880249\n",
      "Step: 21400  \tTraining accuracy: 0.7451450228691101\n",
      "Step: 21400  \tValid loss: 0.4674489498138428\n",
      "Step: 21500  \tTraining loss: 0.4714817702770233\n",
      "Step: 21500  \tTraining accuracy: 0.7451886534690857\n",
      "Step: 21500  \tValid loss: 0.46743112802505493\n",
      "Step: 21600  \tTraining loss: 0.47136974334716797\n",
      "Step: 21600  \tTraining accuracy: 0.7452306747436523\n",
      "Step: 21600  \tValid loss: 0.46741577982902527\n",
      "Step: 21700  \tTraining loss: 0.47125884890556335\n",
      "Step: 21700  \tTraining accuracy: 0.7452772259712219\n",
      "Step: 21700  \tValid loss: 0.4673973619937897\n",
      "Step: 21800  \tTraining loss: 0.4711506962776184\n",
      "Step: 21800  \tTraining accuracy: 0.7453209161758423\n",
      "Step: 21800  \tValid loss: 0.46736037731170654\n",
      "Step: 21900  \tTraining loss: 0.47103700041770935\n",
      "Step: 21900  \tTraining accuracy: 0.7453641891479492\n",
      "Step: 21900  \tValid loss: 0.4673801064491272\n",
      "Step: 22000  \tTraining loss: 0.4709298312664032\n",
      "Step: 22000  \tTraining accuracy: 0.745406448841095\n",
      "Step: 22000  \tValid loss: 0.4673633277416229\n",
      "Step: 22100  \tTraining loss: 0.47082072496414185\n",
      "Step: 22100  \tTraining accuracy: 0.7454537153244019\n",
      "Step: 22100  \tValid loss: 0.46734580397605896\n",
      "Step: 22200  \tTraining loss: 0.4707147181034088\n",
      "Step: 22200  \tTraining accuracy: 0.7455047965049744\n",
      "Step: 22200  \tValid loss: 0.46730858087539673\n",
      "Step: 22300  \tTraining loss: 0.47060832381248474\n",
      "Step: 22300  \tTraining accuracy: 0.7455577850341797\n",
      "Step: 22300  \tValid loss: 0.4673101305961609\n",
      "Step: 22400  \tTraining loss: 0.4704989790916443\n",
      "Step: 22400  \tTraining accuracy: 0.7456102967262268\n",
      "Step: 22400  \tValid loss: 0.4672819972038269\n",
      "Step: 22500  \tTraining loss: 0.47038882970809937\n",
      "Step: 22500  \tTraining accuracy: 0.7456623315811157\n",
      "Step: 22500  \tValid loss: 0.467227578163147\n",
      "Step: 22600  \tTraining loss: 0.47028061747550964\n",
      "Step: 22600  \tTraining accuracy: 0.7457138895988464\n",
      "Step: 22600  \tValid loss: 0.46720176935195923\n",
      "Step: 22700  \tTraining loss: 0.4701756536960602\n",
      "Step: 22700  \tTraining accuracy: 0.7457650303840637\n",
      "Step: 22700  \tValid loss: 0.46719154715538025\n",
      "Step: 22800  \tTraining loss: 0.47007158398628235\n",
      "Step: 22800  \tTraining accuracy: 0.7458156943321228\n",
      "Step: 22800  \tValid loss: 0.4671816825866699\n",
      "Step: 22900  \tTraining loss: 0.4699650704860687\n",
      "Step: 22900  \tTraining accuracy: 0.7458659410476685\n",
      "Step: 22900  \tValid loss: 0.46718883514404297\n",
      "Step: 23000  \tTraining loss: 0.4698629379272461\n",
      "Step: 23000  \tTraining accuracy: 0.7459140419960022\n",
      "Step: 23000  \tValid loss: 0.4671977460384369\n",
      "Step: 23100  \tTraining loss: 0.46975836157798767\n",
      "Step: 23100  \tTraining accuracy: 0.7459599375724792\n",
      "Step: 23100  \tValid loss: 0.46721044182777405\n",
      "Step: 23200  \tTraining loss: 0.46965718269348145\n",
      "Step: 23200  \tTraining accuracy: 0.7460089325904846\n",
      "Step: 23200  \tValid loss: 0.4672050178050995\n",
      "Step: 23300  \tTraining loss: 0.46955612301826477\n",
      "Step: 23300  \tTraining accuracy: 0.746060311794281\n",
      "Step: 23300  \tValid loss: 0.46723228693008423\n",
      "Step: 23400  \tTraining loss: 0.4694550633430481\n",
      "Step: 23400  \tTraining accuracy: 0.7461118102073669\n",
      "Step: 23400  \tValid loss: 0.4672333896160126\n",
      "Step: 23500  \tTraining loss: 0.4693553149700165\n",
      "Step: 23500  \tTraining accuracy: 0.7461651563644409\n",
      "Step: 23500  \tValid loss: 0.467250257730484\n",
      "Step: 23600  \tTraining loss: 0.4692549705505371\n",
      "Step: 23600  \tTraining accuracy: 0.7462180256843567\n",
      "Step: 23600  \tValid loss: 0.4672504663467407\n",
      "Step: 23700  \tTraining loss: 0.46915528178215027\n",
      "Step: 23700  \tTraining accuracy: 0.746270477771759\n",
      "Step: 23700  \tValid loss: 0.4672473967075348\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7463225\n",
      "Precision: 0.78507674\n",
      "Recall: 0.8856222\n",
      "F1 score: 0.7912375\n",
      "AUC: 0.70893383\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.746323   0.785077  0.885622  0.791237  0.708934  0.469117      0.746283   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.467167        0.74625   0.532734      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  23739.0  \n",
      "3\n",
      "(3915, 8)\n",
      "(3915, 1)\n",
      "(2080, 8)\n",
      "(2080, 1)\n",
      "(1690, 8)\n",
      "(1690, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5443854331970215\n",
      "Step: 100  \tTraining accuracy: 0.7402299046516418\n",
      "Step: 100  \tValid loss: 0.5622454285621643\n",
      "Step: 200  \tTraining loss: 0.516231894493103\n",
      "Step: 200  \tTraining accuracy: 0.7402586340904236\n",
      "Step: 200  \tValid loss: 0.5391093492507935\n",
      "Step: 300  \tTraining loss: 0.4887220561504364\n",
      "Step: 300  \tTraining accuracy: 0.7442053556442261\n",
      "Step: 300  \tValid loss: 0.5161009430885315\n",
      "Step: 400  \tTraining loss: 0.4505140781402588\n",
      "Step: 400  \tTraining accuracy: 0.7519466280937195\n",
      "Step: 400  \tValid loss: 0.4840161204338074\n",
      "Step: 500  \tTraining loss: 0.4101460874080658\n",
      "Step: 500  \tTraining accuracy: 0.7598326206207275\n",
      "Step: 500  \tValid loss: 0.4504571855068207\n",
      "Step: 600  \tTraining loss: 0.3893895745277405\n",
      "Step: 600  \tTraining accuracy: 0.7677373886108398\n",
      "Step: 600  \tValid loss: 0.4347098469734192\n",
      "Step: 700  \tTraining loss: 0.3811158835887909\n",
      "Step: 700  \tTraining accuracy: 0.7746726870536804\n",
      "Step: 700  \tValid loss: 0.42928314208984375\n",
      "Step: 800  \tTraining loss: 0.3775855004787445\n",
      "Step: 800  \tTraining accuracy: 0.7803500294685364\n",
      "Step: 800  \tValid loss: 0.4270687401294708\n",
      "Step: 900  \tTraining loss: 0.3756044805049896\n",
      "Step: 900  \tTraining accuracy: 0.7848153710365295\n",
      "Step: 900  \tValid loss: 0.4255731403827667\n",
      "Step: 1000  \tTraining loss: 0.37417545914649963\n",
      "Step: 1000  \tTraining accuracy: 0.7883825898170471\n",
      "Step: 1000  \tValid loss: 0.4241933226585388\n",
      "Step: 1100  \tTraining loss: 0.3729790449142456\n",
      "Step: 1100  \tTraining accuracy: 0.7913947701454163\n",
      "Step: 1100  \tValid loss: 0.4228370785713196\n",
      "Step: 1200  \tTraining loss: 0.37189629673957825\n",
      "Step: 1200  \tTraining accuracy: 0.7938269972801208\n",
      "Step: 1200  \tValid loss: 0.4214938282966614\n",
      "Step: 1300  \tTraining loss: 0.37088045477867126\n",
      "Step: 1300  \tTraining accuracy: 0.7959120273590088\n",
      "Step: 1300  \tValid loss: 0.42018115520477295\n",
      "Step: 1400  \tTraining loss: 0.3699110448360443\n",
      "Step: 1400  \tTraining accuracy: 0.7977461218833923\n",
      "Step: 1400  \tValid loss: 0.4189116358757019\n",
      "Step: 1500  \tTraining loss: 0.36898159980773926\n",
      "Step: 1500  \tTraining accuracy: 0.7993273735046387\n",
      "Step: 1500  \tValid loss: 0.41770458221435547\n",
      "Step: 1600  \tTraining loss: 0.3680887520313263\n",
      "Step: 1600  \tTraining accuracy: 0.8006879687309265\n",
      "Step: 1600  \tValid loss: 0.41657161712646484\n",
      "Step: 1700  \tTraining loss: 0.36723271012306213\n",
      "Step: 1700  \tTraining accuracy: 0.8018600940704346\n",
      "Step: 1700  \tValid loss: 0.41551846265792847\n",
      "Step: 1800  \tTraining loss: 0.366416871547699\n",
      "Step: 1800  \tTraining accuracy: 0.8028760552406311\n",
      "Step: 1800  \tValid loss: 0.4145525395870209\n",
      "Step: 1900  \tTraining loss: 0.36564216017723083\n",
      "Step: 1900  \tTraining accuracy: 0.8038033246994019\n",
      "Step: 1900  \tValid loss: 0.413677841424942\n",
      "Step: 2000  \tTraining loss: 0.36490726470947266\n",
      "Step: 2000  \tTraining accuracy: 0.8047622442245483\n",
      "Step: 2000  \tValid loss: 0.4128935933113098\n",
      "Step: 2100  \tTraining loss: 0.3642096519470215\n",
      "Step: 2100  \tTraining accuracy: 0.8056974411010742\n",
      "Step: 2100  \tValid loss: 0.41219523549079895\n",
      "Step: 2200  \tTraining loss: 0.3635381758213043\n",
      "Step: 2200  \tTraining accuracy: 0.8066121935844421\n",
      "Step: 2200  \tValid loss: 0.41156890988349915\n",
      "Step: 2300  \tTraining loss: 0.3628826141357422\n",
      "Step: 2300  \tTraining accuracy: 0.8074514865875244\n",
      "Step: 2300  \tValid loss: 0.4110001027584076\n",
      "Step: 2400  \tTraining loss: 0.3622472882270813\n",
      "Step: 2400  \tTraining accuracy: 0.8082138895988464\n",
      "Step: 2400  \tValid loss: 0.4104786813259125\n",
      "Step: 2500  \tTraining loss: 0.3616432547569275\n",
      "Step: 2500  \tTraining accuracy: 0.8089246153831482\n",
      "Step: 2500  \tValid loss: 0.41001424193382263\n",
      "Step: 2600  \tTraining loss: 0.3610771894454956\n",
      "Step: 2600  \tTraining accuracy: 0.8095694780349731\n",
      "Step: 2600  \tValid loss: 0.40961092710494995\n",
      "Step: 2700  \tTraining loss: 0.36054959893226624\n",
      "Step: 2700  \tTraining accuracy: 0.810200035572052\n",
      "Step: 2700  \tValid loss: 0.40927553176879883\n",
      "Step: 2800  \tTraining loss: 0.36005455255508423\n",
      "Step: 2800  \tTraining accuracy: 0.8107941746711731\n",
      "Step: 2800  \tValid loss: 0.408988356590271\n",
      "Step: 2900  \tTraining loss: 0.3595810830593109\n",
      "Step: 2900  \tTraining accuracy: 0.8113694787025452\n",
      "Step: 2900  \tValid loss: 0.4087374806404114\n",
      "Step: 3000  \tTraining loss: 0.35911765694618225\n",
      "Step: 3000  \tTraining accuracy: 0.8118969798088074\n",
      "Step: 3000  \tValid loss: 0.4085049629211426\n",
      "Step: 3100  \tTraining loss: 0.3586505055427551\n",
      "Step: 3100  \tTraining accuracy: 0.8123728632926941\n",
      "Step: 3100  \tValid loss: 0.4082660973072052\n",
      "Step: 3200  \tTraining loss: 0.3581658899784088\n",
      "Step: 3200  \tTraining accuracy: 0.8128143548965454\n",
      "Step: 3200  \tValid loss: 0.4080020785331726\n",
      "Step: 3300  \tTraining loss: 0.3576432466506958\n",
      "Step: 3300  \tTraining accuracy: 0.8132287263870239\n",
      "Step: 3300  \tValid loss: 0.407690167427063\n",
      "Step: 3400  \tTraining loss: 0.3570774793624878\n",
      "Step: 3400  \tTraining accuracy: 0.8136261105537415\n",
      "Step: 3400  \tValid loss: 0.40728941559791565\n",
      "Step: 3500  \tTraining loss: 0.3564802408218384\n",
      "Step: 3500  \tTraining accuracy: 0.8139740824699402\n",
      "Step: 3500  \tValid loss: 0.40681958198547363\n",
      "Step: 3600  \tTraining loss: 0.3558635115623474\n",
      "Step: 3600  \tTraining accuracy: 0.8142988085746765\n",
      "Step: 3600  \tValid loss: 0.40632444620132446\n",
      "Step: 3700  \tTraining loss: 0.3552459478378296\n",
      "Step: 3700  \tTraining accuracy: 0.8146235346794128\n",
      "Step: 3700  \tValid loss: 0.4058292806148529\n",
      "Step: 3800  \tTraining loss: 0.3546305298805237\n",
      "Step: 3800  \tTraining accuracy: 0.8149552345275879\n",
      "Step: 3800  \tValid loss: 0.40533778071403503\n",
      "Step: 3900  \tTraining loss: 0.3540198802947998\n",
      "Step: 3900  \tTraining accuracy: 0.8152562379837036\n",
      "Step: 3900  \tValid loss: 0.4048207402229309\n",
      "Step: 4000  \tTraining loss: 0.35341760516166687\n",
      "Step: 4000  \tTraining accuracy: 0.815525472164154\n",
      "Step: 4000  \tValid loss: 0.40436437726020813\n",
      "Step: 4100  \tTraining loss: 0.3528296947479248\n",
      "Step: 4100  \tTraining accuracy: 0.8157878518104553\n",
      "Step: 4100  \tValid loss: 0.4039435386657715\n",
      "Step: 4200  \tTraining loss: 0.35225218534469604\n",
      "Step: 4200  \tTraining accuracy: 0.8160407543182373\n",
      "Step: 4200  \tValid loss: 0.4035545885562897\n",
      "Step: 4300  \tTraining loss: 0.35168927907943726\n",
      "Step: 4300  \tTraining accuracy: 0.8162817358970642\n",
      "Step: 4300  \tValid loss: 0.4031633734703064\n",
      "Step: 4400  \tTraining loss: 0.35111451148986816\n",
      "Step: 4400  \tTraining accuracy: 0.816520631313324\n",
      "Step: 4400  \tValid loss: 0.4029185175895691\n",
      "Step: 4500  \tTraining loss: 0.35052451491355896\n",
      "Step: 4500  \tTraining accuracy: 0.8167546391487122\n",
      "Step: 4500  \tValid loss: 0.40272441506385803\n",
      "Step: 4600  \tTraining loss: 0.34996718168258667\n",
      "Step: 4600  \tTraining accuracy: 0.8169783353805542\n",
      "Step: 4600  \tValid loss: 0.4023961126804352\n",
      "Step: 4700  \tTraining loss: 0.3494281768798828\n",
      "Step: 4700  \tTraining accuracy: 0.8171980381011963\n",
      "Step: 4700  \tValid loss: 0.4020065367221832\n",
      "Step: 4800  \tTraining loss: 0.34890463948249817\n",
      "Step: 4800  \tTraining accuracy: 0.8174029588699341\n",
      "Step: 4800  \tValid loss: 0.40158432722091675\n",
      "Step: 4900  \tTraining loss: 0.3483940362930298\n",
      "Step: 4900  \tTraining accuracy: 0.8175994753837585\n",
      "Step: 4900  \tValid loss: 0.40116068720817566\n",
      "Step: 5000  \tTraining loss: 0.34789708256721497\n",
      "Step: 5000  \tTraining accuracy: 0.8177985548973083\n",
      "Step: 5000  \tValid loss: 0.40071427822113037\n",
      "Step: 5100  \tTraining loss: 0.34741079807281494\n",
      "Step: 5100  \tTraining accuracy: 0.8179923295974731\n",
      "Step: 5100  \tValid loss: 0.4003041088581085\n",
      "Step: 5200  \tTraining loss: 0.3468473255634308\n",
      "Step: 5200  \tTraining accuracy: 0.8181735277175903\n",
      "Step: 5200  \tValid loss: 0.3994458019733429\n",
      "Step: 5300  \tTraining loss: 0.3463311791419983\n",
      "Step: 5300  \tTraining accuracy: 0.8183577656745911\n",
      "Step: 5300  \tValid loss: 0.3991895914077759\n",
      "Step: 5400  \tTraining loss: 0.34580448269844055\n",
      "Step: 5400  \tTraining accuracy: 0.8185375332832336\n",
      "Step: 5400  \tValid loss: 0.39853718876838684\n",
      "Step: 5500  \tTraining loss: 0.34527191519737244\n",
      "Step: 5500  \tTraining accuracy: 0.8187154531478882\n",
      "Step: 5500  \tValid loss: 0.397899866104126\n",
      "Step: 5600  \tTraining loss: 0.3447655439376831\n",
      "Step: 5600  \tTraining accuracy: 0.8188682198524475\n",
      "Step: 5600  \tValid loss: 0.3973901867866516\n",
      "Step: 5700  \tTraining loss: 0.3442719578742981\n",
      "Step: 5700  \tTraining accuracy: 0.8190132975578308\n",
      "Step: 5700  \tValid loss: 0.3969210088253021\n",
      "Step: 5800  \tTraining loss: 0.34378981590270996\n",
      "Step: 5800  \tTraining accuracy: 0.8191623687744141\n",
      "Step: 5800  \tValid loss: 0.3964685797691345\n",
      "Step: 5900  \tTraining loss: 0.34331706166267395\n",
      "Step: 5900  \tTraining accuracy: 0.8193063139915466\n",
      "Step: 5900  \tValid loss: 0.39601781964302063\n",
      "Step: 6000  \tTraining loss: 0.34284523129463196\n",
      "Step: 6000  \tTraining accuracy: 0.8194454908370972\n",
      "Step: 6000  \tValid loss: 0.39554083347320557\n",
      "Step: 6100  \tTraining loss: 0.34237971901893616\n",
      "Step: 6100  \tTraining accuracy: 0.819586455821991\n",
      "Step: 6100  \tValid loss: 0.395079106092453\n",
      "Step: 6200  \tTraining loss: 0.34191790223121643\n",
      "Step: 6200  \tTraining accuracy: 0.8197080492973328\n",
      "Step: 6200  \tValid loss: 0.39462825655937195\n",
      "Step: 6300  \tTraining loss: 0.34145891666412354\n",
      "Step: 6300  \tTraining accuracy: 0.8198341131210327\n",
      "Step: 6300  \tValid loss: 0.3942022919654846\n",
      "Step: 6400  \tTraining loss: 0.34100309014320374\n",
      "Step: 6400  \tTraining accuracy: 0.8199623227119446\n",
      "Step: 6400  \tValid loss: 0.3937855064868927\n",
      "Step: 6500  \tTraining loss: 0.34055066108703613\n",
      "Step: 6500  \tTraining accuracy: 0.8200945854187012\n",
      "Step: 6500  \tValid loss: 0.39337411522865295\n",
      "Step: 6600  \tTraining loss: 0.34010136127471924\n",
      "Step: 6600  \tTraining accuracy: 0.8202268481254578\n",
      "Step: 6600  \tValid loss: 0.392995148897171\n",
      "Step: 6700  \tTraining loss: 0.33965879678726196\n",
      "Step: 6700  \tTraining accuracy: 0.8203609585762024\n",
      "Step: 6700  \tValid loss: 0.39262911677360535\n",
      "Step: 6800  \tTraining loss: 0.3392154276371002\n",
      "Step: 6800  \tTraining accuracy: 0.820483386516571\n",
      "Step: 6800  \tValid loss: 0.39227405190467834\n",
      "Step: 6900  \tTraining loss: 0.3387793004512787\n",
      "Step: 6900  \tTraining accuracy: 0.820602297782898\n",
      "Step: 6900  \tValid loss: 0.39192289113998413\n",
      "Step: 7000  \tTraining loss: 0.3383471369743347\n",
      "Step: 7000  \tTraining accuracy: 0.8207083940505981\n",
      "Step: 7000  \tValid loss: 0.3915860652923584\n",
      "Step: 7100  \tTraining loss: 0.33792781829833984\n",
      "Step: 7100  \tTraining accuracy: 0.8208059072494507\n",
      "Step: 7100  \tValid loss: 0.39126086235046387\n",
      "Step: 7200  \tTraining loss: 0.3375014364719391\n",
      "Step: 7200  \tTraining accuracy: 0.8209043741226196\n",
      "Step: 7200  \tValid loss: 0.3909205496311188\n",
      "Step: 7300  \tTraining loss: 0.3370867371559143\n",
      "Step: 7300  \tTraining accuracy: 0.8210055232048035\n",
      "Step: 7300  \tValid loss: 0.39061906933784485\n",
      "Step: 7400  \tTraining loss: 0.3366723954677582\n",
      "Step: 7400  \tTraining accuracy: 0.8211003541946411\n",
      "Step: 7400  \tValid loss: 0.39028307795524597\n",
      "Step: 7500  \tTraining loss: 0.3358460068702698\n",
      "Step: 7500  \tTraining accuracy: 0.8212048411369324\n",
      "Step: 7500  \tValid loss: 0.3895648419857025\n",
      "Step: 7600  \tTraining loss: 0.3353131413459778\n",
      "Step: 7600  \tTraining accuracy: 0.821330726146698\n",
      "Step: 7600  \tValid loss: 0.38898327946662903\n",
      "Step: 7700  \tTraining loss: 0.3347899913787842\n",
      "Step: 7700  \tTraining accuracy: 0.8214566707611084\n",
      "Step: 7700  \tValid loss: 0.3885607421398163\n",
      "Step: 7800  \tTraining loss: 0.3342800736427307\n",
      "Step: 7800  \tTraining accuracy: 0.8215659856796265\n",
      "Step: 7800  \tValid loss: 0.3881892263889313\n",
      "Step: 7900  \tTraining loss: 0.3338056802749634\n",
      "Step: 7900  \tTraining accuracy: 0.8216542601585388\n",
      "Step: 7900  \tValid loss: 0.38784894347190857\n",
      "Step: 8000  \tTraining loss: 0.33336302638053894\n",
      "Step: 8000  \tTraining accuracy: 0.8217534422874451\n",
      "Step: 8000  \tValid loss: 0.38758620619773865\n",
      "Step: 8100  \tTraining loss: 0.3329433798789978\n",
      "Step: 8100  \tTraining accuracy: 0.8218469023704529\n",
      "Step: 8100  \tValid loss: 0.38731008768081665\n",
      "Step: 8200  \tTraining loss: 0.3325384855270386\n",
      "Step: 8200  \tTraining accuracy: 0.8219316601753235\n",
      "Step: 8200  \tValid loss: 0.3870738744735718\n",
      "Step: 8300  \tTraining loss: 0.3321459889411926\n",
      "Step: 8300  \tTraining accuracy: 0.8220160007476807\n",
      "Step: 8300  \tValid loss: 0.38686853647232056\n",
      "Step: 8400  \tTraining loss: 0.3317701518535614\n",
      "Step: 8400  \tTraining accuracy: 0.8221029043197632\n",
      "Step: 8400  \tValid loss: 0.3866428732872009\n",
      "Step: 8500  \tTraining loss: 0.3314090669155121\n",
      "Step: 8500  \tTraining accuracy: 0.8221847414970398\n",
      "Step: 8500  \tValid loss: 0.38644173741340637\n",
      "Step: 8600  \tTraining loss: 0.33105483651161194\n",
      "Step: 8600  \tTraining accuracy: 0.8222707509994507\n",
      "Step: 8600  \tValid loss: 0.38622352480888367\n",
      "Step: 8700  \tTraining loss: 0.33071449398994446\n",
      "Step: 8700  \tTraining accuracy: 0.8223577737808228\n",
      "Step: 8700  \tValid loss: 0.386075884103775\n",
      "Step: 8800  \tTraining loss: 0.3303762674331665\n",
      "Step: 8800  \tTraining accuracy: 0.8224472403526306\n",
      "Step: 8800  \tValid loss: 0.3859129846096039\n",
      "Step: 8900  \tTraining loss: 0.3300477862358093\n",
      "Step: 8900  \tTraining accuracy: 0.8225435614585876\n",
      "Step: 8900  \tValid loss: 0.38574719429016113\n",
      "Step: 9000  \tTraining loss: 0.3297252357006073\n",
      "Step: 9000  \tTraining accuracy: 0.822640597820282\n",
      "Step: 9000  \tValid loss: 0.385648638010025\n",
      "Step: 9100  \tTraining loss: 0.3294094204902649\n",
      "Step: 9100  \tTraining accuracy: 0.8227340579032898\n",
      "Step: 9100  \tValid loss: 0.3855492174625397\n",
      "Step: 9200  \tTraining loss: 0.3290995657444\n",
      "Step: 9200  \tTraining accuracy: 0.8228311538696289\n",
      "Step: 9200  \tValid loss: 0.38543859124183655\n",
      "Step: 9300  \tTraining loss: 0.3287948966026306\n",
      "Step: 9300  \tTraining accuracy: 0.8229387998580933\n",
      "Step: 9300  \tValid loss: 0.3853498697280884\n",
      "Step: 9400  \tTraining loss: 0.3284958600997925\n",
      "Step: 9400  \tTraining accuracy: 0.8230441808700562\n",
      "Step: 9400  \tValid loss: 0.385263592004776\n",
      "Step: 9500  \tTraining loss: 0.3282020092010498\n",
      "Step: 9500  \tTraining accuracy: 0.8231472969055176\n",
      "Step: 9500  \tValid loss: 0.3852120041847229\n",
      "Step: 9600  \tTraining loss: 0.3279126286506653\n",
      "Step: 9600  \tTraining accuracy: 0.8232578039169312\n",
      "Step: 9600  \tValid loss: 0.3851250112056732\n",
      "Step: 9700  \tTraining loss: 0.32762810587882996\n",
      "Step: 9700  \tTraining accuracy: 0.823371410369873\n",
      "Step: 9700  \tValid loss: 0.38508498668670654\n",
      "Step: 9800  \tTraining loss: 0.32734864950180054\n",
      "Step: 9800  \tTraining accuracy: 0.8234880566596985\n",
      "Step: 9800  \tValid loss: 0.3850329518318176\n",
      "Step: 9900  \tTraining loss: 0.32707399129867554\n",
      "Step: 9900  \tTraining accuracy: 0.8236023187637329\n",
      "Step: 9900  \tValid loss: 0.3849901258945465\n",
      "Step: 10000  \tTraining loss: 0.32680296897888184\n",
      "Step: 10000  \tTraining accuracy: 0.8237181901931763\n",
      "Step: 10000  \tValid loss: 0.3849537670612335\n",
      "Step: 10100  \tTraining loss: 0.32653677463531494\n",
      "Step: 10100  \tTraining accuracy: 0.8238343596458435\n",
      "Step: 10100  \tValid loss: 0.38492149114608765\n",
      "Step: 10200  \tTraining loss: 0.3262752294540405\n",
      "Step: 10200  \tTraining accuracy: 0.8239443898200989\n",
      "Step: 10200  \tValid loss: 0.38489818572998047\n",
      "Step: 10300  \tTraining loss: 0.3260173201560974\n",
      "Step: 10300  \tTraining accuracy: 0.824049711227417\n",
      "Step: 10300  \tValid loss: 0.38488808274269104\n",
      "Step: 10400  \tTraining loss: 0.32576271891593933\n",
      "Step: 10400  \tTraining accuracy: 0.8241530060768127\n",
      "Step: 10400  \tValid loss: 0.38491082191467285\n",
      "Step: 10500  \tTraining loss: 0.32551389932632446\n",
      "Step: 10500  \tTraining accuracy: 0.8242518305778503\n",
      "Step: 10500  \tValid loss: 0.38487064838409424\n",
      "Step: 10600  \tTraining loss: 0.3252660036087036\n",
      "Step: 10600  \tTraining accuracy: 0.824342668056488\n",
      "Step: 10600  \tValid loss: 0.3848561942577362\n",
      "Step: 10700  \tTraining loss: 0.32502448558807373\n",
      "Step: 10700  \tTraining accuracy: 0.8244280815124512\n",
      "Step: 10700  \tValid loss: 0.3848455250263214\n",
      "Step: 10800  \tTraining loss: 0.32478442788124084\n",
      "Step: 10800  \tTraining accuracy: 0.8245179653167725\n",
      "Step: 10800  \tValid loss: 0.38482755422592163\n",
      "Step: 10900  \tTraining loss: 0.32454901933670044\n",
      "Step: 10900  \tTraining accuracy: 0.8246074318885803\n",
      "Step: 10900  \tValid loss: 0.38478803634643555\n",
      "Step: 11000  \tTraining loss: 0.32432112097740173\n",
      "Step: 11000  \tTraining accuracy: 0.82469642162323\n",
      "Step: 11000  \tValid loss: 0.38480326533317566\n",
      "Step: 11100  \tTraining loss: 0.3240867257118225\n",
      "Step: 11100  \tTraining accuracy: 0.8247908353805542\n",
      "Step: 11100  \tValid loss: 0.38478416204452515\n",
      "Step: 11200  \tTraining loss: 0.3238604664802551\n",
      "Step: 11200  \tTraining accuracy: 0.8248777389526367\n",
      "Step: 11200  \tValid loss: 0.3847880959510803\n",
      "Step: 11300  \tTraining loss: 0.323635458946228\n",
      "Step: 11300  \tTraining accuracy: 0.8249492645263672\n",
      "Step: 11300  \tValid loss: 0.3848150372505188\n",
      "Step: 11400  \tTraining loss: 0.3234129250049591\n",
      "Step: 11400  \tTraining accuracy: 0.8250194787979126\n",
      "Step: 11400  \tValid loss: 0.38477107882499695\n",
      "Step: 11500  \tTraining loss: 0.323192298412323\n",
      "Step: 11500  \tTraining accuracy: 0.8250873684883118\n",
      "Step: 11500  \tValid loss: 0.3847697675228119\n",
      "Step: 11600  \tTraining loss: 0.3229762017726898\n",
      "Step: 11600  \tTraining accuracy: 0.8251540660858154\n",
      "Step: 11600  \tValid loss: 0.3847775459289551\n",
      "Step: 11700  \tTraining loss: 0.3227577209472656\n",
      "Step: 11700  \tTraining accuracy: 0.8252195715904236\n",
      "Step: 11700  \tValid loss: 0.3847368359565735\n",
      "Step: 11800  \tTraining loss: 0.32254353165626526\n",
      "Step: 11800  \tTraining accuracy: 0.8252840638160706\n",
      "Step: 11800  \tValid loss: 0.3847495913505554\n",
      "Step: 11900  \tTraining loss: 0.3223291039466858\n",
      "Step: 11900  \tTraining accuracy: 0.8253440856933594\n",
      "Step: 11900  \tValid loss: 0.3847416043281555\n",
      "Step: 12000  \tTraining loss: 0.3221159279346466\n",
      "Step: 12000  \tTraining accuracy: 0.8254075050354004\n",
      "Step: 12000  \tValid loss: 0.38474419713020325\n",
      "Step: 12100  \tTraining loss: 0.32190462946891785\n",
      "Step: 12100  \tTraining accuracy: 0.8254687786102295\n",
      "Step: 12100  \tValid loss: 0.38470736145973206\n",
      "Step: 12200  \tTraining loss: 0.32170024514198303\n",
      "Step: 12200  \tTraining accuracy: 0.8255193829536438\n",
      "Step: 12200  \tValid loss: 0.38476091623306274\n",
      "Step: 12300  \tTraining loss: 0.321489542722702\n",
      "Step: 12300  \tTraining accuracy: 0.8255628347396851\n",
      "Step: 12300  \tValid loss: 0.38471606373786926\n",
      "Step: 12400  \tTraining loss: 0.3212828040122986\n",
      "Step: 12400  \tTraining accuracy: 0.8256108164787292\n",
      "Step: 12400  \tValid loss: 0.38467535376548767\n",
      "Step: 12500  \tTraining loss: 0.3210776150226593\n",
      "Step: 12500  \tTraining accuracy: 0.8256590366363525\n",
      "Step: 12500  \tValid loss: 0.3846632242202759\n",
      "Step: 12600  \tTraining loss: 0.3208731710910797\n",
      "Step: 12600  \tTraining accuracy: 0.8257055282592773\n",
      "Step: 12600  \tValid loss: 0.38459959626197815\n",
      "Step: 12700  \tTraining loss: 0.32067182660102844\n",
      "Step: 12700  \tTraining accuracy: 0.8257502317428589\n",
      "Step: 12700  \tValid loss: 0.3845897316932678\n",
      "Step: 12800  \tTraining loss: 0.32047027349472046\n",
      "Step: 12800  \tTraining accuracy: 0.8257962465286255\n",
      "Step: 12800  \tValid loss: 0.3845628798007965\n",
      "Step: 12900  \tTraining loss: 0.3202727735042572\n",
      "Step: 12900  \tTraining accuracy: 0.8258435726165771\n",
      "Step: 12900  \tValid loss: 0.38450610637664795\n",
      "Step: 13000  \tTraining loss: 0.3200761675834656\n",
      "Step: 13000  \tTraining accuracy: 0.8258882164955139\n",
      "Step: 13000  \tValid loss: 0.3844428062438965\n",
      "Step: 13100  \tTraining loss: 0.31988340616226196\n",
      "Step: 13100  \tTraining accuracy: 0.8259351253509521\n",
      "Step: 13100  \tValid loss: 0.38442733883857727\n",
      "Step: 13200  \tTraining loss: 0.3196929097175598\n",
      "Step: 13200  \tTraining accuracy: 0.8259872198104858\n",
      "Step: 13200  \tValid loss: 0.38438573479652405\n",
      "Step: 13300  \tTraining loss: 0.31950631737709045\n",
      "Step: 13300  \tTraining accuracy: 0.8260385990142822\n",
      "Step: 13300  \tValid loss: 0.38441208004951477\n",
      "Step: 13400  \tTraining loss: 0.3193223774433136\n",
      "Step: 13400  \tTraining accuracy: 0.8260920643806458\n",
      "Step: 13400  \tValid loss: 0.38436204195022583\n",
      "Step: 13500  \tTraining loss: 0.3191417157649994\n",
      "Step: 13500  \tTraining accuracy: 0.8261438012123108\n",
      "Step: 13500  \tValid loss: 0.3843514025211334\n",
      "Step: 13600  \tTraining loss: 0.31896382570266724\n",
      "Step: 13600  \tTraining accuracy: 0.8261928558349609\n",
      "Step: 13600  \tValid loss: 0.3843218982219696\n",
      "Step: 13700  \tTraining loss: 0.3187890946865082\n",
      "Step: 13700  \tTraining accuracy: 0.8262345194816589\n",
      "Step: 13700  \tValid loss: 0.38428255915641785\n",
      "Step: 13800  \tTraining loss: 0.31861644983291626\n",
      "Step: 13800  \tTraining accuracy: 0.826277494430542\n",
      "Step: 13800  \tValid loss: 0.3842792212963104\n",
      "Step: 13900  \tTraining loss: 0.31844663619995117\n",
      "Step: 13900  \tTraining accuracy: 0.8263235688209534\n",
      "Step: 13900  \tValid loss: 0.38427478075027466\n",
      "Step: 14000  \tTraining loss: 0.3182791769504547\n",
      "Step: 14000  \tTraining accuracy: 0.8263745903968811\n",
      "Step: 14000  \tValid loss: 0.38426142930984497\n",
      "Step: 14100  \tTraining loss: 0.3181140720844269\n",
      "Step: 14100  \tTraining accuracy: 0.8264248967170715\n",
      "Step: 14100  \tValid loss: 0.38424426317214966\n",
      "Step: 14200  \tTraining loss: 0.3179514706134796\n",
      "Step: 14200  \tTraining accuracy: 0.8264781832695007\n",
      "Step: 14200  \tValid loss: 0.384238600730896\n",
      "Step: 14300  \tTraining loss: 0.31779128313064575\n",
      "Step: 14300  \tTraining accuracy: 0.8265324831008911\n",
      "Step: 14300  \tValid loss: 0.384238600730896\n",
      "Step: 14400  \tTraining loss: 0.31763240694999695\n",
      "Step: 14400  \tTraining accuracy: 0.8265969753265381\n",
      "Step: 14400  \tValid loss: 0.3842187821865082\n",
      "Step: 14500  \tTraining loss: 0.3174760341644287\n",
      "Step: 14500  \tTraining accuracy: 0.8266578316688538\n",
      "Step: 14500  \tValid loss: 0.38421767950057983\n",
      "Step: 14600  \tTraining loss: 0.3173218071460724\n",
      "Step: 14600  \tTraining accuracy: 0.8267205357551575\n",
      "Step: 14600  \tValid loss: 0.384210467338562\n",
      "Step: 14700  \tTraining loss: 0.3171694278717041\n",
      "Step: 14700  \tTraining accuracy: 0.8267797827720642\n",
      "Step: 14700  \tValid loss: 0.38420772552490234\n",
      "Step: 14800  \tTraining loss: 0.3170188367366791\n",
      "Step: 14800  \tTraining accuracy: 0.8268355131149292\n",
      "Step: 14800  \tValid loss: 0.3841991424560547\n",
      "Step: 14900  \tTraining loss: 0.31687068939208984\n",
      "Step: 14900  \tTraining accuracy: 0.8268870115280151\n",
      "Step: 14900  \tValid loss: 0.38419201970100403\n",
      "Step: 15000  \tTraining loss: 0.3167246878147125\n",
      "Step: 15000  \tTraining accuracy: 0.8269351720809937\n",
      "Step: 15000  \tValid loss: 0.3842066526412964\n",
      "Step: 15100  \tTraining loss: 0.31658005714416504\n",
      "Step: 15100  \tTraining accuracy: 0.8269827365875244\n",
      "Step: 15100  \tValid loss: 0.3842235207557678\n",
      "Step: 15200  \tTraining loss: 0.31643736362457275\n",
      "Step: 15200  \tTraining accuracy: 0.8270296454429626\n",
      "Step: 15200  \tValid loss: 0.3842374384403229\n",
      "Step: 15300  \tTraining loss: 0.3162955343723297\n",
      "Step: 15300  \tTraining accuracy: 0.8270759582519531\n",
      "Step: 15300  \tValid loss: 0.38422295451164246\n",
      "Step: 15400  \tTraining loss: 0.31615644693374634\n",
      "Step: 15400  \tTraining accuracy: 0.8271191716194153\n",
      "Step: 15400  \tValid loss: 0.3842449188232422\n",
      "Step: 15500  \tTraining loss: 0.31601688265800476\n",
      "Step: 15500  \tTraining accuracy: 0.8271600604057312\n",
      "Step: 15500  \tValid loss: 0.38421785831451416\n",
      "Step: 15600  \tTraining loss: 0.3158790171146393\n",
      "Step: 15600  \tTraining accuracy: 0.8271996378898621\n",
      "Step: 15600  \tValid loss: 0.3842134475708008\n",
      "Step: 15700  \tTraining loss: 0.31574299931526184\n",
      "Step: 15700  \tTraining accuracy: 0.8272353410720825\n",
      "Step: 15700  \tValid loss: 0.3842141926288605\n",
      "Step: 15800  \tTraining loss: 0.3156079053878784\n",
      "Step: 15800  \tTraining accuracy: 0.8272706270217896\n",
      "Step: 15800  \tValid loss: 0.3842165470123291\n",
      "Step: 15900  \tTraining loss: 0.3154737055301666\n",
      "Step: 15900  \tTraining accuracy: 0.8273054361343384\n",
      "Step: 15900  \tValid loss: 0.3841668963432312\n",
      "Step: 16000  \tTraining loss: 0.31534022092819214\n",
      "Step: 16000  \tTraining accuracy: 0.8273398280143738\n",
      "Step: 16000  \tValid loss: 0.38417595624923706\n",
      "Step: 16100  \tTraining loss: 0.3152095377445221\n",
      "Step: 16100  \tTraining accuracy: 0.8273802995681763\n",
      "Step: 16100  \tValid loss: 0.38417938351631165\n",
      "Step: 16200  \tTraining loss: 0.3150777220726013\n",
      "Step: 16200  \tTraining accuracy: 0.8274202346801758\n",
      "Step: 16200  \tValid loss: 0.38415244221687317\n",
      "Step: 16300  \tTraining loss: 0.3149467408657074\n",
      "Step: 16300  \tTraining accuracy: 0.8274669051170349\n",
      "Step: 16300  \tValid loss: 0.38412144780158997\n",
      "Step: 16400  \tTraining loss: 0.31481799483299255\n",
      "Step: 16400  \tTraining accuracy: 0.8275193572044373\n",
      "Step: 16400  \tValid loss: 0.3841139078140259\n",
      "Step: 16500  \tTraining loss: 0.31469014286994934\n",
      "Step: 16500  \tTraining accuracy: 0.8275711536407471\n",
      "Step: 16500  \tValid loss: 0.3840875029563904\n",
      "Step: 16600  \tTraining loss: 0.31456276774406433\n",
      "Step: 16600  \tTraining accuracy: 0.8276223540306091\n",
      "Step: 16600  \tValid loss: 0.3840516209602356\n",
      "Step: 16700  \tTraining loss: 0.31443652510643005\n",
      "Step: 16700  \tTraining accuracy: 0.8276729583740234\n",
      "Step: 16700  \tValid loss: 0.3840350806713104\n",
      "Step: 16800  \tTraining loss: 0.3143097460269928\n",
      "Step: 16800  \tTraining accuracy: 0.8277252316474915\n",
      "Step: 16800  \tValid loss: 0.38399866223335266\n",
      "Step: 16900  \tTraining loss: 0.3141838312149048\n",
      "Step: 16900  \tTraining accuracy: 0.8277815580368042\n",
      "Step: 16900  \tValid loss: 0.3839796781539917\n",
      "Step: 17000  \tTraining loss: 0.3140594959259033\n",
      "Step: 17000  \tTraining accuracy: 0.8278380036354065\n",
      "Step: 17000  \tValid loss: 0.3839488625526428\n",
      "Step: 17100  \tTraining loss: 0.31393635272979736\n",
      "Step: 17100  \tTraining accuracy: 0.8278937339782715\n",
      "Step: 17100  \tValid loss: 0.3838872015476227\n",
      "Step: 17200  \tTraining loss: 0.3138129413127899\n",
      "Step: 17200  \tTraining accuracy: 0.8279488682746887\n",
      "Step: 17200  \tValid loss: 0.3838455080986023\n",
      "Step: 17300  \tTraining loss: 0.3136901557445526\n",
      "Step: 17300  \tTraining accuracy: 0.8280063271522522\n",
      "Step: 17300  \tValid loss: 0.38382843136787415\n",
      "Step: 17400  \tTraining loss: 0.31356820464134216\n",
      "Step: 17400  \tTraining accuracy: 0.8280661702156067\n",
      "Step: 17400  \tValid loss: 0.38378220796585083\n",
      "Step: 17500  \tTraining loss: 0.31344664096832275\n",
      "Step: 17500  \tTraining accuracy: 0.8281267881393433\n",
      "Step: 17500  \tValid loss: 0.38370317220687866\n",
      "Step: 17600  \tTraining loss: 0.3133258819580078\n",
      "Step: 17600  \tTraining accuracy: 0.8281904458999634\n",
      "Step: 17600  \tValid loss: 0.3836614489555359\n",
      "Step: 17700  \tTraining loss: 0.3132052719593048\n",
      "Step: 17700  \tTraining accuracy: 0.8282533884048462\n",
      "Step: 17700  \tValid loss: 0.38360339403152466\n",
      "Step: 17800  \tTraining loss: 0.31308531761169434\n",
      "Step: 17800  \tTraining accuracy: 0.8283156156539917\n",
      "Step: 17800  \tValid loss: 0.38355734944343567\n",
      "Step: 17900  \tTraining loss: 0.3129662573337555\n",
      "Step: 17900  \tTraining accuracy: 0.8283742070198059\n",
      "Step: 17900  \tValid loss: 0.3834924101829529\n",
      "Step: 18000  \tTraining loss: 0.3128467798233032\n",
      "Step: 18000  \tTraining accuracy: 0.8284270763397217\n",
      "Step: 18000  \tValid loss: 0.3834354877471924\n",
      "Step: 18100  \tTraining loss: 0.3127293586730957\n",
      "Step: 18100  \tTraining accuracy: 0.8284743428230286\n",
      "Step: 18100  \tValid loss: 0.383360892534256\n",
      "Step: 18200  \tTraining loss: 0.31261077523231506\n",
      "Step: 18200  \tTraining accuracy: 0.8285195827484131\n",
      "Step: 18200  \tValid loss: 0.38331764936447144\n",
      "Step: 18300  \tTraining loss: 0.3124934434890747\n",
      "Step: 18300  \tTraining accuracy: 0.8285644054412842\n",
      "Step: 18300  \tValid loss: 0.3832709789276123\n",
      "Step: 18400  \tTraining loss: 0.31237661838531494\n",
      "Step: 18400  \tTraining accuracy: 0.8286086916923523\n",
      "Step: 18400  \tValid loss: 0.383199542760849\n",
      "Step: 18500  \tTraining loss: 0.31226032972335815\n",
      "Step: 18500  \tTraining accuracy: 0.8286504149436951\n",
      "Step: 18500  \tValid loss: 0.38315537571907043\n",
      "Step: 18600  \tTraining loss: 0.31214359402656555\n",
      "Step: 18600  \tTraining accuracy: 0.8286916613578796\n",
      "Step: 18600  \tValid loss: 0.3830626606941223\n",
      "Step: 18700  \tTraining loss: 0.3120284676551819\n",
      "Step: 18700  \tTraining accuracy: 0.8287345767021179\n",
      "Step: 18700  \tValid loss: 0.3829856812953949\n",
      "Step: 18800  \tTraining loss: 0.31191372871398926\n",
      "Step: 18800  \tTraining accuracy: 0.828777015209198\n",
      "Step: 18800  \tValid loss: 0.38293352723121643\n",
      "Step: 18900  \tTraining loss: 0.311799019575119\n",
      "Step: 18900  \tTraining accuracy: 0.8288190364837646\n",
      "Step: 18900  \tValid loss: 0.3828642666339874\n",
      "Step: 19000  \tTraining loss: 0.3116563856601715\n",
      "Step: 19000  \tTraining accuracy: 0.8288633227348328\n",
      "Step: 19000  \tValid loss: 0.38267239928245544\n",
      "Step: 19100  \tTraining loss: 0.3115307688713074\n",
      "Step: 19100  \tTraining accuracy: 0.8289058208465576\n",
      "Step: 19100  \tValid loss: 0.382488489151001\n",
      "Step: 19200  \tTraining loss: 0.31141045689582825\n",
      "Step: 19200  \tTraining accuracy: 0.8289478421211243\n",
      "Step: 19200  \tValid loss: 0.3823416531085968\n",
      "Step: 19300  \tTraining loss: 0.3112891912460327\n",
      "Step: 19300  \tTraining accuracy: 0.828988790512085\n",
      "Step: 19300  \tValid loss: 0.3821786642074585\n",
      "Step: 19400  \tTraining loss: 0.31116557121276855\n",
      "Step: 19400  \tTraining accuracy: 0.8290286064147949\n",
      "Step: 19400  \tValid loss: 0.38208943605422974\n",
      "Step: 19500  \tTraining loss: 0.31104031205177307\n",
      "Step: 19500  \tTraining accuracy: 0.8290680050849915\n",
      "Step: 19500  \tValid loss: 0.38193416595458984\n",
      "Step: 19600  \tTraining loss: 0.31091856956481934\n",
      "Step: 19600  \tTraining accuracy: 0.829105019569397\n",
      "Step: 19600  \tValid loss: 0.3817775547504425\n",
      "Step: 19700  \tTraining loss: 0.3107972741127014\n",
      "Step: 19700  \tTraining accuracy: 0.829140305519104\n",
      "Step: 19700  \tValid loss: 0.3816491365432739\n",
      "Step: 19800  \tTraining loss: 0.3106769323348999\n",
      "Step: 19800  \tTraining accuracy: 0.8291752934455872\n",
      "Step: 19800  \tValid loss: 0.38152411580085754\n",
      "Step: 19900  \tTraining loss: 0.31055688858032227\n",
      "Step: 19900  \tTraining accuracy: 0.8292098641395569\n",
      "Step: 19900  \tValid loss: 0.3814103901386261\n",
      "Step: 20000  \tTraining loss: 0.3104383647441864\n",
      "Step: 20000  \tTraining accuracy: 0.8292441368103027\n",
      "Step: 20000  \tValid loss: 0.3812812268733978\n",
      "Step: 20100  \tTraining loss: 0.31031736731529236\n",
      "Step: 20100  \tTraining accuracy: 0.8292780518531799\n",
      "Step: 20100  \tValid loss: 0.3811437487602234\n",
      "Step: 20200  \tTraining loss: 0.3102012574672699\n",
      "Step: 20200  \tTraining accuracy: 0.8293135762214661\n",
      "Step: 20200  \tValid loss: 0.38099536299705505\n",
      "Step: 20300  \tTraining loss: 0.3100847005844116\n",
      "Step: 20300  \tTraining accuracy: 0.8293532133102417\n",
      "Step: 20300  \tValid loss: 0.38093140721321106\n",
      "Step: 20400  \tTraining loss: 0.30996638536453247\n",
      "Step: 20400  \tTraining accuracy: 0.8293924927711487\n",
      "Step: 20400  \tValid loss: 0.3808087408542633\n",
      "Step: 20500  \tTraining loss: 0.30985015630722046\n",
      "Step: 20500  \tTraining accuracy: 0.829431414604187\n",
      "Step: 20500  \tValid loss: 0.38068804144859314\n",
      "Step: 20600  \tTraining loss: 0.3097369372844696\n",
      "Step: 20600  \tTraining accuracy: 0.8294699192047119\n",
      "Step: 20600  \tValid loss: 0.38061442971229553\n",
      "Step: 20700  \tTraining loss: 0.30962181091308594\n",
      "Step: 20700  \tTraining accuracy: 0.8295080661773682\n",
      "Step: 20700  \tValid loss: 0.3804817199707031\n",
      "Step: 20800  \tTraining loss: 0.3095073103904724\n",
      "Step: 20800  \tTraining accuracy: 0.829545795917511\n",
      "Step: 20800  \tValid loss: 0.38039684295654297\n",
      "Step: 20900  \tTraining loss: 0.3093911409378052\n",
      "Step: 20900  \tTraining accuracy: 0.8295807242393494\n",
      "Step: 20900  \tValid loss: 0.38029512763023376\n",
      "Step: 21000  \tTraining loss: 0.30927881598472595\n",
      "Step: 21000  \tTraining accuracy: 0.8296146988868713\n",
      "Step: 21000  \tValid loss: 0.3801783621311188\n",
      "Step: 21100  \tTraining loss: 0.3091646134853363\n",
      "Step: 21100  \tTraining accuracy: 0.8296501636505127\n",
      "Step: 21100  \tValid loss: 0.3801191449165344\n",
      "Step: 21200  \tTraining loss: 0.3090527355670929\n",
      "Step: 21200  \tTraining accuracy: 0.8296890258789062\n",
      "Step: 21200  \tValid loss: 0.38003331422805786\n",
      "Step: 21300  \tTraining loss: 0.3089420199394226\n",
      "Step: 21300  \tTraining accuracy: 0.8297299146652222\n",
      "Step: 21300  \tValid loss: 0.3799712657928467\n",
      "Step: 21400  \tTraining loss: 0.30883046984672546\n",
      "Step: 21400  \tTraining accuracy: 0.8297705054283142\n",
      "Step: 21400  \tValid loss: 0.37986791133880615\n",
      "Step: 21500  \tTraining loss: 0.30871865153312683\n",
      "Step: 21500  \tTraining accuracy: 0.8298088312149048\n",
      "Step: 21500  \tValid loss: 0.3797850012779236\n",
      "Step: 21600  \tTraining loss: 0.30861032009124756\n",
      "Step: 21600  \tTraining accuracy: 0.8298467993736267\n",
      "Step: 21600  \tValid loss: 0.37973299622535706\n",
      "Step: 21700  \tTraining loss: 0.3084988296031952\n",
      "Step: 21700  \tTraining accuracy: 0.8298844695091248\n",
      "Step: 21700  \tValid loss: 0.37962767481803894\n",
      "Step: 21800  \tTraining loss: 0.3083893954753876\n",
      "Step: 21800  \tTraining accuracy: 0.8299241662025452\n",
      "Step: 21800  \tValid loss: 0.37953218817710876\n",
      "Step: 21900  \tTraining loss: 0.3082781732082367\n",
      "Step: 21900  \tTraining accuracy: 0.8299658298492432\n",
      "Step: 21900  \tValid loss: 0.37947094440460205\n",
      "Step: 22000  \tTraining loss: 0.30816781520843506\n",
      "Step: 22000  \tTraining accuracy: 0.8300083875656128\n",
      "Step: 22000  \tValid loss: 0.3793739378452301\n",
      "Step: 22100  \tTraining loss: 0.3080584704875946\n",
      "Step: 22100  \tTraining accuracy: 0.8300504684448242\n",
      "Step: 22100  \tValid loss: 0.3792853057384491\n",
      "Step: 22200  \tTraining loss: 0.30795037746429443\n",
      "Step: 22200  \tTraining accuracy: 0.8300922513008118\n",
      "Step: 22200  \tValid loss: 0.3792110085487366\n",
      "Step: 22300  \tTraining loss: 0.30784109234809875\n",
      "Step: 22300  \tTraining accuracy: 0.8301336169242859\n",
      "Step: 22300  \tValid loss: 0.37916818261146545\n",
      "Step: 22400  \tTraining loss: 0.3077335059642792\n",
      "Step: 22400  \tTraining accuracy: 0.8301734328269958\n",
      "Step: 22400  \tValid loss: 0.379083514213562\n",
      "Step: 22500  \tTraining loss: 0.30762946605682373\n",
      "Step: 22500  \tTraining accuracy: 0.8302111625671387\n",
      "Step: 22500  \tValid loss: 0.3789995610713959\n",
      "Step: 22600  \tTraining loss: 0.3075222671031952\n",
      "Step: 22600  \tTraining accuracy: 0.8302485942840576\n",
      "Step: 22600  \tValid loss: 0.3789447844028473\n",
      "Step: 22700  \tTraining loss: 0.3074157238006592\n",
      "Step: 22700  \tTraining accuracy: 0.8302839398384094\n",
      "Step: 22700  \tValid loss: 0.37889719009399414\n",
      "Step: 22800  \tTraining loss: 0.3073098957538605\n",
      "Step: 22800  \tTraining accuracy: 0.8303178548812866\n",
      "Step: 22800  \tValid loss: 0.3788453936576843\n",
      "Step: 22900  \tTraining loss: 0.3072008788585663\n",
      "Step: 22900  \tTraining accuracy: 0.830348014831543\n",
      "Step: 22900  \tValid loss: 0.37879058718681335\n",
      "Step: 23000  \tTraining loss: 0.3070976138114929\n",
      "Step: 23000  \tTraining accuracy: 0.8303756713867188\n",
      "Step: 23000  \tValid loss: 0.37870705127716064\n",
      "Step: 23100  \tTraining loss: 0.30699044466018677\n",
      "Step: 23100  \tTraining accuracy: 0.8304014205932617\n",
      "Step: 23100  \tValid loss: 0.3786400854587555\n",
      "Step: 23200  \tTraining loss: 0.30688372254371643\n",
      "Step: 23200  \tTraining accuracy: 0.8304257392883301\n",
      "Step: 23200  \tValid loss: 0.37856388092041016\n",
      "Step: 23300  \tTraining loss: 0.3067843019962311\n",
      "Step: 23300  \tTraining accuracy: 0.8304516077041626\n",
      "Step: 23300  \tValid loss: 0.378531277179718\n",
      "Step: 23400  \tTraining loss: 0.30667608976364136\n",
      "Step: 23400  \tTraining accuracy: 0.8304800391197205\n",
      "Step: 23400  \tValid loss: 0.3784063458442688\n",
      "Step: 23500  \tTraining loss: 0.3065708875656128\n",
      "Step: 23500  \tTraining accuracy: 0.8305093050003052\n",
      "Step: 23500  \tValid loss: 0.37835168838500977\n",
      "Step: 23600  \tTraining loss: 0.3064633309841156\n",
      "Step: 23600  \tTraining accuracy: 0.8305383324623108\n",
      "Step: 23600  \tValid loss: 0.37828925251960754\n",
      "Step: 23700  \tTraining loss: 0.30636289715766907\n",
      "Step: 23700  \tTraining accuracy: 0.8305671215057373\n",
      "Step: 23700  \tValid loss: 0.3781908452510834\n",
      "Step: 23800  \tTraining loss: 0.3062556982040405\n",
      "Step: 23800  \tTraining accuracy: 0.830593466758728\n",
      "Step: 23800  \tValid loss: 0.3781663477420807\n",
      "Step: 23900  \tTraining loss: 0.30615076422691345\n",
      "Step: 23900  \tTraining accuracy: 0.8306190371513367\n",
      "Step: 23900  \tValid loss: 0.37809091806411743\n",
      "Step: 24000  \tTraining loss: 0.3060474097728729\n",
      "Step: 24000  \tTraining accuracy: 0.830644428730011\n",
      "Step: 24000  \tValid loss: 0.3780059814453125\n",
      "Step: 24100  \tTraining loss: 0.3059437572956085\n",
      "Step: 24100  \tTraining accuracy: 0.8306711912155151\n",
      "Step: 24100  \tValid loss: 0.37796157598495483\n",
      "Step: 24200  \tTraining loss: 0.30583661794662476\n",
      "Step: 24200  \tTraining accuracy: 0.8306987881660461\n",
      "Step: 24200  \tValid loss: 0.37789130210876465\n",
      "Step: 24300  \tTraining loss: 0.30573299527168274\n",
      "Step: 24300  \tTraining accuracy: 0.8307262063026428\n",
      "Step: 24300  \tValid loss: 0.377817302942276\n",
      "Step: 24400  \tTraining loss: 0.3056296706199646\n",
      "Step: 24400  \tTraining accuracy: 0.8307517766952515\n",
      "Step: 24400  \tValid loss: 0.37775611877441406\n",
      "Step: 24500  \tTraining loss: 0.30552610754966736\n",
      "Step: 24500  \tTraining accuracy: 0.8307760953903198\n",
      "Step: 24500  \tValid loss: 0.3776688873767853\n",
      "Step: 24600  \tTraining loss: 0.30542394518852234\n",
      "Step: 24600  \tTraining accuracy: 0.8308002352714539\n",
      "Step: 24600  \tValid loss: 0.37760859727859497\n",
      "Step: 24700  \tTraining loss: 0.30532053112983704\n",
      "Step: 24700  \tTraining accuracy: 0.8308267593383789\n",
      "Step: 24700  \tValid loss: 0.37751635909080505\n",
      "Step: 24800  \tTraining loss: 0.3052213788032532\n",
      "Step: 24800  \tTraining accuracy: 0.8308557271957397\n",
      "Step: 24800  \tValid loss: 0.37750086188316345\n",
      "Step: 24900  \tTraining loss: 0.30511438846588135\n",
      "Step: 24900  \tTraining accuracy: 0.8308844566345215\n",
      "Step: 24900  \tValid loss: 0.37736940383911133\n",
      "Step: 25000  \tTraining loss: 0.30501309037208557\n",
      "Step: 25000  \tTraining accuracy: 0.8309129476547241\n",
      "Step: 25000  \tValid loss: 0.3772881031036377\n",
      "Step: 25100  \tTraining loss: 0.3049089014530182\n",
      "Step: 25100  \tTraining accuracy: 0.8309412598609924\n",
      "Step: 25100  \tValid loss: 0.377236008644104\n",
      "Step: 25200  \tTraining loss: 0.3048076331615448\n",
      "Step: 25200  \tTraining accuracy: 0.8309692740440369\n",
      "Step: 25200  \tValid loss: 0.3771587610244751\n",
      "Step: 25300  \tTraining loss: 0.3047054409980774\n",
      "Step: 25300  \tTraining accuracy: 0.830997109413147\n",
      "Step: 25300  \tValid loss: 0.3770703673362732\n",
      "Step: 25400  \tTraining loss: 0.30460456013679504\n",
      "Step: 25400  \tTraining accuracy: 0.831024706363678\n",
      "Step: 25400  \tValid loss: 0.37699878215789795\n",
      "Step: 25500  \tTraining loss: 0.30450350046157837\n",
      "Step: 25500  \tTraining accuracy: 0.8310521245002747\n",
      "Step: 25500  \tValid loss: 0.376898854970932\n",
      "Step: 25600  \tTraining loss: 0.3044019341468811\n",
      "Step: 25600  \tTraining accuracy: 0.8310793042182922\n",
      "Step: 25600  \tValid loss: 0.37681785225868225\n",
      "Step: 25700  \tTraining loss: 0.3043017089366913\n",
      "Step: 25700  \tTraining accuracy: 0.8311092853546143\n",
      "Step: 25700  \tValid loss: 0.37673264741897583\n",
      "Step: 25800  \tTraining loss: 0.3042023181915283\n",
      "Step: 25800  \tTraining accuracy: 0.8311411142349243\n",
      "Step: 25800  \tValid loss: 0.3766607940196991\n",
      "Step: 25900  \tTraining loss: 0.3041013777256012\n",
      "Step: 25900  \tTraining accuracy: 0.8311726450920105\n",
      "Step: 25900  \tValid loss: 0.3765772879123688\n",
      "Step: 26000  \tTraining loss: 0.30400219559669495\n",
      "Step: 26000  \tTraining accuracy: 0.8312039375305176\n",
      "Step: 26000  \tValid loss: 0.3764839172363281\n",
      "Step: 26100  \tTraining loss: 0.3039035499095917\n",
      "Step: 26100  \tTraining accuracy: 0.8312350511550903\n",
      "Step: 26100  \tValid loss: 0.37640005350112915\n",
      "Step: 26200  \tTraining loss: 0.3038051128387451\n",
      "Step: 26200  \tTraining accuracy: 0.8312658667564392\n",
      "Step: 26200  \tValid loss: 0.3762855529785156\n",
      "Step: 26300  \tTraining loss: 0.30370768904685974\n",
      "Step: 26300  \tTraining accuracy: 0.831296443939209\n",
      "Step: 26300  \tValid loss: 0.3762156665325165\n",
      "Step: 26400  \tTraining loss: 0.3036099672317505\n",
      "Step: 26400  \tTraining accuracy: 0.8313268423080444\n",
      "Step: 26400  \tValid loss: 0.3761250078678131\n",
      "Step: 26500  \tTraining loss: 0.3035133481025696\n",
      "Step: 26500  \tTraining accuracy: 0.8313579559326172\n",
      "Step: 26500  \tValid loss: 0.37604326009750366\n",
      "Step: 26600  \tTraining loss: 0.3034142255783081\n",
      "Step: 26600  \tTraining accuracy: 0.8313902616500854\n",
      "Step: 26600  \tValid loss: 0.3759390115737915\n",
      "Step: 26700  \tTraining loss: 0.3033178448677063\n",
      "Step: 26700  \tTraining accuracy: 0.8314223885536194\n",
      "Step: 26700  \tValid loss: 0.3758411705493927\n",
      "Step: 26800  \tTraining loss: 0.30322161316871643\n",
      "Step: 26800  \tTraining accuracy: 0.8314542770385742\n",
      "Step: 26800  \tValid loss: 0.3757565915584564\n",
      "Step: 26900  \tTraining loss: 0.3031262159347534\n",
      "Step: 26900  \tTraining accuracy: 0.8314839601516724\n",
      "Step: 26900  \tValid loss: 0.37565141916275024\n",
      "Step: 27000  \tTraining loss: 0.3030308187007904\n",
      "Step: 27000  \tTraining accuracy: 0.8315129280090332\n",
      "Step: 27000  \tValid loss: 0.3755568265914917\n",
      "Step: 27100  \tTraining loss: 0.30293506383895874\n",
      "Step: 27100  \tTraining accuracy: 0.8315417170524597\n",
      "Step: 27100  \tValid loss: 0.3754580616950989\n",
      "Step: 27200  \tTraining loss: 0.3028407394886017\n",
      "Step: 27200  \tTraining accuracy: 0.8315708041191101\n",
      "Step: 27200  \tValid loss: 0.3753482401371002\n",
      "Step: 27300  \tTraining loss: 0.3027458190917969\n",
      "Step: 27300  \tTraining accuracy: 0.8316024541854858\n",
      "Step: 27300  \tValid loss: 0.3752731680870056\n",
      "Step: 27400  \tTraining loss: 0.3026546835899353\n",
      "Step: 27400  \tTraining accuracy: 0.8316386938095093\n",
      "Step: 27400  \tValid loss: 0.3751829266548157\n",
      "Step: 27500  \tTraining loss: 0.30255982279777527\n",
      "Step: 27500  \tTraining accuracy: 0.8316760659217834\n",
      "Step: 27500  \tValid loss: 0.3750765919685364\n",
      "Step: 27600  \tTraining loss: 0.3024698495864868\n",
      "Step: 27600  \tTraining accuracy: 0.8317131996154785\n",
      "Step: 27600  \tValid loss: 0.3749947249889374\n",
      "Step: 27700  \tTraining loss: 0.3023742735385895\n",
      "Step: 27700  \tTraining accuracy: 0.8317500352859497\n",
      "Step: 27700  \tValid loss: 0.3748782277107239\n",
      "Step: 27800  \tTraining loss: 0.30228373408317566\n",
      "Step: 27800  \tTraining accuracy: 0.8317894339561462\n",
      "Step: 27800  \tValid loss: 0.3747822642326355\n",
      "Step: 27900  \tTraining loss: 0.3021913766860962\n",
      "Step: 27900  \tTraining accuracy: 0.8318294286727905\n",
      "Step: 27900  \tValid loss: 0.374648779630661\n",
      "Step: 28000  \tTraining loss: 0.30209922790527344\n",
      "Step: 28000  \tTraining accuracy: 0.8318668603897095\n",
      "Step: 28000  \tValid loss: 0.3745828866958618\n",
      "Step: 28100  \tTraining loss: 0.3020084798336029\n",
      "Step: 28100  \tTraining accuracy: 0.8319044709205627\n",
      "Step: 28100  \tValid loss: 0.3744669556617737\n",
      "Step: 28200  \tTraining loss: 0.3019184470176697\n",
      "Step: 28200  \tTraining accuracy: 0.8319423198699951\n",
      "Step: 28200  \tValid loss: 0.3743635416030884\n",
      "Step: 28300  \tTraining loss: 0.30182820558547974\n",
      "Step: 28300  \tTraining accuracy: 0.8319798707962036\n",
      "Step: 28300  \tValid loss: 0.3742745518684387\n",
      "Step: 28400  \tTraining loss: 0.3017388880252838\n",
      "Step: 28400  \tTraining accuracy: 0.8320184946060181\n",
      "Step: 28400  \tValid loss: 0.3741642236709595\n",
      "Step: 28500  \tTraining loss: 0.3016510605812073\n",
      "Step: 28500  \tTraining accuracy: 0.8320578336715698\n",
      "Step: 28500  \tValid loss: 0.37406206130981445\n",
      "Step: 28600  \tTraining loss: 0.3015621602535248\n",
      "Step: 28600  \tTraining accuracy: 0.8320977687835693\n",
      "Step: 28600  \tValid loss: 0.3739517331123352\n",
      "Step: 28700  \tTraining loss: 0.3014737069606781\n",
      "Step: 28700  \tTraining accuracy: 0.8321342468261719\n",
      "Step: 28700  \tValid loss: 0.3738445043563843\n",
      "Step: 28800  \tTraining loss: 0.3013858497142792\n",
      "Step: 28800  \tTraining accuracy: 0.8321704864501953\n",
      "Step: 28800  \tValid loss: 0.37374454736709595\n",
      "Step: 28900  \tTraining loss: 0.30129778385162354\n",
      "Step: 28900  \tTraining accuracy: 0.8322064876556396\n",
      "Step: 28900  \tValid loss: 0.37364742159843445\n",
      "Step: 29000  \tTraining loss: 0.3012107312679291\n",
      "Step: 29000  \tTraining accuracy: 0.8322439789772034\n",
      "Step: 29000  \tValid loss: 0.37353116273880005\n",
      "Step: 29100  \tTraining loss: 0.301123708486557\n",
      "Step: 29100  \tTraining accuracy: 0.8322834968566895\n",
      "Step: 29100  \tValid loss: 0.3734463155269623\n",
      "Step: 29200  \tTraining loss: 0.3010382354259491\n",
      "Step: 29200  \tTraining accuracy: 0.8323231935501099\n",
      "Step: 29200  \tValid loss: 0.3733704388141632\n",
      "Step: 29300  \tTraining loss: 0.3009502589702606\n",
      "Step: 29300  \tTraining accuracy: 0.8323647975921631\n",
      "Step: 29300  \tValid loss: 0.3732529580593109\n",
      "Step: 29400  \tTraining loss: 0.3008643090724945\n",
      "Step: 29400  \tTraining accuracy: 0.8324061632156372\n",
      "Step: 29400  \tValid loss: 0.3731549084186554\n",
      "Step: 29500  \tTraining loss: 0.3007785975933075\n",
      "Step: 29500  \tTraining accuracy: 0.8324472308158875\n",
      "Step: 29500  \tValid loss: 0.37304845452308655\n",
      "Step: 29600  \tTraining loss: 0.3006943464279175\n",
      "Step: 29600  \tTraining accuracy: 0.8324880599975586\n",
      "Step: 29600  \tValid loss: 0.3729613125324249\n",
      "Step: 29700  \tTraining loss: 0.30060797929763794\n",
      "Step: 29700  \tTraining accuracy: 0.8325285315513611\n",
      "Step: 29700  \tValid loss: 0.3728654384613037\n",
      "Step: 29800  \tTraining loss: 0.3005220592021942\n",
      "Step: 29800  \tTraining accuracy: 0.8325696587562561\n",
      "Step: 29800  \tValid loss: 0.37276434898376465\n",
      "Step: 29900  \tTraining loss: 0.30043932795524597\n",
      "Step: 29900  \tTraining accuracy: 0.8326117992401123\n",
      "Step: 29900  \tValid loss: 0.37267979979515076\n",
      "Step: 30000  \tTraining loss: 0.3003532290458679\n",
      "Step: 30000  \tTraining accuracy: 0.8326537013053894\n",
      "Step: 30000  \tValid loss: 0.37254956364631653\n",
      "Step: 30100  \tTraining loss: 0.3002655804157257\n",
      "Step: 30100  \tTraining accuracy: 0.8326953053474426\n",
      "Step: 30100  \tValid loss: 0.3724512457847595\n",
      "Step: 30200  \tTraining loss: 0.3001776933670044\n",
      "Step: 30200  \tTraining accuracy: 0.832736611366272\n",
      "Step: 30200  \tValid loss: 0.3723645508289337\n",
      "Step: 30300  \tTraining loss: 0.2996690273284912\n",
      "Step: 30300  \tTraining accuracy: 0.832780659198761\n",
      "Step: 30300  \tValid loss: 0.3720107972621918\n",
      "Step: 30400  \tTraining loss: 0.2994508147239685\n",
      "Step: 30400  \tTraining accuracy: 0.8328312635421753\n",
      "Step: 30400  \tValid loss: 0.3721911311149597\n",
      "Step: 30500  \tTraining loss: 0.29929572343826294\n",
      "Step: 30500  \tTraining accuracy: 0.8328807353973389\n",
      "Step: 30500  \tValid loss: 0.3722643256187439\n",
      "Step: 30600  \tTraining loss: 0.29915884137153625\n",
      "Step: 30600  \tTraining accuracy: 0.8329285383224487\n",
      "Step: 30600  \tValid loss: 0.3722105026245117\n",
      "Step: 30700  \tTraining loss: 0.29903504252433777\n",
      "Step: 30700  \tTraining accuracy: 0.8329769372940063\n",
      "Step: 30700  \tValid loss: 0.37214043736457825\n",
      "Step: 30800  \tTraining loss: 0.29891836643218994\n",
      "Step: 30800  \tTraining accuracy: 0.8330245614051819\n",
      "Step: 30800  \tValid loss: 0.37208542227745056\n",
      "Step: 30900  \tTraining loss: 0.2988058030605316\n",
      "Step: 30900  \tTraining accuracy: 0.8330702185630798\n",
      "Step: 30900  \tValid loss: 0.37198787927627563\n",
      "Step: 31000  \tTraining loss: 0.2986984848976135\n",
      "Step: 31000  \tTraining accuracy: 0.8331155776977539\n",
      "Step: 31000  \tValid loss: 0.37191107869148254\n",
      "Step: 31100  \tTraining loss: 0.2985924482345581\n",
      "Step: 31100  \tTraining accuracy: 0.8331605792045593\n",
      "Step: 31100  \tValid loss: 0.371808260679245\n",
      "Step: 31200  \tTraining loss: 0.298488587141037\n",
      "Step: 31200  \tTraining accuracy: 0.8332053422927856\n",
      "Step: 31200  \tValid loss: 0.3717104196548462\n",
      "Step: 31300  \tTraining loss: 0.2983892261981964\n",
      "Step: 31300  \tTraining accuracy: 0.8332498669624329\n",
      "Step: 31300  \tValid loss: 0.37162482738494873\n",
      "Step: 31400  \tTraining loss: 0.29828834533691406\n",
      "Step: 31400  \tTraining accuracy: 0.8332940340042114\n",
      "Step: 31400  \tValid loss: 0.3715238571166992\n",
      "Step: 31500  \tTraining loss: 0.2981853485107422\n",
      "Step: 31500  \tTraining accuracy: 0.833341658115387\n",
      "Step: 31500  \tValid loss: 0.3714047968387604\n",
      "Step: 31600  \tTraining loss: 0.29808729887008667\n",
      "Step: 31600  \tTraining accuracy: 0.833392322063446\n",
      "Step: 31600  \tValid loss: 0.3713191747665405\n",
      "Step: 31700  \tTraining loss: 0.29798758029937744\n",
      "Step: 31700  \tTraining accuracy: 0.8334434628486633\n",
      "Step: 31700  \tValid loss: 0.3712072968482971\n",
      "Step: 31800  \tTraining loss: 0.2978880703449249\n",
      "Step: 31800  \tTraining accuracy: 0.8334954977035522\n",
      "Step: 31800  \tValid loss: 0.37109559774398804\n",
      "Step: 31900  \tTraining loss: 0.2977895438671112\n",
      "Step: 31900  \tTraining accuracy: 0.8335443735122681\n",
      "Step: 31900  \tValid loss: 0.3709966838359833\n",
      "Step: 32000  \tTraining loss: 0.2976905107498169\n",
      "Step: 32000  \tTraining accuracy: 0.8335928916931152\n",
      "Step: 32000  \tValid loss: 0.3709038197994232\n",
      "Step: 32100  \tTraining loss: 0.2975940406322479\n",
      "Step: 32100  \tTraining accuracy: 0.8336423635482788\n",
      "Step: 32100  \tValid loss: 0.3707876205444336\n",
      "Step: 32200  \tTraining loss: 0.29749542474746704\n",
      "Step: 32200  \tTraining accuracy: 0.8336923122406006\n",
      "Step: 32200  \tValid loss: 0.3706967830657959\n",
      "Step: 32300  \tTraining loss: 0.2973979413509369\n",
      "Step: 32300  \tTraining accuracy: 0.8337427973747253\n",
      "Step: 32300  \tValid loss: 0.37060311436653137\n",
      "Step: 32400  \tTraining loss: 0.29730308055877686\n",
      "Step: 32400  \tTraining accuracy: 0.8337913155555725\n",
      "Step: 32400  \tValid loss: 0.3704856336116791\n",
      "Step: 32500  \tTraining loss: 0.2972055673599243\n",
      "Step: 32500  \tTraining accuracy: 0.833842396736145\n",
      "Step: 32500  \tValid loss: 0.370414137840271\n",
      "Step: 32600  \tTraining loss: 0.2970775365829468\n",
      "Step: 32600  \tTraining accuracy: 0.8338910937309265\n",
      "Step: 32600  \tValid loss: 0.37031489610671997\n",
      "Step: 32700  \tTraining loss: 0.29697349667549133\n",
      "Step: 32700  \tTraining accuracy: 0.8339415788650513\n",
      "Step: 32700  \tValid loss: 0.3703994154930115\n",
      "Step: 32800  \tTraining loss: 0.296869695186615\n",
      "Step: 32800  \tTraining accuracy: 0.8339881300926208\n",
      "Step: 32800  \tValid loss: 0.3703210949897766\n",
      "Step: 32900  \tTraining loss: 0.2967732548713684\n",
      "Step: 32900  \tTraining accuracy: 0.8340351581573486\n",
      "Step: 32900  \tValid loss: 0.37023741006851196\n",
      "Step: 33000  \tTraining loss: 0.2966752350330353\n",
      "Step: 33000  \tTraining accuracy: 0.8340827226638794\n",
      "Step: 33000  \tValid loss: 0.3701359033584595\n",
      "Step: 33100  \tTraining loss: 0.2965748906135559\n",
      "Step: 33100  \tTraining accuracy: 0.8341299891471863\n",
      "Step: 33100  \tValid loss: 0.37003251910209656\n",
      "Step: 33200  \tTraining loss: 0.296478807926178\n",
      "Step: 33200  \tTraining accuracy: 0.8341769576072693\n",
      "Step: 33200  \tValid loss: 0.3699343800544739\n",
      "Step: 33300  \tTraining loss: 0.296381413936615\n",
      "Step: 33300  \tTraining accuracy: 0.8342236876487732\n",
      "Step: 33300  \tValid loss: 0.36983269453048706\n",
      "Step: 33400  \tTraining loss: 0.29628312587738037\n",
      "Step: 33400  \tTraining accuracy: 0.8342701196670532\n",
      "Step: 33400  \tValid loss: 0.3697640001773834\n",
      "Step: 33500  \tTraining loss: 0.296190470457077\n",
      "Step: 33500  \tTraining accuracy: 0.8343162536621094\n",
      "Step: 33500  \tValid loss: 0.36968159675598145\n",
      "Step: 33600  \tTraining loss: 0.29609304666519165\n",
      "Step: 33600  \tTraining accuracy: 0.8343621492385864\n",
      "Step: 33600  \tValid loss: 0.3696068227291107\n",
      "Step: 33700  \tTraining loss: 0.29599595069885254\n",
      "Step: 33700  \tTraining accuracy: 0.8344077467918396\n",
      "Step: 33700  \tValid loss: 0.36951982975006104\n",
      "Step: 33800  \tTraining loss: 0.295899361371994\n",
      "Step: 33800  \tTraining accuracy: 0.8344530463218689\n",
      "Step: 33800  \tValid loss: 0.3694160580635071\n",
      "Step: 33900  \tTraining loss: 0.29580405354499817\n",
      "Step: 33900  \tTraining accuracy: 0.8344981074333191\n",
      "Step: 33900  \tValid loss: 0.36935630440711975\n",
      "Step: 34000  \tTraining loss: 0.2957097589969635\n",
      "Step: 34000  \tTraining accuracy: 0.8345429301261902\n",
      "Step: 34000  \tValid loss: 0.3692774176597595\n",
      "Step: 34100  \tTraining loss: 0.29561087489128113\n",
      "Step: 34100  \tTraining accuracy: 0.8345893621444702\n",
      "Step: 34100  \tValid loss: 0.3691720962524414\n",
      "Step: 34200  \tTraining loss: 0.29551807045936584\n",
      "Step: 34200  \tTraining accuracy: 0.8346355557441711\n",
      "Step: 34200  \tValid loss: 0.36908605694770813\n",
      "Step: 34300  \tTraining loss: 0.29542428255081177\n",
      "Step: 34300  \tTraining accuracy: 0.8346803188323975\n",
      "Step: 34300  \tValid loss: 0.36903926730155945\n",
      "Step: 34400  \tTraining loss: 0.29532450437545776\n",
      "Step: 34400  \tTraining accuracy: 0.834725558757782\n",
      "Step: 34400  \tValid loss: 0.36892837285995483\n",
      "Step: 34500  \tTraining loss: 0.2952304780483246\n",
      "Step: 34500  \tTraining accuracy: 0.8347709774971008\n",
      "Step: 34500  \tValid loss: 0.36886122822761536\n",
      "Step: 34600  \tTraining loss: 0.2951342463493347\n",
      "Step: 34600  \tTraining accuracy: 0.8348168134689331\n",
      "Step: 34600  \tValid loss: 0.3687728941440582\n",
      "Step: 34700  \tTraining loss: 0.29504162073135376\n",
      "Step: 34700  \tTraining accuracy: 0.834863543510437\n",
      "Step: 34700  \tValid loss: 0.36870890855789185\n",
      "Step: 34800  \tTraining loss: 0.29494166374206543\n",
      "Step: 34800  \tTraining accuracy: 0.834909975528717\n",
      "Step: 34800  \tValid loss: 0.36861008405685425\n",
      "Step: 34900  \tTraining loss: 0.2948494851589203\n",
      "Step: 34900  \tTraining accuracy: 0.8349546790122986\n",
      "Step: 34900  \tValid loss: 0.3685212731361389\n",
      "Step: 35000  \tTraining loss: 0.29475051164627075\n",
      "Step: 35000  \tTraining accuracy: 0.8349998593330383\n",
      "Step: 35000  \tValid loss: 0.3684641420841217\n",
      "Step: 35100  \tTraining loss: 0.29465430974960327\n",
      "Step: 35100  \tTraining accuracy: 0.835043728351593\n",
      "Step: 35100  \tValid loss: 0.3683721423149109\n",
      "Step: 35200  \tTraining loss: 0.29455775022506714\n",
      "Step: 35200  \tTraining accuracy: 0.8350872993469238\n",
      "Step: 35200  \tValid loss: 0.36830273270606995\n",
      "Step: 35300  \tTraining loss: 0.29446420073509216\n",
      "Step: 35300  \tTraining accuracy: 0.8351294994354248\n",
      "Step: 35300  \tValid loss: 0.3682522773742676\n",
      "Step: 35400  \tTraining loss: 0.29436665773391724\n",
      "Step: 35400  \tTraining accuracy: 0.8351725339889526\n",
      "Step: 35400  \tValid loss: 0.3681437373161316\n",
      "Step: 35500  \tTraining loss: 0.29427433013916016\n",
      "Step: 35500  \tTraining accuracy: 0.8352153897285461\n",
      "Step: 35500  \tValid loss: 0.36806148290634155\n",
      "Step: 35600  \tTraining loss: 0.2941710948944092\n",
      "Step: 35600  \tTraining accuracy: 0.8352580070495605\n",
      "Step: 35600  \tValid loss: 0.36800238490104675\n",
      "Step: 35700  \tTraining loss: 0.29407498240470886\n",
      "Step: 35700  \tTraining accuracy: 0.8352996110916138\n",
      "Step: 35700  \tValid loss: 0.3679238557815552\n",
      "Step: 35800  \tTraining loss: 0.2939755618572235\n",
      "Step: 35800  \tTraining accuracy: 0.83534175157547\n",
      "Step: 35800  \tValid loss: 0.3678745627403259\n",
      "Step: 35900  \tTraining loss: 0.293876975774765\n",
      "Step: 35900  \tTraining accuracy: 0.8353835940361023\n",
      "Step: 35900  \tValid loss: 0.36778420209884644\n",
      "Step: 36000  \tTraining loss: 0.29378440976142883\n",
      "Step: 36000  \tTraining accuracy: 0.8354252576828003\n",
      "Step: 36000  \tValid loss: 0.36776530742645264\n",
      "Step: 36100  \tTraining loss: 0.29368099570274353\n",
      "Step: 36100  \tTraining accuracy: 0.8354670405387878\n",
      "Step: 36100  \tValid loss: 0.3676505982875824\n",
      "Step: 36200  \tTraining loss: 0.29358232021331787\n",
      "Step: 36200  \tTraining accuracy: 0.8355100154876709\n",
      "Step: 36200  \tValid loss: 0.3675861954689026\n",
      "Step: 36300  \tTraining loss: 0.2934824526309967\n",
      "Step: 36300  \tTraining accuracy: 0.8355510234832764\n",
      "Step: 36300  \tValid loss: 0.36750665307044983\n",
      "Step: 36400  \tTraining loss: 0.2933894097805023\n",
      "Step: 36400  \tTraining accuracy: 0.835591733455658\n",
      "Step: 36400  \tValid loss: 0.3674241602420807\n",
      "Step: 36500  \tTraining loss: 0.2932853102684021\n",
      "Step: 36500  \tTraining accuracy: 0.8356322646141052\n",
      "Step: 36500  \tValid loss: 0.3673931956291199\n",
      "Step: 36600  \tTraining loss: 0.29318466782569885\n",
      "Step: 36600  \tTraining accuracy: 0.8356725573539734\n",
      "Step: 36600  \tValid loss: 0.3673230707645416\n",
      "Step: 36700  \tTraining loss: 0.29308420419692993\n",
      "Step: 36700  \tTraining accuracy: 0.8357126116752625\n",
      "Step: 36700  \tValid loss: 0.36725538969039917\n",
      "Step: 36800  \tTraining loss: 0.2929854691028595\n",
      "Step: 36800  \tTraining accuracy: 0.8357524275779724\n",
      "Step: 36800  \tValid loss: 0.36720573902130127\n",
      "Step: 36900  \tTraining loss: 0.2928829491138458\n",
      "Step: 36900  \tTraining accuracy: 0.835792064666748\n",
      "Step: 36900  \tValid loss: 0.36713719367980957\n",
      "Step: 37000  \tTraining loss: 0.2927842438220978\n",
      "Step: 37000  \tTraining accuracy: 0.8358315229415894\n",
      "Step: 37000  \tValid loss: 0.3670770823955536\n",
      "Step: 37100  \tTraining loss: 0.2926824390888214\n",
      "Step: 37100  \tTraining accuracy: 0.8358706831932068\n",
      "Step: 37100  \tValid loss: 0.3670238256454468\n",
      "Step: 37200  \tTraining loss: 0.29258206486701965\n",
      "Step: 37200  \tTraining accuracy: 0.8359097242355347\n",
      "Step: 37200  \tValid loss: 0.3669731914997101\n",
      "Step: 37300  \tTraining loss: 0.2924775779247284\n",
      "Step: 37300  \tTraining accuracy: 0.8359484672546387\n",
      "Step: 37300  \tValid loss: 0.36690521240234375\n",
      "Step: 37400  \tTraining loss: 0.29237625002861023\n",
      "Step: 37400  \tTraining accuracy: 0.8359870910644531\n",
      "Step: 37400  \tValid loss: 0.3668559193611145\n",
      "Step: 37500  \tTraining loss: 0.29227209091186523\n",
      "Step: 37500  \tTraining accuracy: 0.8360254168510437\n",
      "Step: 37500  \tValid loss: 0.36679211258888245\n",
      "Step: 37600  \tTraining loss: 0.2921724021434784\n",
      "Step: 37600  \tTraining accuracy: 0.8360618948936462\n",
      "Step: 37600  \tValid loss: 0.3667570948600769\n",
      "Step: 37700  \tTraining loss: 0.29206666350364685\n",
      "Step: 37700  \tTraining accuracy: 0.8360981345176697\n",
      "Step: 37700  \tValid loss: 0.3666536808013916\n",
      "Step: 37800  \tTraining loss: 0.2919639050960541\n",
      "Step: 37800  \tTraining accuracy: 0.8361341953277588\n",
      "Step: 37800  \tValid loss: 0.36660075187683105\n",
      "Step: 37900  \tTraining loss: 0.2918560802936554\n",
      "Step: 37900  \tTraining accuracy: 0.8361703753471375\n",
      "Step: 37900  \tValid loss: 0.36657941341400146\n",
      "Step: 38000  \tTraining loss: 0.29175272583961487\n",
      "Step: 38000  \tTraining accuracy: 0.836207389831543\n",
      "Step: 38000  \tValid loss: 0.3665069043636322\n",
      "Step: 38100  \tTraining loss: 0.2916499972343445\n",
      "Step: 38100  \tTraining accuracy: 0.8362445831298828\n",
      "Step: 38100  \tValid loss: 0.3665040135383606\n",
      "Step: 38200  \tTraining loss: 0.2915410101413727\n",
      "Step: 38200  \tTraining accuracy: 0.8362815976142883\n",
      "Step: 38200  \tValid loss: 0.3664201498031616\n",
      "Step: 38300  \tTraining loss: 0.2914348840713501\n",
      "Step: 38300  \tTraining accuracy: 0.8363183736801147\n",
      "Step: 38300  \tValid loss: 0.3663707971572876\n",
      "Step: 38400  \tTraining loss: 0.29133182764053345\n",
      "Step: 38400  \tTraining accuracy: 0.8363543152809143\n",
      "Step: 38400  \tValid loss: 0.3663613498210907\n",
      "Step: 38500  \tTraining loss: 0.29122355580329895\n",
      "Step: 38500  \tTraining accuracy: 0.8363890647888184\n",
      "Step: 38500  \tValid loss: 0.3662773370742798\n",
      "Step: 38600  \tTraining loss: 0.2911187708377838\n",
      "Step: 38600  \tTraining accuracy: 0.836426317691803\n",
      "Step: 38600  \tValid loss: 0.36624860763549805\n",
      "Step: 38700  \tTraining loss: 0.2910133898258209\n",
      "Step: 38700  \tTraining accuracy: 0.8364640474319458\n",
      "Step: 38700  \tValid loss: 0.3662160336971283\n",
      "Step: 38800  \tTraining loss: 0.2909092307090759\n",
      "Step: 38800  \tTraining accuracy: 0.8365015387535095\n",
      "Step: 38800  \tValid loss: 0.3661426901817322\n",
      "Step: 38900  \tTraining loss: 0.29080498218536377\n",
      "Step: 38900  \tTraining accuracy: 0.8365378975868225\n",
      "Step: 38900  \tValid loss: 0.3661075532436371\n",
      "Step: 39000  \tTraining loss: 0.29070261120796204\n",
      "Step: 39000  \tTraining accuracy: 0.8365733623504639\n",
      "Step: 39000  \tValid loss: 0.3660622537136078\n",
      "Step: 39100  \tTraining loss: 0.2905999422073364\n",
      "Step: 39100  \tTraining accuracy: 0.8366086483001709\n",
      "Step: 39100  \tValid loss: 0.36605265736579895\n",
      "Step: 39200  \tTraining loss: 0.2904967963695526\n",
      "Step: 39200  \tTraining accuracy: 0.8366438150405884\n",
      "Step: 39200  \tValid loss: 0.3659975528717041\n",
      "Step: 39300  \tTraining loss: 0.29039666056632996\n",
      "Step: 39300  \tTraining accuracy: 0.8366787433624268\n",
      "Step: 39300  \tValid loss: 0.36592745780944824\n",
      "Step: 39400  \tTraining loss: 0.2902969717979431\n",
      "Step: 39400  \tTraining accuracy: 0.8367134928703308\n",
      "Step: 39400  \tValid loss: 0.3659207224845886\n",
      "Step: 39500  \tTraining loss: 0.29019737243652344\n",
      "Step: 39500  \tTraining accuracy: 0.8367484211921692\n",
      "Step: 39500  \tValid loss: 0.36586299538612366\n",
      "Step: 39600  \tTraining loss: 0.29010120034217834\n",
      "Step: 39600  \tTraining accuracy: 0.8367828130722046\n",
      "Step: 39600  \tValid loss: 0.3658364415168762\n",
      "Step: 39700  \tTraining loss: 0.2900031507015228\n",
      "Step: 39700  \tTraining accuracy: 0.8368170261383057\n",
      "Step: 39700  \tValid loss: 0.3657756447792053\n",
      "Step: 39800  \tTraining loss: 0.28990638256073\n",
      "Step: 39800  \tTraining accuracy: 0.8368494510650635\n",
      "Step: 39800  \tValid loss: 0.36573782563209534\n",
      "Step: 39900  \tTraining loss: 0.2898111939430237\n",
      "Step: 39900  \tTraining accuracy: 0.8368827104568481\n",
      "Step: 39900  \tValid loss: 0.3656958043575287\n",
      "Step: 40000  \tTraining loss: 0.2897160053253174\n",
      "Step: 40000  \tTraining accuracy: 0.836916446685791\n",
      "Step: 40000  \tValid loss: 0.365658700466156\n",
      "Step: 40100  \tTraining loss: 0.2896219491958618\n",
      "Step: 40100  \tTraining accuracy: 0.8369500041007996\n",
      "Step: 40100  \tValid loss: 0.36562278866767883\n",
      "Step: 40200  \tTraining loss: 0.2895292341709137\n",
      "Step: 40200  \tTraining accuracy: 0.8369833827018738\n",
      "Step: 40200  \tValid loss: 0.36555254459381104\n",
      "Step: 40300  \tTraining loss: 0.28943708539009094\n",
      "Step: 40300  \tTraining accuracy: 0.837017297744751\n",
      "Step: 40300  \tValid loss: 0.3655185401439667\n",
      "Step: 40400  \tTraining loss: 0.2893466055393219\n",
      "Step: 40400  \tTraining accuracy: 0.8370519280433655\n",
      "Step: 40400  \tValid loss: 0.36548087000846863\n",
      "Step: 40500  \tTraining loss: 0.2892555892467499\n",
      "Step: 40500  \tTraining accuracy: 0.8370851874351501\n",
      "Step: 40500  \tValid loss: 0.36548322439193726\n",
      "Step: 40600  \tTraining loss: 0.28916582465171814\n",
      "Step: 40600  \tTraining accuracy: 0.8371179103851318\n",
      "Step: 40600  \tValid loss: 0.36542022228240967\n",
      "Step: 40700  \tTraining loss: 0.2890743315219879\n",
      "Step: 40700  \tTraining accuracy: 0.8371504545211792\n",
      "Step: 40700  \tValid loss: 0.36538517475128174\n",
      "Step: 40800  \tTraining loss: 0.2889840602874756\n",
      "Step: 40800  \tTraining accuracy: 0.837182879447937\n",
      "Step: 40800  \tValid loss: 0.3653552830219269\n",
      "Step: 40900  \tTraining loss: 0.28889521956443787\n",
      "Step: 40900  \tTraining accuracy: 0.8372151255607605\n",
      "Step: 40900  \tValid loss: 0.3653290271759033\n",
      "Step: 41000  \tTraining loss: 0.28880876302719116\n",
      "Step: 41000  \tTraining accuracy: 0.8372472524642944\n",
      "Step: 41000  \tValid loss: 0.36530011892318726\n",
      "Step: 41100  \tTraining loss: 0.28872010111808777\n",
      "Step: 41100  \tTraining accuracy: 0.8372775912284851\n",
      "Step: 41100  \tValid loss: 0.36535000801086426\n",
      "Step: 41200  \tTraining loss: 0.2886374592781067\n",
      "Step: 41200  \tTraining accuracy: 0.8373078107833862\n",
      "Step: 41200  \tValid loss: 0.36537134647369385\n",
      "Step: 41300  \tTraining loss: 0.2885484993457794\n",
      "Step: 41300  \tTraining accuracy: 0.837337851524353\n",
      "Step: 41300  \tValid loss: 0.3653288185596466\n",
      "Step: 41400  \tTraining loss: 0.28846389055252075\n",
      "Step: 41400  \tTraining accuracy: 0.8373677730560303\n",
      "Step: 41400  \tValid loss: 0.3653298020362854\n",
      "Step: 41500  \tTraining loss: 0.2883775532245636\n",
      "Step: 41500  \tTraining accuracy: 0.8373978137969971\n",
      "Step: 41500  \tValid loss: 0.36531582474708557\n",
      "Step: 41600  \tTraining loss: 0.2882895767688751\n",
      "Step: 41600  \tTraining accuracy: 0.8374274373054504\n",
      "Step: 41600  \tValid loss: 0.36527779698371887\n",
      "Step: 41700  \tTraining loss: 0.2882041931152344\n",
      "Step: 41700  \tTraining accuracy: 0.8374569416046143\n",
      "Step: 41700  \tValid loss: 0.3652675747871399\n",
      "Step: 41800  \tTraining loss: 0.2881165146827698\n",
      "Step: 41800  \tTraining accuracy: 0.8374862670898438\n",
      "Step: 41800  \tValid loss: 0.3652312755584717\n",
      "Step: 41900  \tTraining loss: 0.2880323529243469\n",
      "Step: 41900  \tTraining accuracy: 0.8375154733657837\n",
      "Step: 41900  \tValid loss: 0.36520707607269287\n",
      "Step: 42000  \tTraining loss: 0.28795063495635986\n",
      "Step: 42000  \tTraining accuracy: 0.8375445604324341\n",
      "Step: 42000  \tValid loss: 0.36522024869918823\n",
      "Step: 42100  \tTraining loss: 0.287865549325943\n",
      "Step: 42100  \tTraining accuracy: 0.8375734686851501\n",
      "Step: 42100  \tValid loss: 0.3651699423789978\n",
      "Step: 42200  \tTraining loss: 0.2877838611602783\n",
      "Step: 42200  \tTraining accuracy: 0.8376022577285767\n",
      "Step: 42200  \tValid loss: 0.36521217226982117\n",
      "Step: 42300  \tTraining loss: 0.28770002722740173\n",
      "Step: 42300  \tTraining accuracy: 0.8376309275627136\n",
      "Step: 42300  \tValid loss: 0.36520159244537354\n",
      "Step: 42400  \tTraining loss: 0.2876225709915161\n",
      "Step: 42400  \tTraining accuracy: 0.8376594185829163\n",
      "Step: 42400  \tValid loss: 0.36523905396461487\n",
      "Step: 42500  \tTraining loss: 0.28753653168678284\n",
      "Step: 42500  \tTraining accuracy: 0.8376877903938293\n",
      "Step: 42500  \tValid loss: 0.3652055263519287\n",
      "Step: 42600  \tTraining loss: 0.28745731711387634\n",
      "Step: 42600  \tTraining accuracy: 0.8377148509025574\n",
      "Step: 42600  \tValid loss: 0.36520734429359436\n",
      "Step: 42700  \tTraining loss: 0.28737491369247437\n",
      "Step: 42700  \tTraining accuracy: 0.8377414345741272\n",
      "Step: 42700  \tValid loss: 0.3651989698410034\n",
      "Step: 42800  \tTraining loss: 0.28729504346847534\n",
      "Step: 42800  \tTraining accuracy: 0.8377678990364075\n",
      "Step: 42800  \tValid loss: 0.3652019202709198\n",
      "Step: 42900  \tTraining loss: 0.2872152626514435\n",
      "Step: 42900  \tTraining accuracy: 0.8377942442893982\n",
      "Step: 42900  \tValid loss: 0.3652161657810211\n",
      "Step: 43000  \tTraining loss: 0.28713923692703247\n",
      "Step: 43000  \tTraining accuracy: 0.8378204703330994\n",
      "Step: 43000  \tValid loss: 0.365174800157547\n",
      "Step: 43100  \tTraining loss: 0.28705811500549316\n",
      "Step: 43100  \tTraining accuracy: 0.837846577167511\n",
      "Step: 43100  \tValid loss: 0.36525973677635193\n",
      "Step: 43200  \tTraining loss: 0.28697848320007324\n",
      "Step: 43200  \tTraining accuracy: 0.8378725051879883\n",
      "Step: 43200  \tValid loss: 0.36526691913604736\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8378987\n",
      "Precision: 0.90399724\n",
      "Recall: 0.9130435\n",
      "F1 score: 0.8420703\n",
      "AUC: 0.81837034\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0  0.837899   0.903997  0.913043   0.84207  0.81837  0.286935      0.837868   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.365128       0.837877   0.322862      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  43251.0  \n",
      "4\n",
      "(3625, 8)\n",
      "(3625, 1)\n",
      "(2000, 8)\n",
      "(2000, 1)\n",
      "(1625, 8)\n",
      "(1625, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5059772729873657\n",
      "Step: 100  \tTraining accuracy: 0.789793074131012\n",
      "Step: 100  \tValid loss: 0.5125677585601807\n",
      "Step: 200  \tTraining loss: 0.45009520649909973\n",
      "Step: 200  \tTraining accuracy: 0.7866666913032532\n",
      "Step: 200  \tValid loss: 0.4692537784576416\n",
      "Step: 300  \tTraining loss: 0.4317542016506195\n",
      "Step: 300  \tTraining accuracy: 0.7885241508483887\n",
      "Step: 300  \tValid loss: 0.4508286118507385\n",
      "Step: 400  \tTraining loss: 0.40983930230140686\n",
      "Step: 400  \tTraining accuracy: 0.7903448343276978\n",
      "Step: 400  \tValid loss: 0.4279695451259613\n",
      "Step: 500  \tTraining loss: 0.38105177879333496\n",
      "Step: 500  \tTraining accuracy: 0.7923065423965454\n",
      "Step: 500  \tValid loss: 0.3980466425418854\n",
      "Step: 600  \tTraining loss: 0.34569019079208374\n",
      "Step: 600  \tTraining accuracy: 0.7974169254302979\n",
      "Step: 600  \tValid loss: 0.3620009124279022\n",
      "Step: 700  \tTraining loss: 0.31178945302963257\n",
      "Step: 700  \tTraining accuracy: 0.8040955066680908\n",
      "Step: 700  \tValid loss: 0.3287279009819031\n",
      "Step: 800  \tTraining loss: 0.28863441944122314\n",
      "Step: 800  \tTraining accuracy: 0.8112735748291016\n",
      "Step: 800  \tValid loss: 0.30693623423576355\n",
      "Step: 900  \tTraining loss: 0.27574941515922546\n",
      "Step: 900  \tTraining accuracy: 0.8180608749389648\n",
      "Step: 900  \tValid loss: 0.2951629161834717\n",
      "Step: 1000  \tTraining loss: 0.2684690058231354\n",
      "Step: 1000  \tTraining accuracy: 0.8236951231956482\n",
      "Step: 1000  \tValid loss: 0.28848814964294434\n",
      "Step: 1100  \tTraining loss: 0.2639514207839966\n",
      "Step: 1100  \tTraining accuracy: 0.8287422060966492\n",
      "Step: 1100  \tValid loss: 0.2842697203159332\n",
      "Step: 1200  \tTraining loss: 0.2609493136405945\n",
      "Step: 1200  \tTraining accuracy: 0.8331274390220642\n",
      "Step: 1200  \tValid loss: 0.2814677059650421\n",
      "Step: 1300  \tTraining loss: 0.2588671147823334\n",
      "Step: 1300  \tTraining accuracy: 0.8368441462516785\n",
      "Step: 1300  \tValid loss: 0.2795930504798889\n",
      "Step: 1400  \tTraining loss: 0.25737106800079346\n",
      "Step: 1400  \tTraining accuracy: 0.8400306701660156\n",
      "Step: 1400  \tValid loss: 0.27834972739219666\n",
      "Step: 1500  \tTraining loss: 0.2562582790851593\n",
      "Step: 1500  \tTraining accuracy: 0.8429869413375854\n",
      "Step: 1500  \tValid loss: 0.2775385081768036\n",
      "Step: 1600  \tTraining loss: 0.2553977966308594\n",
      "Step: 1600  \tTraining accuracy: 0.8456863164901733\n",
      "Step: 1600  \tValid loss: 0.2770192325115204\n",
      "Step: 1700  \tTraining loss: 0.2547048032283783\n",
      "Step: 1700  \tTraining accuracy: 0.8481839299201965\n",
      "Step: 1700  \tValid loss: 0.27669650316238403\n",
      "Step: 1800  \tTraining loss: 0.2541258931159973\n",
      "Step: 1800  \tTraining accuracy: 0.8503566384315491\n",
      "Step: 1800  \tValid loss: 0.2765059769153595\n",
      "Step: 1900  \tTraining loss: 0.2536259591579437\n",
      "Step: 1900  \tTraining accuracy: 0.8523094058036804\n",
      "Step: 1900  \tValid loss: 0.2764032781124115\n",
      "Step: 2000  \tTraining loss: 0.25318193435668945\n",
      "Step: 2000  \tTraining accuracy: 0.8540335893630981\n",
      "Step: 2000  \tValid loss: 0.2763572335243225\n",
      "Step: 2100  \tTraining loss: 0.2527786195278168\n",
      "Step: 2100  \tTraining accuracy: 0.8556097745895386\n",
      "Step: 2100  \tValid loss: 0.27634522318840027\n",
      "Step: 2200  \tTraining loss: 0.2524053156375885\n",
      "Step: 2200  \tTraining accuracy: 0.857064962387085\n",
      "Step: 2200  \tValid loss: 0.2763497233390808\n",
      "Step: 2300  \tTraining loss: 0.2520544230937958\n",
      "Step: 2300  \tTraining accuracy: 0.8583908081054688\n",
      "Step: 2300  \tValid loss: 0.2763569951057434\n",
      "Step: 2400  \tTraining loss: 0.25172045826911926\n",
      "Step: 2400  \tTraining accuracy: 0.8596155643463135\n",
      "Step: 2400  \tValid loss: 0.2763553559780121\n",
      "Step: 2500  \tTraining loss: 0.25139856338500977\n",
      "Step: 2500  \tTraining accuracy: 0.8607515692710876\n",
      "Step: 2500  \tValid loss: 0.2763350307941437\n",
      "Step: 2600  \tTraining loss: 0.25108495354652405\n",
      "Step: 2600  \tTraining accuracy: 0.8617985248565674\n",
      "Step: 2600  \tValid loss: 0.2762877941131592\n",
      "Step: 2700  \tTraining loss: 0.250776082277298\n",
      "Step: 2700  \tTraining accuracy: 0.8627768158912659\n",
      "Step: 2700  \tValid loss: 0.2762068510055542\n",
      "Step: 2800  \tTraining loss: 0.2504686713218689\n",
      "Step: 2800  \tTraining accuracy: 0.863673985004425\n",
      "Step: 2800  \tValid loss: 0.2760868966579437\n",
      "Step: 2900  \tTraining loss: 0.2501591742038727\n",
      "Step: 2900  \tTraining accuracy: 0.8645033240318298\n",
      "Step: 2900  \tValid loss: 0.2759244441986084\n",
      "Step: 3000  \tTraining loss: 0.24984487891197205\n",
      "Step: 3000  \tTraining accuracy: 0.8652858138084412\n",
      "Step: 3000  \tValid loss: 0.2757166624069214\n",
      "Step: 3100  \tTraining loss: 0.24952219426631927\n",
      "Step: 3100  \tTraining accuracy: 0.8660395741462708\n",
      "Step: 3100  \tValid loss: 0.27546262741088867\n",
      "Step: 3200  \tTraining loss: 0.24918776750564575\n",
      "Step: 3200  \tTraining accuracy: 0.8667279481887817\n",
      "Step: 3200  \tValid loss: 0.2751632034778595\n",
      "Step: 3300  \tTraining loss: 0.2488381415605545\n",
      "Step: 3300  \tTraining accuracy: 0.8673740029335022\n",
      "Step: 3300  \tValid loss: 0.27482062578201294\n",
      "Step: 3400  \tTraining loss: 0.2484700232744217\n",
      "Step: 3400  \tTraining accuracy: 0.868014395236969\n",
      "Step: 3400  \tValid loss: 0.27443909645080566\n",
      "Step: 3500  \tTraining loss: 0.2480808049440384\n",
      "Step: 3500  \tTraining accuracy: 0.8686016798019409\n",
      "Step: 3500  \tValid loss: 0.2740249037742615\n",
      "Step: 3600  \tTraining loss: 0.24766966700553894\n",
      "Step: 3600  \tTraining accuracy: 0.8691520094871521\n",
      "Step: 3600  \tValid loss: 0.2735861837863922\n",
      "Step: 3700  \tTraining loss: 0.24723932147026062\n",
      "Step: 3700  \tTraining accuracy: 0.8696797490119934\n",
      "Step: 3700  \tValid loss: 0.27313363552093506\n",
      "Step: 3800  \tTraining loss: 0.2467934638261795\n",
      "Step: 3800  \tTraining accuracy: 0.8701572418212891\n",
      "Step: 3800  \tValid loss: 0.2726789116859436\n",
      "Step: 3900  \tTraining loss: 0.24633628129959106\n",
      "Step: 3900  \tTraining accuracy: 0.8706350326538086\n",
      "Step: 3900  \tValid loss: 0.2722329795360565\n",
      "Step: 4000  \tTraining loss: 0.24587062001228333\n",
      "Step: 4000  \tTraining accuracy: 0.8710781335830688\n",
      "Step: 4000  \tValid loss: 0.27180394530296326\n",
      "Step: 4100  \tTraining loss: 0.24539914727210999\n",
      "Step: 4100  \tTraining accuracy: 0.8715027570724487\n",
      "Step: 4100  \tValid loss: 0.2713969051837921\n",
      "Step: 4200  \tTraining loss: 0.24492447078227997\n",
      "Step: 4200  \tTraining accuracy: 0.8719069361686707\n",
      "Step: 4200  \tValid loss: 0.2710147798061371\n",
      "Step: 4300  \tTraining loss: 0.24444955587387085\n",
      "Step: 4300  \tTraining accuracy: 0.8722823262214661\n",
      "Step: 4300  \tValid loss: 0.27065908908843994\n",
      "Step: 4400  \tTraining loss: 0.24397599697113037\n",
      "Step: 4400  \tTraining accuracy: 0.8726563453674316\n",
      "Step: 4400  \tValid loss: 0.27032923698425293\n",
      "Step: 4500  \tTraining loss: 0.24350489675998688\n",
      "Step: 4500  \tTraining accuracy: 0.8730507493019104\n",
      "Step: 4500  \tValid loss: 0.270021915435791\n",
      "Step: 4600  \tTraining loss: 0.24303683638572693\n",
      "Step: 4600  \tTraining accuracy: 0.8734005093574524\n",
      "Step: 4600  \tValid loss: 0.26973292231559753\n",
      "Step: 4700  \tTraining loss: 0.24257135391235352\n",
      "Step: 4700  \tTraining accuracy: 0.8737145066261292\n",
      "Step: 4700  \tValid loss: 0.26945760846138\n",
      "Step: 4800  \tTraining loss: 0.24210892617702484\n",
      "Step: 4800  \tTraining accuracy: 0.8740094304084778\n",
      "Step: 4800  \tValid loss: 0.26919233798980713\n",
      "Step: 4900  \tTraining loss: 0.24165017902851105\n",
      "Step: 4900  \tTraining accuracy: 0.8743149638175964\n",
      "Step: 4900  \tValid loss: 0.2689345180988312\n",
      "Step: 5000  \tTraining loss: 0.2411956489086151\n",
      "Step: 5000  \tTraining accuracy: 0.8746053576469421\n",
      "Step: 5000  \tValid loss: 0.26868170499801636\n",
      "Step: 5100  \tTraining loss: 0.24074649810791016\n",
      "Step: 5100  \tTraining accuracy: 0.8748897314071655\n",
      "Step: 5100  \tValid loss: 0.2684318721294403\n",
      "Step: 5200  \tTraining loss: 0.24030336737632751\n",
      "Step: 5200  \tTraining accuracy: 0.8751898407936096\n",
      "Step: 5200  \tValid loss: 0.2681831121444702\n",
      "Step: 5300  \tTraining loss: 0.2398671805858612\n",
      "Step: 5300  \tTraining accuracy: 0.8754863739013672\n",
      "Step: 5300  \tValid loss: 0.26793327927589417\n",
      "Step: 5400  \tTraining loss: 0.23943793773651123\n",
      "Step: 5400  \tTraining accuracy: 0.8757821321487427\n",
      "Step: 5400  \tValid loss: 0.2676808536052704\n",
      "Step: 5500  \tTraining loss: 0.23901602625846863\n",
      "Step: 5500  \tTraining accuracy: 0.876056969165802\n",
      "Step: 5500  \tValid loss: 0.26742449402809143\n",
      "Step: 5600  \tTraining loss: 0.23860108852386475\n",
      "Step: 5600  \tTraining accuracy: 0.8763342499732971\n",
      "Step: 5600  \tValid loss: 0.26716363430023193\n",
      "Step: 5700  \tTraining loss: 0.23819313943386078\n",
      "Step: 5700  \tTraining accuracy: 0.8766188621520996\n",
      "Step: 5700  \tValid loss: 0.2668980062007904\n",
      "Step: 5800  \tTraining loss: 0.23779180645942688\n",
      "Step: 5800  \tTraining accuracy: 0.8768935799598694\n",
      "Step: 5800  \tValid loss: 0.26662784814834595\n",
      "Step: 5900  \tTraining loss: 0.23739708960056305\n",
      "Step: 5900  \tTraining accuracy: 0.8771517872810364\n",
      "Step: 5900  \tValid loss: 0.2663537561893463\n",
      "Step: 6000  \tTraining loss: 0.237008735537529\n",
      "Step: 6000  \tTraining accuracy: 0.8774105906486511\n",
      "Step: 6000  \tValid loss: 0.26607659459114075\n",
      "Step: 6100  \tTraining loss: 0.23662666976451874\n",
      "Step: 6100  \tTraining accuracy: 0.8776677250862122\n",
      "Step: 6100  \tValid loss: 0.2657971680164337\n",
      "Step: 6200  \tTraining loss: 0.23625098168849945\n",
      "Step: 6200  \tTraining accuracy: 0.8779299259185791\n",
      "Step: 6200  \tValid loss: 0.2655170261859894\n",
      "Step: 6300  \tTraining loss: 0.23588144779205322\n",
      "Step: 6300  \tTraining accuracy: 0.8781771063804626\n",
      "Step: 6300  \tValid loss: 0.26523715257644653\n",
      "Step: 6400  \tTraining loss: 0.2355184406042099\n",
      "Step: 6400  \tTraining accuracy: 0.8784295320510864\n",
      "Step: 6400  \tValid loss: 0.26495927572250366\n",
      "Step: 6500  \tTraining loss: 0.235161691904068\n",
      "Step: 6500  \tTraining accuracy: 0.8786869645118713\n",
      "Step: 6500  \tValid loss: 0.2646847069263458\n",
      "Step: 6600  \tTraining loss: 0.23481179773807526\n",
      "Step: 6600  \tTraining accuracy: 0.8789365887641907\n",
      "Step: 6600  \tValid loss: 0.26441508531570435\n",
      "Step: 6700  \tTraining loss: 0.23446856439113617\n",
      "Step: 6700  \tTraining accuracy: 0.879182755947113\n",
      "Step: 6700  \tValid loss: 0.26415184140205383\n",
      "Step: 6800  \tTraining loss: 0.23413224518299103\n",
      "Step: 6800  \tTraining accuracy: 0.879438042640686\n",
      "Step: 6800  \tValid loss: 0.2638964056968689\n",
      "Step: 6900  \tTraining loss: 0.23380248248577118\n",
      "Step: 6900  \tTraining accuracy: 0.8796798586845398\n",
      "Step: 6900  \tValid loss: 0.26364970207214355\n",
      "Step: 7000  \tTraining loss: 0.2334795743227005\n",
      "Step: 7000  \tTraining accuracy: 0.8799146413803101\n",
      "Step: 7000  \tValid loss: 0.2634129822254181\n",
      "Step: 7100  \tTraining loss: 0.23316331207752228\n",
      "Step: 7100  \tTraining accuracy: 0.8801428079605103\n",
      "Step: 7100  \tValid loss: 0.2631870210170746\n",
      "Step: 7200  \tTraining loss: 0.2328534722328186\n",
      "Step: 7200  \tTraining accuracy: 0.8803742527961731\n",
      "Step: 7200  \tValid loss: 0.26297253370285034\n",
      "Step: 7300  \tTraining loss: 0.23254987597465515\n",
      "Step: 7300  \tTraining accuracy: 0.8805992603302002\n",
      "Step: 7300  \tValid loss: 0.262769877910614\n",
      "Step: 7400  \tTraining loss: 0.23225240409374237\n",
      "Step: 7400  \tTraining accuracy: 0.8808144330978394\n",
      "Step: 7400  \tValid loss: 0.26257938146591187\n",
      "Step: 7500  \tTraining loss: 0.23196075856685638\n",
      "Step: 7500  \tTraining accuracy: 0.8810331225395203\n",
      "Step: 7500  \tValid loss: 0.2624010741710663\n",
      "Step: 7600  \tTraining loss: 0.231674462556839\n",
      "Step: 7600  \tTraining accuracy: 0.8812587261199951\n",
      "Step: 7600  \tValid loss: 0.2622346878051758\n",
      "Step: 7700  \tTraining loss: 0.23139353096485138\n",
      "Step: 7700  \tTraining accuracy: 0.881478488445282\n",
      "Step: 7700  \tValid loss: 0.2620798945426941\n",
      "Step: 7800  \tTraining loss: 0.23111753165721893\n",
      "Step: 7800  \tTraining accuracy: 0.8816925287246704\n",
      "Step: 7800  \tValid loss: 0.26193615794181824\n",
      "Step: 7900  \tTraining loss: 0.23084613680839539\n",
      "Step: 7900  \tTraining accuracy: 0.8819064497947693\n",
      "Step: 7900  \tValid loss: 0.2618025541305542\n",
      "Step: 8000  \tTraining loss: 0.23057904839515686\n",
      "Step: 8000  \tTraining accuracy: 0.8821149468421936\n",
      "Step: 8000  \tValid loss: 0.2616782784461975\n",
      "Step: 8100  \tTraining loss: 0.23031572997570038\n",
      "Step: 8100  \tTraining accuracy: 0.8823182582855225\n",
      "Step: 8100  \tValid loss: 0.26156264543533325\n",
      "Step: 8200  \tTraining loss: 0.2300558090209961\n",
      "Step: 8200  \tTraining accuracy: 0.8825216889381409\n",
      "Step: 8200  \tValid loss: 0.26145440340042114\n",
      "Step: 8300  \tTraining loss: 0.22979925572872162\n",
      "Step: 8300  \tTraining accuracy: 0.8827118277549744\n",
      "Step: 8300  \tValid loss: 0.26135289669036865\n",
      "Step: 8400  \tTraining loss: 0.22954504191875458\n",
      "Step: 8400  \tTraining accuracy: 0.8828907608985901\n",
      "Step: 8400  \tValid loss: 0.2612571716308594\n",
      "Step: 8500  \tTraining loss: 0.22929349541664124\n",
      "Step: 8500  \tTraining accuracy: 0.8830638527870178\n",
      "Step: 8500  \tValid loss: 0.26116618514060974\n",
      "Step: 8600  \tTraining loss: 0.22904342412948608\n",
      "Step: 8600  \tTraining accuracy: 0.8832361102104187\n",
      "Step: 8600  \tValid loss: 0.26107898354530334\n",
      "Step: 8700  \tTraining loss: 0.2287951409816742\n",
      "Step: 8700  \tTraining accuracy: 0.8834108114242554\n",
      "Step: 8700  \tValid loss: 0.26099497079849243\n",
      "Step: 8800  \tTraining loss: 0.22854766249656677\n",
      "Step: 8800  \tTraining accuracy: 0.8835814595222473\n",
      "Step: 8800  \tValid loss: 0.2609134018421173\n",
      "Step: 8900  \tTraining loss: 0.2283010631799698\n",
      "Step: 8900  \tTraining accuracy: 0.8837451934814453\n",
      "Step: 8900  \tValid loss: 0.2608336806297302\n",
      "Step: 9000  \tTraining loss: 0.22805489599704742\n",
      "Step: 9000  \tTraining accuracy: 0.8839036822319031\n",
      "Step: 9000  \tValid loss: 0.26075494289398193\n",
      "Step: 9100  \tTraining loss: 0.22780869901180267\n",
      "Step: 9100  \tTraining accuracy: 0.8840693235397339\n",
      "Step: 9100  \tValid loss: 0.260676771402359\n",
      "Step: 9200  \tTraining loss: 0.22756235301494598\n",
      "Step: 9200  \tTraining accuracy: 0.8842359185218811\n",
      "Step: 9200  \tValid loss: 0.260598748922348\n",
      "Step: 9300  \tTraining loss: 0.22731539607048035\n",
      "Step: 9300  \tTraining accuracy: 0.884398877620697\n",
      "Step: 9300  \tValid loss: 0.2605205774307251\n",
      "Step: 9400  \tTraining loss: 0.2270677238702774\n",
      "Step: 9400  \tTraining accuracy: 0.8845524787902832\n",
      "Step: 9400  \tValid loss: 0.2604416012763977\n",
      "Step: 9500  \tTraining loss: 0.22681939601898193\n",
      "Step: 9500  \tTraining accuracy: 0.8846983909606934\n",
      "Step: 9500  \tValid loss: 0.2603616416454315\n",
      "Step: 9600  \tTraining loss: 0.22656984627246857\n",
      "Step: 9600  \tTraining accuracy: 0.8848369717597961\n",
      "Step: 9600  \tValid loss: 0.26028069853782654\n",
      "Step: 9700  \tTraining loss: 0.22631926834583282\n",
      "Step: 9700  \tTraining accuracy: 0.8849712610244751\n",
      "Step: 9700  \tValid loss: 0.26019829511642456\n",
      "Step: 9800  \tTraining loss: 0.2260671854019165\n",
      "Step: 9800  \tTraining accuracy: 0.8851027488708496\n",
      "Step: 9800  \tValid loss: 0.2601144015789032\n",
      "Step: 9900  \tTraining loss: 0.22581380605697632\n",
      "Step: 9900  \tTraining accuracy: 0.8852315545082092\n",
      "Step: 9900  \tValid loss: 0.26002898812294006\n",
      "Step: 10000  \tTraining loss: 0.22555899620056152\n",
      "Step: 10000  \tTraining accuracy: 0.8853661417961121\n",
      "Step: 10000  \tValid loss: 0.2599419057369232\n",
      "Step: 10100  \tTraining loss: 0.22530262172222137\n",
      "Step: 10100  \tTraining accuracy: 0.8855035305023193\n",
      "Step: 10100  \tValid loss: 0.25985321402549744\n",
      "Step: 10200  \tTraining loss: 0.2250448763370514\n",
      "Step: 10200  \tTraining accuracy: 0.8856409192085266\n",
      "Step: 10200  \tValid loss: 0.2597629427909851\n",
      "Step: 10300  \tTraining loss: 0.22478562593460083\n",
      "Step: 10300  \tTraining accuracy: 0.885775625705719\n",
      "Step: 10300  \tValid loss: 0.25967106223106384\n",
      "Step: 10400  \tTraining loss: 0.22452504932880402\n",
      "Step: 10400  \tTraining accuracy: 0.8859077095985413\n",
      "Step: 10400  \tValid loss: 0.25957781076431274\n",
      "Step: 10500  \tTraining loss: 0.2242630124092102\n",
      "Step: 10500  \tTraining accuracy: 0.886037290096283\n",
      "Step: 10500  \tValid loss: 0.2594832181930542\n",
      "Step: 10600  \tTraining loss: 0.22399963438510895\n",
      "Step: 10600  \tTraining accuracy: 0.8861631155014038\n",
      "Step: 10600  \tValid loss: 0.2593875825405121\n",
      "Step: 10700  \tTraining loss: 0.2237352579832077\n",
      "Step: 10700  \tTraining accuracy: 0.8862865567207336\n",
      "Step: 10700  \tValid loss: 0.2592909038066864\n",
      "Step: 10800  \tTraining loss: 0.2234697788953781\n",
      "Step: 10800  \tTraining accuracy: 0.8864076733589172\n",
      "Step: 10800  \tValid loss: 0.2591932415962219\n",
      "Step: 10900  \tTraining loss: 0.2232033610343933\n",
      "Step: 10900  \tTraining accuracy: 0.8865177035331726\n",
      "Step: 10900  \tValid loss: 0.25909510254859924\n",
      "Step: 11000  \tTraining loss: 0.22293604910373688\n",
      "Step: 11000  \tTraining accuracy: 0.8866257071495056\n",
      "Step: 11000  \tValid loss: 0.2589959502220154\n",
      "Step: 11100  \tTraining loss: 0.2226680964231491\n",
      "Step: 11100  \tTraining accuracy: 0.8867367506027222\n",
      "Step: 11100  \tValid loss: 0.2588961720466614\n",
      "Step: 11200  \tTraining loss: 0.22239944338798523\n",
      "Step: 11200  \tTraining accuracy: 0.8868495225906372\n",
      "Step: 11200  \tValid loss: 0.25879546999931335\n",
      "Step: 11300  \tTraining loss: 0.2221301645040512\n",
      "Step: 11300  \tTraining accuracy: 0.886967658996582\n",
      "Step: 11300  \tValid loss: 0.25869396328926086\n",
      "Step: 11400  \tTraining loss: 0.2218606323003769\n",
      "Step: 11400  \tTraining accuracy: 0.8870849013328552\n",
      "Step: 11400  \tValid loss: 0.2585916817188263\n",
      "Step: 11500  \tTraining loss: 0.2215907722711563\n",
      "Step: 11500  \tTraining accuracy: 0.887200117111206\n",
      "Step: 11500  \tValid loss: 0.25848880410194397\n",
      "Step: 11600  \tTraining loss: 0.22131749987602234\n",
      "Step: 11600  \tTraining accuracy: 0.8873133063316345\n",
      "Step: 11600  \tValid loss: 0.25835102796554565\n",
      "Step: 11700  \tTraining loss: 0.22103963792324066\n",
      "Step: 11700  \tTraining accuracy: 0.8874233961105347\n",
      "Step: 11700  \tValid loss: 0.258187472820282\n",
      "Step: 11800  \tTraining loss: 0.22076237201690674\n",
      "Step: 11800  \tTraining accuracy: 0.8875269293785095\n",
      "Step: 11800  \tValid loss: 0.258034884929657\n",
      "Step: 11900  \tTraining loss: 0.22048573195934296\n",
      "Step: 11900  \tTraining accuracy: 0.8876286745071411\n",
      "Step: 11900  \tValid loss: 0.25789082050323486\n",
      "Step: 12000  \tTraining loss: 0.22019648551940918\n",
      "Step: 12000  \tTraining accuracy: 0.887732207775116\n",
      "Step: 12000  \tValid loss: 0.2577746510505676\n",
      "Step: 12100  \tTraining loss: 0.219930961728096\n",
      "Step: 12100  \tTraining accuracy: 0.8878363370895386\n",
      "Step: 12100  \tValid loss: 0.25771620869636536\n",
      "Step: 12200  \tTraining loss: 0.2196694314479828\n",
      "Step: 12200  \tTraining accuracy: 0.8879352807998657\n",
      "Step: 12200  \tValid loss: 0.25764569640159607\n",
      "Step: 12300  \tTraining loss: 0.21941208839416504\n",
      "Step: 12300  \tTraining accuracy: 0.8880214095115662\n",
      "Step: 12300  \tValid loss: 0.2575646936893463\n",
      "Step: 12400  \tTraining loss: 0.21915939450263977\n",
      "Step: 12400  \tTraining accuracy: 0.8881027698516846\n",
      "Step: 12400  \tValid loss: 0.2574756145477295\n",
      "Step: 12500  \tTraining loss: 0.21891169250011444\n",
      "Step: 12500  \tTraining accuracy: 0.8881850242614746\n",
      "Step: 12500  \tValid loss: 0.25738099217414856\n",
      "Step: 12600  \tTraining loss: 0.21866931021213531\n",
      "Step: 12600  \tTraining accuracy: 0.8882681727409363\n",
      "Step: 12600  \tValid loss: 0.25728240609169006\n",
      "Step: 12700  \tTraining loss: 0.21843238174915314\n",
      "Step: 12700  \tTraining accuracy: 0.8883587121963501\n",
      "Step: 12700  \tValid loss: 0.257180392742157\n",
      "Step: 12800  \tTraining loss: 0.21820053458213806\n",
      "Step: 12800  \tTraining accuracy: 0.8884533047676086\n",
      "Step: 12800  \tValid loss: 0.2570754289627075\n",
      "Step: 12900  \tTraining loss: 0.2179735153913498\n",
      "Step: 12900  \tTraining accuracy: 0.8885517120361328\n",
      "Step: 12900  \tValid loss: 0.25696685910224915\n",
      "Step: 13000  \tTraining loss: 0.21775084733963013\n",
      "Step: 13000  \tTraining accuracy: 0.8886486291885376\n",
      "Step: 13000  \tValid loss: 0.256853848695755\n",
      "Step: 13100  \tTraining loss: 0.2175317406654358\n",
      "Step: 13100  \tTraining accuracy: 0.8887441158294678\n",
      "Step: 13100  \tValid loss: 0.25673580169677734\n",
      "Step: 13200  \tTraining loss: 0.21731597185134888\n",
      "Step: 13200  \tTraining accuracy: 0.8888380527496338\n",
      "Step: 13200  \tValid loss: 0.25661206245422363\n",
      "Step: 13300  \tTraining loss: 0.217102512717247\n",
      "Step: 13300  \tTraining accuracy: 0.8889296054840088\n",
      "Step: 13300  \tValid loss: 0.2564815580844879\n",
      "Step: 13400  \tTraining loss: 0.21689103543758392\n",
      "Step: 13400  \tTraining accuracy: 0.889019787311554\n",
      "Step: 13400  \tValid loss: 0.25634413957595825\n",
      "Step: 13500  \tTraining loss: 0.2166803777217865\n",
      "Step: 13500  \tTraining accuracy: 0.8891096115112305\n",
      "Step: 13500  \tValid loss: 0.25619837641716003\n",
      "Step: 13600  \tTraining loss: 0.21646994352340698\n",
      "Step: 13600  \tTraining accuracy: 0.8891981244087219\n",
      "Step: 13600  \tValid loss: 0.25604313611984253\n",
      "Step: 13700  \tTraining loss: 0.21625865995883942\n",
      "Step: 13700  \tTraining accuracy: 0.8892853260040283\n",
      "Step: 13700  \tValid loss: 0.2558802366256714\n",
      "Step: 13800  \tTraining loss: 0.21604564785957336\n",
      "Step: 13800  \tTraining accuracy: 0.8893762826919556\n",
      "Step: 13800  \tValid loss: 0.25569984316825867\n",
      "Step: 13900  \tTraining loss: 0.21583005785942078\n",
      "Step: 13900  \tTraining accuracy: 0.8894669413566589\n",
      "Step: 13900  \tValid loss: 0.25550296902656555\n",
      "Step: 14000  \tTraining loss: 0.21561068296432495\n",
      "Step: 14000  \tTraining accuracy: 0.8895533084869385\n",
      "Step: 14000  \tValid loss: 0.2552887201309204\n",
      "Step: 14100  \tTraining loss: 0.21538633108139038\n",
      "Step: 14100  \tTraining accuracy: 0.8896365165710449\n",
      "Step: 14100  \tValid loss: 0.255056232213974\n",
      "Step: 14200  \tTraining loss: 0.2151561677455902\n",
      "Step: 14200  \tTraining accuracy: 0.8897214531898499\n",
      "Step: 14200  \tValid loss: 0.25480568408966064\n",
      "Step: 14300  \tTraining loss: 0.21491996943950653\n",
      "Step: 14300  \tTraining accuracy: 0.8898051977157593\n",
      "Step: 14300  \tValid loss: 0.25453224778175354\n",
      "Step: 14400  \tTraining loss: 0.21467776596546173\n",
      "Step: 14400  \tTraining accuracy: 0.8898839354515076\n",
      "Step: 14400  \tValid loss: 0.2542244493961334\n",
      "Step: 14500  \tTraining loss: 0.21443036198616028\n",
      "Step: 14500  \tTraining accuracy: 0.8899625539779663\n",
      "Step: 14500  \tValid loss: 0.2539537250995636\n",
      "Step: 14600  \tTraining loss: 0.21417880058288574\n",
      "Step: 14600  \tTraining accuracy: 0.8900400400161743\n",
      "Step: 14600  \tValid loss: 0.25367066264152527\n",
      "Step: 14700  \tTraining loss: 0.21392391622066498\n",
      "Step: 14700  \tTraining accuracy: 0.8901165127754211\n",
      "Step: 14700  \tValid loss: 0.25335603952407837\n",
      "Step: 14800  \tTraining loss: 0.21366718411445618\n",
      "Step: 14800  \tTraining accuracy: 0.8901947140693665\n",
      "Step: 14800  \tValid loss: 0.25305965542793274\n",
      "Step: 14900  \tTraining loss: 0.21340858936309814\n",
      "Step: 14900  \tTraining accuracy: 0.8902719020843506\n",
      "Step: 14900  \tValid loss: 0.2527228593826294\n",
      "Step: 15000  \tTraining loss: 0.21314826607704163\n",
      "Step: 15000  \tTraining accuracy: 0.8903508186340332\n",
      "Step: 15000  \tValid loss: 0.25245752930641174\n",
      "Step: 15100  \tTraining loss: 0.21288427710533142\n",
      "Step: 15100  \tTraining accuracy: 0.8904305100440979\n",
      "Step: 15100  \tValid loss: 0.25209301710128784\n",
      "Step: 15200  \tTraining loss: 0.21261683106422424\n",
      "Step: 15200  \tTraining accuracy: 0.8905054926872253\n",
      "Step: 15200  \tValid loss: 0.2517867684364319\n",
      "Step: 15300  \tTraining loss: 0.21234463155269623\n",
      "Step: 15300  \tTraining accuracy: 0.8905795216560364\n",
      "Step: 15300  \tValid loss: 0.2514551281929016\n",
      "Step: 15400  \tTraining loss: 0.2120669186115265\n",
      "Step: 15400  \tTraining accuracy: 0.890652596950531\n",
      "Step: 15400  \tValid loss: 0.25112423300743103\n",
      "Step: 15500  \tTraining loss: 0.21178361773490906\n",
      "Step: 15500  \tTraining accuracy: 0.8907211422920227\n",
      "Step: 15500  \tValid loss: 0.25078752636909485\n",
      "Step: 15600  \tTraining loss: 0.21149347722530365\n",
      "Step: 15600  \tTraining accuracy: 0.8907896876335144\n",
      "Step: 15600  \tValid loss: 0.25044721364974976\n",
      "Step: 15700  \tTraining loss: 0.2111961841583252\n",
      "Step: 15700  \tTraining accuracy: 0.8908547163009644\n",
      "Step: 15700  \tValid loss: 0.2500724196434021\n",
      "Step: 15800  \tTraining loss: 0.21089138090610504\n",
      "Step: 15800  \tTraining accuracy: 0.8909215331077576\n",
      "Step: 15800  \tValid loss: 0.24976348876953125\n",
      "Step: 15900  \tTraining loss: 0.21057787537574768\n",
      "Step: 15900  \tTraining accuracy: 0.8909883499145508\n",
      "Step: 15900  \tValid loss: 0.24936041235923767\n",
      "Step: 16000  \tTraining loss: 0.2102569341659546\n",
      "Step: 16000  \tTraining accuracy: 0.8910621404647827\n",
      "Step: 16000  \tValid loss: 0.2490178346633911\n",
      "Step: 16100  \tTraining loss: 0.20992933213710785\n",
      "Step: 16100  \tTraining accuracy: 0.89113849401474\n",
      "Step: 16100  \tValid loss: 0.24860188364982605\n",
      "Step: 16200  \tTraining loss: 0.20959581434726715\n",
      "Step: 16200  \tTraining accuracy: 0.8912104368209839\n",
      "Step: 16200  \tValid loss: 0.24815021455287933\n",
      "Step: 16300  \tTraining loss: 0.20925989747047424\n",
      "Step: 16300  \tTraining accuracy: 0.8912848830223083\n",
      "Step: 16300  \tValid loss: 0.24772515892982483\n",
      "Step: 16400  \tTraining loss: 0.20892195403575897\n",
      "Step: 16400  \tTraining accuracy: 0.8913601040840149\n",
      "Step: 16400  \tValid loss: 0.24725347757339478\n",
      "Step: 16500  \tTraining loss: 0.20858465135097504\n",
      "Step: 16500  \tTraining accuracy: 0.8914344310760498\n",
      "Step: 16500  \tValid loss: 0.24673880636692047\n",
      "Step: 16600  \tTraining loss: 0.20825035870075226\n",
      "Step: 16600  \tTraining accuracy: 0.8915103673934937\n",
      "Step: 16600  \tValid loss: 0.24626001715660095\n",
      "Step: 16700  \tTraining loss: 0.20791979134082794\n",
      "Step: 16700  \tTraining accuracy: 0.8915829062461853\n",
      "Step: 16700  \tValid loss: 0.24567779898643494\n",
      "Step: 16800  \tTraining loss: 0.2075883001089096\n",
      "Step: 16800  \tTraining accuracy: 0.8916537165641785\n",
      "Step: 16800  \tValid loss: 0.24513676762580872\n",
      "Step: 16900  \tTraining loss: 0.20726266503334045\n",
      "Step: 16900  \tTraining accuracy: 0.8917237520217896\n",
      "Step: 16900  \tValid loss: 0.24457043409347534\n",
      "Step: 17000  \tTraining loss: 0.20694872736930847\n",
      "Step: 17000  \tTraining accuracy: 0.891792893409729\n",
      "Step: 17000  \tValid loss: 0.2441045194864273\n",
      "Step: 17100  \tTraining loss: 0.20664054155349731\n",
      "Step: 17100  \tTraining accuracy: 0.8918564319610596\n",
      "Step: 17100  \tValid loss: 0.24349632859230042\n",
      "Step: 17200  \tTraining loss: 0.2063368409872055\n",
      "Step: 17200  \tTraining accuracy: 0.8919151425361633\n",
      "Step: 17200  \tValid loss: 0.24293942749500275\n",
      "Step: 17300  \tTraining loss: 0.20603948831558228\n",
      "Step: 17300  \tTraining accuracy: 0.8919731974601746\n",
      "Step: 17300  \tValid loss: 0.24242563545703888\n",
      "Step: 17400  \tTraining loss: 0.2057477980852127\n",
      "Step: 17400  \tTraining accuracy: 0.8920305967330933\n",
      "Step: 17400  \tValid loss: 0.2418704330921173\n",
      "Step: 17500  \tTraining loss: 0.20546172559261322\n",
      "Step: 17500  \tTraining accuracy: 0.8920865654945374\n",
      "Step: 17500  \tValid loss: 0.24136313796043396\n",
      "Step: 17600  \tTraining loss: 0.20518247783184052\n",
      "Step: 17600  \tTraining accuracy: 0.892142653465271\n",
      "Step: 17600  \tValid loss: 0.24090465903282166\n",
      "Step: 17700  \tTraining loss: 0.20491020381450653\n",
      "Step: 17700  \tTraining accuracy: 0.8922043442726135\n",
      "Step: 17700  \tValid loss: 0.2403569519519806\n",
      "Step: 17800  \tTraining loss: 0.2046438455581665\n",
      "Step: 17800  \tTraining accuracy: 0.892270028591156\n",
      "Step: 17800  \tValid loss: 0.23993168771266937\n",
      "Step: 17900  \tTraining loss: 0.20438314974308014\n",
      "Step: 17900  \tTraining accuracy: 0.8923357725143433\n",
      "Step: 17900  \tValid loss: 0.23955604434013367\n",
      "Step: 18000  \tTraining loss: 0.204129159450531\n",
      "Step: 18000  \tTraining accuracy: 0.8923946022987366\n",
      "Step: 18000  \tValid loss: 0.23913533985614777\n",
      "Step: 18100  \tTraining loss: 0.2038825899362564\n",
      "Step: 18100  \tTraining accuracy: 0.8924527764320374\n",
      "Step: 18100  \tValid loss: 0.23883840441703796\n",
      "Step: 18200  \tTraining loss: 0.20364101231098175\n",
      "Step: 18200  \tTraining accuracy: 0.8925141096115112\n",
      "Step: 18200  \tValid loss: 0.23840801417827606\n",
      "Step: 18300  \tTraining loss: 0.20340505242347717\n",
      "Step: 18300  \tTraining accuracy: 0.8925785422325134\n",
      "Step: 18300  \tValid loss: 0.23805662989616394\n",
      "Step: 18400  \tTraining loss: 0.20317426323890686\n",
      "Step: 18400  \tTraining accuracy: 0.8926438093185425\n",
      "Step: 18400  \tValid loss: 0.23775321245193481\n",
      "Step: 18500  \tTraining loss: 0.20294705033302307\n",
      "Step: 18500  \tTraining accuracy: 0.8927090764045715\n",
      "Step: 18500  \tValid loss: 0.23738336563110352\n",
      "Step: 18600  \tTraining loss: 0.20272397994995117\n",
      "Step: 18600  \tTraining accuracy: 0.8927751779556274\n",
      "Step: 18600  \tValid loss: 0.23703601956367493\n",
      "Step: 18700  \tTraining loss: 0.20250669121742249\n",
      "Step: 18700  \tTraining accuracy: 0.8928412795066833\n",
      "Step: 18700  \tValid loss: 0.23657019436359406\n",
      "Step: 18800  \tTraining loss: 0.20229138433933258\n",
      "Step: 18800  \tTraining accuracy: 0.8929081559181213\n",
      "Step: 18800  \tValid loss: 0.23638099431991577\n",
      "Step: 18900  \tTraining loss: 0.2020801454782486\n",
      "Step: 18900  \tTraining accuracy: 0.8929750323295593\n",
      "Step: 18900  \tValid loss: 0.23594874143600464\n",
      "Step: 19000  \tTraining loss: 0.20187242329120636\n",
      "Step: 19000  \tTraining accuracy: 0.89304119348526\n",
      "Step: 19000  \tValid loss: 0.23560982942581177\n",
      "Step: 19100  \tTraining loss: 0.20166517794132233\n",
      "Step: 19100  \tTraining accuracy: 0.8931081295013428\n",
      "Step: 19100  \tValid loss: 0.23526769876480103\n",
      "Step: 19200  \tTraining loss: 0.20146146416664124\n",
      "Step: 19200  \tTraining accuracy: 0.893171489238739\n",
      "Step: 19200  \tValid loss: 0.23489998281002045\n",
      "Step: 19300  \tTraining loss: 0.20126137137413025\n",
      "Step: 19300  \tTraining accuracy: 0.8932291865348816\n",
      "Step: 19300  \tValid loss: 0.23454642295837402\n",
      "Step: 19400  \tTraining loss: 0.20106323063373566\n",
      "Step: 19400  \tTraining accuracy: 0.8932834267616272\n",
      "Step: 19400  \tValid loss: 0.23431850969791412\n",
      "Step: 19500  \tTraining loss: 0.20086920261383057\n",
      "Step: 19500  \tTraining accuracy: 0.8933364152908325\n",
      "Step: 19500  \tValid loss: 0.23387011885643005\n",
      "Step: 19600  \tTraining loss: 0.20067699253559113\n",
      "Step: 19600  \tTraining accuracy: 0.8933874368667603\n",
      "Step: 19600  \tValid loss: 0.2335001528263092\n",
      "Step: 19700  \tTraining loss: 0.20048756897449493\n",
      "Step: 19700  \tTraining accuracy: 0.893437922000885\n",
      "Step: 19700  \tValid loss: 0.2332226186990738\n",
      "Step: 19800  \tTraining loss: 0.20030352473258972\n",
      "Step: 19800  \tTraining accuracy: 0.8934913873672485\n",
      "Step: 19800  \tValid loss: 0.2327653169631958\n",
      "Step: 19900  \tTraining loss: 0.2001194953918457\n",
      "Step: 19900  \tTraining accuracy: 0.893549919128418\n",
      "Step: 19900  \tValid loss: 0.2324838936328888\n",
      "Step: 20000  \tTraining loss: 0.19993950426578522\n",
      "Step: 20000  \tTraining accuracy: 0.8936071395874023\n",
      "Step: 20000  \tValid loss: 0.23213745653629303\n",
      "Step: 20100  \tTraining loss: 0.1997612863779068\n",
      "Step: 20100  \tTraining accuracy: 0.893660306930542\n",
      "Step: 20100  \tValid loss: 0.23184503614902496\n",
      "Step: 20200  \tTraining loss: 0.19958548247814178\n",
      "Step: 20200  \tTraining accuracy: 0.8937102556228638\n",
      "Step: 20200  \tValid loss: 0.2314557284116745\n",
      "Step: 20300  \tTraining loss: 0.1994113028049469\n",
      "Step: 20300  \tTraining accuracy: 0.8937597274780273\n",
      "Step: 20300  \tValid loss: 0.23112963140010834\n",
      "Step: 20400  \tTraining loss: 0.19923895597457886\n",
      "Step: 20400  \tTraining accuracy: 0.8938046097755432\n",
      "Step: 20400  \tValid loss: 0.23080793023109436\n",
      "Step: 20500  \tTraining loss: 0.1990710347890854\n",
      "Step: 20500  \tTraining accuracy: 0.8938484191894531\n",
      "Step: 20500  \tValid loss: 0.2303534895181656\n",
      "Step: 20600  \tTraining loss: 0.1989022046327591\n",
      "Step: 20600  \tTraining accuracy: 0.8938931226730347\n",
      "Step: 20600  \tValid loss: 0.23013025522232056\n",
      "Step: 20700  \tTraining loss: 0.1987365484237671\n",
      "Step: 20700  \tTraining accuracy: 0.893932044506073\n",
      "Step: 20700  \tValid loss: 0.22982817888259888\n",
      "Step: 20800  \tTraining loss: 0.19857439398765564\n",
      "Step: 20800  \tTraining accuracy: 0.8939685821533203\n",
      "Step: 20800  \tValid loss: 0.22942736744880676\n",
      "Step: 20900  \tTraining loss: 0.19841368496418\n",
      "Step: 20900  \tTraining accuracy: 0.8940034508705139\n",
      "Step: 20900  \tValid loss: 0.22911621630191803\n",
      "Step: 21000  \tTraining loss: 0.19825367629528046\n",
      "Step: 21000  \tTraining accuracy: 0.8940380215644836\n",
      "Step: 21000  \tValid loss: 0.22880807518959045\n",
      "Step: 21100  \tTraining loss: 0.19809648394584656\n",
      "Step: 21100  \tTraining accuracy: 0.8940722346305847\n",
      "Step: 21100  \tValid loss: 0.228499174118042\n",
      "Step: 21200  \tTraining loss: 0.19794365763664246\n",
      "Step: 21200  \tTraining accuracy: 0.8941068053245544\n",
      "Step: 21200  \tValid loss: 0.22812509536743164\n",
      "Step: 21300  \tTraining loss: 0.19779060781002045\n",
      "Step: 21300  \tTraining accuracy: 0.8941410183906555\n",
      "Step: 21300  \tValid loss: 0.22786591947078705\n",
      "Step: 21400  \tTraining loss: 0.1976427584886551\n",
      "Step: 21400  \tTraining accuracy: 0.8941774964332581\n",
      "Step: 21400  \tValid loss: 0.22749246656894684\n",
      "Step: 21500  \tTraining loss: 0.19749405980110168\n",
      "Step: 21500  \tTraining accuracy: 0.8942136764526367\n",
      "Step: 21500  \tValid loss: 0.2273406982421875\n",
      "Step: 21600  \tTraining loss: 0.1973491907119751\n",
      "Step: 21600  \tTraining accuracy: 0.8942475318908691\n",
      "Step: 21600  \tValid loss: 0.22695709764957428\n",
      "Step: 21700  \tTraining loss: 0.1972055733203888\n",
      "Step: 21700  \tTraining accuracy: 0.8942810893058777\n",
      "Step: 21700  \tValid loss: 0.22669385373592377\n",
      "Step: 21800  \tTraining loss: 0.19706369936466217\n",
      "Step: 21800  \tTraining accuracy: 0.8943144083023071\n",
      "Step: 21800  \tValid loss: 0.22636550664901733\n",
      "Step: 21900  \tTraining loss: 0.1969234198331833\n",
      "Step: 21900  \tTraining accuracy: 0.8943473696708679\n",
      "Step: 21900  \tValid loss: 0.2260473370552063\n",
      "Step: 22000  \tTraining loss: 0.1967855989933014\n",
      "Step: 22000  \tTraining accuracy: 0.8943806290626526\n",
      "Step: 22000  \tValid loss: 0.2258368283510208\n",
      "Step: 22100  \tTraining loss: 0.19665181636810303\n",
      "Step: 22100  \tTraining accuracy: 0.8944136500358582\n",
      "Step: 22100  \tValid loss: 0.2255880981683731\n",
      "Step: 22200  \tTraining loss: 0.19651950895786285\n",
      "Step: 22200  \tTraining accuracy: 0.8944463133811951\n",
      "Step: 22200  \tValid loss: 0.22538378834724426\n",
      "Step: 22300  \tTraining loss: 0.1963881403207779\n",
      "Step: 22300  \tTraining accuracy: 0.8944787383079529\n",
      "Step: 22300  \tValid loss: 0.22520877420902252\n",
      "Step: 22400  \tTraining loss: 0.1962600201368332\n",
      "Step: 22400  \tTraining accuracy: 0.8945108652114868\n",
      "Step: 22400  \tValid loss: 0.22487975656986237\n",
      "Step: 22500  \tTraining loss: 0.19613240659236908\n",
      "Step: 22500  \tTraining accuracy: 0.8945451378822327\n",
      "Step: 22500  \tValid loss: 0.22472751140594482\n",
      "Step: 22600  \tTraining loss: 0.19600877165794373\n",
      "Step: 22600  \tTraining accuracy: 0.8945772647857666\n",
      "Step: 22600  \tValid loss: 0.22441956400871277\n",
      "Step: 22700  \tTraining loss: 0.19588495790958405\n",
      "Step: 22700  \tTraining accuracy: 0.8946090936660767\n",
      "Step: 22700  \tValid loss: 0.22420446574687958\n",
      "Step: 22800  \tTraining loss: 0.1957622766494751\n",
      "Step: 22800  \tTraining accuracy: 0.8946437239646912\n",
      "Step: 22800  \tValid loss: 0.22401709854602814\n",
      "Step: 22900  \tTraining loss: 0.19564208388328552\n",
      "Step: 22900  \tTraining accuracy: 0.894679844379425\n",
      "Step: 22900  \tValid loss: 0.22380571067333221\n",
      "Step: 23000  \tTraining loss: 0.19552317261695862\n",
      "Step: 23000  \tTraining accuracy: 0.8947174549102783\n",
      "Step: 23000  \tValid loss: 0.22358685731887817\n",
      "Step: 23100  \tTraining loss: 0.1954050362110138\n",
      "Step: 23100  \tTraining accuracy: 0.8947571516036987\n",
      "Step: 23100  \tValid loss: 0.22348512709140778\n",
      "Step: 23200  \tTraining loss: 0.19528909027576447\n",
      "Step: 23200  \tTraining accuracy: 0.8948000073432922\n",
      "Step: 23200  \tValid loss: 0.22317489981651306\n",
      "Step: 23300  \tTraining loss: 0.19517460465431213\n",
      "Step: 23300  \tTraining accuracy: 0.8948425650596619\n",
      "Step: 23300  \tValid loss: 0.2229350507259369\n",
      "Step: 23400  \tTraining loss: 0.1950598508119583\n",
      "Step: 23400  \tTraining accuracy: 0.8948858976364136\n",
      "Step: 23400  \tValid loss: 0.22277559340000153\n",
      "Step: 23500  \tTraining loss: 0.19494779407978058\n",
      "Step: 23500  \tTraining accuracy: 0.8949283361434937\n",
      "Step: 23500  \tValid loss: 0.22258417308330536\n",
      "Step: 23600  \tTraining loss: 0.19483628869056702\n",
      "Step: 23600  \tTraining accuracy: 0.8949703574180603\n",
      "Step: 23600  \tValid loss: 0.22242847084999084\n",
      "Step: 23700  \tTraining loss: 0.1947265863418579\n",
      "Step: 23700  \tTraining accuracy: 0.8950120210647583\n",
      "Step: 23700  \tValid loss: 0.2221764326095581\n",
      "Step: 23800  \tTraining loss: 0.19461709260940552\n",
      "Step: 23800  \tTraining accuracy: 0.8950533866882324\n",
      "Step: 23800  \tValid loss: 0.22204893827438354\n",
      "Step: 23900  \tTraining loss: 0.19450995326042175\n",
      "Step: 23900  \tTraining accuracy: 0.8950943350791931\n",
      "Step: 23900  \tValid loss: 0.2218492031097412\n",
      "Step: 24000  \tTraining loss: 0.19440245628356934\n",
      "Step: 24000  \tTraining accuracy: 0.8951338529586792\n",
      "Step: 24000  \tValid loss: 0.22166936099529266\n",
      "Step: 24100  \tTraining loss: 0.19429640471935272\n",
      "Step: 24100  \tTraining accuracy: 0.8951730132102966\n",
      "Step: 24100  \tValid loss: 0.22163310647010803\n",
      "Step: 24200  \tTraining loss: 0.19419188797473907\n",
      "Step: 24200  \tTraining accuracy: 0.8952118158340454\n",
      "Step: 24200  \tValid loss: 0.2213899791240692\n",
      "Step: 24300  \tTraining loss: 0.19408808648586273\n",
      "Step: 24300  \tTraining accuracy: 0.8952503204345703\n",
      "Step: 24300  \tValid loss: 0.22125695645809174\n",
      "Step: 24400  \tTraining loss: 0.19398610293865204\n",
      "Step: 24400  \tTraining accuracy: 0.8952879905700684\n",
      "Step: 24400  \tValid loss: 0.22107870876789093\n",
      "Step: 24500  \tTraining loss: 0.19388291239738464\n",
      "Step: 24500  \tTraining accuracy: 0.8953253030776978\n",
      "Step: 24500  \tValid loss: 0.22098489105701447\n",
      "Step: 24600  \tTraining loss: 0.19378264248371124\n",
      "Step: 24600  \tTraining accuracy: 0.8953617811203003\n",
      "Step: 24600  \tValid loss: 0.22077316045761108\n",
      "Step: 24700  \tTraining loss: 0.19368219375610352\n",
      "Step: 24700  \tTraining accuracy: 0.8953979015350342\n",
      "Step: 24700  \tValid loss: 0.22066333889961243\n",
      "Step: 24800  \tTraining loss: 0.19358275830745697\n",
      "Step: 24800  \tTraining accuracy: 0.895433783531189\n",
      "Step: 24800  \tValid loss: 0.22052165865898132\n",
      "Step: 24900  \tTraining loss: 0.193482905626297\n",
      "Step: 24900  \tTraining accuracy: 0.8954693675041199\n",
      "Step: 24900  \tValid loss: 0.220378577709198\n",
      "Step: 25000  \tTraining loss: 0.19338569045066833\n",
      "Step: 25000  \tTraining accuracy: 0.8955057859420776\n",
      "Step: 25000  \tValid loss: 0.2202538698911667\n",
      "Step: 25100  \tTraining loss: 0.1932879388332367\n",
      "Step: 25100  \tTraining accuracy: 0.8955424427986145\n",
      "Step: 25100  \tValid loss: 0.22010210156440735\n",
      "Step: 25200  \tTraining loss: 0.19319121539592743\n",
      "Step: 25200  \tTraining accuracy: 0.8955788016319275\n",
      "Step: 25200  \tValid loss: 0.21997833251953125\n",
      "Step: 25300  \tTraining loss: 0.19309549033641815\n",
      "Step: 25300  \tTraining accuracy: 0.8956137895584106\n",
      "Step: 25300  \tValid loss: 0.21982905268669128\n",
      "Step: 25400  \tTraining loss: 0.19299982488155365\n",
      "Step: 25400  \tTraining accuracy: 0.8956479430198669\n",
      "Step: 25400  \tValid loss: 0.21973425149917603\n",
      "Step: 25500  \tTraining loss: 0.19290581345558167\n",
      "Step: 25500  \tTraining accuracy: 0.8956834673881531\n",
      "Step: 25500  \tValid loss: 0.21957582235336304\n",
      "Step: 25600  \tTraining loss: 0.192811518907547\n",
      "Step: 25600  \tTraining accuracy: 0.8957187533378601\n",
      "Step: 25600  \tValid loss: 0.21947796642780304\n",
      "Step: 25700  \tTraining loss: 0.19271832704544067\n",
      "Step: 25700  \tTraining accuracy: 0.8957553505897522\n",
      "Step: 25700  \tValid loss: 0.21935266256332397\n",
      "Step: 25800  \tTraining loss: 0.19262497127056122\n",
      "Step: 25800  \tTraining accuracy: 0.8957916498184204\n",
      "Step: 25800  \tValid loss: 0.21924230456352234\n",
      "Step: 25900  \tTraining loss: 0.19253318011760712\n",
      "Step: 25900  \tTraining accuracy: 0.8958276510238647\n",
      "Step: 25900  \tValid loss: 0.21911486983299255\n",
      "Step: 26000  \tTraining loss: 0.19244316220283508\n",
      "Step: 26000  \tTraining accuracy: 0.89586341381073\n",
      "Step: 26000  \tValid loss: 0.21899756789207458\n",
      "Step: 26100  \tTraining loss: 0.1923510730266571\n",
      "Step: 26100  \tTraining accuracy: 0.8958988785743713\n",
      "Step: 26100  \tValid loss: 0.2189389318227768\n",
      "Step: 26200  \tTraining loss: 0.1922619491815567\n",
      "Step: 26200  \tTraining accuracy: 0.8959356546401978\n",
      "Step: 26200  \tValid loss: 0.21877850592136383\n",
      "Step: 26300  \tTraining loss: 0.1921718418598175\n",
      "Step: 26300  \tTraining accuracy: 0.8959737420082092\n",
      "Step: 26300  \tValid loss: 0.21872228384017944\n",
      "Step: 26400  \tTraining loss: 0.1920832246541977\n",
      "Step: 26400  \tTraining accuracy: 0.896012544631958\n",
      "Step: 26400  \tValid loss: 0.21855410933494568\n",
      "Step: 26500  \tTraining loss: 0.1919945627450943\n",
      "Step: 26500  \tTraining accuracy: 0.8960521221160889\n",
      "Step: 26500  \tValid loss: 0.2184603214263916\n",
      "Step: 26600  \tTraining loss: 0.1919068992137909\n",
      "Step: 26600  \tTraining accuracy: 0.8960914611816406\n",
      "Step: 26600  \tValid loss: 0.21831448376178741\n",
      "Step: 26700  \tTraining loss: 0.19181858003139496\n",
      "Step: 26700  \tTraining accuracy: 0.8961314558982849\n",
      "Step: 26700  \tValid loss: 0.21827340126037598\n",
      "Step: 26800  \tTraining loss: 0.19173245131969452\n",
      "Step: 26800  \tTraining accuracy: 0.8961727619171143\n",
      "Step: 26800  \tValid loss: 0.2181583195924759\n",
      "Step: 26900  \tTraining loss: 0.1916460394859314\n",
      "Step: 26900  \tTraining accuracy: 0.8962121605873108\n",
      "Step: 26900  \tValid loss: 0.21802853047847748\n",
      "Step: 27000  \tTraining loss: 0.19156016409397125\n",
      "Step: 27000  \tTraining accuracy: 0.8962502479553223\n",
      "Step: 27000  \tValid loss: 0.21796593070030212\n",
      "Step: 27100  \tTraining loss: 0.19147488474845886\n",
      "Step: 27100  \tTraining accuracy: 0.8962880969047546\n",
      "Step: 27100  \tValid loss: 0.21782684326171875\n",
      "Step: 27200  \tTraining loss: 0.19139008224010468\n",
      "Step: 27200  \tTraining accuracy: 0.8963236212730408\n",
      "Step: 27200  \tValid loss: 0.21773794293403625\n",
      "Step: 27300  \tTraining loss: 0.1913067102432251\n",
      "Step: 27300  \tTraining accuracy: 0.8963583707809448\n",
      "Step: 27300  \tValid loss: 0.21762807667255402\n",
      "Step: 27400  \tTraining loss: 0.191221684217453\n",
      "Step: 27400  \tTraining accuracy: 0.8963949084281921\n",
      "Step: 27400  \tValid loss: 0.21754461526870728\n",
      "Step: 27500  \tTraining loss: 0.19113782048225403\n",
      "Step: 27500  \tTraining accuracy: 0.896432638168335\n",
      "Step: 27500  \tValid loss: 0.21748271584510803\n",
      "Step: 27600  \tTraining loss: 0.19105467200279236\n",
      "Step: 27600  \tTraining accuracy: 0.8964701294898987\n",
      "Step: 27600  \tValid loss: 0.21734362840652466\n",
      "Step: 27700  \tTraining loss: 0.1909707635641098\n",
      "Step: 27700  \tTraining accuracy: 0.8965068459510803\n",
      "Step: 27700  \tValid loss: 0.21732787787914276\n",
      "Step: 27800  \tTraining loss: 0.19088943302631378\n",
      "Step: 27800  \tTraining accuracy: 0.8965432643890381\n",
      "Step: 27800  \tValid loss: 0.21717366576194763\n",
      "Step: 27900  \tTraining loss: 0.1908063441514969\n",
      "Step: 27900  \tTraining accuracy: 0.8965794444084167\n",
      "Step: 27900  \tValid loss: 0.21711784601211548\n",
      "Step: 28000  \tTraining loss: 0.19072407484054565\n",
      "Step: 28000  \tTraining accuracy: 0.8966153860092163\n",
      "Step: 28000  \tValid loss: 0.21700286865234375\n",
      "Step: 28100  \tTraining loss: 0.19064220786094666\n",
      "Step: 28100  \tTraining accuracy: 0.896651029586792\n",
      "Step: 28100  \tValid loss: 0.21692925691604614\n",
      "Step: 28200  \tTraining loss: 0.19056035578250885\n",
      "Step: 28200  \tTraining accuracy: 0.8966864943504333\n",
      "Step: 28200  \tValid loss: 0.21684008836746216\n",
      "Step: 28300  \tTraining loss: 0.1904795914888382\n",
      "Step: 28300  \tTraining accuracy: 0.8967216610908508\n",
      "Step: 28300  \tValid loss: 0.21674653887748718\n",
      "Step: 28400  \tTraining loss: 0.19039830565452576\n",
      "Step: 28400  \tTraining accuracy: 0.8967565298080444\n",
      "Step: 28400  \tValid loss: 0.21665383875370026\n",
      "Step: 28500  \tTraining loss: 0.19031724333763123\n",
      "Step: 28500  \tTraining accuracy: 0.8967912197113037\n",
      "Step: 28500  \tValid loss: 0.2166167050600052\n",
      "Step: 28600  \tTraining loss: 0.19023676216602325\n",
      "Step: 28600  \tTraining accuracy: 0.8968237042427063\n",
      "Step: 28600  \tValid loss: 0.21649135649204254\n",
      "Step: 28700  \tTraining loss: 0.1901562362909317\n",
      "Step: 28700  \tTraining accuracy: 0.8968564867973328\n",
      "Step: 28700  \tValid loss: 0.21636691689491272\n",
      "Step: 28800  \tTraining loss: 0.1900751292705536\n",
      "Step: 28800  \tTraining accuracy: 0.8968923687934875\n",
      "Step: 28800  \tValid loss: 0.21634487807750702\n",
      "Step: 28900  \tTraining loss: 0.189994677901268\n",
      "Step: 28900  \tTraining accuracy: 0.8969270586967468\n",
      "Step: 28900  \tValid loss: 0.21622183918952942\n",
      "Step: 29000  \tTraining loss: 0.1899135410785675\n",
      "Step: 29000  \tTraining accuracy: 0.8969614505767822\n",
      "Step: 29000  \tValid loss: 0.21615241467952728\n",
      "Step: 29100  \tTraining loss: 0.18983320891857147\n",
      "Step: 29100  \tTraining accuracy: 0.8969956636428833\n",
      "Step: 29100  \tValid loss: 0.21603180468082428\n",
      "Step: 29200  \tTraining loss: 0.1897507607936859\n",
      "Step: 29200  \tTraining accuracy: 0.8970305919647217\n",
      "Step: 29200  \tValid loss: 0.2159654200077057\n",
      "Step: 29300  \tTraining loss: 0.18966814875602722\n",
      "Step: 29300  \tTraining accuracy: 0.8970652222633362\n",
      "Step: 29300  \tValid loss: 0.21584205329418182\n",
      "Step: 29400  \tTraining loss: 0.18958143889904022\n",
      "Step: 29400  \tTraining accuracy: 0.8970996737480164\n",
      "Step: 29400  \tValid loss: 0.2157025933265686\n",
      "Step: 29500  \tTraining loss: 0.18949277698993683\n",
      "Step: 29500  \tTraining accuracy: 0.8971338868141174\n",
      "Step: 29500  \tValid loss: 0.21558380126953125\n",
      "Step: 29600  \tTraining loss: 0.18941152095794678\n",
      "Step: 29600  \tTraining accuracy: 0.8971678614616394\n",
      "Step: 29600  \tValid loss: 0.21553939580917358\n",
      "Step: 29700  \tTraining loss: 0.18933162093162537\n",
      "Step: 29700  \tTraining accuracy: 0.8972011208534241\n",
      "Step: 29700  \tValid loss: 0.2154155820608139\n",
      "Step: 29800  \tTraining loss: 0.1892508566379547\n",
      "Step: 29800  \tTraining accuracy: 0.8972342014312744\n",
      "Step: 29800  \tValid loss: 0.21538254618644714\n",
      "Step: 29900  \tTraining loss: 0.18917116522789001\n",
      "Step: 29900  \tTraining accuracy: 0.897265613079071\n",
      "Step: 29900  \tValid loss: 0.21529100835323334\n",
      "Step: 30000  \tTraining loss: 0.18909147381782532\n",
      "Step: 30000  \tTraining accuracy: 0.8972986936569214\n",
      "Step: 30000  \tValid loss: 0.21520240604877472\n",
      "Step: 30100  \tTraining loss: 0.1890113204717636\n",
      "Step: 30100  \tTraining accuracy: 0.8973320126533508\n",
      "Step: 30100  \tValid loss: 0.2151356190443039\n",
      "Step: 30200  \tTraining loss: 0.18893083930015564\n",
      "Step: 30200  \tTraining accuracy: 0.897365152835846\n",
      "Step: 30200  \tValid loss: 0.21506333351135254\n",
      "Step: 30300  \tTraining loss: 0.1888495236635208\n",
      "Step: 30300  \tTraining accuracy: 0.8973979949951172\n",
      "Step: 30300  \tValid loss: 0.21496352553367615\n",
      "Step: 30400  \tTraining loss: 0.18876875936985016\n",
      "Step: 30400  \tTraining accuracy: 0.8974306583404541\n",
      "Step: 30400  \tValid loss: 0.21485234797000885\n",
      "Step: 30500  \tTraining loss: 0.18868644535541534\n",
      "Step: 30500  \tTraining accuracy: 0.8974612951278687\n",
      "Step: 30500  \tValid loss: 0.2147885113954544\n",
      "Step: 30600  \tTraining loss: 0.18860436975955963\n",
      "Step: 30600  \tTraining accuracy: 0.8974917531013489\n",
      "Step: 30600  \tValid loss: 0.214725062251091\n",
      "Step: 30700  \tTraining loss: 0.18852147459983826\n",
      "Step: 30700  \tTraining accuracy: 0.89752197265625\n",
      "Step: 30700  \tValid loss: 0.21466857194900513\n",
      "Step: 30800  \tTraining loss: 0.18843922019004822\n",
      "Step: 30800  \tTraining accuracy: 0.8975515365600586\n",
      "Step: 30800  \tValid loss: 0.214572474360466\n",
      "Step: 30900  \tTraining loss: 0.1883581578731537\n",
      "Step: 30900  \tTraining accuracy: 0.8975809812545776\n",
      "Step: 30900  \tValid loss: 0.21440747380256653\n",
      "Step: 31000  \tTraining loss: 0.188273623585701\n",
      "Step: 31000  \tTraining accuracy: 0.8976101875305176\n",
      "Step: 31000  \tValid loss: 0.2144036591053009\n",
      "Step: 31100  \tTraining loss: 0.18819116055965424\n",
      "Step: 31100  \tTraining accuracy: 0.8976391553878784\n",
      "Step: 31100  \tValid loss: 0.21426157653331757\n",
      "Step: 31200  \tTraining loss: 0.18810786306858063\n",
      "Step: 31200  \tTraining accuracy: 0.8976680040359497\n",
      "Step: 31200  \tValid loss: 0.21417468786239624\n",
      "Step: 31300  \tTraining loss: 0.18802335858345032\n",
      "Step: 31300  \tTraining accuracy: 0.8976975679397583\n",
      "Step: 31300  \tValid loss: 0.21409879624843597\n",
      "Step: 31400  \tTraining loss: 0.18793880939483643\n",
      "Step: 31400  \tTraining accuracy: 0.8977268934249878\n",
      "Step: 31400  \tValid loss: 0.21400973200798035\n",
      "Step: 31500  \tTraining loss: 0.1878536194562912\n",
      "Step: 31500  \tTraining accuracy: 0.897756040096283\n",
      "Step: 31500  \tValid loss: 0.2139035016298294\n",
      "Step: 31600  \tTraining loss: 0.1877683848142624\n",
      "Step: 31600  \tTraining accuracy: 0.8977845907211304\n",
      "Step: 31600  \tValid loss: 0.2137930691242218\n",
      "Step: 31700  \tTraining loss: 0.18768242001533508\n",
      "Step: 31700  \tTraining accuracy: 0.8978151082992554\n",
      "Step: 31700  \tValid loss: 0.21370813250541687\n",
      "Step: 31800  \tTraining loss: 0.18759551644325256\n",
      "Step: 31800  \tTraining accuracy: 0.897845447063446\n",
      "Step: 31800  \tValid loss: 0.21360592544078827\n",
      "Step: 31900  \tTraining loss: 0.18750803172588348\n",
      "Step: 31900  \tTraining accuracy: 0.8978756070137024\n",
      "Step: 31900  \tValid loss: 0.21351207792758942\n",
      "Step: 32000  \tTraining loss: 0.18741977214813232\n",
      "Step: 32000  \tTraining accuracy: 0.8979060053825378\n",
      "Step: 32000  \tValid loss: 0.21343021094799042\n",
      "Step: 32100  \tTraining loss: 0.18733179569244385\n",
      "Step: 32100  \tTraining accuracy: 0.8979349136352539\n",
      "Step: 32100  \tValid loss: 0.2132948338985443\n",
      "Step: 32200  \tTraining loss: 0.18724246323108673\n",
      "Step: 32200  \tTraining accuracy: 0.8979636430740356\n",
      "Step: 32200  \tValid loss: 0.2132079005241394\n",
      "Step: 32300  \tTraining loss: 0.18715155124664307\n",
      "Step: 32300  \tTraining accuracy: 0.8979935050010681\n",
      "Step: 32300  \tValid loss: 0.21314309537410736\n",
      "Step: 32400  \tTraining loss: 0.18706205487251282\n",
      "Step: 32400  \tTraining accuracy: 0.8980227112770081\n",
      "Step: 32400  \tValid loss: 0.21297693252563477\n",
      "Step: 32500  \tTraining loss: 0.18697039783000946\n",
      "Step: 32500  \tTraining accuracy: 0.8980517387390137\n",
      "Step: 32500  \tValid loss: 0.2128971815109253\n",
      "Step: 32600  \tTraining loss: 0.1868785321712494\n",
      "Step: 32600  \tTraining accuracy: 0.8980789184570312\n",
      "Step: 32600  \tValid loss: 0.2127799540758133\n",
      "Step: 32700  \tTraining loss: 0.18678505718708038\n",
      "Step: 32700  \tTraining accuracy: 0.898107647895813\n",
      "Step: 32700  \tValid loss: 0.2127167135477066\n",
      "Step: 32800  \tTraining loss: 0.1866919845342636\n",
      "Step: 32800  \tTraining accuracy: 0.8981344699859619\n",
      "Step: 32800  \tValid loss: 0.2125817835330963\n",
      "Step: 32900  \tTraining loss: 0.18659858405590057\n",
      "Step: 32900  \tTraining accuracy: 0.8981611132621765\n",
      "Step: 32900  \tValid loss: 0.2124655544757843\n",
      "Step: 33000  \tTraining loss: 0.18650344014167786\n",
      "Step: 33000  \tTraining accuracy: 0.8981893062591553\n",
      "Step: 33000  \tValid loss: 0.2123807668685913\n",
      "Step: 33100  \tTraining loss: 0.18640922009944916\n",
      "Step: 33100  \tTraining accuracy: 0.898215651512146\n",
      "Step: 33100  \tValid loss: 0.21225307881832123\n",
      "Step: 33200  \tTraining loss: 0.1863153725862503\n",
      "Step: 33200  \tTraining accuracy: 0.8982402086257935\n",
      "Step: 33200  \tValid loss: 0.21207492053508759\n",
      "Step: 33300  \tTraining loss: 0.18621955811977386\n",
      "Step: 33300  \tTraining accuracy: 0.8982633352279663\n",
      "Step: 33300  \tValid loss: 0.2120041698217392\n",
      "Step: 33400  \tTraining loss: 0.18612416088581085\n",
      "Step: 33400  \tTraining accuracy: 0.8982879519462585\n",
      "Step: 33400  \tValid loss: 0.2118874043226242\n",
      "Step: 33500  \tTraining loss: 0.1860291212797165\n",
      "Step: 33500  \tTraining accuracy: 0.8983107805252075\n",
      "Step: 33500  \tValid loss: 0.21175995469093323\n",
      "Step: 33600  \tTraining loss: 0.18593476712703705\n",
      "Step: 33600  \tTraining accuracy: 0.8983335494995117\n",
      "Step: 33600  \tValid loss: 0.21159610152244568\n",
      "Step: 33700  \tTraining loss: 0.1858409196138382\n",
      "Step: 33700  \tTraining accuracy: 0.8983548879623413\n",
      "Step: 33700  \tValid loss: 0.21142321825027466\n",
      "Step: 33800  \tTraining loss: 0.18574412167072296\n",
      "Step: 33800  \tTraining accuracy: 0.8983772993087769\n",
      "Step: 33800  \tValid loss: 0.21138949692249298\n",
      "Step: 33900  \tTraining loss: 0.1856486052274704\n",
      "Step: 33900  \tTraining accuracy: 0.8984020948410034\n",
      "Step: 33900  \tValid loss: 0.21128204464912415\n",
      "Step: 34000  \tTraining loss: 0.18555429577827454\n",
      "Step: 34000  \tTraining accuracy: 0.8984258770942688\n",
      "Step: 34000  \tValid loss: 0.21112942695617676\n",
      "Step: 34100  \tTraining loss: 0.18546031415462494\n",
      "Step: 34100  \tTraining accuracy: 0.8984499573707581\n",
      "Step: 34100  \tValid loss: 0.21095675230026245\n",
      "Step: 34200  \tTraining loss: 0.18536430597305298\n",
      "Step: 34200  \tTraining accuracy: 0.8984726667404175\n",
      "Step: 34200  \tValid loss: 0.21090884506702423\n",
      "Step: 34300  \tTraining loss: 0.1852683573961258\n",
      "Step: 34300  \tTraining accuracy: 0.8984972834587097\n",
      "Step: 34300  \tValid loss: 0.21089273691177368\n",
      "Step: 34400  \tTraining loss: 0.18517659604549408\n",
      "Step: 34400  \tTraining accuracy: 0.8985196948051453\n",
      "Step: 34400  \tValid loss: 0.2105795294046402\n",
      "Step: 34500  \tTraining loss: 0.1850820928812027\n",
      "Step: 34500  \tTraining accuracy: 0.8985416293144226\n",
      "Step: 34500  \tValid loss: 0.21045038104057312\n",
      "Step: 34600  \tTraining loss: 0.18498636782169342\n",
      "Step: 34600  \tTraining accuracy: 0.8985649943351746\n",
      "Step: 34600  \tValid loss: 0.21036380529403687\n",
      "Step: 34700  \tTraining loss: 0.18489070236682892\n",
      "Step: 34700  \tTraining accuracy: 0.898588240146637\n",
      "Step: 34700  \tValid loss: 0.21027350425720215\n",
      "Step: 34800  \tTraining loss: 0.18479762971401215\n",
      "Step: 34800  \tTraining accuracy: 0.8986121416091919\n",
      "Step: 34800  \tValid loss: 0.21008111536502838\n",
      "Step: 34900  \tTraining loss: 0.18470399081707\n",
      "Step: 34900  \tTraining accuracy: 0.8986351490020752\n",
      "Step: 34900  \tValid loss: 0.20992863178253174\n",
      "Step: 35000  \tTraining loss: 0.1846095323562622\n",
      "Step: 35000  \tTraining accuracy: 0.89865642786026\n",
      "Step: 35000  \tValid loss: 0.20977996289730072\n",
      "Step: 35100  \tTraining loss: 0.1845136433839798\n",
      "Step: 35100  \tTraining accuracy: 0.8986791372299194\n",
      "Step: 35100  \tValid loss: 0.20964619517326355\n",
      "Step: 35200  \tTraining loss: 0.18441779911518097\n",
      "Step: 35200  \tTraining accuracy: 0.8987001776695251\n",
      "Step: 35200  \tValid loss: 0.20956622064113617\n",
      "Step: 35300  \tTraining loss: 0.1843239665031433\n",
      "Step: 35300  \tTraining accuracy: 0.8987218141555786\n",
      "Step: 35300  \tValid loss: 0.20941248536109924\n",
      "Step: 35400  \tTraining loss: 0.1842258870601654\n",
      "Step: 35400  \tTraining accuracy: 0.8987430334091187\n",
      "Step: 35400  \tValid loss: 0.20935556292533875\n",
      "Step: 35500  \tTraining loss: 0.1841297447681427\n",
      "Step: 35500  \tTraining accuracy: 0.8987652063369751\n",
      "Step: 35500  \tValid loss: 0.2092358022928238\n",
      "Step: 35600  \tTraining loss: 0.18403343856334686\n",
      "Step: 35600  \tTraining accuracy: 0.8987857699394226\n",
      "Step: 35600  \tValid loss: 0.2091338187456131\n",
      "Step: 35700  \tTraining loss: 0.18393485248088837\n",
      "Step: 35700  \tTraining accuracy: 0.8988077640533447\n",
      "Step: 35700  \tValid loss: 0.20906592905521393\n",
      "Step: 35800  \tTraining loss: 0.18383920192718506\n",
      "Step: 35800  \tTraining accuracy: 0.8988296389579773\n",
      "Step: 35800  \tValid loss: 0.20885980129241943\n",
      "Step: 35900  \tTraining loss: 0.18374136090278625\n",
      "Step: 35900  \tTraining accuracy: 0.8988528847694397\n",
      "Step: 35900  \tValid loss: 0.20874890685081482\n",
      "Step: 36000  \tTraining loss: 0.18364328145980835\n",
      "Step: 36000  \tTraining accuracy: 0.8988737463951111\n",
      "Step: 36000  \tValid loss: 0.2086193710565567\n",
      "Step: 36100  \tTraining loss: 0.1835453361272812\n",
      "Step: 36100  \tTraining accuracy: 0.8988944292068481\n",
      "Step: 36100  \tValid loss: 0.2084624320268631\n",
      "Step: 36200  \tTraining loss: 0.18344423174858093\n",
      "Step: 36200  \tTraining accuracy: 0.8989150524139404\n",
      "Step: 36200  \tValid loss: 0.20841753482818604\n",
      "Step: 36300  \tTraining loss: 0.1833454966545105\n",
      "Step: 36300  \tTraining accuracy: 0.8989363312721252\n",
      "Step: 36300  \tValid loss: 0.20824630558490753\n",
      "Step: 36400  \tTraining loss: 0.18324525654315948\n",
      "Step: 36400  \tTraining accuracy: 0.8989585638046265\n",
      "Step: 36400  \tValid loss: 0.20810705423355103\n",
      "Step: 36500  \tTraining loss: 0.18314316868782043\n",
      "Step: 36500  \tTraining accuracy: 0.8989811539649963\n",
      "Step: 36500  \tValid loss: 0.20802660286426544\n",
      "Step: 36600  \tTraining loss: 0.18304210901260376\n",
      "Step: 36600  \tTraining accuracy: 0.8990035653114319\n",
      "Step: 36600  \tValid loss: 0.20786522328853607\n",
      "Step: 36700  \tTraining loss: 0.182942733168602\n",
      "Step: 36700  \tTraining accuracy: 0.8990254402160645\n",
      "Step: 36700  \tValid loss: 0.20766139030456543\n",
      "Step: 36800  \tTraining loss: 0.18283721804618835\n",
      "Step: 36800  \tTraining accuracy: 0.8990472555160522\n",
      "Step: 36800  \tValid loss: 0.20760391652584076\n",
      "Step: 36900  \tTraining loss: 0.18273332715034485\n",
      "Step: 36900  \tTraining accuracy: 0.8990678191184998\n",
      "Step: 36900  \tValid loss: 0.20746232569217682\n",
      "Step: 37000  \tTraining loss: 0.18262805044651031\n",
      "Step: 37000  \tTraining accuracy: 0.8990874886512756\n",
      "Step: 37000  \tValid loss: 0.20735305547714233\n",
      "Step: 37100  \tTraining loss: 0.18252114951610565\n",
      "Step: 37100  \tTraining accuracy: 0.8991070985794067\n",
      "Step: 37100  \tValid loss: 0.20729495584964752\n",
      "Step: 37200  \tTraining loss: 0.18241560459136963\n",
      "Step: 37200  \tTraining accuracy: 0.8991265892982483\n",
      "Step: 37200  \tValid loss: 0.20712575316429138\n",
      "Step: 37300  \tTraining loss: 0.18230678141117096\n",
      "Step: 37300  \tTraining accuracy: 0.8991433382034302\n",
      "Step: 37300  \tValid loss: 0.20711420476436615\n",
      "Step: 37400  \tTraining loss: 0.1822015941143036\n",
      "Step: 37400  \tTraining accuracy: 0.8991600275039673\n",
      "Step: 37400  \tValid loss: 0.2068495750427246\n",
      "Step: 37500  \tTraining loss: 0.18209053575992584\n",
      "Step: 37500  \tTraining accuracy: 0.8991755247116089\n",
      "Step: 37500  \tValid loss: 0.20682744681835175\n",
      "Step: 37600  \tTraining loss: 0.18198338150978088\n",
      "Step: 37600  \tTraining accuracy: 0.8991906046867371\n",
      "Step: 37600  \tValid loss: 0.20659960806369781\n",
      "Step: 37700  \tTraining loss: 0.1818719059228897\n",
      "Step: 37700  \tTraining accuracy: 0.8992070555686951\n",
      "Step: 37700  \tValid loss: 0.20650692284107208\n",
      "Step: 37800  \tTraining loss: 0.18176130950450897\n",
      "Step: 37800  \tTraining accuracy: 0.8992233872413635\n",
      "Step: 37800  \tValid loss: 0.20634125173091888\n",
      "Step: 37900  \tTraining loss: 0.18164806067943573\n",
      "Step: 37900  \tTraining accuracy: 0.8992396593093872\n",
      "Step: 37900  \tValid loss: 0.2062721997499466\n",
      "Step: 38000  \tTraining loss: 0.18153506517410278\n",
      "Step: 38000  \tTraining accuracy: 0.8992558121681213\n",
      "Step: 38000  \tValid loss: 0.20614799857139587\n",
      "Step: 38100  \tTraining loss: 0.18142499029636383\n",
      "Step: 38100  \tTraining accuracy: 0.8992719054222107\n",
      "Step: 38100  \tValid loss: 0.20590245723724365\n",
      "Step: 38200  \tTraining loss: 0.1813066452741623\n",
      "Step: 38200  \tTraining accuracy: 0.8992879390716553\n",
      "Step: 38200  \tValid loss: 0.20587770640850067\n",
      "Step: 38300  \tTraining loss: 0.18119333684444427\n",
      "Step: 38300  \tTraining accuracy: 0.8993038535118103\n",
      "Step: 38300  \tValid loss: 0.2056836485862732\n",
      "Step: 38400  \tTraining loss: 0.18107356131076813\n",
      "Step: 38400  \tTraining accuracy: 0.8993197083473206\n",
      "Step: 38400  \tValid loss: 0.2056986689567566\n",
      "Step: 38500  \tTraining loss: 0.18095873296260834\n",
      "Step: 38500  \tTraining accuracy: 0.899334728717804\n",
      "Step: 38500  \tValid loss: 0.20546932518482208\n",
      "Step: 38600  \tTraining loss: 0.18084070086479187\n",
      "Step: 38600  \tTraining accuracy: 0.8993496894836426\n",
      "Step: 38600  \tValid loss: 0.20535458624362946\n",
      "Step: 38700  \tTraining loss: 0.1807229071855545\n",
      "Step: 38700  \tTraining accuracy: 0.899366021156311\n",
      "Step: 38700  \tValid loss: 0.20519888401031494\n",
      "Step: 38800  \tTraining loss: 0.18060418963432312\n",
      "Step: 38800  \tTraining accuracy: 0.8993808031082153\n",
      "Step: 38800  \tValid loss: 0.20506714284420013\n",
      "Step: 38900  \tTraining loss: 0.1804841011762619\n",
      "Step: 38900  \tTraining accuracy: 0.8993948698043823\n",
      "Step: 38900  \tValid loss: 0.20494665205478668\n",
      "Step: 39000  \tTraining loss: 0.18036438524723053\n",
      "Step: 39000  \tTraining accuracy: 0.8994081020355225\n",
      "Step: 39000  \tValid loss: 0.20480090379714966\n",
      "Step: 39100  \tTraining loss: 0.18024340271949768\n",
      "Step: 39100  \tTraining accuracy: 0.8994198441505432\n",
      "Step: 39100  \tValid loss: 0.20464779436588287\n",
      "Step: 39200  \tTraining loss: 0.18012259900569916\n",
      "Step: 39200  \tTraining accuracy: 0.8994329571723938\n",
      "Step: 39200  \tValid loss: 0.20450927317142487\n",
      "Step: 39300  \tTraining loss: 0.18000034987926483\n",
      "Step: 39300  \tTraining accuracy: 0.8994442224502563\n",
      "Step: 39300  \tValid loss: 0.20438805222511292\n",
      "Step: 39400  \tTraining loss: 0.17987877130508423\n",
      "Step: 39400  \tTraining accuracy: 0.8994554877281189\n",
      "Step: 39400  \tValid loss: 0.20424288511276245\n",
      "Step: 39500  \tTraining loss: 0.17975717782974243\n",
      "Step: 39500  \tTraining accuracy: 0.8994677066802979\n",
      "Step: 39500  \tValid loss: 0.20405890047550201\n",
      "Step: 39600  \tTraining loss: 0.17963311076164246\n",
      "Step: 39600  \tTraining accuracy: 0.8994780778884888\n",
      "Step: 39600  \tValid loss: 0.20395229756832123\n",
      "Step: 39700  \tTraining loss: 0.17951077222824097\n",
      "Step: 39700  \tTraining accuracy: 0.8994884490966797\n",
      "Step: 39700  \tValid loss: 0.20378336310386658\n",
      "Step: 39800  \tTraining loss: 0.179387629032135\n",
      "Step: 39800  \tTraining accuracy: 0.8994977474212646\n",
      "Step: 39800  \tValid loss: 0.20363092422485352\n",
      "Step: 39900  \tTraining loss: 0.1792641133069992\n",
      "Step: 39900  \tTraining accuracy: 0.8995069265365601\n",
      "Step: 39900  \tValid loss: 0.20348158478736877\n",
      "Step: 40000  \tTraining loss: 0.17913879454135895\n",
      "Step: 40000  \tTraining accuracy: 0.8995157480239868\n",
      "Step: 40000  \tValid loss: 0.20347069203853607\n",
      "Step: 40100  \tTraining loss: 0.17901842296123505\n",
      "Step: 40100  \tTraining accuracy: 0.8995245695114136\n",
      "Step: 40100  \tValid loss: 0.20316505432128906\n",
      "Step: 40200  \tTraining loss: 0.1788955181837082\n",
      "Step: 40200  \tTraining accuracy: 0.8995333313941956\n",
      "Step: 40200  \tValid loss: 0.20301967859268188\n",
      "Step: 40300  \tTraining loss: 0.1787758469581604\n",
      "Step: 40300  \tTraining accuracy: 0.8995433449745178\n",
      "Step: 40300  \tValid loss: 0.20279279351234436\n",
      "Step: 40400  \tTraining loss: 0.1786540150642395\n",
      "Step: 40400  \tTraining accuracy: 0.8995537161827087\n",
      "Step: 40400  \tValid loss: 0.20269721746444702\n",
      "Step: 40500  \tTraining loss: 0.1785300374031067\n",
      "Step: 40500  \tTraining accuracy: 0.899566113948822\n",
      "Step: 40500  \tValid loss: 0.2025541067123413\n",
      "Step: 40600  \tTraining loss: 0.17840930819511414\n",
      "Step: 40600  \tTraining accuracy: 0.8995797634124756\n",
      "Step: 40600  \tValid loss: 0.2023516297340393\n",
      "Step: 40700  \tTraining loss: 0.17828771471977234\n",
      "Step: 40700  \tTraining accuracy: 0.8995933532714844\n",
      "Step: 40700  \tValid loss: 0.20218798518180847\n",
      "Step: 40800  \tTraining loss: 0.17816680669784546\n",
      "Step: 40800  \tTraining accuracy: 0.8996075391769409\n",
      "Step: 40800  \tValid loss: 0.20202305912971497\n",
      "Step: 40900  \tTraining loss: 0.17804618179798126\n",
      "Step: 40900  \tTraining accuracy: 0.8996216654777527\n",
      "Step: 40900  \tValid loss: 0.20186756551265717\n",
      "Step: 41000  \tTraining loss: 0.17792478203773499\n",
      "Step: 41000  \tTraining accuracy: 0.8996350765228271\n",
      "Step: 41000  \tValid loss: 0.2017526924610138\n",
      "Step: 41100  \tTraining loss: 0.17780578136444092\n",
      "Step: 41100  \tTraining accuracy: 0.8996483683586121\n",
      "Step: 41100  \tValid loss: 0.20156025886535645\n",
      "Step: 41200  \tTraining loss: 0.17768578231334686\n",
      "Step: 41200  \tTraining accuracy: 0.8996613025665283\n",
      "Step: 41200  \tValid loss: 0.20140287280082703\n",
      "Step: 41300  \tTraining loss: 0.17757080495357513\n",
      "Step: 41300  \tTraining accuracy: 0.8996741771697998\n",
      "Step: 41300  \tValid loss: 0.20123088359832764\n",
      "Step: 41400  \tTraining loss: 0.17745241522789001\n",
      "Step: 41400  \tTraining accuracy: 0.8996869325637817\n",
      "Step: 41400  \tValid loss: 0.20114563405513763\n",
      "Step: 41500  \tTraining loss: 0.1773376166820526\n",
      "Step: 41500  \tTraining accuracy: 0.8996996879577637\n",
      "Step: 41500  \tValid loss: 0.20089185237884521\n",
      "Step: 41600  \tTraining loss: 0.1772220879793167\n",
      "Step: 41600  \tTraining accuracy: 0.899712324142456\n",
      "Step: 41600  \tValid loss: 0.20072592794895172\n",
      "Step: 41700  \tTraining loss: 0.17710493505001068\n",
      "Step: 41700  \tTraining accuracy: 0.8997232913970947\n",
      "Step: 41700  \tValid loss: 0.20073793828487396\n",
      "Step: 41800  \tTraining loss: 0.17699354887008667\n",
      "Step: 41800  \tTraining accuracy: 0.8997341990470886\n",
      "Step: 41800  \tValid loss: 0.20039483904838562\n",
      "Step: 41900  \tTraining loss: 0.17687979340553284\n",
      "Step: 41900  \tTraining accuracy: 0.8997450470924377\n",
      "Step: 41900  \tValid loss: 0.2002905011177063\n",
      "Step: 42000  \tTraining loss: 0.17676608264446259\n",
      "Step: 42000  \tTraining accuracy: 0.8997558951377869\n",
      "Step: 42000  \tValid loss: 0.20019547641277313\n",
      "Step: 42100  \tTraining loss: 0.17665834724903107\n",
      "Step: 42100  \tTraining accuracy: 0.8997675776481628\n",
      "Step: 42100  \tValid loss: 0.1999378353357315\n",
      "Step: 42200  \tTraining loss: 0.17654648423194885\n",
      "Step: 42200  \tTraining accuracy: 0.8997799158096313\n",
      "Step: 42200  \tValid loss: 0.19983656704425812\n",
      "Step: 42300  \tTraining loss: 0.17643535137176514\n",
      "Step: 42300  \tTraining accuracy: 0.89979088306427\n",
      "Step: 42300  \tValid loss: 0.19977743923664093\n",
      "Step: 42400  \tTraining loss: 0.17632734775543213\n",
      "Step: 42400  \tTraining accuracy: 0.8998050689697266\n",
      "Step: 42400  \tValid loss: 0.19959786534309387\n",
      "Step: 42500  \tTraining loss: 0.1762191206216812\n",
      "Step: 42500  \tTraining accuracy: 0.8998178839683533\n",
      "Step: 42500  \tValid loss: 0.199553444981575\n",
      "Step: 42600  \tTraining loss: 0.17611481249332428\n",
      "Step: 42600  \tTraining accuracy: 0.8998319506645203\n",
      "Step: 42600  \tValid loss: 0.19923356175422668\n",
      "Step: 42700  \tTraining loss: 0.17600828409194946\n",
      "Step: 42700  \tTraining accuracy: 0.8998456001281738\n",
      "Step: 42700  \tValid loss: 0.19924114644527435\n",
      "Step: 42800  \tTraining loss: 0.17590449750423431\n",
      "Step: 42800  \tTraining accuracy: 0.8998578786849976\n",
      "Step: 42800  \tValid loss: 0.19900229573249817\n",
      "Step: 42900  \tTraining loss: 0.1758040338754654\n",
      "Step: 42900  \tTraining accuracy: 0.8998727202415466\n",
      "Step: 42900  \tValid loss: 0.19877712428569794\n",
      "Step: 43000  \tTraining loss: 0.1757003664970398\n",
      "Step: 43000  \tTraining accuracy: 0.8998861312866211\n",
      "Step: 43000  \tValid loss: 0.1988411694765091\n",
      "Step: 43100  \tTraining loss: 0.17560593783855438\n",
      "Step: 43100  \tTraining accuracy: 0.8999008536338806\n",
      "Step: 43100  \tValid loss: 0.19843557476997375\n",
      "Step: 43200  \tTraining loss: 0.17550118267536163\n",
      "Step: 43200  \tTraining accuracy: 0.8999138474464417\n",
      "Step: 43200  \tValid loss: 0.19849444925785065\n",
      "Step: 43300  \tTraining loss: 0.17540428042411804\n",
      "Step: 43300  \tTraining accuracy: 0.8999287486076355\n",
      "Step: 43300  \tValid loss: 0.19831417500972748\n",
      "Step: 43400  \tTraining loss: 0.17530621588230133\n",
      "Step: 43400  \tTraining accuracy: 0.8999406695365906\n",
      "Step: 43400  \tValid loss: 0.19817803800106049\n",
      "Step: 43500  \tTraining loss: 0.17520837485790253\n",
      "Step: 43500  \tTraining accuracy: 0.8999515771865845\n",
      "Step: 43500  \tValid loss: 0.1980733573436737\n",
      "Step: 43600  \tTraining loss: 0.1751098334789276\n",
      "Step: 43600  \tTraining accuracy: 0.8999624848365784\n",
      "Step: 43600  \tValid loss: 0.19808098673820496\n",
      "Step: 43700  \tTraining loss: 0.17501665651798248\n",
      "Step: 43700  \tTraining accuracy: 0.8999732732772827\n",
      "Step: 43700  \tValid loss: 0.19783523678779602\n",
      "Step: 43800  \tTraining loss: 0.17492251098155975\n",
      "Step: 43800  \tTraining accuracy: 0.8999840617179871\n",
      "Step: 43800  \tValid loss: 0.1977066695690155\n",
      "Step: 43900  \tTraining loss: 0.17482762038707733\n",
      "Step: 43900  \tTraining accuracy: 0.8999938368797302\n",
      "Step: 43900  \tValid loss: 0.19762203097343445\n",
      "Step: 44000  \tTraining loss: 0.17473328113555908\n",
      "Step: 44000  \tTraining accuracy: 0.9000029563903809\n",
      "Step: 44000  \tValid loss: 0.19757746160030365\n",
      "Step: 44100  \tTraining loss: 0.17464078962802887\n",
      "Step: 44100  \tTraining accuracy: 0.9000108242034912\n",
      "Step: 44100  \tValid loss: 0.19749398529529572\n",
      "Step: 44200  \tTraining loss: 0.17455150187015533\n",
      "Step: 44200  \tTraining accuracy: 0.9000179767608643\n",
      "Step: 44200  \tValid loss: 0.197279691696167\n",
      "Step: 44300  \tTraining loss: 0.17445948719978333\n",
      "Step: 44300  \tTraining accuracy: 0.9000250697135925\n",
      "Step: 44300  \tValid loss: 0.19723384082317352\n",
      "Step: 44400  \tTraining loss: 0.17436742782592773\n",
      "Step: 44400  \tTraining accuracy: 0.9000328183174133\n",
      "Step: 44400  \tValid loss: 0.1972375512123108\n",
      "Step: 44500  \tTraining loss: 0.17427833378314972\n",
      "Step: 44500  \tTraining accuracy: 0.9000405073165894\n",
      "Step: 44500  \tValid loss: 0.19711105525493622\n",
      "Step: 44600  \tTraining loss: 0.17418956756591797\n",
      "Step: 44600  \tTraining accuracy: 0.9000475406646729\n",
      "Step: 44600  \tValid loss: 0.1969704031944275\n",
      "Step: 44700  \tTraining loss: 0.174103781580925\n",
      "Step: 44700  \tTraining accuracy: 0.9000548124313354\n",
      "Step: 44700  \tValid loss: 0.19678331911563873\n",
      "Step: 44800  \tTraining loss: 0.1740160435438156\n",
      "Step: 44800  \tTraining accuracy: 0.900062084197998\n",
      "Step: 44800  \tValid loss: 0.1966925859451294\n",
      "Step: 44900  \tTraining loss: 0.17392787337303162\n",
      "Step: 44900  \tTraining accuracy: 0.9000693559646606\n",
      "Step: 44900  \tValid loss: 0.1965930163860321\n",
      "Step: 45000  \tTraining loss: 0.17384086549282074\n",
      "Step: 45000  \tTraining accuracy: 0.9000765681266785\n",
      "Step: 45000  \tValid loss: 0.19649408757686615\n",
      "Step: 45100  \tTraining loss: 0.17375163733959198\n",
      "Step: 45100  \tTraining accuracy: 0.9000828266143799\n",
      "Step: 45100  \tValid loss: 0.1964685022830963\n",
      "Step: 45200  \tTraining loss: 0.17366842925548553\n",
      "Step: 45200  \tTraining accuracy: 0.9000902771949768\n",
      "Step: 45200  \tValid loss: 0.19629712402820587\n",
      "Step: 45300  \tTraining loss: 0.1735796332359314\n",
      "Step: 45300  \tTraining accuracy: 0.900097668170929\n",
      "Step: 45300  \tValid loss: 0.1962769627571106\n",
      "Step: 45400  \tTraining loss: 0.173495352268219\n",
      "Step: 45400  \tTraining accuracy: 0.9001050591468811\n",
      "Step: 45400  \tValid loss: 0.1961938440799713\n",
      "Step: 45500  \tTraining loss: 0.1734085977077484\n",
      "Step: 45500  \tTraining accuracy: 0.9001124501228333\n",
      "Step: 45500  \tValid loss: 0.19611628353595734\n",
      "Step: 45600  \tTraining loss: 0.17332278192043304\n",
      "Step: 45600  \tTraining accuracy: 0.9001197814941406\n",
      "Step: 45600  \tValid loss: 0.1960422694683075\n",
      "Step: 45700  \tTraining loss: 0.17323942482471466\n",
      "Step: 45700  \tTraining accuracy: 0.9001270532608032\n",
      "Step: 45700  \tValid loss: 0.1959010362625122\n",
      "Step: 45800  \tTraining loss: 0.17315442860126495\n",
      "Step: 45800  \tTraining accuracy: 0.9001343250274658\n",
      "Step: 45800  \tValid loss: 0.1958186775445938\n",
      "Step: 45900  \tTraining loss: 0.17306849360466003\n",
      "Step: 45900  \tTraining accuracy: 0.9001415371894836\n",
      "Step: 45900  \tValid loss: 0.19583545625209808\n",
      "Step: 46000  \tTraining loss: 0.17298611998558044\n",
      "Step: 46000  \tTraining accuracy: 0.9001472592353821\n",
      "Step: 46000  \tValid loss: 0.19564418494701385\n",
      "Step: 46100  \tTraining loss: 0.1729038804769516\n",
      "Step: 46100  \tTraining accuracy: 0.9001526236534119\n",
      "Step: 46100  \tValid loss: 0.19547782838344574\n",
      "Step: 46200  \tTraining loss: 0.17281799018383026\n",
      "Step: 46200  \tTraining accuracy: 0.9001579284667969\n",
      "Step: 46200  \tValid loss: 0.19547930359840393\n",
      "Step: 46300  \tTraining loss: 0.17274537682533264\n",
      "Step: 46300  \tTraining accuracy: 0.9001632928848267\n",
      "Step: 46300  \tValid loss: 0.19517001509666443\n",
      "Step: 46400  \tTraining loss: 0.17265062034130096\n",
      "Step: 46400  \tTraining accuracy: 0.9001685976982117\n",
      "Step: 46400  \tValid loss: 0.19528022408485413\n",
      "Step: 46500  \tTraining loss: 0.17256392538547516\n",
      "Step: 46500  \tTraining accuracy: 0.9001744389533997\n",
      "Step: 46500  \tValid loss: 0.19529405236244202\n",
      "Step: 46600  \tTraining loss: 0.17248184978961945\n",
      "Step: 46600  \tTraining accuracy: 0.9001796841621399\n",
      "Step: 46600  \tValid loss: 0.19515971839427948\n",
      "Step: 46700  \tTraining loss: 0.17239953577518463\n",
      "Step: 46700  \tTraining accuracy: 0.9001837372779846\n",
      "Step: 46700  \tValid loss: 0.19503337144851685\n",
      "Step: 46800  \tTraining loss: 0.1723148673772812\n",
      "Step: 46800  \tTraining accuracy: 0.90018630027771\n",
      "Step: 46800  \tValid loss: 0.19499947130680084\n",
      "Step: 46900  \tTraining loss: 0.1722331941127777\n",
      "Step: 46900  \tTraining accuracy: 0.9001903533935547\n",
      "Step: 46900  \tValid loss: 0.1948515921831131\n",
      "Step: 47000  \tTraining loss: 0.17214767634868622\n",
      "Step: 47000  \tTraining accuracy: 0.9001940488815308\n",
      "Step: 47000  \tValid loss: 0.1949702352285385\n",
      "Step: 47100  \tTraining loss: 0.17206765711307526\n",
      "Step: 47100  \tTraining accuracy: 0.9001980423927307\n",
      "Step: 47100  \tValid loss: 0.1946975141763687\n",
      "Step: 47200  \tTraining loss: 0.17198802530765533\n",
      "Step: 47200  \tTraining accuracy: 0.9002019762992859\n",
      "Step: 47200  \tValid loss: 0.19452141225337982\n",
      "Step: 47300  \tTraining loss: 0.1719045341014862\n",
      "Step: 47300  \tTraining accuracy: 0.9002059698104858\n",
      "Step: 47300  \tValid loss: 0.19447559118270874\n",
      "Step: 47400  \tTraining loss: 0.1718222200870514\n",
      "Step: 47400  \tTraining accuracy: 0.9002093076705933\n",
      "Step: 47400  \tValid loss: 0.19440704584121704\n",
      "Step: 47500  \tTraining loss: 0.17173996567726135\n",
      "Step: 47500  \tTraining accuracy: 0.9002129435539246\n",
      "Step: 47500  \tValid loss: 0.19434276223182678\n",
      "Step: 47600  \tTraining loss: 0.1716562956571579\n",
      "Step: 47600  \tTraining accuracy: 0.9002165198326111\n",
      "Step: 47600  \tValid loss: 0.19434228539466858\n",
      "Step: 47700  \tTraining loss: 0.17157725989818573\n",
      "Step: 47700  \tTraining accuracy: 0.9002201557159424\n",
      "Step: 47700  \tValid loss: 0.19416747987270355\n",
      "Step: 47800  \tTraining loss: 0.17149360477924347\n",
      "Step: 47800  \tTraining accuracy: 0.9002245664596558\n",
      "Step: 47800  \tValid loss: 0.1942891627550125\n",
      "Step: 47900  \tTraining loss: 0.17141512036323547\n",
      "Step: 47900  \tTraining accuracy: 0.9002281427383423\n",
      "Step: 47900  \tValid loss: 0.19402632117271423\n",
      "Step: 48000  \tTraining loss: 0.17133179306983948\n",
      "Step: 48000  \tTraining accuracy: 0.9002325534820557\n",
      "Step: 48000  \tValid loss: 0.19400574266910553\n",
      "Step: 48100  \tTraining loss: 0.17125222086906433\n",
      "Step: 48100  \tTraining accuracy: 0.9002358317375183\n",
      "Step: 48100  \tValid loss: 0.19383344054222107\n",
      "Step: 48200  \tTraining loss: 0.17117053270339966\n",
      "Step: 48200  \tTraining accuracy: 0.9002390503883362\n",
      "Step: 48200  \tValid loss: 0.19371652603149414\n",
      "Step: 48300  \tTraining loss: 0.1710820496082306\n",
      "Step: 48300  \tTraining accuracy: 0.9002431035041809\n",
      "Step: 48300  \tValid loss: 0.19377762079238892\n",
      "Step: 48400  \tTraining loss: 0.17100591957569122\n",
      "Step: 48400  \tTraining accuracy: 0.9002457857131958\n",
      "Step: 48400  \tValid loss: 0.19354385137557983\n",
      "Step: 48500  \tTraining loss: 0.17090485990047455\n",
      "Step: 48500  \tTraining accuracy: 0.9002498388290405\n",
      "Step: 48500  \tValid loss: 0.19365140795707703\n",
      "Step: 48600  \tTraining loss: 0.1708143651485443\n",
      "Step: 48600  \tTraining accuracy: 0.9002538323402405\n",
      "Step: 48600  \tValid loss: 0.19354842603206635\n",
      "Step: 48700  \tTraining loss: 0.17071470618247986\n",
      "Step: 48700  \tTraining accuracy: 0.9002578854560852\n",
      "Step: 48700  \tValid loss: 0.19355209171772003\n",
      "Step: 48800  \tTraining loss: 0.1706230789422989\n",
      "Step: 48800  \tTraining accuracy: 0.9002618789672852\n",
      "Step: 48800  \tValid loss: 0.1934150755405426\n",
      "Step: 48900  \tTraining loss: 0.17053456604480743\n",
      "Step: 48900  \tTraining accuracy: 0.9002658128738403\n",
      "Step: 48900  \tValid loss: 0.19339047372341156\n",
      "Step: 49000  \tTraining loss: 0.1704452484846115\n",
      "Step: 49000  \tTraining accuracy: 0.9002698063850403\n",
      "Step: 49000  \tValid loss: 0.19328506290912628\n",
      "Step: 49100  \tTraining loss: 0.1703597605228424\n",
      "Step: 49100  \tTraining accuracy: 0.9002732038497925\n",
      "Step: 49100  \tValid loss: 0.19304978847503662\n",
      "Step: 49200  \tTraining loss: 0.17024385929107666\n",
      "Step: 49200  \tTraining accuracy: 0.9002782702445984\n",
      "Step: 49200  \tValid loss: 0.19319839775562286\n",
      "Step: 49300  \tTraining loss: 0.17013221979141235\n",
      "Step: 49300  \tTraining accuracy: 0.9002835750579834\n",
      "Step: 49300  \tValid loss: 0.19309654831886292\n",
      "Step: 49400  \tTraining loss: 0.17001914978027344\n",
      "Step: 49400  \tTraining accuracy: 0.9002894163131714\n",
      "Step: 49400  \tValid loss: 0.19304779171943665\n",
      "Step: 49500  \tTraining loss: 0.16992463171482086\n",
      "Step: 49500  \tTraining accuracy: 0.9002966284751892\n",
      "Step: 49500  \tValid loss: 0.19294482469558716\n",
      "Step: 49600  \tTraining loss: 0.16983121633529663\n",
      "Step: 49600  \tTraining accuracy: 0.9003030061721802\n",
      "Step: 49600  \tValid loss: 0.19293463230133057\n",
      "Step: 49700  \tTraining loss: 0.16974098980426788\n",
      "Step: 49700  \tTraining accuracy: 0.9003104567527771\n",
      "Step: 49700  \tValid loss: 0.19289323687553406\n",
      "Step: 49800  \tTraining loss: 0.1696530282497406\n",
      "Step: 49800  \tTraining accuracy: 0.9003178477287292\n",
      "Step: 49800  \tValid loss: 0.192832350730896\n",
      "Step: 49900  \tTraining loss: 0.16955886781215668\n",
      "Step: 49900  \tTraining accuracy: 0.9003252387046814\n",
      "Step: 49900  \tValid loss: 0.19293271005153656\n",
      "Step: 50000  \tTraining loss: 0.16947053372859955\n",
      "Step: 50000  \tTraining accuracy: 0.9003326296806335\n",
      "Step: 50000  \tValid loss: 0.19287024438381195\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.90033996\n",
      "Precision: 0.89642286\n",
      "Recall: 0.94485086\n",
      "F1 score: 0.92402005\n",
      "AUC: 0.9199363\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0   0.90034   0.896423  0.944851   0.92402  0.919936  0.169471      0.900321   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.192667       0.900315   0.327444      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  49999.0  \n",
      "5\n",
      "(4205, 8)\n",
      "(4205, 1)\n",
      "(2320, 8)\n",
      "(2320, 1)\n",
      "(1885, 8)\n",
      "(1885, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6601394414901733\n",
      "Step: 100  \tTraining accuracy: 0.5778834819793701\n",
      "Step: 100  \tValid loss: 0.6497406959533691\n",
      "Step: 200  \tTraining loss: 0.6462661623954773\n",
      "Step: 200  \tTraining accuracy: 0.5975425839424133\n",
      "Step: 200  \tValid loss: 0.6389305591583252\n",
      "Step: 300  \tTraining loss: 0.639360785484314\n",
      "Step: 300  \tTraining accuracy: 0.6109869480133057\n",
      "Step: 300  \tValid loss: 0.6321077346801758\n",
      "Step: 400  \tTraining loss: 0.6308640241622925\n",
      "Step: 400  \tTraining accuracy: 0.6195345520973206\n",
      "Step: 400  \tValid loss: 0.6217405796051025\n",
      "Step: 500  \tTraining loss: 0.6190350651741028\n",
      "Step: 500  \tTraining accuracy: 0.626767098903656\n",
      "Step: 500  \tValid loss: 0.6078482270240784\n",
      "Step: 600  \tTraining loss: 0.6063826084136963\n",
      "Step: 600  \tTraining accuracy: 0.6343098282814026\n",
      "Step: 600  \tValid loss: 0.5940039157867432\n",
      "Step: 700  \tTraining loss: 0.596228837966919\n",
      "Step: 700  \tTraining accuracy: 0.641232967376709\n",
      "Step: 700  \tValid loss: 0.5831160545349121\n",
      "Step: 800  \tTraining loss: 0.588765025138855\n",
      "Step: 800  \tTraining accuracy: 0.6471343636512756\n",
      "Step: 800  \tValid loss: 0.5759193897247314\n",
      "Step: 900  \tTraining loss: 0.5830683708190918\n",
      "Step: 900  \tTraining accuracy: 0.6520388722419739\n",
      "Step: 900  \tValid loss: 0.5702369213104248\n",
      "Step: 1000  \tTraining loss: 0.5787449479103088\n",
      "Step: 1000  \tTraining accuracy: 0.6561486721038818\n",
      "Step: 1000  \tValid loss: 0.5664424300193787\n",
      "Step: 1100  \tTraining loss: 0.5754089951515198\n",
      "Step: 1100  \tTraining accuracy: 0.6595323085784912\n",
      "Step: 1100  \tValid loss: 0.5637377500534058\n",
      "Step: 1200  \tTraining loss: 0.5725688338279724\n",
      "Step: 1200  \tTraining accuracy: 0.6623067855834961\n",
      "Step: 1200  \tValid loss: 0.5617989301681519\n",
      "Step: 1300  \tTraining loss: 0.5701363682746887\n",
      "Step: 1300  \tTraining accuracy: 0.6646848917007446\n",
      "Step: 1300  \tValid loss: 0.5601677894592285\n",
      "Step: 1400  \tTraining loss: 0.5679869651794434\n",
      "Step: 1400  \tTraining accuracy: 0.6668339967727661\n",
      "Step: 1400  \tValid loss: 0.5589589476585388\n",
      "Step: 1500  \tTraining loss: 0.5660427808761597\n",
      "Step: 1500  \tTraining accuracy: 0.6687440872192383\n",
      "Step: 1500  \tValid loss: 0.5575113892555237\n",
      "Step: 1600  \tTraining loss: 0.5643194913864136\n",
      "Step: 1600  \tTraining accuracy: 0.670637845993042\n",
      "Step: 1600  \tValid loss: 0.5564021468162537\n",
      "Step: 1700  \tTraining loss: 0.5625817775726318\n",
      "Step: 1700  \tTraining accuracy: 0.6722444295883179\n",
      "Step: 1700  \tValid loss: 0.5547836422920227\n",
      "Step: 1800  \tTraining loss: 0.5609636306762695\n",
      "Step: 1800  \tTraining accuracy: 0.6736538410186768\n",
      "Step: 1800  \tValid loss: 0.5537583231925964\n",
      "Step: 1900  \tTraining loss: 0.5593522191047668\n",
      "Step: 1900  \tTraining accuracy: 0.674885094165802\n",
      "Step: 1900  \tValid loss: 0.5525100231170654\n",
      "Step: 2000  \tTraining loss: 0.5577090382575989\n",
      "Step: 2000  \tTraining accuracy: 0.676020622253418\n",
      "Step: 2000  \tValid loss: 0.5514443516731262\n",
      "Step: 2100  \tTraining loss: 0.556079089641571\n",
      "Step: 2100  \tTraining accuracy: 0.6771497130393982\n",
      "Step: 2100  \tValid loss: 0.5504372715950012\n",
      "Step: 2200  \tTraining loss: 0.5545734763145447\n",
      "Step: 2200  \tTraining accuracy: 0.6782568097114563\n",
      "Step: 2200  \tValid loss: 0.5493715405464172\n",
      "Step: 2300  \tTraining loss: 0.5532450675964355\n",
      "Step: 2300  \tTraining accuracy: 0.6794292330741882\n",
      "Step: 2300  \tValid loss: 0.54852694272995\n",
      "Step: 2400  \tTraining loss: 0.5521021485328674\n",
      "Step: 2400  \tTraining accuracy: 0.6805576086044312\n",
      "Step: 2400  \tValid loss: 0.5477597713470459\n",
      "Step: 2500  \tTraining loss: 0.5511151552200317\n",
      "Step: 2500  \tTraining accuracy: 0.6816326379776001\n",
      "Step: 2500  \tValid loss: 0.5471614003181458\n",
      "Step: 2600  \tTraining loss: 0.5502619743347168\n",
      "Step: 2600  \tTraining accuracy: 0.6826886534690857\n",
      "Step: 2600  \tValid loss: 0.5466524362564087\n",
      "Step: 2700  \tTraining loss: 0.5495020151138306\n",
      "Step: 2700  \tTraining accuracy: 0.6837098598480225\n",
      "Step: 2700  \tValid loss: 0.5461653470993042\n",
      "Step: 2800  \tTraining loss: 0.5488206744194031\n",
      "Step: 2800  \tTraining accuracy: 0.6846481561660767\n",
      "Step: 2800  \tValid loss: 0.5456734895706177\n",
      "Step: 2900  \tTraining loss: 0.5481706261634827\n",
      "Step: 2900  \tTraining accuracy: 0.6855456233024597\n",
      "Step: 2900  \tValid loss: 0.5451886653900146\n",
      "Step: 3000  \tTraining loss: 0.5474487543106079\n",
      "Step: 3000  \tTraining accuracy: 0.686366081237793\n",
      "Step: 3000  \tValid loss: 0.5446982383728027\n",
      "Step: 3100  \tTraining loss: 0.546852707862854\n",
      "Step: 3100  \tTraining accuracy: 0.6870937943458557\n",
      "Step: 3100  \tValid loss: 0.5444706082344055\n",
      "Step: 3200  \tTraining loss: 0.5460105538368225\n",
      "Step: 3200  \tTraining accuracy: 0.6877678036689758\n",
      "Step: 3200  \tValid loss: 0.5442430377006531\n",
      "Step: 3300  \tTraining loss: 0.5454115271568298\n",
      "Step: 3300  \tTraining accuracy: 0.688477098941803\n",
      "Step: 3300  \tValid loss: 0.5435958504676819\n",
      "Step: 3400  \tTraining loss: 0.5448490977287292\n",
      "Step: 3400  \tTraining accuracy: 0.6891334056854248\n",
      "Step: 3400  \tValid loss: 0.5429356098175049\n",
      "Step: 3500  \tTraining loss: 0.5443037748336792\n",
      "Step: 3500  \tTraining accuracy: 0.689793050289154\n",
      "Step: 3500  \tValid loss: 0.5424165725708008\n",
      "Step: 3600  \tTraining loss: 0.5435781478881836\n",
      "Step: 3600  \tTraining accuracy: 0.6904489994049072\n",
      "Step: 3600  \tValid loss: 0.5417600870132446\n",
      "Step: 3700  \tTraining loss: 0.5428483486175537\n",
      "Step: 3700  \tTraining accuracy: 0.6911439299583435\n",
      "Step: 3700  \tValid loss: 0.5410654544830322\n",
      "Step: 3800  \tTraining loss: 0.5422311425209045\n",
      "Step: 3800  \tTraining accuracy: 0.6918081641197205\n",
      "Step: 3800  \tValid loss: 0.5405585169792175\n",
      "Step: 3900  \tTraining loss: 0.5416788458824158\n",
      "Step: 3900  \tTraining accuracy: 0.692425549030304\n",
      "Step: 3900  \tValid loss: 0.5401366949081421\n",
      "Step: 4000  \tTraining loss: 0.5411777496337891\n",
      "Step: 4000  \tTraining accuracy: 0.6929875612258911\n",
      "Step: 4000  \tValid loss: 0.5398240685462952\n",
      "Step: 4100  \tTraining loss: 0.5406681895256042\n",
      "Step: 4100  \tTraining accuracy: 0.693513035774231\n",
      "Step: 4100  \tValid loss: 0.5394942760467529\n",
      "Step: 4200  \tTraining loss: 0.5401408672332764\n",
      "Step: 4200  \tTraining accuracy: 0.6940016746520996\n",
      "Step: 4200  \tValid loss: 0.5393046140670776\n",
      "Step: 4300  \tTraining loss: 0.5395013093948364\n",
      "Step: 4300  \tTraining accuracy: 0.6944869756698608\n",
      "Step: 4300  \tValid loss: 0.5390757322311401\n",
      "Step: 4400  \tTraining loss: 0.5389208197593689\n",
      "Step: 4400  \tTraining accuracy: 0.694999098777771\n",
      "Step: 4400  \tValid loss: 0.5386111736297607\n",
      "Step: 4500  \tTraining loss: 0.5383661389350891\n",
      "Step: 4500  \tTraining accuracy: 0.695480227470398\n",
      "Step: 4500  \tValid loss: 0.5380021929740906\n",
      "Step: 4600  \tTraining loss: 0.5378514528274536\n",
      "Step: 4600  \tTraining accuracy: 0.695966362953186\n",
      "Step: 4600  \tValid loss: 0.5376401543617249\n",
      "Step: 4700  \tTraining loss: 0.537283182144165\n",
      "Step: 4700  \tTraining accuracy: 0.6964520215988159\n",
      "Step: 4700  \tValid loss: 0.5368531346321106\n",
      "Step: 4800  \tTraining loss: 0.5366350412368774\n",
      "Step: 4800  \tTraining accuracy: 0.6969372034072876\n",
      "Step: 4800  \tValid loss: 0.536307156085968\n",
      "Step: 4900  \tTraining loss: 0.5359744429588318\n",
      "Step: 4900  \tTraining accuracy: 0.6974220871925354\n",
      "Step: 4900  \tValid loss: 0.536108672618866\n",
      "Step: 5000  \tTraining loss: 0.5354183316230774\n",
      "Step: 5000  \tTraining accuracy: 0.6978728771209717\n",
      "Step: 5000  \tValid loss: 0.5357359647750854\n",
      "Step: 5100  \tTraining loss: 0.5349135398864746\n",
      "Step: 5100  \tTraining accuracy: 0.6982846856117249\n",
      "Step: 5100  \tValid loss: 0.5353577733039856\n",
      "Step: 5200  \tTraining loss: 0.5343679189682007\n",
      "Step: 5200  \tTraining accuracy: 0.6986689567565918\n",
      "Step: 5200  \tValid loss: 0.534896731376648\n",
      "Step: 5300  \tTraining loss: 0.5336750745773315\n",
      "Step: 5300  \tTraining accuracy: 0.6990589499473572\n",
      "Step: 5300  \tValid loss: 0.5344414710998535\n",
      "Step: 5400  \tTraining loss: 0.5331002473831177\n",
      "Step: 5400  \tTraining accuracy: 0.6994721293449402\n",
      "Step: 5400  \tValid loss: 0.5339326858520508\n",
      "Step: 5500  \tTraining loss: 0.5325199961662292\n",
      "Step: 5500  \tTraining accuracy: 0.6998592615127563\n",
      "Step: 5500  \tValid loss: 0.5337944626808167\n",
      "Step: 5600  \tTraining loss: 0.5316272974014282\n",
      "Step: 5600  \tTraining accuracy: 0.7002303004264832\n",
      "Step: 5600  \tValid loss: 0.5332253575325012\n",
      "Step: 5700  \tTraining loss: 0.530865490436554\n",
      "Step: 5700  \tTraining accuracy: 0.7005524635314941\n",
      "Step: 5700  \tValid loss: 0.5327733755111694\n",
      "Step: 5800  \tTraining loss: 0.5301467776298523\n",
      "Step: 5800  \tTraining accuracy: 0.7008509635925293\n",
      "Step: 5800  \tValid loss: 0.5320272445678711\n",
      "Step: 5900  \tTraining loss: 0.5294894576072693\n",
      "Step: 5900  \tTraining accuracy: 0.7011535167694092\n",
      "Step: 5900  \tValid loss: 0.5311591029167175\n",
      "Step: 6000  \tTraining loss: 0.5289534330368042\n",
      "Step: 6000  \tTraining accuracy: 0.7014538645744324\n",
      "Step: 6000  \tValid loss: 0.5305351614952087\n",
      "Step: 6100  \tTraining loss: 0.5284894108772278\n",
      "Step: 6100  \tTraining accuracy: 0.7017364501953125\n",
      "Step: 6100  \tValid loss: 0.5300551056861877\n",
      "Step: 6200  \tTraining loss: 0.528075635433197\n",
      "Step: 6200  \tTraining accuracy: 0.702019453048706\n",
      "Step: 6200  \tValid loss: 0.5296306610107422\n",
      "Step: 6300  \tTraining loss: 0.5276932716369629\n",
      "Step: 6300  \tTraining accuracy: 0.7022801637649536\n",
      "Step: 6300  \tValid loss: 0.529336154460907\n",
      "Step: 6400  \tTraining loss: 0.5273528099060059\n",
      "Step: 6400  \tTraining accuracy: 0.7025232315063477\n",
      "Step: 6400  \tValid loss: 0.5290998220443726\n",
      "Step: 6500  \tTraining loss: 0.5270249843597412\n",
      "Step: 6500  \tTraining accuracy: 0.702756941318512\n",
      "Step: 6500  \tValid loss: 0.5288774967193604\n",
      "Step: 6600  \tTraining loss: 0.5267292857170105\n",
      "Step: 6600  \tTraining accuracy: 0.7029726505279541\n",
      "Step: 6600  \tValid loss: 0.5287190675735474\n",
      "Step: 6700  \tTraining loss: 0.5264276266098022\n",
      "Step: 6700  \tTraining accuracy: 0.7031657695770264\n",
      "Step: 6700  \tValid loss: 0.5284741520881653\n",
      "Step: 6800  \tTraining loss: 0.5261427164077759\n",
      "Step: 6800  \tTraining accuracy: 0.7033584117889404\n",
      "Step: 6800  \tValid loss: 0.5281639099121094\n",
      "Step: 6900  \tTraining loss: 0.5258294343948364\n",
      "Step: 6900  \tTraining accuracy: 0.703559398651123\n",
      "Step: 6900  \tValid loss: 0.528005838394165\n",
      "Step: 7000  \tTraining loss: 0.5255299806594849\n",
      "Step: 7000  \tTraining accuracy: 0.7037819027900696\n",
      "Step: 7000  \tValid loss: 0.5278542637825012\n",
      "Step: 7100  \tTraining loss: 0.5252493023872375\n",
      "Step: 7100  \tTraining accuracy: 0.7039896845817566\n",
      "Step: 7100  \tValid loss: 0.5277670621871948\n",
      "Step: 7200  \tTraining loss: 0.5249928832054138\n",
      "Step: 7200  \tTraining accuracy: 0.704188346862793\n",
      "Step: 7200  \tValid loss: 0.5276833176612854\n",
      "Step: 7300  \tTraining loss: 0.5247333645820618\n",
      "Step: 7300  \tTraining accuracy: 0.7043814659118652\n",
      "Step: 7300  \tValid loss: 0.5275232195854187\n",
      "Step: 7400  \tTraining loss: 0.5244787335395813\n",
      "Step: 7400  \tTraining accuracy: 0.7045855522155762\n",
      "Step: 7400  \tValid loss: 0.5273773670196533\n",
      "Step: 7500  \tTraining loss: 0.5241764783859253\n",
      "Step: 7500  \tTraining accuracy: 0.7047761678695679\n",
      "Step: 7500  \tValid loss: 0.52711021900177\n",
      "Step: 7600  \tTraining loss: 0.523889422416687\n",
      "Step: 7600  \tTraining accuracy: 0.704950749874115\n",
      "Step: 7600  \tValid loss: 0.5269471406936646\n",
      "Step: 7700  \tTraining loss: 0.5235740542411804\n",
      "Step: 7700  \tTraining accuracy: 0.7051036357879639\n",
      "Step: 7700  \tValid loss: 0.5266764163970947\n",
      "Step: 7800  \tTraining loss: 0.5233227610588074\n",
      "Step: 7800  \tTraining accuracy: 0.7052448987960815\n",
      "Step: 7800  \tValid loss: 0.5264190435409546\n",
      "Step: 7900  \tTraining loss: 0.5230793952941895\n",
      "Step: 7900  \tTraining accuracy: 0.7053871154785156\n",
      "Step: 7900  \tValid loss: 0.5263298153877258\n",
      "Step: 8000  \tTraining loss: 0.5228471755981445\n",
      "Step: 8000  \tTraining accuracy: 0.7055497169494629\n",
      "Step: 8000  \tValid loss: 0.5261724591255188\n",
      "Step: 8100  \tTraining loss: 0.522605836391449\n",
      "Step: 8100  \tTraining accuracy: 0.7057155966758728\n",
      "Step: 8100  \tValid loss: 0.5260776877403259\n",
      "Step: 8200  \tTraining loss: 0.5223536491394043\n",
      "Step: 8200  \tTraining accuracy: 0.7058672308921814\n",
      "Step: 8200  \tValid loss: 0.525945246219635\n",
      "Step: 8300  \tTraining loss: 0.5220900774002075\n",
      "Step: 8300  \tTraining accuracy: 0.7060108780860901\n",
      "Step: 8300  \tValid loss: 0.525770902633667\n",
      "Step: 8400  \tTraining loss: 0.5217992663383484\n",
      "Step: 8400  \tTraining accuracy: 0.7061454057693481\n",
      "Step: 8400  \tValid loss: 0.52549809217453\n",
      "Step: 8500  \tTraining loss: 0.521545946598053\n",
      "Step: 8500  \tTraining accuracy: 0.7062767148017883\n",
      "Step: 8500  \tValid loss: 0.5252916812896729\n",
      "Step: 8600  \tTraining loss: 0.5212866067886353\n",
      "Step: 8600  \tTraining accuracy: 0.7063882350921631\n",
      "Step: 8600  \tValid loss: 0.5251352190971375\n",
      "Step: 8700  \tTraining loss: 0.5210463404655457\n",
      "Step: 8700  \tTraining accuracy: 0.7065137028694153\n",
      "Step: 8700  \tValid loss: 0.5250891447067261\n",
      "Step: 8800  \tTraining loss: 0.5207924246788025\n",
      "Step: 8800  \tTraining accuracy: 0.7066417336463928\n",
      "Step: 8800  \tValid loss: 0.5249544978141785\n",
      "Step: 8900  \tTraining loss: 0.5205471515655518\n",
      "Step: 8900  \tTraining accuracy: 0.7067709565162659\n",
      "Step: 8900  \tValid loss: 0.5248134732246399\n",
      "Step: 9000  \tTraining loss: 0.520319938659668\n",
      "Step: 9000  \tTraining accuracy: 0.706903874874115\n",
      "Step: 9000  \tValid loss: 0.5247490406036377\n",
      "Step: 9100  \tTraining loss: 0.5200833082199097\n",
      "Step: 9100  \tTraining accuracy: 0.7070430517196655\n",
      "Step: 9100  \tValid loss: 0.5246627330780029\n",
      "Step: 9200  \tTraining loss: 0.51986163854599\n",
      "Step: 9200  \tTraining accuracy: 0.7071921825408936\n",
      "Step: 9200  \tValid loss: 0.5246187448501587\n",
      "Step: 9300  \tTraining loss: 0.5196527242660522\n",
      "Step: 9300  \tTraining accuracy: 0.7073394060134888\n",
      "Step: 9300  \tValid loss: 0.5247328281402588\n",
      "Step: 9400  \tTraining loss: 0.519415020942688\n",
      "Step: 9400  \tTraining accuracy: 0.7074796557426453\n",
      "Step: 9400  \tValid loss: 0.5247888565063477\n",
      "Step: 9500  \tTraining loss: 0.5191962122917175\n",
      "Step: 9500  \tTraining accuracy: 0.70760178565979\n",
      "Step: 9500  \tValid loss: 0.5247863531112671\n",
      "Step: 9600  \tTraining loss: 0.5189717411994934\n",
      "Step: 9600  \tTraining accuracy: 0.7077276706695557\n",
      "Step: 9600  \tValid loss: 0.5246875882148743\n",
      "Step: 9700  \tTraining loss: 0.5187694430351257\n",
      "Step: 9700  \tTraining accuracy: 0.7078533172607422\n",
      "Step: 9700  \tValid loss: 0.5246569514274597\n",
      "Step: 9800  \tTraining loss: 0.5185746550559998\n",
      "Step: 9800  \tTraining accuracy: 0.7079715728759766\n",
      "Step: 9800  \tValid loss: 0.5246403813362122\n",
      "Step: 9900  \tTraining loss: 0.518382728099823\n",
      "Step: 9900  \tTraining accuracy: 0.7080898284912109\n",
      "Step: 9900  \tValid loss: 0.5246652364730835\n",
      "Step: 10000  \tTraining loss: 0.5181897282600403\n",
      "Step: 10000  \tTraining accuracy: 0.7082033157348633\n",
      "Step: 10000  \tValid loss: 0.5246716141700745\n",
      "Step: 10100  \tTraining loss: 0.518010139465332\n",
      "Step: 10100  \tTraining accuracy: 0.7083062529563904\n",
      "Step: 10100  \tValid loss: 0.5246947407722473\n",
      "Step: 10200  \tTraining loss: 0.5178351998329163\n",
      "Step: 10200  \tTraining accuracy: 0.7084013223648071\n",
      "Step: 10200  \tValid loss: 0.5247384309768677\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7084841\n",
      "Precision: 0.74671304\n",
      "Recall: 0.7946502\n",
      "F1 score: 0.73051834\n",
      "AUC: 0.712818\n",
      "   accuracy  precision   recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.708484   0.746713  0.79465  0.730518  0.712818  0.517711        0.7084   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.524576       0.708355   0.574472      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  10275.0  \n",
      "6\n",
      "(5655, 8)\n",
      "(5655, 1)\n",
      "(3120, 8)\n",
      "(3120, 1)\n",
      "(2535, 8)\n",
      "(2535, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4654049873352051\n",
      "Step: 100  \tTraining accuracy: 0.8279398679733276\n",
      "Step: 100  \tValid loss: 0.4642217457294464\n",
      "Step: 200  \tTraining loss: 0.41400906443595886\n",
      "Step: 200  \tTraining accuracy: 0.8357205986976624\n",
      "Step: 200  \tValid loss: 0.41245344281196594\n",
      "Step: 300  \tTraining loss: 0.40119385719299316\n",
      "Step: 300  \tTraining accuracy: 0.8396463394165039\n",
      "Step: 300  \tValid loss: 0.40010756254196167\n",
      "Step: 400  \tTraining loss: 0.3916477560997009\n",
      "Step: 400  \tTraining accuracy: 0.841581404209137\n",
      "Step: 400  \tValid loss: 0.39090415835380554\n",
      "Step: 500  \tTraining loss: 0.38311195373535156\n",
      "Step: 500  \tTraining accuracy: 0.8430690765380859\n",
      "Step: 500  \tValid loss: 0.38328516483306885\n",
      "Step: 600  \tTraining loss: 0.3740507960319519\n",
      "Step: 600  \tTraining accuracy: 0.844224750995636\n",
      "Step: 600  \tValid loss: 0.37561681866645813\n",
      "Step: 700  \tTraining loss: 0.36532798409461975\n",
      "Step: 700  \tTraining accuracy: 0.8453105092048645\n",
      "Step: 700  \tValid loss: 0.36850398778915405\n",
      "Step: 800  \tTraining loss: 0.35688915848731995\n",
      "Step: 800  \tTraining accuracy: 0.8465664386749268\n",
      "Step: 800  \tValid loss: 0.361910343170166\n",
      "Step: 900  \tTraining loss: 0.3492797017097473\n",
      "Step: 900  \tTraining accuracy: 0.8480470180511475\n",
      "Step: 900  \tValid loss: 0.3565385043621063\n",
      "Step: 1000  \tTraining loss: 0.3430017828941345\n",
      "Step: 1000  \tTraining accuracy: 0.8496161103248596\n",
      "Step: 1000  \tValid loss: 0.3526133596897125\n",
      "Step: 1100  \tTraining loss: 0.3375603258609772\n",
      "Step: 1100  \tTraining accuracy: 0.8512399196624756\n",
      "Step: 1100  \tValid loss: 0.3491520881652832\n",
      "Step: 1200  \tTraining loss: 0.33345940709114075\n",
      "Step: 1200  \tTraining accuracy: 0.8526736497879028\n",
      "Step: 1200  \tValid loss: 0.34730878472328186\n",
      "Step: 1300  \tTraining loss: 0.330454558134079\n",
      "Step: 1300  \tTraining accuracy: 0.8539062738418579\n",
      "Step: 1300  \tValid loss: 0.34600067138671875\n",
      "Step: 1400  \tTraining loss: 0.3279881477355957\n",
      "Step: 1400  \tTraining accuracy: 0.8551003932952881\n",
      "Step: 1400  \tValid loss: 0.34472352266311646\n",
      "Step: 1500  \tTraining loss: 0.32584148645401\n",
      "Step: 1500  \tTraining accuracy: 0.8562639355659485\n",
      "Step: 1500  \tValid loss: 0.343799889087677\n",
      "Step: 1600  \tTraining loss: 0.32384178042411804\n",
      "Step: 1600  \tTraining accuracy: 0.8573172688484192\n",
      "Step: 1600  \tValid loss: 0.34319379925727844\n",
      "Step: 1700  \tTraining loss: 0.3218938708305359\n",
      "Step: 1700  \tTraining accuracy: 0.8582535982131958\n",
      "Step: 1700  \tValid loss: 0.3427446484565735\n",
      "Step: 1800  \tTraining loss: 0.3200732171535492\n",
      "Step: 1800  \tTraining accuracy: 0.859012246131897\n",
      "Step: 1800  \tValid loss: 0.3424220681190491\n",
      "Step: 1900  \tTraining loss: 0.31835073232650757\n",
      "Step: 1900  \tTraining accuracy: 0.8596171736717224\n",
      "Step: 1900  \tValid loss: 0.3419235646724701\n",
      "Step: 2000  \tTraining loss: 0.31611141562461853\n",
      "Step: 2000  \tTraining accuracy: 0.8602008819580078\n",
      "Step: 2000  \tValid loss: 0.3410027027130127\n",
      "Step: 2100  \tTraining loss: 0.31435713171958923\n",
      "Step: 2100  \tTraining accuracy: 0.8607621192932129\n",
      "Step: 2100  \tValid loss: 0.34101226925849915\n",
      "Step: 2200  \tTraining loss: 0.313268780708313\n",
      "Step: 2200  \tTraining accuracy: 0.8612505793571472\n",
      "Step: 2200  \tValid loss: 0.341190904378891\n",
      "Step: 2300  \tTraining loss: 0.3124101459980011\n",
      "Step: 2300  \tTraining accuracy: 0.8617153167724609\n",
      "Step: 2300  \tValid loss: 0.34152713418006897\n",
      "Step: 2400  \tTraining loss: 0.3116762638092041\n",
      "Step: 2400  \tTraining accuracy: 0.8621743321418762\n",
      "Step: 2400  \tValid loss: 0.34187403321266174\n",
      "Step: 2500  \tTraining loss: 0.3110511600971222\n",
      "Step: 2500  \tTraining accuracy: 0.8626067042350769\n",
      "Step: 2500  \tValid loss: 0.3421177268028259\n",
      "Step: 2600  \tTraining loss: 0.31050413846969604\n",
      "Step: 2600  \tTraining accuracy: 0.8630120754241943\n",
      "Step: 2600  \tValid loss: 0.34223854541778564\n",
      "Step: 2700  \tTraining loss: 0.3100316524505615\n",
      "Step: 2700  \tTraining accuracy: 0.8634035587310791\n",
      "Step: 2700  \tValid loss: 0.3423342704772949\n",
      "Step: 2800  \tTraining loss: 0.30960142612457275\n",
      "Step: 2800  \tTraining accuracy: 0.8637955188751221\n",
      "Step: 2800  \tValid loss: 0.34245288372039795\n",
      "Step: 2900  \tTraining loss: 0.309198260307312\n",
      "Step: 2900  \tTraining accuracy: 0.8641940951347351\n",
      "Step: 2900  \tValid loss: 0.34243282675743103\n",
      "Step: 3000  \tTraining loss: 0.30880847573280334\n",
      "Step: 3000  \tTraining accuracy: 0.86457759141922\n",
      "Step: 3000  \tValid loss: 0.34234514832496643\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8649447\n",
      "Precision: 0.8521959\n",
      "Recall: 0.85544723\n",
      "F1 score: 0.8665915\n",
      "AUC: 0.874629\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.864945   0.852196  0.855447  0.866592  0.874629  0.308742       0.86473   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.340771         0.8647   0.306309      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3015.0  \n",
      "7\n",
      "(4930, 8)\n",
      "(4930, 1)\n",
      "(2720, 8)\n",
      "(2720, 1)\n",
      "(2210, 8)\n",
      "(2210, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.644347608089447\n",
      "Step: 100  \tTraining accuracy: 0.6168357133865356\n",
      "Step: 100  \tValid loss: 0.6274144053459167\n",
      "Step: 200  \tTraining loss: 0.5583990812301636\n",
      "Step: 200  \tTraining accuracy: 0.6548343300819397\n",
      "Step: 200  \tValid loss: 0.5538362860679626\n",
      "Step: 300  \tTraining loss: 0.5254440903663635\n",
      "Step: 300  \tTraining accuracy: 0.68405681848526\n",
      "Step: 300  \tValid loss: 0.530711829662323\n",
      "Step: 400  \tTraining loss: 0.5194811224937439\n",
      "Step: 400  \tTraining accuracy: 0.7000289559364319\n",
      "Step: 400  \tValid loss: 0.5278096199035645\n",
      "Step: 500  \tTraining loss: 0.5162373781204224\n",
      "Step: 500  \tTraining accuracy: 0.7094883918762207\n",
      "Step: 500  \tValid loss: 0.5249394774436951\n",
      "Step: 600  \tTraining loss: 0.5133307576179504\n",
      "Step: 600  \tTraining accuracy: 0.7158768177032471\n",
      "Step: 600  \tValid loss: 0.5218123197555542\n",
      "Step: 700  \tTraining loss: 0.5107613205909729\n",
      "Step: 700  \tTraining accuracy: 0.7203620076179504\n",
      "Step: 700  \tValid loss: 0.5190683603286743\n",
      "Step: 800  \tTraining loss: 0.508666455745697\n",
      "Step: 800  \tTraining accuracy: 0.7238404154777527\n",
      "Step: 800  \tValid loss: 0.5173144936561584\n",
      "Step: 900  \tTraining loss: 0.5068546533584595\n",
      "Step: 900  \tTraining accuracy: 0.7266674637794495\n",
      "Step: 900  \tValid loss: 0.5159304738044739\n",
      "Step: 1000  \tTraining loss: 0.5053386092185974\n",
      "Step: 1000  \tTraining accuracy: 0.7290807962417603\n",
      "Step: 1000  \tValid loss: 0.5145540237426758\n",
      "Step: 1100  \tTraining loss: 0.5041823983192444\n",
      "Step: 1100  \tTraining accuracy: 0.7309185862541199\n",
      "Step: 1100  \tValid loss: 0.5136887431144714\n",
      "Step: 1200  \tTraining loss: 0.5030392408370972\n",
      "Step: 1200  \tTraining accuracy: 0.7327365875244141\n",
      "Step: 1200  \tValid loss: 0.5129164457321167\n",
      "Step: 1300  \tTraining loss: 0.502001941204071\n",
      "Step: 1300  \tTraining accuracy: 0.7345151901245117\n",
      "Step: 1300  \tValid loss: 0.512008786201477\n",
      "Step: 1400  \tTraining loss: 0.5010082721710205\n",
      "Step: 1400  \tTraining accuracy: 0.7362557053565979\n",
      "Step: 1400  \tValid loss: 0.5113107562065125\n",
      "Step: 1500  \tTraining loss: 0.49973031878471375\n",
      "Step: 1500  \tTraining accuracy: 0.7377631664276123\n",
      "Step: 1500  \tValid loss: 0.5109608173370361\n",
      "Step: 1600  \tTraining loss: 0.49882104992866516\n",
      "Step: 1600  \tTraining accuracy: 0.7389518022537231\n",
      "Step: 1600  \tValid loss: 0.510607123374939\n",
      "Step: 1700  \tTraining loss: 0.49807989597320557\n",
      "Step: 1700  \tTraining accuracy: 0.7399901747703552\n",
      "Step: 1700  \tValid loss: 0.5100914239883423\n",
      "Step: 1800  \tTraining loss: 0.49741944670677185\n",
      "Step: 1800  \tTraining accuracy: 0.7410084009170532\n",
      "Step: 1800  \tValid loss: 0.5097129940986633\n",
      "Step: 1900  \tTraining loss: 0.49674466252326965\n",
      "Step: 1900  \tTraining accuracy: 0.7419439554214478\n",
      "Step: 1900  \tValid loss: 0.5094559788703918\n",
      "Step: 2000  \tTraining loss: 0.49599456787109375\n",
      "Step: 2000  \tTraining accuracy: 0.7428044080734253\n",
      "Step: 2000  \tValid loss: 0.509270429611206\n",
      "Step: 2100  \tTraining loss: 0.495378315448761\n",
      "Step: 2100  \tTraining accuracy: 0.7436847686767578\n",
      "Step: 2100  \tValid loss: 0.5091701745986938\n",
      "Step: 2200  \tTraining loss: 0.4947853088378906\n",
      "Step: 2200  \tTraining accuracy: 0.7445021271705627\n",
      "Step: 2200  \tValid loss: 0.5089705586433411\n",
      "Step: 2300  \tTraining loss: 0.4941653907299042\n",
      "Step: 2300  \tTraining accuracy: 0.7452197670936584\n",
      "Step: 2300  \tValid loss: 0.5088870525360107\n",
      "Step: 2400  \tTraining loss: 0.4936167001724243\n",
      "Step: 2400  \tTraining accuracy: 0.7458892464637756\n",
      "Step: 2400  \tValid loss: 0.5087924003601074\n",
      "Step: 2500  \tTraining loss: 0.49308422207832336\n",
      "Step: 2500  \tTraining accuracy: 0.7465124130249023\n",
      "Step: 2500  \tValid loss: 0.5087184906005859\n",
      "Step: 2600  \tTraining loss: 0.49253469705581665\n",
      "Step: 2600  \tTraining accuracy: 0.7470866441726685\n",
      "Step: 2600  \tValid loss: 0.508638858795166\n",
      "Step: 2700  \tTraining loss: 0.4920240342617035\n",
      "Step: 2700  \tTraining accuracy: 0.7476099133491516\n",
      "Step: 2700  \tValid loss: 0.5086197853088379\n",
      "Step: 2800  \tTraining loss: 0.4915233254432678\n",
      "Step: 2800  \tTraining accuracy: 0.7480951547622681\n",
      "Step: 2800  \tValid loss: 0.508496880531311\n",
      "Step: 2900  \tTraining loss: 0.49100008606910706\n",
      "Step: 2900  \tTraining accuracy: 0.7485498785972595\n",
      "Step: 2900  \tValid loss: 0.5084956884384155\n",
      "Step: 3000  \tTraining loss: 0.4904099702835083\n",
      "Step: 3000  \tTraining accuracy: 0.7489668726921082\n",
      "Step: 3000  \tValid loss: 0.5083236694335938\n",
      "Step: 3100  \tTraining loss: 0.4898415803909302\n",
      "Step: 3100  \tTraining accuracy: 0.7493997812271118\n",
      "Step: 3100  \tValid loss: 0.5081908106803894\n",
      "Step: 3200  \tTraining loss: 0.48929139971733093\n",
      "Step: 3200  \tTraining accuracy: 0.7498663663864136\n",
      "Step: 3200  \tValid loss: 0.5080447196960449\n",
      "Step: 3300  \tTraining loss: 0.488701730966568\n",
      "Step: 3300  \tTraining accuracy: 0.7503260970115662\n",
      "Step: 3300  \tValid loss: 0.5079936981201172\n",
      "Step: 3400  \tTraining loss: 0.4880494773387909\n",
      "Step: 3400  \tTraining accuracy: 0.7507311105728149\n",
      "Step: 3400  \tValid loss: 0.5079407691955566\n",
      "Step: 3500  \tTraining loss: 0.4874646067619324\n",
      "Step: 3500  \tTraining accuracy: 0.7511361837387085\n",
      "Step: 3500  \tValid loss: 0.5081446170806885\n",
      "Step: 3600  \tTraining loss: 0.4868599772453308\n",
      "Step: 3600  \tTraining accuracy: 0.7515384554862976\n",
      "Step: 3600  \tValid loss: 0.5082467198371887\n",
      "Step: 3700  \tTraining loss: 0.4863138496875763\n",
      "Step: 3700  \tTraining accuracy: 0.7518991827964783\n",
      "Step: 3700  \tValid loss: 0.5083226561546326\n",
      "Step: 3800  \tTraining loss: 0.48566919565200806\n",
      "Step: 3800  \tTraining accuracy: 0.7522109746932983\n",
      "Step: 3800  \tValid loss: 0.5080273151397705\n",
      "Step: 3900  \tTraining loss: 0.48509785532951355\n",
      "Step: 3900  \tTraining accuracy: 0.7524906992912292\n",
      "Step: 3900  \tValid loss: 0.5079911351203918\n",
      "Step: 4000  \tTraining loss: 0.48456689715385437\n",
      "Step: 4000  \tTraining accuracy: 0.7527177929878235\n",
      "Step: 4000  \tValid loss: 0.5081620812416077\n",
      "Step: 4100  \tTraining loss: 0.4840969443321228\n",
      "Step: 4100  \tTraining accuracy: 0.7529011368751526\n",
      "Step: 4100  \tValid loss: 0.508345365524292\n",
      "Step: 4200  \tTraining loss: 0.4836380183696747\n",
      "Step: 4200  \tTraining accuracy: 0.7530413866043091\n",
      "Step: 4200  \tValid loss: 0.508216142654419\n",
      "Step: 4300  \tTraining loss: 0.4832249581813812\n",
      "Step: 4300  \tTraining accuracy: 0.7531869411468506\n",
      "Step: 4300  \tValid loss: 0.5084238052368164\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.75334454\n",
      "Precision: 0.717302\n",
      "Recall: 0.6474325\n",
      "F1 score: 0.714776\n",
      "AUC: 0.744466\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.753345   0.717302  0.647433  0.714776  0.744466  0.482874      0.753184   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.507855       0.753133   0.526849      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  4375.0  \n",
      "8\n",
      "(13340, 8)\n",
      "(13340, 1)\n",
      "(7360, 8)\n",
      "(7360, 1)\n",
      "(5980, 8)\n",
      "(5980, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5926355719566345\n",
      "Step: 100  \tTraining accuracy: 0.730434775352478\n",
      "Step: 100  \tValid loss: 0.5894840955734253\n",
      "Step: 200  \tTraining loss: 0.44504985213279724\n",
      "Step: 200  \tTraining accuracy: 0.7715892195701599\n",
      "Step: 200  \tValid loss: 0.43358850479125977\n",
      "Step: 300  \tTraining loss: 0.37034034729003906\n",
      "Step: 300  \tTraining accuracy: 0.8068965673446655\n",
      "Step: 300  \tValid loss: 0.353275865316391\n",
      "Step: 400  \tTraining loss: 0.3399108052253723\n",
      "Step: 400  \tTraining accuracy: 0.8238487839698792\n",
      "Step: 400  \tValid loss: 0.31966838240623474\n",
      "Step: 500  \tTraining loss: 0.3271574378013611\n",
      "Step: 500  \tTraining accuracy: 0.8333083391189575\n",
      "Step: 500  \tValid loss: 0.3047550916671753\n",
      "Step: 600  \tTraining loss: 0.32125067710876465\n",
      "Step: 600  \tTraining accuracy: 0.8393349051475525\n",
      "Step: 600  \tValid loss: 0.29763028025627136\n",
      "Step: 700  \tTraining loss: 0.31822484731674194\n",
      "Step: 700  \tTraining accuracy: 0.8436166644096375\n",
      "Step: 700  \tValid loss: 0.2938302755355835\n",
      "Step: 800  \tTraining loss: 0.31623750925064087\n",
      "Step: 800  \tTraining accuracy: 0.8468515872955322\n",
      "Step: 800  \tValid loss: 0.2912546694278717\n",
      "Step: 900  \tTraining loss: 0.31476786732673645\n",
      "Step: 900  \tTraining accuracy: 0.849378228187561\n",
      "Step: 900  \tValid loss: 0.2894173562526703\n",
      "Step: 1000  \tTraining loss: 0.31355738639831543\n",
      "Step: 1000  \tTraining accuracy: 0.8514637351036072\n",
      "Step: 1000  \tValid loss: 0.28798410296440125\n",
      "Step: 1100  \tTraining loss: 0.3124632239341736\n",
      "Step: 1100  \tTraining accuracy: 0.8532341122627258\n",
      "Step: 1100  \tValid loss: 0.2868559658527374\n",
      "Step: 1200  \tTraining loss: 0.31143274903297424\n",
      "Step: 1200  \tTraining accuracy: 0.8547943234443665\n",
      "Step: 1200  \tValid loss: 0.2858608067035675\n",
      "Step: 1300  \tTraining loss: 0.3104192912578583\n",
      "Step: 1300  \tTraining accuracy: 0.8561829328536987\n",
      "Step: 1300  \tValid loss: 0.28498369455337524\n",
      "Step: 1400  \tTraining loss: 0.3094302713871002\n",
      "Step: 1400  \tTraining accuracy: 0.8573518991470337\n",
      "Step: 1400  \tValid loss: 0.28419071435928345\n",
      "Step: 1500  \tTraining loss: 0.3085358440876007\n",
      "Step: 1500  \tTraining accuracy: 0.8583544492721558\n",
      "Step: 1500  \tValid loss: 0.2835312485694885\n",
      "Step: 1600  \tTraining loss: 0.30775782465934753\n",
      "Step: 1600  \tTraining accuracy: 0.8592566847801208\n",
      "Step: 1600  \tValid loss: 0.2829650938510895\n",
      "Step: 1700  \tTraining loss: 0.30708637833595276\n",
      "Step: 1700  \tTraining accuracy: 0.8600495457649231\n",
      "Step: 1700  \tValid loss: 0.282470703125\n",
      "Step: 1800  \tTraining loss: 0.30646586418151855\n",
      "Step: 1800  \tTraining accuracy: 0.8607324957847595\n",
      "Step: 1800  \tValid loss: 0.2819650173187256\n",
      "Step: 1900  \tTraining loss: 0.30590206384658813\n",
      "Step: 1900  \tTraining accuracy: 0.8613497018814087\n",
      "Step: 1900  \tValid loss: 0.28150200843811035\n",
      "Step: 2000  \tTraining loss: 0.30536898970603943\n",
      "Step: 2000  \tTraining accuracy: 0.8618959784507751\n",
      "Step: 2000  \tValid loss: 0.2810633182525635\n",
      "Step: 2100  \tTraining loss: 0.30485260486602783\n",
      "Step: 2100  \tTraining accuracy: 0.8623926043510437\n",
      "Step: 2100  \tValid loss: 0.28060248494148254\n",
      "Step: 2200  \tTraining loss: 0.3043364882469177\n",
      "Step: 2200  \tTraining accuracy: 0.8628412485122681\n",
      "Step: 2200  \tValid loss: 0.28013893961906433\n",
      "Step: 2300  \tTraining loss: 0.3038206994533539\n",
      "Step: 2300  \tTraining accuracy: 0.8632250428199768\n",
      "Step: 2300  \tValid loss: 0.279677152633667\n",
      "Step: 2400  \tTraining loss: 0.3033001720905304\n",
      "Step: 2400  \tTraining accuracy: 0.8635522723197937\n",
      "Step: 2400  \tValid loss: 0.2792116105556488\n",
      "Step: 2500  \tTraining loss: 0.30277785658836365\n",
      "Step: 2500  \tTraining accuracy: 0.8638619184494019\n",
      "Step: 2500  \tValid loss: 0.278743177652359\n",
      "Step: 2600  \tTraining loss: 0.30225634574890137\n",
      "Step: 2600  \tTraining accuracy: 0.8641341328620911\n",
      "Step: 2600  \tValid loss: 0.2782810926437378\n",
      "Step: 2700  \tTraining loss: 0.301736980676651\n",
      "Step: 2700  \tTraining accuracy: 0.864364504814148\n",
      "Step: 2700  \tValid loss: 0.27782630920410156\n",
      "Step: 2800  \tTraining loss: 0.3012278974056244\n",
      "Step: 2800  \tTraining accuracy: 0.8645836114883423\n",
      "Step: 2800  \tValid loss: 0.2773575186729431\n",
      "Step: 2900  \tTraining loss: 0.30073943734169006\n",
      "Step: 2900  \tTraining accuracy: 0.8647978901863098\n",
      "Step: 2900  \tValid loss: 0.27691495418548584\n",
      "Step: 3000  \tTraining loss: 0.30027079582214355\n",
      "Step: 3000  \tTraining accuracy: 0.8650178909301758\n",
      "Step: 3000  \tValid loss: 0.27649274468421936\n",
      "Step: 3100  \tTraining loss: 0.2998279631137848\n",
      "Step: 3100  \tTraining accuracy: 0.8652419447898865\n",
      "Step: 3100  \tValid loss: 0.2761261761188507\n",
      "Step: 3200  \tTraining loss: 0.2994122803211212\n",
      "Step: 3200  \tTraining accuracy: 0.865470826625824\n",
      "Step: 3200  \tValid loss: 0.27578628063201904\n",
      "Step: 3300  \tTraining loss: 0.2990220785140991\n",
      "Step: 3300  \tTraining accuracy: 0.8656890988349915\n",
      "Step: 3300  \tValid loss: 0.2754700481891632\n",
      "Step: 3400  \tTraining loss: 0.29865723848342896\n",
      "Step: 3400  \tTraining accuracy: 0.8658887147903442\n",
      "Step: 3400  \tValid loss: 0.2751842439174652\n",
      "Step: 3500  \tTraining loss: 0.2983163893222809\n",
      "Step: 3500  \tTraining accuracy: 0.8660767674446106\n",
      "Step: 3500  \tValid loss: 0.2749190032482147\n",
      "Step: 3600  \tTraining loss: 0.2979974150657654\n",
      "Step: 3600  \tTraining accuracy: 0.8662562966346741\n",
      "Step: 3600  \tValid loss: 0.27467796206474304\n",
      "Step: 3700  \tTraining loss: 0.2976871132850647\n",
      "Step: 3700  \tTraining accuracy: 0.8664209246635437\n",
      "Step: 3700  \tValid loss: 0.27443066239356995\n",
      "Step: 3800  \tTraining loss: 0.2973997890949249\n",
      "Step: 3800  \tTraining accuracy: 0.8665717244148254\n",
      "Step: 3800  \tValid loss: 0.2742408812046051\n",
      "Step: 3900  \tTraining loss: 0.29712599515914917\n",
      "Step: 3900  \tTraining accuracy: 0.866715669631958\n",
      "Step: 3900  \tValid loss: 0.2740587890148163\n",
      "Step: 4000  \tTraining loss: 0.2968669533729553\n",
      "Step: 4000  \tTraining accuracy: 0.8668627738952637\n",
      "Step: 4000  \tValid loss: 0.27389341592788696\n",
      "Step: 4100  \tTraining loss: 0.29662346839904785\n",
      "Step: 4100  \tTraining accuracy: 0.8670072555541992\n",
      "Step: 4100  \tValid loss: 0.2737460136413574\n",
      "Step: 4200  \tTraining loss: 0.29639366269111633\n",
      "Step: 4200  \tTraining accuracy: 0.8671402335166931\n",
      "Step: 4200  \tValid loss: 0.27361106872558594\n",
      "Step: 4300  \tTraining loss: 0.2961752414703369\n",
      "Step: 4300  \tTraining accuracy: 0.8672687411308289\n",
      "Step: 4300  \tValid loss: 0.2734799385070801\n",
      "Step: 4400  \tTraining loss: 0.29596394300460815\n",
      "Step: 4400  \tTraining accuracy: 0.8674007654190063\n",
      "Step: 4400  \tValid loss: 0.2733554244041443\n",
      "Step: 4500  \tTraining loss: 0.29575613141059875\n",
      "Step: 4500  \tTraining accuracy: 0.8675201535224915\n",
      "Step: 4500  \tValid loss: 0.27323758602142334\n",
      "Step: 4600  \tTraining loss: 0.29554101824760437\n",
      "Step: 4600  \tTraining accuracy: 0.8676310181617737\n",
      "Step: 4600  \tValid loss: 0.27310433983802795\n",
      "Step: 4700  \tTraining loss: 0.29529088735580444\n",
      "Step: 4700  \tTraining accuracy: 0.8677330613136292\n",
      "Step: 4700  \tValid loss: 0.2729245722293854\n",
      "Step: 4800  \tTraining loss: 0.2950098216533661\n",
      "Step: 4800  \tTraining accuracy: 0.8678331971168518\n",
      "Step: 4800  \tValid loss: 0.2727183699607849\n",
      "Step: 4900  \tTraining loss: 0.29475536942481995\n",
      "Step: 4900  \tTraining accuracy: 0.867926836013794\n",
      "Step: 4900  \tValid loss: 0.27257663011550903\n",
      "Step: 5000  \tTraining loss: 0.29451945424079895\n",
      "Step: 5000  \tTraining accuracy: 0.8680213093757629\n",
      "Step: 5000  \tValid loss: 0.2724634110927582\n",
      "Step: 5100  \tTraining loss: 0.29429617524147034\n",
      "Step: 5100  \tTraining accuracy: 0.8681246042251587\n",
      "Step: 5100  \tValid loss: 0.27236899733543396\n",
      "Step: 5200  \tTraining loss: 0.29394766688346863\n",
      "Step: 5200  \tTraining accuracy: 0.8682304620742798\n",
      "Step: 5200  \tValid loss: 0.2721642255783081\n",
      "Step: 5300  \tTraining loss: 0.2934703826904297\n",
      "Step: 5300  \tTraining accuracy: 0.8683451414108276\n",
      "Step: 5300  \tValid loss: 0.27178242802619934\n",
      "Step: 5400  \tTraining loss: 0.2929818034172058\n",
      "Step: 5400  \tTraining accuracy: 0.868467390537262\n",
      "Step: 5400  \tValid loss: 0.27133625745773315\n",
      "Step: 5500  \tTraining loss: 0.2924220860004425\n",
      "Step: 5500  \tTraining accuracy: 0.8685852289199829\n",
      "Step: 5500  \tValid loss: 0.27078840136528015\n",
      "Step: 5600  \tTraining loss: 0.2920084297657013\n",
      "Step: 5600  \tTraining accuracy: 0.8687014579772949\n",
      "Step: 5600  \tValid loss: 0.27041521668434143\n",
      "Step: 5700  \tTraining loss: 0.29166415333747864\n",
      "Step: 5700  \tTraining accuracy: 0.8688142895698547\n",
      "Step: 5700  \tValid loss: 0.2701621949672699\n",
      "Step: 5800  \tTraining loss: 0.2913648784160614\n",
      "Step: 5800  \tTraining accuracy: 0.8689231276512146\n",
      "Step: 5800  \tValid loss: 0.2699827253818512\n",
      "Step: 5900  \tTraining loss: 0.2911047637462616\n",
      "Step: 5900  \tTraining accuracy: 0.8690257668495178\n",
      "Step: 5900  \tValid loss: 0.26982223987579346\n",
      "Step: 6000  \tTraining loss: 0.29087430238723755\n",
      "Step: 6000  \tTraining accuracy: 0.8691217303276062\n",
      "Step: 6000  \tValid loss: 0.2696940004825592\n",
      "Step: 6100  \tTraining loss: 0.2906688451766968\n",
      "Step: 6100  \tTraining accuracy: 0.8692182898521423\n",
      "Step: 6100  \tValid loss: 0.2695766091346741\n",
      "Step: 6200  \tTraining loss: 0.29048240184783936\n",
      "Step: 6200  \tTraining accuracy: 0.869306206703186\n",
      "Step: 6200  \tValid loss: 0.26949363946914673\n",
      "Step: 6300  \tTraining loss: 0.290311723947525\n",
      "Step: 6300  \tTraining accuracy: 0.8693907260894775\n",
      "Step: 6300  \tValid loss: 0.2694181799888611\n",
      "Step: 6400  \tTraining loss: 0.290149062871933\n",
      "Step: 6400  \tTraining accuracy: 0.8694760799407959\n",
      "Step: 6400  \tValid loss: 0.2693585455417633\n",
      "Step: 6500  \tTraining loss: 0.28999367356300354\n",
      "Step: 6500  \tTraining accuracy: 0.8695605397224426\n",
      "Step: 6500  \tValid loss: 0.2693108022212982\n",
      "Step: 6600  \tTraining loss: 0.2898435592651367\n",
      "Step: 6600  \tTraining accuracy: 0.8696482181549072\n",
      "Step: 6600  \tValid loss: 0.2692525088787079\n",
      "Step: 6700  \tTraining loss: 0.2896982431411743\n",
      "Step: 6700  \tTraining accuracy: 0.869742214679718\n",
      "Step: 6700  \tValid loss: 0.26917505264282227\n",
      "Step: 6800  \tTraining loss: 0.28954577445983887\n",
      "Step: 6800  \tTraining accuracy: 0.869846761226654\n",
      "Step: 6800  \tValid loss: 0.26910170912742615\n",
      "Step: 6900  \tTraining loss: 0.2894074022769928\n",
      "Step: 6900  \tTraining accuracy: 0.8699488043785095\n",
      "Step: 6900  \tValid loss: 0.2690685987472534\n",
      "Step: 7000  \tTraining loss: 0.289277583360672\n",
      "Step: 7000  \tTraining accuracy: 0.8700354695320129\n",
      "Step: 7000  \tValid loss: 0.2690322697162628\n",
      "Step: 7100  \tTraining loss: 0.289151668548584\n",
      "Step: 7100  \tTraining accuracy: 0.8701112270355225\n",
      "Step: 7100  \tValid loss: 0.26897624135017395\n",
      "Step: 7200  \tTraining loss: 0.2890244126319885\n",
      "Step: 7200  \tTraining accuracy: 0.8701843023300171\n",
      "Step: 7200  \tValid loss: 0.2689691483974457\n",
      "Step: 7300  \tTraining loss: 0.288904070854187\n",
      "Step: 7300  \tTraining accuracy: 0.870252788066864\n",
      "Step: 7300  \tValid loss: 0.26892876625061035\n",
      "Step: 7400  \tTraining loss: 0.28878551721572876\n",
      "Step: 7400  \tTraining accuracy: 0.8703199625015259\n",
      "Step: 7400  \tValid loss: 0.26888105273246765\n",
      "Step: 7500  \tTraining loss: 0.2886669933795929\n",
      "Step: 7500  \tTraining accuracy: 0.8703852891921997\n",
      "Step: 7500  \tValid loss: 0.2688724100589752\n",
      "Step: 7600  \tTraining loss: 0.2885517179965973\n",
      "Step: 7600  \tTraining accuracy: 0.8704493641853333\n",
      "Step: 7600  \tValid loss: 0.26883837580680847\n",
      "Step: 7700  \tTraining loss: 0.2884395718574524\n",
      "Step: 7700  \tTraining accuracy: 0.8705112934112549\n",
      "Step: 7700  \tValid loss: 0.2687917649745941\n",
      "Step: 7800  \tTraining loss: 0.28832805156707764\n",
      "Step: 7800  \tTraining accuracy: 0.8705716729164124\n",
      "Step: 7800  \tValid loss: 0.2687520682811737\n",
      "Step: 7900  \tTraining loss: 0.28821057081222534\n",
      "Step: 7900  \tTraining accuracy: 0.8706309199333191\n",
      "Step: 7900  \tValid loss: 0.2686575651168823\n",
      "Step: 8000  \tTraining loss: 0.2880992591381073\n",
      "Step: 8000  \tTraining accuracy: 0.8706892132759094\n",
      "Step: 8000  \tValid loss: 0.2686169743537903\n",
      "Step: 8100  \tTraining loss: 0.28798991441726685\n",
      "Step: 8100  \tTraining accuracy: 0.8707460165023804\n",
      "Step: 8100  \tValid loss: 0.2685815691947937\n",
      "Step: 8200  \tTraining loss: 0.28788208961486816\n",
      "Step: 8200  \tTraining accuracy: 0.8708027601242065\n",
      "Step: 8200  \tValid loss: 0.26850801706314087\n",
      "Step: 8300  \tTraining loss: 0.2877727746963501\n",
      "Step: 8300  \tTraining accuracy: 0.8708590865135193\n",
      "Step: 8300  \tValid loss: 0.2684803307056427\n",
      "Step: 8400  \tTraining loss: 0.28766530752182007\n",
      "Step: 8400  \tTraining accuracy: 0.870914101600647\n",
      "Step: 8400  \tValid loss: 0.26843389868736267\n",
      "Step: 8500  \tTraining loss: 0.28755995631217957\n",
      "Step: 8500  \tTraining accuracy: 0.8709655404090881\n",
      "Step: 8500  \tValid loss: 0.2683948576450348\n",
      "Step: 8600  \tTraining loss: 0.2874559164047241\n",
      "Step: 8600  \tTraining accuracy: 0.871016263961792\n",
      "Step: 8600  \tValid loss: 0.26833656430244446\n",
      "Step: 8700  \tTraining loss: 0.28735339641571045\n",
      "Step: 8700  \tTraining accuracy: 0.8710640072822571\n",
      "Step: 8700  \tValid loss: 0.26828521490097046\n",
      "Step: 8800  \tTraining loss: 0.28725340962409973\n",
      "Step: 8800  \tTraining accuracy: 0.8711099028587341\n",
      "Step: 8800  \tValid loss: 0.2682327330112457\n",
      "Step: 8900  \tTraining loss: 0.28715232014656067\n",
      "Step: 8900  \tTraining accuracy: 0.8711538314819336\n",
      "Step: 8900  \tValid loss: 0.2681865692138672\n",
      "Step: 9000  \tTraining loss: 0.2870529294013977\n",
      "Step: 9000  \tTraining accuracy: 0.8712013959884644\n",
      "Step: 9000  \tValid loss: 0.26813510060310364\n",
      "Step: 9100  \tTraining loss: 0.28695619106292725\n",
      "Step: 9100  \tTraining accuracy: 0.8712500333786011\n",
      "Step: 9100  \tValid loss: 0.2680901288986206\n",
      "Step: 9200  \tTraining loss: 0.28685957193374634\n",
      "Step: 9200  \tTraining accuracy: 0.8712979555130005\n",
      "Step: 9200  \tValid loss: 0.26803529262542725\n",
      "Step: 9300  \tTraining loss: 0.28676310181617737\n",
      "Step: 9300  \tTraining accuracy: 0.8713428378105164\n",
      "Step: 9300  \tValid loss: 0.2679838538169861\n",
      "Step: 9400  \tTraining loss: 0.2866702973842621\n",
      "Step: 9400  \tTraining accuracy: 0.871383547782898\n",
      "Step: 9400  \tValid loss: 0.2679345905780792\n",
      "Step: 9500  \tTraining loss: 0.28657400608062744\n",
      "Step: 9500  \tTraining accuracy: 0.8714230060577393\n",
      "Step: 9500  \tValid loss: 0.26788848638534546\n",
      "Step: 9600  \tTraining loss: 0.2864815592765808\n",
      "Step: 9600  \tTraining accuracy: 0.8714635968208313\n",
      "Step: 9600  \tValid loss: 0.26783785223960876\n",
      "Step: 9700  \tTraining loss: 0.2863909602165222\n",
      "Step: 9700  \tTraining accuracy: 0.8715029954910278\n",
      "Step: 9700  \tValid loss: 0.2677821218967438\n",
      "Step: 9800  \tTraining loss: 0.28630027174949646\n",
      "Step: 9800  \tTraining accuracy: 0.8715426921844482\n",
      "Step: 9800  \tValid loss: 0.2677302956581116\n",
      "Step: 9900  \tTraining loss: 0.2862115800380707\n",
      "Step: 9900  \tTraining accuracy: 0.8715816140174866\n",
      "Step: 9900  \tValid loss: 0.267673522233963\n",
      "Step: 10000  \tTraining loss: 0.2861279845237732\n",
      "Step: 10000  \tTraining accuracy: 0.8716204762458801\n",
      "Step: 10000  \tValid loss: 0.2676013112068176\n",
      "Step: 10100  \tTraining loss: 0.28603675961494446\n",
      "Step: 10100  \tTraining accuracy: 0.8716596961021423\n",
      "Step: 10100  \tValid loss: 0.26756754517555237\n",
      "Step: 10200  \tTraining loss: 0.28594788908958435\n",
      "Step: 10200  \tTraining accuracy: 0.8717014789581299\n",
      "Step: 10200  \tValid loss: 0.26750603318214417\n",
      "Step: 10300  \tTraining loss: 0.2858579754829407\n",
      "Step: 10300  \tTraining accuracy: 0.8717453479766846\n",
      "Step: 10300  \tValid loss: 0.2674087584018707\n",
      "Step: 10400  \tTraining loss: 0.2857707142829895\n",
      "Step: 10400  \tTraining accuracy: 0.8717913031578064\n",
      "Step: 10400  \tValid loss: 0.2673502266407013\n",
      "Step: 10500  \tTraining loss: 0.2856844961643219\n",
      "Step: 10500  \tTraining accuracy: 0.8718363046646118\n",
      "Step: 10500  \tValid loss: 0.2672940194606781\n",
      "Step: 10600  \tTraining loss: 0.2856021225452423\n",
      "Step: 10600  \tTraining accuracy: 0.8718801736831665\n",
      "Step: 10600  \tValid loss: 0.2672369182109833\n",
      "Step: 10700  \tTraining loss: 0.28551939129829407\n",
      "Step: 10700  \tTraining accuracy: 0.871923565864563\n",
      "Step: 10700  \tValid loss: 0.26717472076416016\n",
      "Step: 10800  \tTraining loss: 0.28543075919151306\n",
      "Step: 10800  \tTraining accuracy: 0.8719667792320251\n",
      "Step: 10800  \tValid loss: 0.2670231759548187\n",
      "Step: 10900  \tTraining loss: 0.2853185534477234\n",
      "Step: 10900  \tTraining accuracy: 0.87200927734375\n",
      "Step: 10900  \tValid loss: 0.26689696311950684\n",
      "Step: 11000  \tTraining loss: 0.2852262258529663\n",
      "Step: 11000  \tTraining accuracy: 0.8720458149909973\n",
      "Step: 11000  \tValid loss: 0.2667935788631439\n",
      "Step: 11100  \tTraining loss: 0.28514617681503296\n",
      "Step: 11100  \tTraining accuracy: 0.8720813989639282\n",
      "Step: 11100  \tValid loss: 0.2667619585990906\n",
      "Step: 11200  \tTraining loss: 0.28506436944007874\n",
      "Step: 11200  \tTraining accuracy: 0.8721153140068054\n",
      "Step: 11200  \tValid loss: 0.2667282819747925\n",
      "Step: 11300  \tTraining loss: 0.2849869728088379\n",
      "Step: 11300  \tTraining accuracy: 0.8721479177474976\n",
      "Step: 11300  \tValid loss: 0.2666758596897125\n",
      "Step: 11400  \tTraining loss: 0.2849118113517761\n",
      "Step: 11400  \tTraining accuracy: 0.8721776604652405\n",
      "Step: 11400  \tValid loss: 0.2666466534137726\n",
      "Step: 11500  \tTraining loss: 0.28483763337135315\n",
      "Step: 11500  \tTraining accuracy: 0.8722082376480103\n",
      "Step: 11500  \tValid loss: 0.26661428809165955\n",
      "Step: 11600  \tTraining loss: 0.2847598195075989\n",
      "Step: 11600  \tTraining accuracy: 0.8722427487373352\n",
      "Step: 11600  \tValid loss: 0.2665444016456604\n",
      "Step: 11700  \tTraining loss: 0.28468388319015503\n",
      "Step: 11700  \tTraining accuracy: 0.8722819089889526\n",
      "Step: 11700  \tValid loss: 0.26651009917259216\n",
      "Step: 11800  \tTraining loss: 0.2846115529537201\n",
      "Step: 11800  \tTraining accuracy: 0.872323215007782\n",
      "Step: 11800  \tValid loss: 0.266478031873703\n",
      "Step: 11900  \tTraining loss: 0.28454142808914185\n",
      "Step: 11900  \tTraining accuracy: 0.8723647594451904\n",
      "Step: 11900  \tValid loss: 0.2664456367492676\n",
      "Step: 12000  \tTraining loss: 0.28447049856185913\n",
      "Step: 12000  \tTraining accuracy: 0.8724053502082825\n",
      "Step: 12000  \tValid loss: 0.2664031982421875\n",
      "Step: 12100  \tTraining loss: 0.2844015657901764\n",
      "Step: 12100  \tTraining accuracy: 0.8724455237388611\n",
      "Step: 12100  \tValid loss: 0.2663789391517639\n",
      "Step: 12200  \tTraining loss: 0.2843347489833832\n",
      "Step: 12200  \tTraining accuracy: 0.8724850416183472\n",
      "Step: 12200  \tValid loss: 0.2663387656211853\n",
      "Step: 12300  \tTraining loss: 0.28426796197891235\n",
      "Step: 12300  \tTraining accuracy: 0.8725239634513855\n",
      "Step: 12300  \tValid loss: 0.2662978768348694\n",
      "Step: 12400  \tTraining loss: 0.284201055765152\n",
      "Step: 12400  \tTraining accuracy: 0.8725640177726746\n",
      "Step: 12400  \tValid loss: 0.2662888169288635\n",
      "Step: 12500  \tTraining loss: 0.2841339111328125\n",
      "Step: 12500  \tTraining accuracy: 0.8726046681404114\n",
      "Step: 12500  \tValid loss: 0.2662472128868103\n",
      "Step: 12600  \tTraining loss: 0.28406867384910583\n",
      "Step: 12600  \tTraining accuracy: 0.8726446628570557\n",
      "Step: 12600  \tValid loss: 0.2662149965763092\n",
      "Step: 12700  \tTraining loss: 0.2840038537979126\n",
      "Step: 12700  \tTraining accuracy: 0.8726840019226074\n",
      "Step: 12700  \tValid loss: 0.26618438959121704\n",
      "Step: 12800  \tTraining loss: 0.28393974900245667\n",
      "Step: 12800  \tTraining accuracy: 0.8727218508720398\n",
      "Step: 12800  \tValid loss: 0.26615431904792786\n",
      "Step: 12900  \tTraining loss: 0.28387758135795593\n",
      "Step: 12900  \tTraining accuracy: 0.872759997844696\n",
      "Step: 12900  \tValid loss: 0.26610058546066284\n",
      "Step: 13000  \tTraining loss: 0.2838180363178253\n",
      "Step: 13000  \tTraining accuracy: 0.8727970123291016\n",
      "Step: 13000  \tValid loss: 0.26606568694114685\n",
      "Step: 13100  \tTraining loss: 0.28375735878944397\n",
      "Step: 13100  \tTraining accuracy: 0.8728334307670593\n",
      "Step: 13100  \tValid loss: 0.266018807888031\n",
      "Step: 13200  \tTraining loss: 0.2836982011795044\n",
      "Step: 13200  \tTraining accuracy: 0.872870683670044\n",
      "Step: 13200  \tValid loss: 0.26598119735717773\n",
      "Step: 13300  \tTraining loss: 0.2836403548717499\n",
      "Step: 13300  \tTraining accuracy: 0.8729079961776733\n",
      "Step: 13300  \tValid loss: 0.2659514248371124\n",
      "Step: 13400  \tTraining loss: 0.2835823893547058\n",
      "Step: 13400  \tTraining accuracy: 0.8729458451271057\n",
      "Step: 13400  \tValid loss: 0.2659190595149994\n",
      "Step: 13500  \tTraining loss: 0.2835261821746826\n",
      "Step: 13500  \tTraining accuracy: 0.8729833960533142\n",
      "Step: 13500  \tValid loss: 0.26586997509002686\n",
      "Step: 13600  \tTraining loss: 0.28347060084342957\n",
      "Step: 13600  \tTraining accuracy: 0.8730195760726929\n",
      "Step: 13600  \tValid loss: 0.26583507657051086\n",
      "Step: 13700  \tTraining loss: 0.28341639041900635\n",
      "Step: 13700  \tTraining accuracy: 0.8730546832084656\n",
      "Step: 13700  \tValid loss: 0.2658022344112396\n",
      "Step: 13800  \tTraining loss: 0.2833631634712219\n",
      "Step: 13800  \tTraining accuracy: 0.8730887174606323\n",
      "Step: 13800  \tValid loss: 0.26575884222984314\n",
      "Step: 13900  \tTraining loss: 0.28330835700035095\n",
      "Step: 13900  \tTraining accuracy: 0.8731220364570618\n",
      "Step: 13900  \tValid loss: 0.2657316327095032\n",
      "Step: 14000  \tTraining loss: 0.2832571864128113\n",
      "Step: 14000  \tTraining accuracy: 0.8731551170349121\n",
      "Step: 14000  \tValid loss: 0.2656938433647156\n",
      "Step: 14100  \tTraining loss: 0.2832029461860657\n",
      "Step: 14100  \tTraining accuracy: 0.8731882572174072\n",
      "Step: 14100  \tValid loss: 0.2656519114971161\n",
      "Step: 14200  \tTraining loss: 0.28315040469169617\n",
      "Step: 14200  \tTraining accuracy: 0.8732200860977173\n",
      "Step: 14200  \tValid loss: 0.26561200618743896\n",
      "Step: 14300  \tTraining loss: 0.2830981910228729\n",
      "Step: 14300  \tTraining accuracy: 0.8732517957687378\n",
      "Step: 14300  \tValid loss: 0.26556509733200073\n",
      "Step: 14400  \tTraining loss: 0.28304705023765564\n",
      "Step: 14400  \tTraining accuracy: 0.873281717300415\n",
      "Step: 14400  \tValid loss: 0.2655203640460968\n",
      "Step: 14500  \tTraining loss: 0.28299662470817566\n",
      "Step: 14500  \tTraining accuracy: 0.8733122944831848\n",
      "Step: 14500  \tValid loss: 0.26547032594680786\n",
      "Step: 14600  \tTraining loss: 0.2829461693763733\n",
      "Step: 14600  \tTraining accuracy: 0.8733426928520203\n",
      "Step: 14600  \tValid loss: 0.26541972160339355\n",
      "Step: 14700  \tTraining loss: 0.2828969657421112\n",
      "Step: 14700  \tTraining accuracy: 0.8733721971511841\n",
      "Step: 14700  \tValid loss: 0.26540011167526245\n",
      "Step: 14800  \tTraining loss: 0.2828465700149536\n",
      "Step: 14800  \tTraining accuracy: 0.8734007477760315\n",
      "Step: 14800  \tValid loss: 0.2653577923774719\n",
      "Step: 14900  \tTraining loss: 0.2827984094619751\n",
      "Step: 14900  \tTraining accuracy: 0.8734297156333923\n",
      "Step: 14900  \tValid loss: 0.26531925797462463\n",
      "Step: 15000  \tTraining loss: 0.282748818397522\n",
      "Step: 15000  \tTraining accuracy: 0.8734577298164368\n",
      "Step: 15000  \tValid loss: 0.2652831971645355\n",
      "Step: 15100  \tTraining loss: 0.28269702196121216\n",
      "Step: 15100  \tTraining accuracy: 0.8734866976737976\n",
      "Step: 15100  \tValid loss: 0.2652251720428467\n",
      "Step: 15200  \tTraining loss: 0.2826484441757202\n",
      "Step: 15200  \tTraining accuracy: 0.8735162019729614\n",
      "Step: 15200  \tValid loss: 0.2651819884777069\n",
      "Step: 15300  \tTraining loss: 0.28260132670402527\n",
      "Step: 15300  \tTraining accuracy: 0.8735461235046387\n",
      "Step: 15300  \tValid loss: 0.26515188813209534\n",
      "Step: 15400  \tTraining loss: 0.28255271911621094\n",
      "Step: 15400  \tTraining accuracy: 0.8735777735710144\n",
      "Step: 15400  \tValid loss: 0.2651229798793793\n",
      "Step: 15500  \tTraining loss: 0.2825055718421936\n",
      "Step: 15500  \tTraining accuracy: 0.8736088275909424\n",
      "Step: 15500  \tValid loss: 0.26508620381355286\n",
      "Step: 15600  \tTraining loss: 0.28245964646339417\n",
      "Step: 15600  \tTraining accuracy: 0.8736394643783569\n",
      "Step: 15600  \tValid loss: 0.265059232711792\n",
      "Step: 15700  \tTraining loss: 0.28241437673568726\n",
      "Step: 15700  \tTraining accuracy: 0.8736697435379028\n",
      "Step: 15700  \tValid loss: 0.2650270164012909\n",
      "Step: 15800  \tTraining loss: 0.2823683023452759\n",
      "Step: 15800  \tTraining accuracy: 0.8737003207206726\n",
      "Step: 15800  \tValid loss: 0.2649909555912018\n",
      "Step: 15900  \tTraining loss: 0.2823232114315033\n",
      "Step: 15900  \tTraining accuracy: 0.87373286485672\n",
      "Step: 15900  \tValid loss: 0.26496705412864685\n",
      "Step: 16000  \tTraining loss: 0.2822786271572113\n",
      "Step: 16000  \tTraining accuracy: 0.8737671375274658\n",
      "Step: 16000  \tValid loss: 0.2649395763874054\n",
      "Step: 16100  \tTraining loss: 0.28223374485969543\n",
      "Step: 16100  \tTraining accuracy: 0.8738004565238953\n",
      "Step: 16100  \tValid loss: 0.2649149000644684\n",
      "Step: 16200  \tTraining loss: 0.2821893095970154\n",
      "Step: 16200  \tTraining accuracy: 0.873833417892456\n",
      "Step: 16200  \tValid loss: 0.2648901045322418\n",
      "Step: 16300  \tTraining loss: 0.28214573860168457\n",
      "Step: 16300  \tTraining accuracy: 0.8738659620285034\n",
      "Step: 16300  \tValid loss: 0.2648703157901764\n",
      "Step: 16400  \tTraining loss: 0.28210219740867615\n",
      "Step: 16400  \tTraining accuracy: 0.8738981485366821\n",
      "Step: 16400  \tValid loss: 0.26484617590904236\n",
      "Step: 16500  \tTraining loss: 0.2820579409599304\n",
      "Step: 16500  \tTraining accuracy: 0.8739312887191772\n",
      "Step: 16500  \tValid loss: 0.26484414935112\n",
      "Step: 16600  \tTraining loss: 0.28201693296432495\n",
      "Step: 16600  \tTraining accuracy: 0.873964250087738\n",
      "Step: 16600  \tValid loss: 0.26480019092559814\n",
      "Step: 16700  \tTraining loss: 0.28197434544563293\n",
      "Step: 16700  \tTraining accuracy: 0.8739970326423645\n",
      "Step: 16700  \tValid loss: 0.26477715373039246\n",
      "Step: 16800  \tTraining loss: 0.2819293439388275\n",
      "Step: 16800  \tTraining accuracy: 0.8740291595458984\n",
      "Step: 16800  \tValid loss: 0.2647548317909241\n",
      "Step: 16900  \tTraining loss: 0.28188958764076233\n",
      "Step: 16900  \tTraining accuracy: 0.8740614056587219\n",
      "Step: 16900  \tValid loss: 0.26474684476852417\n",
      "Step: 17000  \tTraining loss: 0.28184598684310913\n",
      "Step: 17000  \tTraining accuracy: 0.8740939497947693\n",
      "Step: 17000  \tValid loss: 0.2647453248500824\n",
      "Step: 17100  \tTraining loss: 0.2818014919757843\n",
      "Step: 17100  \tTraining accuracy: 0.8741264939308167\n",
      "Step: 17100  \tValid loss: 0.2647130489349365\n",
      "Step: 17200  \tTraining loss: 0.281759113073349\n",
      "Step: 17200  \tTraining accuracy: 0.8741586804389954\n",
      "Step: 17200  \tValid loss: 0.264692485332489\n",
      "Step: 17300  \tTraining loss: 0.2817172706127167\n",
      "Step: 17300  \tTraining accuracy: 0.874190092086792\n",
      "Step: 17300  \tValid loss: 0.26468008756637573\n",
      "Step: 17400  \tTraining loss: 0.28167685866355896\n",
      "Step: 17400  \tTraining accuracy: 0.8742217421531677\n",
      "Step: 17400  \tValid loss: 0.2646521031856537\n",
      "Step: 17500  \tTraining loss: 0.28163665533065796\n",
      "Step: 17500  \tTraining accuracy: 0.874253511428833\n",
      "Step: 17500  \tValid loss: 0.2646474242210388\n",
      "Step: 17600  \tTraining loss: 0.28159573674201965\n",
      "Step: 17600  \tTraining accuracy: 0.8742855191230774\n",
      "Step: 17600  \tValid loss: 0.26460421085357666\n",
      "Step: 17700  \tTraining loss: 0.28155648708343506\n",
      "Step: 17700  \tTraining accuracy: 0.8743182420730591\n",
      "Step: 17700  \tValid loss: 0.26459527015686035\n",
      "Step: 17800  \tTraining loss: 0.2815169394016266\n",
      "Step: 17800  \tTraining accuracy: 0.8743505477905273\n",
      "Step: 17800  \tValid loss: 0.26456189155578613\n",
      "Step: 17900  \tTraining loss: 0.28147807717323303\n",
      "Step: 17900  \tTraining accuracy: 0.8743821382522583\n",
      "Step: 17900  \tValid loss: 0.264545202255249\n",
      "Step: 18000  \tTraining loss: 0.28143933415412903\n",
      "Step: 18000  \tTraining accuracy: 0.8744133710861206\n",
      "Step: 18000  \tValid loss: 0.2645331919193268\n",
      "Step: 18100  \tTraining loss: 0.2814027667045593\n",
      "Step: 18100  \tTraining accuracy: 0.8744435906410217\n",
      "Step: 18100  \tValid loss: 0.2645284831523895\n",
      "Step: 18200  \tTraining loss: 0.28136348724365234\n",
      "Step: 18200  \tTraining accuracy: 0.8744739294052124\n",
      "Step: 18200  \tValid loss: 0.2644835412502289\n",
      "Step: 18300  \tTraining loss: 0.28132590651512146\n",
      "Step: 18300  \tTraining accuracy: 0.8745045065879822\n",
      "Step: 18300  \tValid loss: 0.264488160610199\n",
      "Step: 18400  \tTraining loss: 0.2812879681587219\n",
      "Step: 18400  \tTraining accuracy: 0.8745347857475281\n",
      "Step: 18400  \tValid loss: 0.2644631564617157\n",
      "Step: 18500  \tTraining loss: 0.28125089406967163\n",
      "Step: 18500  \tTraining accuracy: 0.8745647668838501\n",
      "Step: 18500  \tValid loss: 0.26444754004478455\n",
      "Step: 18600  \tTraining loss: 0.28121456503868103\n",
      "Step: 18600  \tTraining accuracy: 0.8745943903923035\n",
      "Step: 18600  \tValid loss: 0.26442399621009827\n",
      "Step: 18700  \tTraining loss: 0.2811800539493561\n",
      "Step: 18700  \tTraining accuracy: 0.8746234774589539\n",
      "Step: 18700  \tValid loss: 0.26439332962036133\n",
      "Step: 18800  \tTraining loss: 0.2811431884765625\n",
      "Step: 18800  \tTraining accuracy: 0.8746516704559326\n",
      "Step: 18800  \tValid loss: 0.26438644528388977\n",
      "Step: 18900  \tTraining loss: 0.2811059355735779\n",
      "Step: 18900  \tTraining accuracy: 0.8746811747550964\n",
      "Step: 18900  \tValid loss: 0.264377623796463\n",
      "Step: 19000  \tTraining loss: 0.2810707092285156\n",
      "Step: 19000  \tTraining accuracy: 0.8747105598449707\n",
      "Step: 19000  \tValid loss: 0.2643478810787201\n",
      "Step: 19100  \tTraining loss: 0.28103509545326233\n",
      "Step: 19100  \tTraining accuracy: 0.874739408493042\n",
      "Step: 19100  \tValid loss: 0.26434847712516785\n",
      "Step: 19200  \tTraining loss: 0.28100067377090454\n",
      "Step: 19200  \tTraining accuracy: 0.8747671842575073\n",
      "Step: 19200  \tValid loss: 0.264325350522995\n",
      "Step: 19300  \tTraining loss: 0.2809670567512512\n",
      "Step: 19300  \tTraining accuracy: 0.8747941255569458\n",
      "Step: 19300  \tValid loss: 0.26430726051330566\n",
      "Step: 19400  \tTraining loss: 0.28093111515045166\n",
      "Step: 19400  \tTraining accuracy: 0.8748193979263306\n",
      "Step: 19400  \tValid loss: 0.2642894685268402\n",
      "Step: 19500  \tTraining loss: 0.28089720010757446\n",
      "Step: 19500  \tTraining accuracy: 0.8748443722724915\n",
      "Step: 19500  \tValid loss: 0.264276921749115\n",
      "Step: 19600  \tTraining loss: 0.28086355328559875\n",
      "Step: 19600  \tTraining accuracy: 0.8748693466186523\n",
      "Step: 19600  \tValid loss: 0.2642439901828766\n",
      "Step: 19700  \tTraining loss: 0.2808303236961365\n",
      "Step: 19700  \tTraining accuracy: 0.8748947978019714\n",
      "Step: 19700  \tValid loss: 0.2642238140106201\n",
      "Step: 19800  \tTraining loss: 0.2807973027229309\n",
      "Step: 19800  \tTraining accuracy: 0.8749206066131592\n",
      "Step: 19800  \tValid loss: 0.26420867443084717\n",
      "Step: 19900  \tTraining loss: 0.28076523542404175\n",
      "Step: 19900  \tTraining accuracy: 0.874945342540741\n",
      "Step: 19900  \tValid loss: 0.26420217752456665\n",
      "Step: 20000  \tTraining loss: 0.28073155879974365\n",
      "Step: 20000  \tTraining accuracy: 0.8749688863754272\n",
      "Step: 20000  \tValid loss: 0.26416850090026855\n",
      "Step: 20100  \tTraining loss: 0.2806987166404724\n",
      "Step: 20100  \tTraining accuracy: 0.8749924302101135\n",
      "Step: 20100  \tValid loss: 0.26415395736694336\n",
      "Step: 20200  \tTraining loss: 0.280666321516037\n",
      "Step: 20200  \tTraining accuracy: 0.8750157356262207\n",
      "Step: 20200  \tValid loss: 0.2641333341598511\n",
      "Step: 20300  \tTraining loss: 0.28063729405403137\n",
      "Step: 20300  \tTraining accuracy: 0.8750388026237488\n",
      "Step: 20300  \tValid loss: 0.26412081718444824\n",
      "Step: 20400  \tTraining loss: 0.28060242533683777\n",
      "Step: 20400  \tTraining accuracy: 0.875060498714447\n",
      "Step: 20400  \tValid loss: 0.2641099989414215\n",
      "Step: 20500  \tTraining loss: 0.2805747091770172\n",
      "Step: 20500  \tTraining accuracy: 0.8750820159912109\n",
      "Step: 20500  \tValid loss: 0.2640902101993561\n",
      "Step: 20600  \tTraining loss: 0.28054073452949524\n",
      "Step: 20600  \tTraining accuracy: 0.8751051425933838\n",
      "Step: 20600  \tValid loss: 0.2640566825866699\n",
      "Step: 20700  \tTraining loss: 0.2805095314979553\n",
      "Step: 20700  \tTraining accuracy: 0.875128984451294\n",
      "Step: 20700  \tValid loss: 0.264048308134079\n",
      "Step: 20800  \tTraining loss: 0.2804816961288452\n",
      "Step: 20800  \tTraining accuracy: 0.8751512765884399\n",
      "Step: 20800  \tValid loss: 0.26401636004447937\n",
      "Step: 20900  \tTraining loss: 0.28045037388801575\n",
      "Step: 20900  \tTraining accuracy: 0.8751737475395203\n",
      "Step: 20900  \tValid loss: 0.2639833390712738\n",
      "Step: 21000  \tTraining loss: 0.2804189622402191\n",
      "Step: 21000  \tTraining accuracy: 0.8751972317695618\n",
      "Step: 21000  \tValid loss: 0.2639647424221039\n",
      "Step: 21100  \tTraining loss: 0.2803882658481598\n",
      "Step: 21100  \tTraining accuracy: 0.8752210736274719\n",
      "Step: 21100  \tValid loss: 0.2639639675617218\n",
      "Step: 21200  \tTraining loss: 0.2803620398044586\n",
      "Step: 21200  \tTraining accuracy: 0.8752446174621582\n",
      "Step: 21200  \tValid loss: 0.2639244496822357\n",
      "Step: 21300  \tTraining loss: 0.2803294360637665\n",
      "Step: 21300  \tTraining accuracy: 0.8752683401107788\n",
      "Step: 21300  \tValid loss: 0.2639217674732208\n",
      "Step: 21400  \tTraining loss: 0.28030022978782654\n",
      "Step: 21400  \tTraining accuracy: 0.8752920627593994\n",
      "Step: 21400  \tValid loss: 0.26388803124427795\n",
      "Step: 21500  \tTraining loss: 0.2802720367908478\n",
      "Step: 21500  \tTraining accuracy: 0.8753151297569275\n",
      "Step: 21500  \tValid loss: 0.26387912034988403\n",
      "Step: 21600  \tTraining loss: 0.280245840549469\n",
      "Step: 21600  \tTraining accuracy: 0.8753373026847839\n",
      "Step: 21600  \tValid loss: 0.2638415992259979\n",
      "Step: 21700  \tTraining loss: 0.2802150547504425\n",
      "Step: 21700  \tTraining accuracy: 0.8753591179847717\n",
      "Step: 21700  \tValid loss: 0.26382383704185486\n",
      "Step: 21800  \tTraining loss: 0.2801862955093384\n",
      "Step: 21800  \tTraining accuracy: 0.8753814697265625\n",
      "Step: 21800  \tValid loss: 0.26379749178886414\n",
      "Step: 21900  \tTraining loss: 0.28015652298927307\n",
      "Step: 21900  \tTraining accuracy: 0.8754037022590637\n",
      "Step: 21900  \tValid loss: 0.2637885510921478\n",
      "Step: 22000  \tTraining loss: 0.28012844920158386\n",
      "Step: 22000  \tTraining accuracy: 0.8754251003265381\n",
      "Step: 22000  \tValid loss: 0.2637537121772766\n",
      "Step: 22100  \tTraining loss: 0.2801007926464081\n",
      "Step: 22100  \tTraining accuracy: 0.8754461407661438\n",
      "Step: 22100  \tValid loss: 0.2637105882167816\n",
      "Step: 22200  \tTraining loss: 0.28007274866104126\n",
      "Step: 22200  \tTraining accuracy: 0.8754675984382629\n",
      "Step: 22200  \tValid loss: 0.2637038230895996\n",
      "Step: 22300  \tTraining loss: 0.28004583716392517\n",
      "Step: 22300  \tTraining accuracy: 0.8754897713661194\n",
      "Step: 22300  \tValid loss: 0.26369112730026245\n",
      "Step: 22400  \tTraining loss: 0.28001734614372253\n",
      "Step: 22400  \tTraining accuracy: 0.8755120635032654\n",
      "Step: 22400  \tValid loss: 0.26366502046585083\n",
      "Step: 22500  \tTraining loss: 0.27999016642570496\n",
      "Step: 22500  \tTraining accuracy: 0.8755339980125427\n",
      "Step: 22500  \tValid loss: 0.2636411786079407\n",
      "Step: 22600  \tTraining loss: 0.27996334433555603\n",
      "Step: 22600  \tTraining accuracy: 0.8755553960800171\n",
      "Step: 22600  \tValid loss: 0.2636227607727051\n",
      "Step: 22700  \tTraining loss: 0.27993884682655334\n",
      "Step: 22700  \tTraining accuracy: 0.8755772709846497\n",
      "Step: 22700  \tValid loss: 0.2635997533798218\n",
      "Step: 22800  \tTraining loss: 0.2799120545387268\n",
      "Step: 22800  \tTraining accuracy: 0.8755991458892822\n",
      "Step: 22800  \tValid loss: 0.26358580589294434\n",
      "Step: 22900  \tTraining loss: 0.2798841893672943\n",
      "Step: 22900  \tTraining accuracy: 0.8756203055381775\n",
      "Step: 22900  \tValid loss: 0.2635551691055298\n",
      "Step: 23000  \tTraining loss: 0.27985766530036926\n",
      "Step: 23000  \tTraining accuracy: 0.8756412863731384\n",
      "Step: 23000  \tValid loss: 0.26355019211769104\n",
      "Step: 23100  \tTraining loss: 0.27983197569847107\n",
      "Step: 23100  \tTraining accuracy: 0.8756626844406128\n",
      "Step: 23100  \tValid loss: 0.26352572441101074\n",
      "Step: 23200  \tTraining loss: 0.2798054814338684\n",
      "Step: 23200  \tTraining accuracy: 0.8756841421127319\n",
      "Step: 23200  \tValid loss: 0.26352065801620483\n",
      "Step: 23300  \tTraining loss: 0.27977997064590454\n",
      "Step: 23300  \tTraining accuracy: 0.875705361366272\n",
      "Step: 23300  \tValid loss: 0.2634899914264679\n",
      "Step: 23400  \tTraining loss: 0.27975448966026306\n",
      "Step: 23400  \tTraining accuracy: 0.8757262825965881\n",
      "Step: 23400  \tValid loss: 0.2634764015674591\n",
      "Step: 23500  \tTraining loss: 0.2797304093837738\n",
      "Step: 23500  \tTraining accuracy: 0.8757461905479431\n",
      "Step: 23500  \tValid loss: 0.2634497582912445\n",
      "Step: 23600  \tTraining loss: 0.27970418334007263\n",
      "Step: 23600  \tTraining accuracy: 0.8757659196853638\n",
      "Step: 23600  \tValid loss: 0.2634345293045044\n",
      "Step: 23700  \tTraining loss: 0.2796792685985565\n",
      "Step: 23700  \tTraining accuracy: 0.875785231590271\n",
      "Step: 23700  \tValid loss: 0.2634163498878479\n",
      "Step: 23800  \tTraining loss: 0.27965593338012695\n",
      "Step: 23800  \tTraining accuracy: 0.875804603099823\n",
      "Step: 23800  \tValid loss: 0.26338842511177063\n",
      "Step: 23900  \tTraining loss: 0.2796308994293213\n",
      "Step: 23900  \tTraining accuracy: 0.8758237361907959\n",
      "Step: 23900  \tValid loss: 0.2633805572986603\n",
      "Step: 24000  \tTraining loss: 0.2796025276184082\n",
      "Step: 24000  \tTraining accuracy: 0.8758417367935181\n",
      "Step: 24000  \tValid loss: 0.2633562982082367\n",
      "Step: 24100  \tTraining loss: 0.27957433462142944\n",
      "Step: 24100  \tTraining accuracy: 0.8758594393730164\n",
      "Step: 24100  \tValid loss: 0.2633267641067505\n",
      "Step: 24200  \tTraining loss: 0.2795458137989044\n",
      "Step: 24200  \tTraining accuracy: 0.8758774399757385\n",
      "Step: 24200  \tValid loss: 0.26329734921455383\n",
      "Step: 24300  \tTraining loss: 0.2795177698135376\n",
      "Step: 24300  \tTraining accuracy: 0.8758949637413025\n",
      "Step: 24300  \tValid loss: 0.2632942795753479\n",
      "Step: 24400  \tTraining loss: 0.27949267625808716\n",
      "Step: 24400  \tTraining accuracy: 0.8759128451347351\n",
      "Step: 24400  \tValid loss: 0.2632638216018677\n",
      "Step: 24500  \tTraining loss: 0.27946507930755615\n",
      "Step: 24500  \tTraining accuracy: 0.8759306073188782\n",
      "Step: 24500  \tValid loss: 0.2632414996623993\n",
      "Step: 24600  \tTraining loss: 0.2794394791126251\n",
      "Step: 24600  \tTraining accuracy: 0.8759477138519287\n",
      "Step: 24600  \tValid loss: 0.2632124423980713\n",
      "Step: 24700  \tTraining loss: 0.27941441535949707\n",
      "Step: 24700  \tTraining accuracy: 0.8759649991989136\n",
      "Step: 24700  \tValid loss: 0.26319366693496704\n",
      "Step: 24800  \tTraining loss: 0.27938881516456604\n",
      "Step: 24800  \tTraining accuracy: 0.8759822845458984\n",
      "Step: 24800  \tValid loss: 0.26318982243537903\n",
      "Step: 24900  \tTraining loss: 0.2793642282485962\n",
      "Step: 24900  \tTraining accuracy: 0.8759994506835938\n",
      "Step: 24900  \tValid loss: 0.26316118240356445\n",
      "Step: 25000  \tTraining loss: 0.2793388068675995\n",
      "Step: 25000  \tTraining accuracy: 0.8760167956352234\n",
      "Step: 25000  \tValid loss: 0.2631368339061737\n",
      "Step: 25100  \tTraining loss: 0.27931392192840576\n",
      "Step: 25100  \tTraining accuracy: 0.8760344386100769\n",
      "Step: 25100  \tValid loss: 0.2631218731403351\n",
      "Step: 25200  \tTraining loss: 0.27929121255874634\n",
      "Step: 25200  \tTraining accuracy: 0.8760522603988647\n",
      "Step: 25200  \tValid loss: 0.263110876083374\n",
      "Step: 25300  \tTraining loss: 0.279266357421875\n",
      "Step: 25300  \tTraining accuracy: 0.8760707974433899\n",
      "Step: 25300  \tValid loss: 0.26306378841400146\n",
      "Step: 25400  \tTraining loss: 0.27924275398254395\n",
      "Step: 25400  \tTraining accuracy: 0.8760894536972046\n",
      "Step: 25400  \tValid loss: 0.26305440068244934\n",
      "Step: 25500  \tTraining loss: 0.2792186141014099\n",
      "Step: 25500  \tTraining accuracy: 0.8761079907417297\n",
      "Step: 25500  \tValid loss: 0.2630443274974823\n",
      "Step: 25600  \tTraining loss: 0.27919501066207886\n",
      "Step: 25600  \tTraining accuracy: 0.8761264085769653\n",
      "Step: 25600  \tValid loss: 0.2630157768726349\n",
      "Step: 25700  \tTraining loss: 0.27917465567588806\n",
      "Step: 25700  \tTraining accuracy: 0.8761452436447144\n",
      "Step: 25700  \tValid loss: 0.2629983425140381\n",
      "Step: 25800  \tTraining loss: 0.2791482210159302\n",
      "Step: 25800  \tTraining accuracy: 0.8761640787124634\n",
      "Step: 25800  \tValid loss: 0.2629864513874054\n",
      "Step: 25900  \tTraining loss: 0.27912524342536926\n",
      "Step: 25900  \tTraining accuracy: 0.8761832118034363\n",
      "Step: 25900  \tValid loss: 0.2629643976688385\n",
      "Step: 26000  \tTraining loss: 0.27910277247428894\n",
      "Step: 26000  \tTraining accuracy: 0.8762025237083435\n",
      "Step: 26000  \tValid loss: 0.2629336714744568\n",
      "Step: 26100  \tTraining loss: 0.27907899022102356\n",
      "Step: 26100  \tTraining accuracy: 0.8762216567993164\n",
      "Step: 26100  \tValid loss: 0.2629196047782898\n",
      "Step: 26200  \tTraining loss: 0.27905669808387756\n",
      "Step: 26200  \tTraining accuracy: 0.876240611076355\n",
      "Step: 26200  \tValid loss: 0.2628934979438782\n",
      "Step: 26300  \tTraining loss: 0.27903270721435547\n",
      "Step: 26300  \tTraining accuracy: 0.876259446144104\n",
      "Step: 26300  \tValid loss: 0.2629041075706482\n",
      "Step: 26400  \tTraining loss: 0.2790106534957886\n",
      "Step: 26400  \tTraining accuracy: 0.8762781620025635\n",
      "Step: 26400  \tValid loss: 0.2628646194934845\n",
      "Step: 26500  \tTraining loss: 0.2789885103702545\n",
      "Step: 26500  \tTraining accuracy: 0.8762966990470886\n",
      "Step: 26500  \tValid loss: 0.26285383105278015\n",
      "Step: 26600  \tTraining loss: 0.2789653539657593\n",
      "Step: 26600  \tTraining accuracy: 0.8763151168823242\n",
      "Step: 26600  \tValid loss: 0.2628399431705475\n",
      "Step: 26700  \tTraining loss: 0.2789417803287506\n",
      "Step: 26700  \tTraining accuracy: 0.8763336539268494\n",
      "Step: 26700  \tValid loss: 0.262832373380661\n",
      "Step: 26800  \tTraining loss: 0.2789190113544464\n",
      "Step: 26800  \tTraining accuracy: 0.8763524889945984\n",
      "Step: 26800  \tValid loss: 0.2628045678138733\n",
      "Step: 26900  \tTraining loss: 0.27889707684516907\n",
      "Step: 26900  \tTraining accuracy: 0.876370906829834\n",
      "Step: 26900  \tValid loss: 0.26279088854789734\n",
      "Step: 27000  \tTraining loss: 0.27887338399887085\n",
      "Step: 27000  \tTraining accuracy: 0.8763895630836487\n",
      "Step: 27000  \tValid loss: 0.2627713978290558\n",
      "Step: 27100  \tTraining loss: 0.27885088324546814\n",
      "Step: 27100  \tTraining accuracy: 0.8764082789421082\n",
      "Step: 27100  \tValid loss: 0.2627583146095276\n",
      "Step: 27200  \tTraining loss: 0.27882885932922363\n",
      "Step: 27200  \tTraining accuracy: 0.8764265775680542\n",
      "Step: 27200  \tValid loss: 0.26274603605270386\n",
      "Step: 27300  \tTraining loss: 0.27880677580833435\n",
      "Step: 27300  \tTraining accuracy: 0.8764446973800659\n",
      "Step: 27300  \tValid loss: 0.2627260684967041\n",
      "Step: 27400  \tTraining loss: 0.27878424525260925\n",
      "Step: 27400  \tTraining accuracy: 0.8764627575874329\n",
      "Step: 27400  \tValid loss: 0.2627130448818207\n",
      "Step: 27500  \tTraining loss: 0.27876415848731995\n",
      "Step: 27500  \tTraining accuracy: 0.8764798045158386\n",
      "Step: 27500  \tValid loss: 0.2626813054084778\n",
      "Step: 27600  \tTraining loss: 0.2787420153617859\n",
      "Step: 27600  \tTraining accuracy: 0.8764968514442444\n",
      "Step: 27600  \tValid loss: 0.2626689672470093\n",
      "Step: 27700  \tTraining loss: 0.27871978282928467\n",
      "Step: 27700  \tTraining accuracy: 0.8765138387680054\n",
      "Step: 27700  \tValid loss: 0.26264986395835876\n",
      "Step: 27800  \tTraining loss: 0.2786976397037506\n",
      "Step: 27800  \tTraining accuracy: 0.876530647277832\n",
      "Step: 27800  \tValid loss: 0.26263198256492615\n",
      "Step: 27900  \tTraining loss: 0.2786758840084076\n",
      "Step: 27900  \tTraining accuracy: 0.876547634601593\n",
      "Step: 27900  \tValid loss: 0.2626112699508667\n",
      "Step: 28000  \tTraining loss: 0.27865371108055115\n",
      "Step: 28000  \tTraining accuracy: 0.8765636682510376\n",
      "Step: 28000  \tValid loss: 0.26259589195251465\n",
      "Step: 28100  \tTraining loss: 0.2786337435245514\n",
      "Step: 28100  \tTraining accuracy: 0.8765788078308105\n",
      "Step: 28100  \tValid loss: 0.26256874203681946\n",
      "Step: 28200  \tTraining loss: 0.2786102592945099\n",
      "Step: 28200  \tTraining accuracy: 0.8765951991081238\n",
      "Step: 28200  \tValid loss: 0.26257088780403137\n",
      "Step: 28300  \tTraining loss: 0.27858981490135193\n",
      "Step: 28300  \tTraining accuracy: 0.8766108751296997\n",
      "Step: 28300  \tValid loss: 0.2625419497489929\n",
      "Step: 28400  \tTraining loss: 0.2785678207874298\n",
      "Step: 28400  \tTraining accuracy: 0.8766270279884338\n",
      "Step: 28400  \tValid loss: 0.2625333070755005\n",
      "Step: 28500  \tTraining loss: 0.2785482704639435\n",
      "Step: 28500  \tTraining accuracy: 0.8766417503356934\n",
      "Step: 28500  \tValid loss: 0.2624949514865875\n",
      "Step: 28600  \tTraining loss: 0.27852663397789\n",
      "Step: 28600  \tTraining accuracy: 0.8766556978225708\n",
      "Step: 28600  \tValid loss: 0.26249656081199646\n",
      "Step: 28700  \tTraining loss: 0.2785048186779022\n",
      "Step: 28700  \tTraining accuracy: 0.8766695261001587\n",
      "Step: 28700  \tValid loss: 0.2624649405479431\n",
      "Step: 28800  \tTraining loss: 0.2784842848777771\n",
      "Step: 28800  \tTraining accuracy: 0.8766832947731018\n",
      "Step: 28800  \tValid loss: 0.26245802640914917\n",
      "Step: 28900  \tTraining loss: 0.27846166491508484\n",
      "Step: 28900  \tTraining accuracy: 0.8766981959342957\n",
      "Step: 28900  \tValid loss: 0.2624496817588806\n",
      "Step: 29000  \tTraining loss: 0.2784405052661896\n",
      "Step: 29000  \tTraining accuracy: 0.8767127990722656\n",
      "Step: 29000  \tValid loss: 0.26243436336517334\n",
      "Step: 29100  \tTraining loss: 0.278419554233551\n",
      "Step: 29100  \tTraining accuracy: 0.8767275810241699\n",
      "Step: 29100  \tValid loss: 0.2624036371707916\n",
      "Step: 29200  \tTraining loss: 0.27839821577072144\n",
      "Step: 29200  \tTraining accuracy: 0.8767428398132324\n",
      "Step: 29200  \tValid loss: 0.2624059021472931\n",
      "Step: 29300  \tTraining loss: 0.2783786356449127\n",
      "Step: 29300  \tTraining accuracy: 0.8767571449279785\n",
      "Step: 29300  \tValid loss: 0.26237213611602783\n",
      "Step: 29400  \tTraining loss: 0.27835819125175476\n",
      "Step: 29400  \tTraining accuracy: 0.8767709136009216\n",
      "Step: 29400  \tValid loss: 0.2623535692691803\n",
      "Step: 29500  \tTraining loss: 0.2783395051956177\n",
      "Step: 29500  \tTraining accuracy: 0.8767846822738647\n",
      "Step: 29500  \tValid loss: 0.2623482942581177\n",
      "Step: 29600  \tTraining loss: 0.27831587195396423\n",
      "Step: 29600  \tTraining accuracy: 0.8767982721328735\n",
      "Step: 29600  \tValid loss: 0.26232659816741943\n",
      "Step: 29700  \tTraining loss: 0.2782955467700958\n",
      "Step: 29700  \tTraining accuracy: 0.8768120408058167\n",
      "Step: 29700  \tValid loss: 0.262326717376709\n",
      "Step: 29800  \tTraining loss: 0.27827444672584534\n",
      "Step: 29800  \tTraining accuracy: 0.8768261075019836\n",
      "Step: 29800  \tValid loss: 0.2623140215873718\n",
      "Step: 29900  \tTraining loss: 0.27825528383255005\n",
      "Step: 29900  \tTraining accuracy: 0.8768401145935059\n",
      "Step: 29900  \tValid loss: 0.26228460669517517\n",
      "Step: 30000  \tTraining loss: 0.27823373675346375\n",
      "Step: 30000  \tTraining accuracy: 0.8768540024757385\n",
      "Step: 30000  \tValid loss: 0.2622653543949127\n",
      "Step: 30100  \tTraining loss: 0.2782127857208252\n",
      "Step: 30100  \tTraining accuracy: 0.8768677711486816\n",
      "Step: 30100  \tValid loss: 0.26225745677948\n",
      "Step: 30200  \tTraining loss: 0.27819201350212097\n",
      "Step: 30200  \tTraining accuracy: 0.87688148021698\n",
      "Step: 30200  \tValid loss: 0.26224154233932495\n",
      "Step: 30300  \tTraining loss: 0.278172105550766\n",
      "Step: 30300  \tTraining accuracy: 0.8768950700759888\n",
      "Step: 30300  \tValid loss: 0.26222386956214905\n",
      "Step: 30400  \tTraining loss: 0.2781524956226349\n",
      "Step: 30400  \tTraining accuracy: 0.8769084811210632\n",
      "Step: 30400  \tValid loss: 0.26220226287841797\n",
      "Step: 30500  \tTraining loss: 0.27813342213630676\n",
      "Step: 30500  \tTraining accuracy: 0.8769208788871765\n",
      "Step: 30500  \tValid loss: 0.2622041404247284\n",
      "Step: 30600  \tTraining loss: 0.27811288833618164\n",
      "Step: 30600  \tTraining accuracy: 0.8769323825836182\n",
      "Step: 30600  \tValid loss: 0.26217907667160034\n",
      "Step: 30700  \tTraining loss: 0.2780918478965759\n",
      "Step: 30700  \tTraining accuracy: 0.876943826675415\n",
      "Step: 30700  \tValid loss: 0.26216378808021545\n",
      "Step: 30800  \tTraining loss: 0.27807193994522095\n",
      "Step: 30800  \tTraining accuracy: 0.8769551515579224\n",
      "Step: 30800  \tValid loss: 0.2621544897556305\n",
      "Step: 30900  \tTraining loss: 0.2780519723892212\n",
      "Step: 30900  \tTraining accuracy: 0.8769664764404297\n",
      "Step: 30900  \tValid loss: 0.2621387839317322\n",
      "Step: 31000  \tTraining loss: 0.2780328691005707\n",
      "Step: 31000  \tTraining accuracy: 0.8769775629043579\n",
      "Step: 31000  \tValid loss: 0.26212653517723083\n",
      "Step: 31100  \tTraining loss: 0.2780137360095978\n",
      "Step: 31100  \tTraining accuracy: 0.8769880533218384\n",
      "Step: 31100  \tValid loss: 0.26210471987724304\n",
      "Step: 31200  \tTraining loss: 0.2779957056045532\n",
      "Step: 31200  \tTraining accuracy: 0.8769985437393188\n",
      "Step: 31200  \tValid loss: 0.2620975375175476\n",
      "Step: 31300  \tTraining loss: 0.2779728174209595\n",
      "Step: 31300  \tTraining accuracy: 0.8770089149475098\n",
      "Step: 31300  \tValid loss: 0.26209238171577454\n",
      "Step: 31400  \tTraining loss: 0.2779536247253418\n",
      "Step: 31400  \tTraining accuracy: 0.8770192861557007\n",
      "Step: 31400  \tValid loss: 0.2620774805545807\n",
      "Step: 31500  \tTraining loss: 0.2779345214366913\n",
      "Step: 31500  \tTraining accuracy: 0.877029538154602\n",
      "Step: 31500  \tValid loss: 0.26206016540527344\n",
      "Step: 31600  \tTraining loss: 0.2779153883457184\n",
      "Step: 31600  \tTraining accuracy: 0.8770398497581482\n",
      "Step: 31600  \tValid loss: 0.2620411515235901\n",
      "Step: 31700  \tTraining loss: 0.27789637446403503\n",
      "Step: 31700  \tTraining accuracy: 0.8770504593849182\n",
      "Step: 31700  \tValid loss: 0.26204273104667664\n",
      "Step: 31800  \tTraining loss: 0.27787747979164124\n",
      "Step: 31800  \tTraining accuracy: 0.8770605325698853\n",
      "Step: 31800  \tValid loss: 0.26204320788383484\n",
      "Step: 31900  \tTraining loss: 0.2778550386428833\n",
      "Step: 31900  \tTraining accuracy: 0.8770706653594971\n",
      "Step: 31900  \tValid loss: 0.26200637221336365\n",
      "Step: 32000  \tTraining loss: 0.27783429622650146\n",
      "Step: 32000  \tTraining accuracy: 0.8770815134048462\n",
      "Step: 32000  \tValid loss: 0.2620135545730591\n",
      "Step: 32100  \tTraining loss: 0.2778145968914032\n",
      "Step: 32100  \tTraining accuracy: 0.877092719078064\n",
      "Step: 32100  \tValid loss: 0.2619975805282593\n",
      "Step: 32200  \tTraining loss: 0.2777949571609497\n",
      "Step: 32200  \tTraining accuracy: 0.8771042823791504\n",
      "Step: 32200  \tValid loss: 0.26199623942375183\n",
      "Step: 32300  \tTraining loss: 0.27777522802352905\n",
      "Step: 32300  \tTraining accuracy: 0.8771157264709473\n",
      "Step: 32300  \tValid loss: 0.26198476552963257\n",
      "Step: 32400  \tTraining loss: 0.2777559161186218\n",
      "Step: 32400  \tTraining accuracy: 0.8771271705627441\n",
      "Step: 32400  \tValid loss: 0.2619728446006775\n",
      "Step: 32500  \tTraining loss: 0.27773672342300415\n",
      "Step: 32500  \tTraining accuracy: 0.877138078212738\n",
      "Step: 32500  \tValid loss: 0.2619725167751312\n",
      "Step: 32600  \tTraining loss: 0.27771785855293274\n",
      "Step: 32600  \tTraining accuracy: 0.8771487474441528\n",
      "Step: 32600  \tValid loss: 0.2619585394859314\n",
      "Step: 32700  \tTraining loss: 0.2776981294155121\n",
      "Step: 32700  \tTraining accuracy: 0.8771594762802124\n",
      "Step: 32700  \tValid loss: 0.2619553208351135\n",
      "Step: 32800  \tTraining loss: 0.27767953276634216\n",
      "Step: 32800  \tTraining accuracy: 0.8771710991859436\n",
      "Step: 32800  \tValid loss: 0.26194655895233154\n",
      "Step: 32900  \tTraining loss: 0.2776627838611603\n",
      "Step: 32900  \tTraining accuracy: 0.8771836757659912\n",
      "Step: 32900  \tValid loss: 0.26193466782569885\n",
      "Step: 33000  \tTraining loss: 0.27764222025871277\n",
      "Step: 33000  \tTraining accuracy: 0.8771963715553284\n",
      "Step: 33000  \tValid loss: 0.26193010807037354\n",
      "Step: 33100  \tTraining loss: 0.27762219309806824\n",
      "Step: 33100  \tTraining accuracy: 0.8772085309028625\n",
      "Step: 33100  \tValid loss: 0.2619170546531677\n",
      "Step: 33200  \tTraining loss: 0.27760380506515503\n",
      "Step: 33200  \tTraining accuracy: 0.8772198557853699\n",
      "Step: 33200  \tValid loss: 0.26190364360809326\n",
      "Step: 33300  \tTraining loss: 0.2775851786136627\n",
      "Step: 33300  \tTraining accuracy: 0.8772308826446533\n",
      "Step: 33300  \tValid loss: 0.2618967890739441\n",
      "Step: 33400  \tTraining loss: 0.2775667607784271\n",
      "Step: 33400  \tTraining accuracy: 0.8772416114807129\n",
      "Step: 33400  \tValid loss: 0.26189735531806946\n",
      "Step: 33500  \tTraining loss: 0.2775484323501587\n",
      "Step: 33500  \tTraining accuracy: 0.8772522807121277\n",
      "Step: 33500  \tValid loss: 0.2618752717971802\n",
      "Step: 33600  \tTraining loss: 0.2775298058986664\n",
      "Step: 33600  \tTraining accuracy: 0.8772630095481873\n",
      "Step: 33600  \tValid loss: 0.26187726855278015\n",
      "Step: 33700  \tTraining loss: 0.2775115370750427\n",
      "Step: 33700  \tTraining accuracy: 0.8772740960121155\n",
      "Step: 33700  \tValid loss: 0.2618604302406311\n",
      "Step: 33800  \tTraining loss: 0.27749451994895935\n",
      "Step: 33800  \tTraining accuracy: 0.8772851228713989\n",
      "Step: 33800  \tValid loss: 0.26185324788093567\n",
      "Step: 33900  \tTraining loss: 0.27747541666030884\n",
      "Step: 33900  \tTraining accuracy: 0.8772960901260376\n",
      "Step: 33900  \tValid loss: 0.26184943318367004\n",
      "Step: 34000  \tTraining loss: 0.2774581015110016\n",
      "Step: 34000  \tTraining accuracy: 0.8773069977760315\n",
      "Step: 34000  \tValid loss: 0.26182821393013\n",
      "Step: 34100  \tTraining loss: 0.27743953466415405\n",
      "Step: 34100  \tTraining accuracy: 0.8773178458213806\n",
      "Step: 34100  \tValid loss: 0.261820524930954\n",
      "Step: 34200  \tTraining loss: 0.2774215042591095\n",
      "Step: 34200  \tTraining accuracy: 0.877328634262085\n",
      "Step: 34200  \tValid loss: 0.26181378960609436\n",
      "Step: 34300  \tTraining loss: 0.2774048447608948\n",
      "Step: 34300  \tTraining accuracy: 0.8773393034934998\n",
      "Step: 34300  \tValid loss: 0.2618098258972168\n",
      "Step: 34400  \tTraining loss: 0.2773858904838562\n",
      "Step: 34400  \tTraining accuracy: 0.8773497343063354\n",
      "Step: 34400  \tValid loss: 0.261791855096817\n",
      "Step: 34500  \tTraining loss: 0.2773682475090027\n",
      "Step: 34500  \tTraining accuracy: 0.8773598074913025\n",
      "Step: 34500  \tValid loss: 0.261781245470047\n",
      "Step: 34600  \tTraining loss: 0.27735158801078796\n",
      "Step: 34600  \tTraining accuracy: 0.87736976146698\n",
      "Step: 34600  \tValid loss: 0.2617875933647156\n",
      "Step: 34700  \tTraining loss: 0.277333527803421\n",
      "Step: 34700  \tTraining accuracy: 0.8773793578147888\n",
      "Step: 34700  \tValid loss: 0.26177045702934265\n",
      "Step: 34800  \tTraining loss: 0.2773151099681854\n",
      "Step: 34800  \tTraining accuracy: 0.8773887157440186\n",
      "Step: 34800  \tValid loss: 0.26176032423973083\n",
      "Step: 34900  \tTraining loss: 0.27729713916778564\n",
      "Step: 34900  \tTraining accuracy: 0.8773980140686035\n",
      "Step: 34900  \tValid loss: 0.26178157329559326\n",
      "Step: 35000  \tTraining loss: 0.277279794216156\n",
      "Step: 35000  \tTraining accuracy: 0.8774075508117676\n",
      "Step: 35000  \tValid loss: 0.2617414593696594\n",
      "Step: 35100  \tTraining loss: 0.27726301550865173\n",
      "Step: 35100  \tTraining accuracy: 0.8774169087409973\n",
      "Step: 35100  \tValid loss: 0.2617497444152832\n",
      "Step: 35200  \tTraining loss: 0.2772452235221863\n",
      "Step: 35200  \tTraining accuracy: 0.877426028251648\n",
      "Step: 35200  \tValid loss: 0.2617364525794983\n",
      "Step: 35300  \tTraining loss: 0.2772272229194641\n",
      "Step: 35300  \tTraining accuracy: 0.8774352073669434\n",
      "Step: 35300  \tValid loss: 0.2617250978946686\n",
      "Step: 35400  \tTraining loss: 0.27720996737480164\n",
      "Step: 35400  \tTraining accuracy: 0.8774442076683044\n",
      "Step: 35400  \tValid loss: 0.2617103159427643\n",
      "Step: 35500  \tTraining loss: 0.2771936357021332\n",
      "Step: 35500  \tTraining accuracy: 0.8774532079696655\n",
      "Step: 35500  \tValid loss: 0.26170191168785095\n",
      "Step: 35600  \tTraining loss: 0.27717533707618713\n",
      "Step: 35600  \tTraining accuracy: 0.8774620890617371\n",
      "Step: 35600  \tValid loss: 0.2616986036300659\n",
      "Step: 35700  \tTraining loss: 0.2771576941013336\n",
      "Step: 35700  \tTraining accuracy: 0.8774709701538086\n",
      "Step: 35700  \tValid loss: 0.2616875171661377\n",
      "Step: 35800  \tTraining loss: 0.2771398425102234\n",
      "Step: 35800  \tTraining accuracy: 0.8774794936180115\n",
      "Step: 35800  \tValid loss: 0.26169314980506897\n",
      "Step: 35900  \tTraining loss: 0.2771225571632385\n",
      "Step: 35900  \tTraining accuracy: 0.87748783826828\n",
      "Step: 35900  \tValid loss: 0.2616753876209259\n",
      "Step: 36000  \tTraining loss: 0.27710750699043274\n",
      "Step: 36000  \tTraining accuracy: 0.8774959444999695\n",
      "Step: 36000  \tValid loss: 0.2616645097732544\n",
      "Step: 36100  \tTraining loss: 0.2770880460739136\n",
      "Step: 36100  \tTraining accuracy: 0.8775038719177246\n",
      "Step: 36100  \tValid loss: 0.26165539026260376\n",
      "Step: 36200  \tTraining loss: 0.27707117795944214\n",
      "Step: 36200  \tTraining accuracy: 0.8775111436843872\n",
      "Step: 36200  \tValid loss: 0.26165300607681274\n",
      "Step: 36300  \tTraining loss: 0.27705296874046326\n",
      "Step: 36300  \tTraining accuracy: 0.8775184750556946\n",
      "Step: 36300  \tValid loss: 0.2616542875766754\n",
      "Step: 36400  \tTraining loss: 0.27703601121902466\n",
      "Step: 36400  \tTraining accuracy: 0.8775259852409363\n",
      "Step: 36400  \tValid loss: 0.26163846254348755\n",
      "Step: 36500  \tTraining loss: 0.2770192623138428\n",
      "Step: 36500  \tTraining accuracy: 0.8775334358215332\n",
      "Step: 36500  \tValid loss: 0.2616279125213623\n",
      "Step: 36600  \tTraining loss: 0.27700191736221313\n",
      "Step: 36600  \tTraining accuracy: 0.877540647983551\n",
      "Step: 36600  \tValid loss: 0.2616298496723175\n",
      "Step: 36700  \tTraining loss: 0.2769867777824402\n",
      "Step: 36700  \tTraining accuracy: 0.877547562122345\n",
      "Step: 36700  \tValid loss: 0.2616221606731415\n",
      "Step: 36800  \tTraining loss: 0.27696818113327026\n",
      "Step: 36800  \tTraining accuracy: 0.8775543570518494\n",
      "Step: 36800  \tValid loss: 0.2616283595561981\n",
      "Step: 36900  \tTraining loss: 0.2769519090652466\n",
      "Step: 36900  \tTraining accuracy: 0.8775609731674194\n",
      "Step: 36900  \tValid loss: 0.2616085112094879\n",
      "Step: 37000  \tTraining loss: 0.27693450450897217\n",
      "Step: 37000  \tTraining accuracy: 0.8775672316551208\n",
      "Step: 37000  \tValid loss: 0.26159971952438354\n",
      "Step: 37100  \tTraining loss: 0.2769183814525604\n",
      "Step: 37100  \tTraining accuracy: 0.8775734305381775\n",
      "Step: 37100  \tValid loss: 0.2615812420845032\n",
      "Step: 37200  \tTraining loss: 0.2768993675708771\n",
      "Step: 37200  \tTraining accuracy: 0.8775796294212341\n",
      "Step: 37200  \tValid loss: 0.2615871727466583\n",
      "Step: 37300  \tTraining loss: 0.2768843173980713\n",
      "Step: 37300  \tTraining accuracy: 0.8775857090950012\n",
      "Step: 37300  \tValid loss: 0.2615823745727539\n",
      "Step: 37400  \tTraining loss: 0.27686524391174316\n",
      "Step: 37400  \tTraining accuracy: 0.8775913119316101\n",
      "Step: 37400  \tValid loss: 0.26158007979393005\n",
      "Step: 37500  \tTraining loss: 0.27684879302978516\n",
      "Step: 37500  \tTraining accuracy: 0.877596914768219\n",
      "Step: 37500  \tValid loss: 0.2615610659122467\n",
      "Step: 37600  \tTraining loss: 0.2768315076828003\n",
      "Step: 37600  \tTraining accuracy: 0.8776019811630249\n",
      "Step: 37600  \tValid loss: 0.26158058643341064\n",
      "Step: 37700  \tTraining loss: 0.27681466937065125\n",
      "Step: 37700  \tTraining accuracy: 0.877606987953186\n",
      "Step: 37700  \tValid loss: 0.2615590989589691\n",
      "Step: 37800  \tTraining loss: 0.27679750323295593\n",
      "Step: 37800  \tTraining accuracy: 0.8776119947433472\n",
      "Step: 37800  \tValid loss: 0.26156193017959595\n",
      "Step: 37900  \tTraining loss: 0.2767822742462158\n",
      "Step: 37900  \tTraining accuracy: 0.8776170015335083\n",
      "Step: 37900  \tValid loss: 0.2615499496459961\n",
      "Step: 38000  \tTraining loss: 0.27676448225975037\n",
      "Step: 38000  \tTraining accuracy: 0.8776217699050903\n",
      "Step: 38000  \tValid loss: 0.26155146956443787\n",
      "Step: 38100  \tTraining loss: 0.27674660086631775\n",
      "Step: 38100  \tTraining accuracy: 0.8776264786720276\n",
      "Step: 38100  \tValid loss: 0.2615429162979126\n",
      "Step: 38200  \tTraining loss: 0.27672943472862244\n",
      "Step: 38200  \tTraining accuracy: 0.877631425857544\n",
      "Step: 38200  \tValid loss: 0.26153117418289185\n",
      "Step: 38300  \tTraining loss: 0.2767128050327301\n",
      "Step: 38300  \tTraining accuracy: 0.8776360750198364\n",
      "Step: 38300  \tValid loss: 0.26153838634490967\n",
      "Step: 38400  \tTraining loss: 0.276695191860199\n",
      "Step: 38400  \tTraining accuracy: 0.877640426158905\n",
      "Step: 38400  \tValid loss: 0.26153305172920227\n",
      "Step: 38500  \tTraining loss: 0.27667802572250366\n",
      "Step: 38500  \tTraining accuracy: 0.8776447772979736\n",
      "Step: 38500  \tValid loss: 0.2615257203578949\n",
      "Step: 38600  \tTraining loss: 0.276661217212677\n",
      "Step: 38600  \tTraining accuracy: 0.8776491284370422\n",
      "Step: 38600  \tValid loss: 0.26151883602142334\n",
      "Step: 38700  \tTraining loss: 0.2766450047492981\n",
      "Step: 38700  \tTraining accuracy: 0.8776534199714661\n",
      "Step: 38700  \tValid loss: 0.26150915026664734\n",
      "Step: 38800  \tTraining loss: 0.2766280174255371\n",
      "Step: 38800  \tTraining accuracy: 0.8776578903198242\n",
      "Step: 38800  \tValid loss: 0.2615126371383667\n",
      "Step: 38900  \tTraining loss: 0.2766094207763672\n",
      "Step: 38900  \tTraining accuracy: 0.8776625990867615\n",
      "Step: 38900  \tValid loss: 0.26150932908058167\n",
      "Step: 39000  \tTraining loss: 0.2765776216983795\n",
      "Step: 39000  \tTraining accuracy: 0.8776671290397644\n",
      "Step: 39000  \tValid loss: 0.2614964246749878\n",
      "Step: 39100  \tTraining loss: 0.2765274941921234\n",
      "Step: 39100  \tTraining accuracy: 0.8776710629463196\n",
      "Step: 39100  \tValid loss: 0.2614898979663849\n",
      "Step: 39200  \tTraining loss: 0.27650266885757446\n",
      "Step: 39200  \tTraining accuracy: 0.8776752352714539\n",
      "Step: 39200  \tValid loss: 0.261491984128952\n",
      "Step: 39300  \tTraining loss: 0.2764829993247986\n",
      "Step: 39300  \tTraining accuracy: 0.8776807188987732\n",
      "Step: 39300  \tValid loss: 0.2614978849887848\n",
      "Step: 39400  \tTraining loss: 0.27646419405937195\n",
      "Step: 39400  \tTraining accuracy: 0.8776863217353821\n",
      "Step: 39400  \tValid loss: 0.26149553060531616\n",
      "Step: 39500  \tTraining loss: 0.27644583582878113\n",
      "Step: 39500  \tTraining accuracy: 0.8776914477348328\n",
      "Step: 39500  \tValid loss: 0.26150524616241455\n",
      "Step: 39600  \tTraining loss: 0.2764282524585724\n",
      "Step: 39600  \tTraining accuracy: 0.8776963949203491\n",
      "Step: 39600  \tValid loss: 0.2614848017692566\n",
      "Step: 39700  \tTraining loss: 0.2764087915420532\n",
      "Step: 39700  \tTraining accuracy: 0.8777009844779968\n",
      "Step: 39700  \tValid loss: 0.2614907920360565\n",
      "Step: 39800  \tTraining loss: 0.2763917148113251\n",
      "Step: 39800  \tTraining accuracy: 0.8777055740356445\n",
      "Step: 39800  \tValid loss: 0.26147663593292236\n",
      "Step: 39900  \tTraining loss: 0.27637243270874023\n",
      "Step: 39900  \tTraining accuracy: 0.87771075963974\n",
      "Step: 39900  \tValid loss: 0.2614762485027313\n",
      "Step: 40000  \tTraining loss: 0.2763545513153076\n",
      "Step: 40000  \tTraining accuracy: 0.8777158856391907\n",
      "Step: 40000  \tValid loss: 0.2614617943763733\n",
      "Step: 40100  \tTraining loss: 0.2763376832008362\n",
      "Step: 40100  \tTraining accuracy: 0.8777207732200623\n",
      "Step: 40100  \tValid loss: 0.26145288348197937\n",
      "Step: 40200  \tTraining loss: 0.2763197422027588\n",
      "Step: 40200  \tTraining accuracy: 0.8777256608009338\n",
      "Step: 40200  \tValid loss: 0.26146259903907776\n",
      "Step: 40300  \tTraining loss: 0.27630001306533813\n",
      "Step: 40300  \tTraining accuracy: 0.8777300715446472\n",
      "Step: 40300  \tValid loss: 0.26144739985466003\n",
      "Step: 40400  \tTraining loss: 0.2762819528579712\n",
      "Step: 40400  \tTraining accuracy: 0.8777340650558472\n",
      "Step: 40400  \tValid loss: 0.2614477276802063\n",
      "Step: 40500  \tTraining loss: 0.276263564825058\n",
      "Step: 40500  \tTraining accuracy: 0.877737820148468\n",
      "Step: 40500  \tValid loss: 0.2614392340183258\n",
      "Step: 40600  \tTraining loss: 0.2762448191642761\n",
      "Step: 40600  \tTraining accuracy: 0.8777412176132202\n",
      "Step: 40600  \tValid loss: 0.26144710183143616\n",
      "Step: 40700  \tTraining loss: 0.2762269079685211\n",
      "Step: 40700  \tTraining accuracy: 0.8777446150779724\n",
      "Step: 40700  \tValid loss: 0.2614487409591675\n",
      "Step: 40800  \tTraining loss: 0.2762080132961273\n",
      "Step: 40800  \tTraining accuracy: 0.8777483701705933\n",
      "Step: 40800  \tValid loss: 0.2614227831363678\n",
      "Step: 40900  \tTraining loss: 0.2761900722980499\n",
      "Step: 40900  \tTraining accuracy: 0.8777518272399902\n",
      "Step: 40900  \tValid loss: 0.2614317238330841\n",
      "Step: 41000  \tTraining loss: 0.2761702537536621\n",
      "Step: 41000  \tTraining accuracy: 0.8777549862861633\n",
      "Step: 41000  \tValid loss: 0.2614377737045288\n",
      "Step: 41100  \tTraining loss: 0.27615147829055786\n",
      "Step: 41100  \tTraining accuracy: 0.8777578473091125\n",
      "Step: 41100  \tValid loss: 0.2614312171936035\n",
      "Step: 41200  \tTraining loss: 0.27613377571105957\n",
      "Step: 41200  \tTraining accuracy: 0.8777610659599304\n",
      "Step: 41200  \tValid loss: 0.26142212748527527\n",
      "Step: 41300  \tTraining loss: 0.276114284992218\n",
      "Step: 41300  \tTraining accuracy: 0.8777639269828796\n",
      "Step: 41300  \tValid loss: 0.26143020391464233\n",
      "Step: 41400  \tTraining loss: 0.27609527111053467\n",
      "Step: 41400  \tTraining accuracy: 0.8777663111686707\n",
      "Step: 41400  \tValid loss: 0.2614257037639618\n",
      "Step: 41500  \tTraining loss: 0.276077538728714\n",
      "Step: 41500  \tTraining accuracy: 0.877768874168396\n",
      "Step: 41500  \tValid loss: 0.26141253113746643\n",
      "Step: 41600  \tTraining loss: 0.2760583460330963\n",
      "Step: 41600  \tTraining accuracy: 0.8777716755867004\n",
      "Step: 41600  \tValid loss: 0.2614201605319977\n",
      "Step: 41700  \tTraining loss: 0.2760382294654846\n",
      "Step: 41700  \tTraining accuracy: 0.8777744770050049\n",
      "Step: 41700  \tValid loss: 0.261429101228714\n",
      "Step: 41800  \tTraining loss: 0.276020348072052\n",
      "Step: 41800  \tTraining accuracy: 0.8777772784233093\n",
      "Step: 41800  \tValid loss: 0.2614138126373291\n",
      "Step: 41900  \tTraining loss: 0.27600112557411194\n",
      "Step: 41900  \tTraining accuracy: 0.877780020236969\n",
      "Step: 41900  \tValid loss: 0.261415958404541\n",
      "Step: 42000  \tTraining loss: 0.2759808897972107\n",
      "Step: 42000  \tTraining accuracy: 0.87778240442276\n",
      "Step: 42000  \tValid loss: 0.261432409286499\n",
      "Step: 42100  \tTraining loss: 0.2759631276130676\n",
      "Step: 42100  \tTraining accuracy: 0.8777840733528137\n",
      "Step: 42100  \tValid loss: 0.2614101469516754\n",
      "Step: 42200  \tTraining loss: 0.2759425640106201\n",
      "Step: 42200  \tTraining accuracy: 0.8777862191200256\n",
      "Step: 42200  \tValid loss: 0.2614186108112335\n",
      "Step: 42300  \tTraining loss: 0.27592289447784424\n",
      "Step: 42300  \tTraining accuracy: 0.8777879476547241\n",
      "Step: 42300  \tValid loss: 0.2614205777645111\n",
      "Step: 42400  \tTraining loss: 0.27590394020080566\n",
      "Step: 42400  \tTraining accuracy: 0.877789318561554\n",
      "Step: 42400  \tValid loss: 0.26140567660331726\n",
      "Step: 42500  \tTraining loss: 0.27588382363319397\n",
      "Step: 42500  \tTraining accuracy: 0.8777905106544495\n",
      "Step: 42500  \tValid loss: 0.2614190876483917\n",
      "Step: 42600  \tTraining loss: 0.27586477994918823\n",
      "Step: 42600  \tTraining accuracy: 0.8777914643287659\n",
      "Step: 42600  \tValid loss: 0.2614099681377411\n",
      "Step: 42700  \tTraining loss: 0.2758452892303467\n",
      "Step: 42700  \tTraining accuracy: 0.8777923583984375\n",
      "Step: 42700  \tValid loss: 0.2614056468009949\n",
      "Step: 42800  \tTraining loss: 0.27582499384880066\n",
      "Step: 42800  \tTraining accuracy: 0.8777933120727539\n",
      "Step: 42800  \tValid loss: 0.2614116072654724\n",
      "Step: 42900  \tTraining loss: 0.27580395340919495\n",
      "Step: 42900  \tTraining accuracy: 0.8777943849563599\n",
      "Step: 42900  \tValid loss: 0.26142460107803345\n",
      "Step: 43000  \tTraining loss: 0.2757842540740967\n",
      "Step: 43000  \tTraining accuracy: 0.8777957558631897\n",
      "Step: 43000  \tValid loss: 0.26141104102134705\n",
      "Step: 43100  \tTraining loss: 0.2757647931575775\n",
      "Step: 43100  \tTraining accuracy: 0.8777970671653748\n",
      "Step: 43100  \tValid loss: 0.2614103853702545\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.87779844\n",
      "Precision: 0.8955391\n",
      "Recall: 0.8811227\n",
      "F1 score: 0.8706756\n",
      "AUC: 0.8790495\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.877798   0.895539  0.881123  0.870676  0.879049  0.275762      0.877797   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.261395        0.87779   0.317834      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  43108.0  \n",
      "9\n",
      "(3770, 8)\n",
      "(3770, 1)\n",
      "(2000, 8)\n",
      "(2000, 1)\n",
      "(1625, 8)\n",
      "(1625, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6192120313644409\n",
      "Step: 100  \tTraining accuracy: 0.7456233501434326\n",
      "Step: 100  \tValid loss: 0.6302227973937988\n",
      "Step: 200  \tTraining loss: 0.5858554244041443\n",
      "Step: 200  \tTraining accuracy: 0.7565606832504272\n",
      "Step: 200  \tValid loss: 0.6077165603637695\n",
      "Step: 300  \tTraining loss: 0.5655428767204285\n",
      "Step: 300  \tTraining accuracy: 0.7643857598304749\n",
      "Step: 300  \tValid loss: 0.5877016186714172\n",
      "Step: 400  \tTraining loss: 0.5447300672531128\n",
      "Step: 400  \tTraining accuracy: 0.7688306570053101\n",
      "Step: 400  \tValid loss: 0.566424548625946\n",
      "Step: 500  \tTraining loss: 0.5283345580101013\n",
      "Step: 500  \tTraining accuracy: 0.7715142369270325\n",
      "Step: 500  \tValid loss: 0.549199104309082\n",
      "Step: 600  \tTraining loss: 0.5146909952163696\n",
      "Step: 600  \tTraining accuracy: 0.7732236981391907\n",
      "Step: 600  \tValid loss: 0.5340457558631897\n",
      "Step: 700  \tTraining loss: 0.5050150752067566\n",
      "Step: 700  \tTraining accuracy: 0.7744079828262329\n",
      "Step: 700  \tValid loss: 0.5236628651618958\n",
      "Step: 800  \tTraining loss: 0.4959720969200134\n",
      "Step: 800  \tTraining accuracy: 0.7752768397331238\n",
      "Step: 800  \tValid loss: 0.5166118144989014\n",
      "Step: 900  \tTraining loss: 0.48738792538642883\n",
      "Step: 900  \tTraining accuracy: 0.7759415507316589\n",
      "Step: 900  \tValid loss: 0.5102656483650208\n",
      "Step: 1000  \tTraining loss: 0.4787193238735199\n",
      "Step: 1000  \tTraining accuracy: 0.7765944004058838\n",
      "Step: 1000  \tValid loss: 0.5022275447845459\n",
      "Step: 1100  \tTraining loss: 0.47054457664489746\n",
      "Step: 1100  \tTraining accuracy: 0.7779078483581543\n",
      "Step: 1100  \tValid loss: 0.4948499798774719\n",
      "Step: 1200  \tTraining loss: 0.46341758966445923\n",
      "Step: 1200  \tTraining accuracy: 0.7795453071594238\n",
      "Step: 1200  \tValid loss: 0.48790305852890015\n",
      "Step: 1300  \tTraining loss: 0.45837685465812683\n",
      "Step: 1300  \tTraining accuracy: 0.7810074687004089\n",
      "Step: 1300  \tValid loss: 0.48345980048179626\n",
      "Step: 1400  \tTraining loss: 0.45480743050575256\n",
      "Step: 1400  \tTraining accuracy: 0.7822731733322144\n",
      "Step: 1400  \tValid loss: 0.4803106188774109\n",
      "Step: 1500  \tTraining loss: 0.45195111632347107\n",
      "Step: 1500  \tTraining accuracy: 0.7834669351577759\n",
      "Step: 1500  \tValid loss: 0.4767535924911499\n",
      "Step: 1600  \tTraining loss: 0.44965076446533203\n",
      "Step: 1600  \tTraining accuracy: 0.7844631671905518\n",
      "Step: 1600  \tValid loss: 0.47461551427841187\n",
      "Step: 1700  \tTraining loss: 0.446942001581192\n",
      "Step: 1700  \tTraining accuracy: 0.7853468656539917\n",
      "Step: 1700  \tValid loss: 0.47170621156692505\n",
      "Step: 1800  \tTraining loss: 0.4448893964290619\n",
      "Step: 1800  \tTraining accuracy: 0.7862609624862671\n",
      "Step: 1800  \tValid loss: 0.4700726866722107\n",
      "Step: 1900  \tTraining loss: 0.4429594576358795\n",
      "Step: 1900  \tTraining accuracy: 0.7871201038360596\n",
      "Step: 1900  \tValid loss: 0.46859410405158997\n",
      "Step: 2000  \tTraining loss: 0.441646546125412\n",
      "Step: 2000  \tTraining accuracy: 0.7878704071044922\n",
      "Step: 2000  \tValid loss: 0.4674838185310364\n",
      "Step: 2100  \tTraining loss: 0.440500944852829\n",
      "Step: 2100  \tTraining accuracy: 0.7885211110115051\n",
      "Step: 2100  \tValid loss: 0.46661117672920227\n",
      "Step: 2200  \tTraining loss: 0.4394412636756897\n",
      "Step: 2200  \tTraining accuracy: 0.789092481136322\n",
      "Step: 2200  \tValid loss: 0.4655870199203491\n",
      "Step: 2300  \tTraining loss: 0.4384634792804718\n",
      "Step: 2300  \tTraining accuracy: 0.7896371483802795\n",
      "Step: 2300  \tValid loss: 0.4647102653980255\n",
      "Step: 2400  \tTraining loss: 0.4375368654727936\n",
      "Step: 2400  \tTraining accuracy: 0.79017573595047\n",
      "Step: 2400  \tValid loss: 0.4636979401111603\n",
      "Step: 2500  \tTraining loss: 0.436619371175766\n",
      "Step: 2500  \tTraining accuracy: 0.7907145023345947\n",
      "Step: 2500  \tValid loss: 0.46262145042419434\n",
      "Step: 2600  \tTraining loss: 0.4357156455516815\n",
      "Step: 2600  \tTraining accuracy: 0.7912269234657288\n",
      "Step: 2600  \tValid loss: 0.4616507887840271\n",
      "Step: 2700  \tTraining loss: 0.43479570746421814\n",
      "Step: 2700  \tTraining accuracy: 0.7917006611824036\n",
      "Step: 2700  \tValid loss: 0.4606774151325226\n",
      "Step: 2800  \tTraining loss: 0.4337620139122009\n",
      "Step: 2800  \tTraining accuracy: 0.7921448945999146\n",
      "Step: 2800  \tValid loss: 0.4595804512500763\n",
      "Step: 2900  \tTraining loss: 0.4323681592941284\n",
      "Step: 2900  \tTraining accuracy: 0.7925674915313721\n",
      "Step: 2900  \tValid loss: 0.45739930868148804\n",
      "Step: 3000  \tTraining loss: 0.4312949776649475\n",
      "Step: 3000  \tTraining accuracy: 0.7930071949958801\n",
      "Step: 3000  \tValid loss: 0.45600879192352295\n",
      "Step: 3100  \tTraining loss: 0.43015849590301514\n",
      "Step: 3100  \tTraining accuracy: 0.7934136986732483\n",
      "Step: 3100  \tValid loss: 0.45439356565475464\n",
      "Step: 3200  \tTraining loss: 0.4291991591453552\n",
      "Step: 3200  \tTraining accuracy: 0.7937986850738525\n",
      "Step: 3200  \tValid loss: 0.4531092643737793\n",
      "Step: 3300  \tTraining loss: 0.4282757043838501\n",
      "Step: 3300  \tTraining accuracy: 0.7941641211509705\n",
      "Step: 3300  \tValid loss: 0.4519835412502289\n",
      "Step: 3400  \tTraining loss: 0.42745891213417053\n",
      "Step: 3400  \tTraining accuracy: 0.7945037484169006\n",
      "Step: 3400  \tValid loss: 0.45097771286964417\n",
      "Step: 3500  \tTraining loss: 0.42670881748199463\n",
      "Step: 3500  \tTraining accuracy: 0.7948315143585205\n",
      "Step: 3500  \tValid loss: 0.4500483274459839\n",
      "Step: 3600  \tTraining loss: 0.42600640654563904\n",
      "Step: 3600  \tTraining accuracy: 0.7951560616493225\n",
      "Step: 3600  \tValid loss: 0.449243426322937\n",
      "Step: 3700  \tTraining loss: 0.42533302307128906\n",
      "Step: 3700  \tTraining accuracy: 0.7954701781272888\n",
      "Step: 3700  \tValid loss: 0.4485316276550293\n",
      "Step: 3800  \tTraining loss: 0.42464128136634827\n",
      "Step: 3800  \tTraining accuracy: 0.7957928776741028\n",
      "Step: 3800  \tValid loss: 0.4478607475757599\n",
      "Step: 3900  \tTraining loss: 0.4239496886730194\n",
      "Step: 3900  \tTraining accuracy: 0.7961092591285706\n",
      "Step: 3900  \tValid loss: 0.44729089736938477\n",
      "Step: 4000  \tTraining loss: 0.423334002494812\n",
      "Step: 4000  \tTraining accuracy: 0.7964233756065369\n",
      "Step: 4000  \tValid loss: 0.4466921389102936\n",
      "Step: 4100  \tTraining loss: 0.4227742850780487\n",
      "Step: 4100  \tTraining accuracy: 0.7967453598976135\n",
      "Step: 4100  \tValid loss: 0.44618719816207886\n",
      "Step: 4200  \tTraining loss: 0.42222556471824646\n",
      "Step: 4200  \tTraining accuracy: 0.7970713376998901\n",
      "Step: 4200  \tValid loss: 0.44581717252731323\n",
      "Step: 4300  \tTraining loss: 0.42170071601867676\n",
      "Step: 4300  \tTraining accuracy: 0.7974074482917786\n",
      "Step: 4300  \tValid loss: 0.44542646408081055\n",
      "Step: 4400  \tTraining loss: 0.4211971163749695\n",
      "Step: 4400  \tTraining accuracy: 0.7977156639099121\n",
      "Step: 4400  \tValid loss: 0.4450303912162781\n",
      "Step: 4500  \tTraining loss: 0.4207071363925934\n",
      "Step: 4500  \tTraining accuracy: 0.7980100512504578\n",
      "Step: 4500  \tValid loss: 0.44469738006591797\n",
      "Step: 4600  \tTraining loss: 0.4202047884464264\n",
      "Step: 4600  \tTraining accuracy: 0.7983033657073975\n",
      "Step: 4600  \tValid loss: 0.4443095624446869\n",
      "Step: 4700  \tTraining loss: 0.419547438621521\n",
      "Step: 4700  \tTraining accuracy: 0.7985898852348328\n",
      "Step: 4700  \tValid loss: 0.44395458698272705\n",
      "Step: 4800  \tTraining loss: 0.4189673364162445\n",
      "Step: 4800  \tTraining accuracy: 0.7988443970680237\n",
      "Step: 4800  \tValid loss: 0.4435456395149231\n",
      "Step: 4900  \tTraining loss: 0.41813135147094727\n",
      "Step: 4900  \tTraining accuracy: 0.79908287525177\n",
      "Step: 4900  \tValid loss: 0.44279253482818604\n",
      "Step: 5000  \tTraining loss: 0.4175865352153778\n",
      "Step: 5000  \tTraining accuracy: 0.7993089556694031\n",
      "Step: 5000  \tValid loss: 0.4425674378871918\n",
      "Step: 5100  \tTraining loss: 0.4171276390552521\n",
      "Step: 5100  \tTraining accuracy: 0.7995368242263794\n",
      "Step: 5100  \tValid loss: 0.4424072802066803\n",
      "Step: 5200  \tTraining loss: 0.41668128967285156\n",
      "Step: 5200  \tTraining accuracy: 0.7997637391090393\n",
      "Step: 5200  \tValid loss: 0.4421536326408386\n",
      "Step: 5300  \tTraining loss: 0.4162561595439911\n",
      "Step: 5300  \tTraining accuracy: 0.799974262714386\n",
      "Step: 5300  \tValid loss: 0.4419806897640228\n",
      "Step: 5400  \tTraining loss: 0.4158613979816437\n",
      "Step: 5400  \tTraining accuracy: 0.8001920580863953\n",
      "Step: 5400  \tValid loss: 0.4417518973350525\n",
      "Step: 5500  \tTraining loss: 0.4154907464981079\n",
      "Step: 5500  \tTraining accuracy: 0.8003969192504883\n",
      "Step: 5500  \tValid loss: 0.44152459502220154\n",
      "Step: 5600  \tTraining loss: 0.4151185154914856\n",
      "Step: 5600  \tTraining accuracy: 0.8005700707435608\n",
      "Step: 5600  \tValid loss: 0.4413544535636902\n",
      "Step: 5700  \tTraining loss: 0.41477951407432556\n",
      "Step: 5700  \tTraining accuracy: 0.8007466197013855\n",
      "Step: 5700  \tValid loss: 0.4411809742450714\n",
      "Step: 5800  \tTraining loss: 0.41447150707244873\n",
      "Step: 5800  \tTraining accuracy: 0.8009193539619446\n",
      "Step: 5800  \tValid loss: 0.44107311964035034\n",
      "Step: 5900  \tTraining loss: 0.41415297985076904\n",
      "Step: 5900  \tTraining accuracy: 0.8010908961296082\n",
      "Step: 5900  \tValid loss: 0.4409009516239166\n",
      "Step: 6000  \tTraining loss: 0.4138478934764862\n",
      "Step: 6000  \tTraining accuracy: 0.8012679815292358\n",
      "Step: 6000  \tValid loss: 0.44073304533958435\n",
      "Step: 6100  \tTraining loss: 0.4135081470012665\n",
      "Step: 6100  \tTraining accuracy: 0.8014347553253174\n",
      "Step: 6100  \tValid loss: 0.44060394167900085\n",
      "Step: 6200  \tTraining loss: 0.4131750762462616\n",
      "Step: 6200  \tTraining accuracy: 0.801600456237793\n",
      "Step: 6200  \tValid loss: 0.4404100775718689\n",
      "Step: 6300  \tTraining loss: 0.412864625453949\n",
      "Step: 6300  \tTraining accuracy: 0.8017652630805969\n",
      "Step: 6300  \tValid loss: 0.4402674436569214\n",
      "Step: 6400  \tTraining loss: 0.41256821155548096\n",
      "Step: 6400  \tTraining accuracy: 0.8019333481788635\n",
      "Step: 6400  \tValid loss: 0.4401847720146179\n",
      "Step: 6500  \tTraining loss: 0.412278950214386\n",
      "Step: 6500  \tTraining accuracy: 0.8020920157432556\n",
      "Step: 6500  \tValid loss: 0.4401273727416992\n",
      "Step: 6600  \tTraining loss: 0.41197460889816284\n",
      "Step: 6600  \tTraining accuracy: 0.8022520542144775\n",
      "Step: 6600  \tValid loss: 0.4400065839290619\n",
      "Step: 6700  \tTraining loss: 0.41169023513793945\n",
      "Step: 6700  \tTraining accuracy: 0.8024194836616516\n",
      "Step: 6700  \tValid loss: 0.4398946762084961\n",
      "Step: 6800  \tTraining loss: 0.4114135801792145\n",
      "Step: 6800  \tTraining accuracy: 0.8025819659233093\n",
      "Step: 6800  \tValid loss: 0.4397934675216675\n",
      "Step: 6900  \tTraining loss: 0.4111385941505432\n",
      "Step: 6900  \tTraining accuracy: 0.8027475476264954\n",
      "Step: 6900  \tValid loss: 0.43963518738746643\n",
      "Step: 7000  \tTraining loss: 0.4108813405036926\n",
      "Step: 7000  \tTraining accuracy: 0.8029084205627441\n",
      "Step: 7000  \tValid loss: 0.4396066963672638\n",
      "Step: 7100  \tTraining loss: 0.4106285870075226\n",
      "Step: 7100  \tTraining accuracy: 0.8030627965927124\n",
      "Step: 7100  \tValid loss: 0.4394286870956421\n",
      "Step: 7200  \tTraining loss: 0.4103543758392334\n",
      "Step: 7200  \tTraining accuracy: 0.8032185435295105\n",
      "Step: 7200  \tValid loss: 0.43917882442474365\n",
      "Step: 7300  \tTraining loss: 0.4100806415081024\n",
      "Step: 7300  \tTraining accuracy: 0.803371787071228\n",
      "Step: 7300  \tValid loss: 0.43903788924217224\n",
      "Step: 7400  \tTraining loss: 0.4098241329193115\n",
      "Step: 7400  \tTraining accuracy: 0.8035080432891846\n",
      "Step: 7400  \tValid loss: 0.43887147307395935\n",
      "Step: 7500  \tTraining loss: 0.40955498814582825\n",
      "Step: 7500  \tTraining accuracy: 0.8036406636238098\n",
      "Step: 7500  \tValid loss: 0.4386383295059204\n",
      "Step: 7600  \tTraining loss: 0.409305214881897\n",
      "Step: 7600  \tTraining accuracy: 0.8037751317024231\n",
      "Step: 7600  \tValid loss: 0.43844276666641235\n",
      "Step: 7700  \tTraining loss: 0.40900933742523193\n",
      "Step: 7700  \tTraining accuracy: 0.8039095997810364\n",
      "Step: 7700  \tValid loss: 0.4378560185432434\n",
      "Step: 7800  \tTraining loss: 0.4087374806404114\n",
      "Step: 7800  \tTraining accuracy: 0.8040632605552673\n",
      "Step: 7800  \tValid loss: 0.43768224120140076\n",
      "Step: 7900  \tTraining loss: 0.4084862470626831\n",
      "Step: 7900  \tTraining accuracy: 0.8042095899581909\n",
      "Step: 7900  \tValid loss: 0.43749818205833435\n",
      "Step: 8000  \tTraining loss: 0.40823906660079956\n",
      "Step: 8000  \tTraining accuracy: 0.8043573498725891\n",
      "Step: 8000  \tValid loss: 0.43729013204574585\n",
      "Step: 8100  \tTraining loss: 0.4080018997192383\n",
      "Step: 8100  \tTraining accuracy: 0.804498016834259\n",
      "Step: 8100  \tValid loss: 0.43714639544487\n",
      "Step: 8200  \tTraining loss: 0.40773290395736694\n",
      "Step: 8200  \tTraining accuracy: 0.8046369552612305\n",
      "Step: 8200  \tValid loss: 0.4369620978832245\n",
      "Step: 8300  \tTraining loss: 0.40735188126564026\n",
      "Step: 8300  \tTraining accuracy: 0.8047757744789124\n",
      "Step: 8300  \tValid loss: 0.4368194043636322\n",
      "Step: 8400  \tTraining loss: 0.40700605511665344\n",
      "Step: 8400  \tTraining accuracy: 0.8049015998840332\n",
      "Step: 8400  \tValid loss: 0.4366350769996643\n",
      "Step: 8500  \tTraining loss: 0.4066671133041382\n",
      "Step: 8500  \tTraining accuracy: 0.8050147891044617\n",
      "Step: 8500  \tValid loss: 0.43644121289253235\n",
      "Step: 8600  \tTraining loss: 0.4063616991043091\n",
      "Step: 8600  \tTraining accuracy: 0.805126965045929\n",
      "Step: 8600  \tValid loss: 0.4362125098705292\n",
      "Step: 8700  \tTraining loss: 0.40607020258903503\n",
      "Step: 8700  \tTraining accuracy: 0.8052411675453186\n",
      "Step: 8700  \tValid loss: 0.435970276594162\n",
      "Step: 8800  \tTraining loss: 0.4057924151420593\n",
      "Step: 8800  \tTraining accuracy: 0.8053574562072754\n",
      "Step: 8800  \tValid loss: 0.4357469975948334\n",
      "Step: 8900  \tTraining loss: 0.4055355191230774\n",
      "Step: 8900  \tTraining accuracy: 0.8054757118225098\n",
      "Step: 8900  \tValid loss: 0.43551522493362427\n",
      "Step: 9000  \tTraining loss: 0.405294269323349\n",
      "Step: 9000  \tTraining accuracy: 0.805594265460968\n",
      "Step: 9000  \tValid loss: 0.4353494942188263\n",
      "Step: 9100  \tTraining loss: 0.40506502985954285\n",
      "Step: 9100  \tTraining accuracy: 0.8057102560997009\n",
      "Step: 9100  \tValid loss: 0.43511682748794556\n",
      "Step: 9200  \tTraining loss: 0.4048486649990082\n",
      "Step: 9200  \tTraining accuracy: 0.8058266639709473\n",
      "Step: 9200  \tValid loss: 0.4348975718021393\n",
      "Step: 9300  \tTraining loss: 0.40464040637016296\n",
      "Step: 9300  \tTraining accuracy: 0.8059449791908264\n",
      "Step: 9300  \tValid loss: 0.43473780155181885\n",
      "Step: 9400  \tTraining loss: 0.4044341444969177\n",
      "Step: 9400  \tTraining accuracy: 0.8060592412948608\n",
      "Step: 9400  \tValid loss: 0.43449148535728455\n",
      "Step: 9500  \tTraining loss: 0.4042346775531769\n",
      "Step: 9500  \tTraining accuracy: 0.8061725497245789\n",
      "Step: 9500  \tValid loss: 0.4342840015888214\n",
      "Step: 9600  \tTraining loss: 0.40404340624809265\n",
      "Step: 9600  \tTraining accuracy: 0.8062834739685059\n",
      "Step: 9600  \tValid loss: 0.4340590834617615\n",
      "Step: 9700  \tTraining loss: 0.40385326743125916\n",
      "Step: 9700  \tTraining accuracy: 0.8063977360725403\n",
      "Step: 9700  \tValid loss: 0.43386608362197876\n",
      "Step: 9800  \tTraining loss: 0.4036668837070465\n",
      "Step: 9800  \tTraining accuracy: 0.8065110445022583\n",
      "Step: 9800  \tValid loss: 0.4335920214653015\n",
      "Step: 9900  \tTraining loss: 0.4034889340400696\n",
      "Step: 9900  \tTraining accuracy: 0.8066220283508301\n",
      "Step: 9900  \tValid loss: 0.43339595198631287\n",
      "Step: 10000  \tTraining loss: 0.4033082127571106\n",
      "Step: 10000  \tTraining accuracy: 0.8067334890365601\n",
      "Step: 10000  \tValid loss: 0.4331530034542084\n",
      "Step: 10100  \tTraining loss: 0.4031331539154053\n",
      "Step: 10100  \tTraining accuracy: 0.8068386912345886\n",
      "Step: 10100  \tValid loss: 0.43293580412864685\n",
      "Step: 10200  \tTraining loss: 0.40295758843421936\n",
      "Step: 10200  \tTraining accuracy: 0.806936502456665\n",
      "Step: 10200  \tValid loss: 0.43269461393356323\n",
      "Step: 10300  \tTraining loss: 0.40278854966163635\n",
      "Step: 10300  \tTraining accuracy: 0.8070337176322937\n",
      "Step: 10300  \tValid loss: 0.4324781894683838\n",
      "Step: 10400  \tTraining loss: 0.4026198387145996\n",
      "Step: 10400  \tTraining accuracy: 0.8071212768554688\n",
      "Step: 10400  \tValid loss: 0.43223047256469727\n",
      "Step: 10500  \tTraining loss: 0.4024535119533539\n",
      "Step: 10500  \tTraining accuracy: 0.8072083592414856\n",
      "Step: 10500  \tValid loss: 0.43194788694381714\n",
      "Step: 10600  \tTraining loss: 0.4022815525531769\n",
      "Step: 10600  \tTraining accuracy: 0.807299017906189\n",
      "Step: 10600  \tValid loss: 0.4317070245742798\n",
      "Step: 10700  \tTraining loss: 0.40211230516433716\n",
      "Step: 10700  \tTraining accuracy: 0.8073866367340088\n",
      "Step: 10700  \tValid loss: 0.43147391080856323\n",
      "Step: 10800  \tTraining loss: 0.4019466042518616\n",
      "Step: 10800  \tTraining accuracy: 0.8074650764465332\n",
      "Step: 10800  \tValid loss: 0.4312032461166382\n",
      "Step: 10900  \tTraining loss: 0.4017984867095947\n",
      "Step: 10900  \tTraining accuracy: 0.8075445890426636\n",
      "Step: 10900  \tValid loss: 0.43102389574050903\n",
      "Step: 11000  \tTraining loss: 0.40161368250846863\n",
      "Step: 11000  \tTraining accuracy: 0.807617723941803\n",
      "Step: 11000  \tValid loss: 0.43070852756500244\n",
      "Step: 11100  \tTraining loss: 0.4014575481414795\n",
      "Step: 11100  \tTraining accuracy: 0.8076870441436768\n",
      "Step: 11100  \tValid loss: 0.43051353096961975\n",
      "Step: 11200  \tTraining loss: 0.4012814164161682\n",
      "Step: 11200  \tTraining accuracy: 0.807751476764679\n",
      "Step: 11200  \tValid loss: 0.4301976263523102\n",
      "Step: 11300  \tTraining loss: 0.40111956000328064\n",
      "Step: 11300  \tTraining accuracy: 0.8078196048736572\n",
      "Step: 11300  \tValid loss: 0.42994555830955505\n",
      "Step: 11400  \tTraining loss: 0.4009511172771454\n",
      "Step: 11400  \tTraining accuracy: 0.80788654088974\n",
      "Step: 11400  \tValid loss: 0.4296768605709076\n",
      "Step: 11500  \tTraining loss: 0.40078267455101013\n",
      "Step: 11500  \tTraining accuracy: 0.8079522848129272\n",
      "Step: 11500  \tValid loss: 0.4294201135635376\n",
      "Step: 11600  \tTraining loss: 0.4006366431713104\n",
      "Step: 11600  \tTraining accuracy: 0.8080168962478638\n",
      "Step: 11600  \tValid loss: 0.4291490614414215\n",
      "Step: 11700  \tTraining loss: 0.4004442095756531\n",
      "Step: 11700  \tTraining accuracy: 0.8080804347991943\n",
      "Step: 11700  \tValid loss: 0.4289063513278961\n",
      "Step: 11800  \tTraining loss: 0.40028730034828186\n",
      "Step: 11800  \tTraining accuracy: 0.8081428408622742\n",
      "Step: 11800  \tValid loss: 0.428707093000412\n",
      "Step: 11900  \tTraining loss: 0.40010756254196167\n",
      "Step: 11900  \tTraining accuracy: 0.8082064986228943\n",
      "Step: 11900  \tValid loss: 0.4284118711948395\n",
      "Step: 12000  \tTraining loss: 0.3999394178390503\n",
      "Step: 12000  \tTraining accuracy: 0.8082758784294128\n",
      "Step: 12000  \tValid loss: 0.42808347940444946\n",
      "Step: 12100  \tTraining loss: 0.3997705280780792\n",
      "Step: 12100  \tTraining accuracy: 0.8083463311195374\n",
      "Step: 12100  \tValid loss: 0.4279034435749054\n",
      "Step: 12200  \tTraining loss: 0.3996022641658783\n",
      "Step: 12200  \tTraining accuracy: 0.8084156513214111\n",
      "Step: 12200  \tValid loss: 0.4276587665081024\n",
      "Step: 12300  \tTraining loss: 0.3994397819042206\n",
      "Step: 12300  \tTraining accuracy: 0.8084838390350342\n",
      "Step: 12300  \tValid loss: 0.4273565411567688\n",
      "Step: 12400  \tTraining loss: 0.39926251769065857\n",
      "Step: 12400  \tTraining accuracy: 0.8085508942604065\n",
      "Step: 12400  \tValid loss: 0.4271845817565918\n",
      "Step: 12500  \tTraining loss: 0.3991089463233948\n",
      "Step: 12500  \tTraining accuracy: 0.8086168766021729\n",
      "Step: 12500  \tValid loss: 0.42692506313323975\n",
      "Step: 12600  \tTraining loss: 0.3989420533180237\n",
      "Step: 12600  \tTraining accuracy: 0.808681845664978\n",
      "Step: 12600  \tValid loss: 0.42680251598358154\n",
      "Step: 12700  \tTraining loss: 0.39875268936157227\n",
      "Step: 12700  \tTraining accuracy: 0.8087468147277832\n",
      "Step: 12700  \tValid loss: 0.42656293511390686\n",
      "Step: 12800  \tTraining loss: 0.3985845148563385\n",
      "Step: 12800  \tTraining accuracy: 0.8088139891624451\n",
      "Step: 12800  \tValid loss: 0.42634129524230957\n",
      "Step: 12900  \tTraining loss: 0.3984116017818451\n",
      "Step: 12900  \tTraining accuracy: 0.8088758587837219\n",
      "Step: 12900  \tValid loss: 0.4260987937450409\n",
      "Step: 13000  \tTraining loss: 0.3982468545436859\n",
      "Step: 13000  \tTraining accuracy: 0.8089367747306824\n",
      "Step: 13000  \tValid loss: 0.42593875527381897\n",
      "Step: 13100  \tTraining loss: 0.3980812430381775\n",
      "Step: 13100  \tTraining accuracy: 0.8089967966079712\n",
      "Step: 13100  \tValid loss: 0.4257156252861023\n",
      "Step: 13200  \tTraining loss: 0.39792999625205994\n",
      "Step: 13200  \tTraining accuracy: 0.8090579509735107\n",
      "Step: 13200  \tValid loss: 0.4254923164844513\n",
      "Step: 13300  \tTraining loss: 0.3977765738964081\n",
      "Step: 13300  \tTraining accuracy: 0.8091212511062622\n",
      "Step: 13300  \tValid loss: 0.4252917468547821\n",
      "Step: 13400  \tTraining loss: 0.3975941836833954\n",
      "Step: 13400  \tTraining accuracy: 0.8091835975646973\n",
      "Step: 13400  \tValid loss: 0.425090491771698\n",
      "Step: 13500  \tTraining loss: 0.3974493145942688\n",
      "Step: 13500  \tTraining accuracy: 0.8092449903488159\n",
      "Step: 13500  \tValid loss: 0.4249034523963928\n",
      "Step: 13600  \tTraining loss: 0.3972795009613037\n",
      "Step: 13600  \tTraining accuracy: 0.8093035221099854\n",
      "Step: 13600  \tValid loss: 0.42467203736305237\n",
      "Step: 13700  \tTraining loss: 0.39712780714035034\n",
      "Step: 13700  \tTraining accuracy: 0.8093581795692444\n",
      "Step: 13700  \tValid loss: 0.4244730472564697\n",
      "Step: 13800  \tTraining loss: 0.3969637453556061\n",
      "Step: 13800  \tTraining accuracy: 0.8094120621681213\n",
      "Step: 13800  \tValid loss: 0.4243113696575165\n",
      "Step: 13900  \tTraining loss: 0.3968367278575897\n",
      "Step: 13900  \tTraining accuracy: 0.8094651699066162\n",
      "Step: 13900  \tValid loss: 0.4241258502006531\n",
      "Step: 14000  \tTraining loss: 0.39667391777038574\n",
      "Step: 14000  \tTraining accuracy: 0.809517502784729\n",
      "Step: 14000  \tValid loss: 0.42385241389274597\n",
      "Step: 14100  \tTraining loss: 0.39650800824165344\n",
      "Step: 14100  \tTraining accuracy: 0.8095691204071045\n",
      "Step: 14100  \tValid loss: 0.4236376881599426\n",
      "Step: 14200  \tTraining loss: 0.3963530361652374\n",
      "Step: 14200  \tTraining accuracy: 0.809616208076477\n",
      "Step: 14200  \tValid loss: 0.4234810173511505\n",
      "Step: 14300  \tTraining loss: 0.3962040841579437\n",
      "Step: 14300  \tTraining accuracy: 0.8096654415130615\n",
      "Step: 14300  \tValid loss: 0.423239529132843\n",
      "Step: 14400  \tTraining loss: 0.3960418105125427\n",
      "Step: 14400  \tTraining accuracy: 0.8097149133682251\n",
      "Step: 14400  \tValid loss: 0.4230397045612335\n",
      "Step: 14500  \tTraining loss: 0.3958778977394104\n",
      "Step: 14500  \tTraining accuracy: 0.8097665309906006\n",
      "Step: 14500  \tValid loss: 0.42284324765205383\n",
      "Step: 14600  \tTraining loss: 0.39569658041000366\n",
      "Step: 14600  \tTraining accuracy: 0.8098192811012268\n",
      "Step: 14600  \tValid loss: 0.4226672351360321\n",
      "Step: 14700  \tTraining loss: 0.3955441415309906\n",
      "Step: 14700  \tTraining accuracy: 0.8098713159561157\n",
      "Step: 14700  \tValid loss: 0.422529935836792\n",
      "Step: 14800  \tTraining loss: 0.39535507559776306\n",
      "Step: 14800  \tTraining accuracy: 0.8099235892295837\n",
      "Step: 14800  \tValid loss: 0.42236724495887756\n",
      "Step: 14900  \tTraining loss: 0.39520972967147827\n",
      "Step: 14900  \tTraining accuracy: 0.8099787831306458\n",
      "Step: 14900  \tValid loss: 0.42221400141716003\n",
      "Step: 15000  \tTraining loss: 0.3950643837451935\n",
      "Step: 15000  \tTraining accuracy: 0.8100332617759705\n",
      "Step: 15000  \tValid loss: 0.4219817519187927\n",
      "Step: 15100  \tTraining loss: 0.3949202597141266\n",
      "Step: 15100  \tTraining accuracy: 0.8100869655609131\n",
      "Step: 15100  \tValid loss: 0.4217979311943054\n",
      "Step: 15200  \tTraining loss: 0.39477527141571045\n",
      "Step: 15200  \tTraining accuracy: 0.8101400136947632\n",
      "Step: 15200  \tValid loss: 0.4216821491718292\n",
      "Step: 15300  \tTraining loss: 0.3946402966976166\n",
      "Step: 15300  \tTraining accuracy: 0.810192346572876\n",
      "Step: 15300  \tValid loss: 0.4215177893638611\n",
      "Step: 15400  \tTraining loss: 0.39450910687446594\n",
      "Step: 15400  \tTraining accuracy: 0.8102439641952515\n",
      "Step: 15400  \tValid loss: 0.421317458152771\n",
      "Step: 15500  \tTraining loss: 0.3943822383880615\n",
      "Step: 15500  \tTraining accuracy: 0.8102949261665344\n",
      "Step: 15500  \tValid loss: 0.42122766375541687\n",
      "Step: 15600  \tTraining loss: 0.39425888657569885\n",
      "Step: 15600  \tTraining accuracy: 0.8103452920913696\n",
      "Step: 15600  \tValid loss: 0.4210413098335266\n",
      "Step: 15700  \tTraining loss: 0.39413031935691833\n",
      "Step: 15700  \tTraining accuracy: 0.8103949427604675\n",
      "Step: 15700  \tValid loss: 0.420960396528244\n",
      "Step: 15800  \tTraining loss: 0.39403724670410156\n",
      "Step: 15800  \tTraining accuracy: 0.8104448318481445\n",
      "Step: 15800  \tValid loss: 0.4208495318889618\n",
      "Step: 15900  \tTraining loss: 0.39387133717536926\n",
      "Step: 15900  \tTraining accuracy: 0.8104966878890991\n",
      "Step: 15900  \tValid loss: 0.4207233190536499\n",
      "Step: 16000  \tTraining loss: 0.3937431871891022\n",
      "Step: 16000  \tTraining accuracy: 0.8105453252792358\n",
      "Step: 16000  \tValid loss: 0.42055338621139526\n",
      "Step: 16100  \tTraining loss: 0.3936215043067932\n",
      "Step: 16100  \tTraining accuracy: 0.8105967044830322\n",
      "Step: 16100  \tValid loss: 0.4204445779323578\n",
      "Step: 16200  \tTraining loss: 0.39350631833076477\n",
      "Step: 16200  \tTraining accuracy: 0.8106474876403809\n",
      "Step: 16200  \tValid loss: 0.4203336834907532\n",
      "Step: 16300  \tTraining loss: 0.39340388774871826\n",
      "Step: 16300  \tTraining accuracy: 0.810697615146637\n",
      "Step: 16300  \tValid loss: 0.4202595353126526\n",
      "Step: 16400  \tTraining loss: 0.39326798915863037\n",
      "Step: 16400  \tTraining accuracy: 0.8107471466064453\n",
      "Step: 16400  \tValid loss: 0.42013654112815857\n",
      "Step: 16500  \tTraining loss: 0.39315101504325867\n",
      "Step: 16500  \tTraining accuracy: 0.8107960820198059\n",
      "Step: 16500  \tValid loss: 0.41998374462127686\n",
      "Step: 16600  \tTraining loss: 0.3930552005767822\n",
      "Step: 16600  \tTraining accuracy: 0.8108468651771545\n",
      "Step: 16600  \tValid loss: 0.41981884837150574\n",
      "Step: 16700  \tTraining loss: 0.3929339051246643\n",
      "Step: 16700  \tTraining accuracy: 0.8108987212181091\n",
      "Step: 16700  \tValid loss: 0.4197468161582947\n",
      "Step: 16800  \tTraining loss: 0.3928042948246002\n",
      "Step: 16800  \tTraining accuracy: 0.8109498620033264\n",
      "Step: 16800  \tValid loss: 0.4196207821369171\n",
      "Step: 16900  \tTraining loss: 0.3926942050457001\n",
      "Step: 16900  \tTraining accuracy: 0.8109964728355408\n",
      "Step: 16900  \tValid loss: 0.4195214807987213\n",
      "Step: 17000  \tTraining loss: 0.392583966255188\n",
      "Step: 17000  \tTraining accuracy: 0.8110432624816895\n",
      "Step: 17000  \tValid loss: 0.4194186329841614\n",
      "Step: 17100  \tTraining loss: 0.3925204873085022\n",
      "Step: 17100  \tTraining accuracy: 0.8110902905464172\n",
      "Step: 17100  \tValid loss: 0.4194014072418213\n",
      "Step: 17200  \tTraining loss: 0.3923758566379547\n",
      "Step: 17200  \tTraining accuracy: 0.8111391663551331\n",
      "Step: 17200  \tValid loss: 0.4191613495349884\n",
      "Step: 17300  \tTraining loss: 0.39226213097572327\n",
      "Step: 17300  \tTraining accuracy: 0.8111875057220459\n",
      "Step: 17300  \tValid loss: 0.4190675616264343\n",
      "Step: 17400  \tTraining loss: 0.3921513259410858\n",
      "Step: 17400  \tTraining accuracy: 0.8112329244613647\n",
      "Step: 17400  \tValid loss: 0.41892170906066895\n",
      "Step: 17500  \tTraining loss: 0.3920401930809021\n",
      "Step: 17500  \tTraining accuracy: 0.8112785816192627\n",
      "Step: 17500  \tValid loss: 0.41882288455963135\n",
      "Step: 17600  \tTraining loss: 0.39193660020828247\n",
      "Step: 17600  \tTraining accuracy: 0.8113252520561218\n",
      "Step: 17600  \tValid loss: 0.41873258352279663\n",
      "Step: 17700  \tTraining loss: 0.39183130860328674\n",
      "Step: 17700  \tTraining accuracy: 0.8113714456558228\n",
      "Step: 17700  \tValid loss: 0.41864117980003357\n",
      "Step: 17800  \tTraining loss: 0.39173606038093567\n",
      "Step: 17800  \tTraining accuracy: 0.8114170432090759\n",
      "Step: 17800  \tValid loss: 0.41858428716659546\n",
      "Step: 17900  \tTraining loss: 0.3916385769844055\n",
      "Step: 17900  \tTraining accuracy: 0.8114621639251709\n",
      "Step: 17900  \tValid loss: 0.41844961047172546\n",
      "Step: 18000  \tTraining loss: 0.3915337026119232\n",
      "Step: 18000  \tTraining accuracy: 0.8115068078041077\n",
      "Step: 18000  \tValid loss: 0.4183763861656189\n",
      "Step: 18100  \tTraining loss: 0.39142751693725586\n",
      "Step: 18100  \tTraining accuracy: 0.8115509152412415\n",
      "Step: 18100  \tValid loss: 0.4182743430137634\n",
      "Step: 18200  \tTraining loss: 0.3913290202617645\n",
      "Step: 18200  \tTraining accuracy: 0.8115975856781006\n",
      "Step: 18200  \tValid loss: 0.41818371415138245\n",
      "Step: 18300  \tTraining loss: 0.3912317156791687\n",
      "Step: 18300  \tTraining accuracy: 0.8116414546966553\n",
      "Step: 18300  \tValid loss: 0.4180958867073059\n",
      "Step: 18400  \tTraining loss: 0.3911350667476654\n",
      "Step: 18400  \tTraining accuracy: 0.8116870522499084\n",
      "Step: 18400  \tValid loss: 0.41799119114875793\n",
      "Step: 18500  \tTraining loss: 0.3910352885723114\n",
      "Step: 18500  \tTraining accuracy: 0.8117329478263855\n",
      "Step: 18500  \tValid loss: 0.41790154576301575\n",
      "Step: 18600  \tTraining loss: 0.39096200466156006\n",
      "Step: 18600  \tTraining accuracy: 0.8117805123329163\n",
      "Step: 18600  \tValid loss: 0.4178553521633148\n",
      "Step: 18700  \tTraining loss: 0.3908710479736328\n",
      "Step: 18700  \tTraining accuracy: 0.8118239045143127\n",
      "Step: 18700  \tValid loss: 0.4176912009716034\n",
      "Step: 18800  \tTraining loss: 0.39074447751045227\n",
      "Step: 18800  \tTraining accuracy: 0.8118646740913391\n",
      "Step: 18800  \tValid loss: 0.41769275069236755\n",
      "Step: 18900  \tTraining loss: 0.3906814455986023\n",
      "Step: 18900  \tTraining accuracy: 0.811907947063446\n",
      "Step: 18900  \tValid loss: 0.4175523817539215\n",
      "Step: 19000  \tTraining loss: 0.39055952429771423\n",
      "Step: 19000  \tTraining accuracy: 0.8119485378265381\n",
      "Step: 19000  \tValid loss: 0.4174686372280121\n",
      "Step: 19100  \tTraining loss: 0.3904804289340973\n",
      "Step: 19100  \tTraining accuracy: 0.8119908571243286\n",
      "Step: 19100  \tValid loss: 0.4173761308193207\n",
      "Step: 19200  \tTraining loss: 0.3903748095035553\n",
      "Step: 19200  \tTraining accuracy: 0.812033474445343\n",
      "Step: 19200  \tValid loss: 0.4173446595668793\n",
      "Step: 19300  \tTraining loss: 0.39028510451316833\n",
      "Step: 19300  \tTraining accuracy: 0.8120756149291992\n",
      "Step: 19300  \tValid loss: 0.4171936511993408\n",
      "Step: 19400  \tTraining loss: 0.39018118381500244\n",
      "Step: 19400  \tTraining accuracy: 0.812117338180542\n",
      "Step: 19400  \tValid loss: 0.4171546399593353\n",
      "Step: 19500  \tTraining loss: 0.39009082317352295\n",
      "Step: 19500  \tTraining accuracy: 0.8121586441993713\n",
      "Step: 19500  \tValid loss: 0.4170685410499573\n",
      "Step: 19600  \tTraining loss: 0.390001505613327\n",
      "Step: 19600  \tTraining accuracy: 0.8121995329856873\n",
      "Step: 19600  \tValid loss: 0.4170133173465729\n",
      "Step: 19700  \tTraining loss: 0.3899243175983429\n",
      "Step: 19700  \tTraining accuracy: 0.8122434020042419\n",
      "Step: 19700  \tValid loss: 0.416901171207428\n",
      "Step: 19800  \tTraining loss: 0.38983598351478577\n",
      "Step: 19800  \tTraining accuracy: 0.8122868537902832\n",
      "Step: 19800  \tValid loss: 0.416807621717453\n",
      "Step: 19900  \tTraining loss: 0.3897339105606079\n",
      "Step: 19900  \tTraining accuracy: 0.8123312592506409\n",
      "Step: 19900  \tValid loss: 0.4167358875274658\n",
      "Step: 20000  \tTraining loss: 0.3896559476852417\n",
      "Step: 20000  \tTraining accuracy: 0.8123772144317627\n",
      "Step: 20000  \tValid loss: 0.4166108965873718\n",
      "Step: 20100  \tTraining loss: 0.38957083225250244\n",
      "Step: 20100  \tTraining accuracy: 0.8124226927757263\n",
      "Step: 20100  \tValid loss: 0.41653040051460266\n",
      "Step: 20200  \tTraining loss: 0.3894905149936676\n",
      "Step: 20200  \tTraining accuracy: 0.8124677538871765\n",
      "Step: 20200  \tValid loss: 0.4164513945579529\n",
      "Step: 20300  \tTraining loss: 0.38937291502952576\n",
      "Step: 20300  \tTraining accuracy: 0.8125123381614685\n",
      "Step: 20300  \tValid loss: 0.4163757264614105\n",
      "Step: 20400  \tTraining loss: 0.3893240690231323\n",
      "Step: 20400  \tTraining accuracy: 0.8125558495521545\n",
      "Step: 20400  \tValid loss: 0.4163019359111786\n",
      "Step: 20500  \tTraining loss: 0.3892248272895813\n",
      "Step: 20500  \tTraining accuracy: 0.8125962615013123\n",
      "Step: 20500  \tValid loss: 0.41623660922050476\n",
      "Step: 20600  \tTraining loss: 0.38914862275123596\n",
      "Step: 20600  \tTraining accuracy: 0.8126363158226013\n",
      "Step: 20600  \tValid loss: 0.41619378328323364\n",
      "Step: 20700  \tTraining loss: 0.3890290856361389\n",
      "Step: 20700  \tTraining accuracy: 0.812675952911377\n",
      "Step: 20700  \tValid loss: 0.4161212742328644\n",
      "Step: 20800  \tTraining loss: 0.3889508843421936\n",
      "Step: 20800  \tTraining accuracy: 0.8127152323722839\n",
      "Step: 20800  \tValid loss: 0.4159698188304901\n",
      "Step: 20900  \tTraining loss: 0.38888975977897644\n",
      "Step: 20900  \tTraining accuracy: 0.8127541542053223\n",
      "Step: 20900  \tValid loss: 0.4159807860851288\n",
      "Step: 21000  \tTraining loss: 0.3887861371040344\n",
      "Step: 21000  \tTraining accuracy: 0.8127926588058472\n",
      "Step: 21000  \tValid loss: 0.4158283472061157\n",
      "Step: 21100  \tTraining loss: 0.38868871331214905\n",
      "Step: 21100  \tTraining accuracy: 0.8128308057785034\n",
      "Step: 21100  \tValid loss: 0.41571465134620667\n",
      "Step: 21200  \tTraining loss: 0.38860228657722473\n",
      "Step: 21200  \tTraining accuracy: 0.812868595123291\n",
      "Step: 21200  \tValid loss: 0.41567176580429077\n",
      "Step: 21300  \tTraining loss: 0.38851454854011536\n",
      "Step: 21300  \tTraining accuracy: 0.8129041790962219\n",
      "Step: 21300  \tValid loss: 0.41554731130599976\n",
      "Step: 21400  \tTraining loss: 0.38842812180519104\n",
      "Step: 21400  \tTraining accuracy: 0.8129380941390991\n",
      "Step: 21400  \tValid loss: 0.4154791235923767\n",
      "Step: 21500  \tTraining loss: 0.38835880160331726\n",
      "Step: 21500  \tTraining accuracy: 0.8129717111587524\n",
      "Step: 21500  \tValid loss: 0.41538140177726746\n",
      "Step: 21600  \tTraining loss: 0.3882634937763214\n",
      "Step: 21600  \tTraining accuracy: 0.8130062818527222\n",
      "Step: 21600  \tValid loss: 0.4153176248073578\n",
      "Step: 21700  \tTraining loss: 0.3882104456424713\n",
      "Step: 21700  \tTraining accuracy: 0.8130423426628113\n",
      "Step: 21700  \tValid loss: 0.4152155816555023\n",
      "Step: 21800  \tTraining loss: 0.3880910277366638\n",
      "Step: 21800  \tTraining accuracy: 0.8130781650543213\n",
      "Step: 21800  \tValid loss: 0.4151294529438019\n",
      "Step: 21900  \tTraining loss: 0.3880140781402588\n",
      "Step: 21900  \tTraining accuracy: 0.8131135702133179\n",
      "Step: 21900  \tValid loss: 0.415045827627182\n",
      "Step: 22000  \tTraining loss: 0.3879246413707733\n",
      "Step: 22000  \tTraining accuracy: 0.8131487369537354\n",
      "Step: 22000  \tValid loss: 0.414926141500473\n",
      "Step: 22100  \tTraining loss: 0.3878409266471863\n",
      "Step: 22100  \tTraining accuracy: 0.8131835460662842\n",
      "Step: 22100  \tValid loss: 0.41482558846473694\n",
      "Step: 22200  \tTraining loss: 0.3877682387828827\n",
      "Step: 22200  \tTraining accuracy: 0.8132180571556091\n",
      "Step: 22200  \tValid loss: 0.41481539607048035\n",
      "Step: 22300  \tTraining loss: 0.3877031207084656\n",
      "Step: 22300  \tTraining accuracy: 0.8132522106170654\n",
      "Step: 22300  \tValid loss: 0.4147331118583679\n",
      "Step: 22400  \tTraining loss: 0.3876197636127472\n",
      "Step: 22400  \tTraining accuracy: 0.8132861256599426\n",
      "Step: 22400  \tValid loss: 0.4146699011325836\n",
      "Step: 22500  \tTraining loss: 0.38750794529914856\n",
      "Step: 22500  \tTraining accuracy: 0.8133196830749512\n",
      "Step: 22500  \tValid loss: 0.4145330488681793\n",
      "Step: 22600  \tTraining loss: 0.38745376467704773\n",
      "Step: 22600  \tTraining accuracy: 0.8133529424667358\n",
      "Step: 22600  \tValid loss: 0.41448378562927246\n",
      "Step: 22700  \tTraining loss: 0.38734546303749084\n",
      "Step: 22700  \tTraining accuracy: 0.8133859634399414\n",
      "Step: 22700  \tValid loss: 0.4143671989440918\n",
      "Step: 22800  \tTraining loss: 0.3872818946838379\n",
      "Step: 22800  \tTraining accuracy: 0.8134186267852783\n",
      "Step: 22800  \tValid loss: 0.4142850339412689\n",
      "Step: 22900  \tTraining loss: 0.38718387484550476\n",
      "Step: 22900  \tTraining accuracy: 0.8134522438049316\n",
      "Step: 22900  \tValid loss: 0.41417524218559265\n",
      "Step: 23000  \tTraining loss: 0.3871157765388489\n",
      "Step: 23000  \tTraining accuracy: 0.8134872913360596\n",
      "Step: 23000  \tValid loss: 0.41409289836883545\n",
      "Step: 23100  \tTraining loss: 0.38700634241104126\n",
      "Step: 23100  \tTraining accuracy: 0.8135221004486084\n",
      "Step: 23100  \tValid loss: 0.41407087445259094\n",
      "Step: 23200  \tTraining loss: 0.3869313895702362\n",
      "Step: 23200  \tTraining accuracy: 0.8135565519332886\n",
      "Step: 23200  \tValid loss: 0.4139821231365204\n",
      "Step: 23300  \tTraining loss: 0.38685065507888794\n",
      "Step: 23300  \tTraining accuracy: 0.8135907053947449\n",
      "Step: 23300  \tValid loss: 0.41387712955474854\n",
      "Step: 23400  \tTraining loss: 0.3867553472518921\n",
      "Step: 23400  \tTraining accuracy: 0.8136245608329773\n",
      "Step: 23400  \tValid loss: 0.41383886337280273\n",
      "Step: 23500  \tTraining loss: 0.3867042660713196\n",
      "Step: 23500  \tTraining accuracy: 0.8136581778526306\n",
      "Step: 23500  \tValid loss: 0.41371095180511475\n",
      "Step: 23600  \tTraining loss: 0.38660430908203125\n",
      "Step: 23600  \tTraining accuracy: 0.8136926293373108\n",
      "Step: 23600  \tValid loss: 0.41361451148986816\n",
      "Step: 23700  \tTraining loss: 0.38650843501091003\n",
      "Step: 23700  \tTraining accuracy: 0.8137285113334656\n",
      "Step: 23700  \tValid loss: 0.413532018661499\n",
      "Step: 23800  \tTraining loss: 0.3864375352859497\n",
      "Step: 23800  \tTraining accuracy: 0.8137634992599487\n",
      "Step: 23800  \tValid loss: 0.41346466541290283\n",
      "Step: 23900  \tTraining loss: 0.3863319456577301\n",
      "Step: 23900  \tTraining accuracy: 0.8137947916984558\n",
      "Step: 23900  \tValid loss: 0.4133846163749695\n",
      "Step: 24000  \tTraining loss: 0.38624730706214905\n",
      "Step: 24000  \tTraining accuracy: 0.8138241767883301\n",
      "Step: 24000  \tValid loss: 0.41331711411476135\n",
      "Step: 24100  \tTraining loss: 0.38614150881767273\n",
      "Step: 24100  \tTraining accuracy: 0.8138532638549805\n",
      "Step: 24100  \tValid loss: 0.41323840618133545\n",
      "Step: 24200  \tTraining loss: 0.38607943058013916\n",
      "Step: 24200  \tTraining accuracy: 0.8138832449913025\n",
      "Step: 24200  \tValid loss: 0.4130948483943939\n",
      "Step: 24300  \tTraining loss: 0.3859633803367615\n",
      "Step: 24300  \tTraining accuracy: 0.8139146566390991\n",
      "Step: 24300  \tValid loss: 0.41298454999923706\n",
      "Step: 24400  \tTraining loss: 0.385875403881073\n",
      "Step: 24400  \tTraining accuracy: 0.8139458298683167\n",
      "Step: 24400  \tValid loss: 0.41292938590049744\n",
      "Step: 24500  \tTraining loss: 0.38578274846076965\n",
      "Step: 24500  \tTraining accuracy: 0.8139767050743103\n",
      "Step: 24500  \tValid loss: 0.4127943813800812\n",
      "Step: 24600  \tTraining loss: 0.38566839694976807\n",
      "Step: 24600  \tTraining accuracy: 0.8140073418617249\n",
      "Step: 24600  \tValid loss: 0.4127238690853119\n",
      "Step: 24700  \tTraining loss: 0.3855724036693573\n",
      "Step: 24700  \tTraining accuracy: 0.8140377402305603\n",
      "Step: 24700  \tValid loss: 0.41263312101364136\n",
      "Step: 24800  \tTraining loss: 0.38549554347991943\n",
      "Step: 24800  \tTraining accuracy: 0.8140679001808167\n",
      "Step: 24800  \tValid loss: 0.41259005665779114\n",
      "Step: 24900  \tTraining loss: 0.38538679480552673\n",
      "Step: 24900  \tTraining accuracy: 0.8140978217124939\n",
      "Step: 24900  \tValid loss: 0.41237711906433105\n",
      "Step: 25000  \tTraining loss: 0.3852929174900055\n",
      "Step: 25000  \tTraining accuracy: 0.814127504825592\n",
      "Step: 25000  \tValid loss: 0.4123048186302185\n",
      "Step: 25100  \tTraining loss: 0.3851895034313202\n",
      "Step: 25100  \tTraining accuracy: 0.8141569495201111\n",
      "Step: 25100  \tValid loss: 0.41215887665748596\n",
      "Step: 25200  \tTraining loss: 0.38508766889572144\n",
      "Step: 25200  \tTraining accuracy: 0.814186155796051\n",
      "Step: 25200  \tValid loss: 0.41213977336883545\n",
      "Step: 25300  \tTraining loss: 0.38502874970436096\n",
      "Step: 25300  \tTraining accuracy: 0.8142151236534119\n",
      "Step: 25300  \tValid loss: 0.4119642972946167\n",
      "Step: 25400  \tTraining loss: 0.3848609924316406\n",
      "Step: 25400  \tTraining accuracy: 0.8142438530921936\n",
      "Step: 25400  \tValid loss: 0.4119100868701935\n",
      "Step: 25500  \tTraining loss: 0.38477519154548645\n",
      "Step: 25500  \tTraining accuracy: 0.8142697215080261\n",
      "Step: 25500  \tValid loss: 0.41179361939430237\n",
      "Step: 25600  \tTraining loss: 0.384704053401947\n",
      "Step: 25600  \tTraining accuracy: 0.8142927289009094\n",
      "Step: 25600  \tValid loss: 0.41162511706352234\n",
      "Step: 25700  \tTraining loss: 0.38459280133247375\n",
      "Step: 25700  \tTraining accuracy: 0.8143155574798584\n",
      "Step: 25700  \tValid loss: 0.4115348756313324\n",
      "Step: 25800  \tTraining loss: 0.3845699429512024\n",
      "Step: 25800  \tTraining accuracy: 0.814338207244873\n",
      "Step: 25800  \tValid loss: 0.4114513695240021\n",
      "Step: 25900  \tTraining loss: 0.38444530963897705\n",
      "Step: 25900  \tTraining accuracy: 0.8143606781959534\n",
      "Step: 25900  \tValid loss: 0.41139018535614014\n",
      "Step: 26000  \tTraining loss: 0.3843511939048767\n",
      "Step: 26000  \tTraining accuracy: 0.8143829703330994\n",
      "Step: 26000  \tValid loss: 0.41122645139694214\n",
      "Step: 26100  \tTraining loss: 0.3842584788799286\n",
      "Step: 26100  \tTraining accuracy: 0.814406156539917\n",
      "Step: 26100  \tValid loss: 0.4111635684967041\n",
      "Step: 26200  \tTraining loss: 0.38419923186302185\n",
      "Step: 26200  \tTraining accuracy: 0.8144307136535645\n",
      "Step: 26200  \tValid loss: 0.4110572338104248\n",
      "Step: 26300  \tTraining loss: 0.3840973675251007\n",
      "Step: 26300  \tTraining accuracy: 0.8144550323486328\n",
      "Step: 26300  \tValid loss: 0.4110233187675476\n",
      "Step: 26400  \tTraining loss: 0.3840472996234894\n",
      "Step: 26400  \tTraining accuracy: 0.8144792318344116\n",
      "Step: 26400  \tValid loss: 0.41097021102905273\n",
      "Step: 26500  \tTraining loss: 0.38395264744758606\n",
      "Step: 26500  \tTraining accuracy: 0.8145031929016113\n",
      "Step: 26500  \tValid loss: 0.41085025668144226\n",
      "Step: 26600  \tTraining loss: 0.3839007318019867\n",
      "Step: 26600  \tTraining accuracy: 0.8145260214805603\n",
      "Step: 26600  \tValid loss: 0.41080421209335327\n",
      "Step: 26700  \tTraining loss: 0.3837980329990387\n",
      "Step: 26700  \tTraining accuracy: 0.8145491480827332\n",
      "Step: 26700  \tValid loss: 0.4107336401939392\n",
      "Step: 26800  \tTraining loss: 0.38373449444770813\n",
      "Step: 26800  \tTraining accuracy: 0.8145725727081299\n",
      "Step: 26800  \tValid loss: 0.41065356135368347\n",
      "Step: 26900  \tTraining loss: 0.3836703300476074\n",
      "Step: 26900  \tTraining accuracy: 0.8145958781242371\n",
      "Step: 26900  \tValid loss: 0.4105335474014282\n",
      "Step: 27000  \tTraining loss: 0.38360702991485596\n",
      "Step: 27000  \tTraining accuracy: 0.8146190047264099\n",
      "Step: 27000  \tValid loss: 0.41054245829582214\n",
      "Step: 27100  \tTraining loss: 0.38350963592529297\n",
      "Step: 27100  \tTraining accuracy: 0.8146419525146484\n",
      "Step: 27100  \tValid loss: 0.41048234701156616\n",
      "Step: 27200  \tTraining loss: 0.38343140482902527\n",
      "Step: 27200  \tTraining accuracy: 0.8146647214889526\n",
      "Step: 27200  \tValid loss: 0.4103338122367859\n",
      "Step: 27300  \tTraining loss: 0.3833753764629364\n",
      "Step: 27300  \tTraining accuracy: 0.8146873116493225\n",
      "Step: 27300  \tValid loss: 0.4103180468082428\n",
      "Step: 27400  \tTraining loss: 0.3832845687866211\n",
      "Step: 27400  \tTraining accuracy: 0.8147097229957581\n",
      "Step: 27400  \tValid loss: 0.4102131426334381\n",
      "Step: 27500  \tTraining loss: 0.38323017954826355\n",
      "Step: 27500  \tTraining accuracy: 0.814732015132904\n",
      "Step: 27500  \tValid loss: 0.41017669439315796\n",
      "Step: 27600  \tTraining loss: 0.38317087292671204\n",
      "Step: 27600  \tTraining accuracy: 0.8147541284561157\n",
      "Step: 27600  \tValid loss: 0.4100838601589203\n",
      "Step: 27700  \tTraining loss: 0.38308826088905334\n",
      "Step: 27700  \tTraining accuracy: 0.8147780299186707\n",
      "Step: 27700  \tValid loss: 0.41008302569389343\n",
      "Step: 27800  \tTraining loss: 0.38301774859428406\n",
      "Step: 27800  \tTraining accuracy: 0.8148007988929749\n",
      "Step: 27800  \tValid loss: 0.4100099503993988\n",
      "Step: 27900  \tTraining loss: 0.38296037912368774\n",
      "Step: 27900  \tTraining accuracy: 0.8148229122161865\n",
      "Step: 27900  \tValid loss: 0.4098358154296875\n",
      "Step: 28000  \tTraining loss: 0.3828786313533783\n",
      "Step: 28000  \tTraining accuracy: 0.8148468136787415\n",
      "Step: 28000  \tValid loss: 0.40984225273132324\n",
      "Step: 28100  \tTraining loss: 0.38283541798591614\n",
      "Step: 28100  \tTraining accuracy: 0.8148705363273621\n",
      "Step: 28100  \tValid loss: 0.40978601574897766\n",
      "Step: 28200  \tTraining loss: 0.3827504813671112\n",
      "Step: 28200  \tTraining accuracy: 0.814895510673523\n",
      "Step: 28200  \tValid loss: 0.4097995460033417\n",
      "Step: 28300  \tTraining loss: 0.38268330693244934\n",
      "Step: 28300  \tTraining accuracy: 0.8149213194847107\n",
      "Step: 28300  \tValid loss: 0.4097276031970978\n",
      "Step: 28400  \tTraining loss: 0.38262003660202026\n",
      "Step: 28400  \tTraining accuracy: 0.8149468898773193\n",
      "Step: 28400  \tValid loss: 0.4096219837665558\n",
      "Step: 28500  \tTraining loss: 0.38255593180656433\n",
      "Step: 28500  \tTraining accuracy: 0.8149723410606384\n",
      "Step: 28500  \tValid loss: 0.4095659852027893\n",
      "Step: 28600  \tTraining loss: 0.3824944496154785\n",
      "Step: 28600  \tTraining accuracy: 0.814998984336853\n",
      "Step: 28600  \tValid loss: 0.40949079394340515\n",
      "Step: 28700  \tTraining loss: 0.38243386149406433\n",
      "Step: 28700  \tTraining accuracy: 0.8150264024734497\n",
      "Step: 28700  \tValid loss: 0.4094945788383484\n",
      "Step: 28800  \tTraining loss: 0.3823760449886322\n",
      "Step: 28800  \tTraining accuracy: 0.8150526881217957\n",
      "Step: 28800  \tValid loss: 0.4093377888202667\n",
      "Step: 28900  \tTraining loss: 0.382312536239624\n",
      "Step: 28900  \tTraining accuracy: 0.8150773644447327\n",
      "Step: 28900  \tValid loss: 0.4094012975692749\n",
      "Step: 29000  \tTraining loss: 0.38225629925727844\n",
      "Step: 29000  \tTraining accuracy: 0.8151018619537354\n",
      "Step: 29000  \tValid loss: 0.40931418538093567\n",
      "Step: 29100  \tTraining loss: 0.38218796253204346\n",
      "Step: 29100  \tTraining accuracy: 0.8151262402534485\n",
      "Step: 29100  \tValid loss: 0.4092782735824585\n",
      "Step: 29200  \tTraining loss: 0.38213762640953064\n",
      "Step: 29200  \tTraining accuracy: 0.8151504397392273\n",
      "Step: 29200  \tValid loss: 0.409251868724823\n",
      "Step: 29300  \tTraining loss: 0.38209161162376404\n",
      "Step: 29300  \tTraining accuracy: 0.8151744604110718\n",
      "Step: 29300  \tValid loss: 0.4091547131538391\n",
      "Step: 29400  \tTraining loss: 0.3820491135120392\n",
      "Step: 29400  \tTraining accuracy: 0.8151983022689819\n",
      "Step: 29400  \tValid loss: 0.40915626287460327\n",
      "Step: 29500  \tTraining loss: 0.38196903467178345\n",
      "Step: 29500  \tTraining accuracy: 0.8152220249176025\n",
      "Step: 29500  \tValid loss: 0.40911033749580383\n",
      "Step: 29600  \tTraining loss: 0.381899893283844\n",
      "Step: 29600  \tTraining accuracy: 0.815245509147644\n",
      "Step: 29600  \tValid loss: 0.4090729057788849\n",
      "Step: 29700  \tTraining loss: 0.3818424642086029\n",
      "Step: 29700  \tTraining accuracy: 0.8152689337730408\n",
      "Step: 29700  \tValid loss: 0.4090617597103119\n",
      "Step: 29800  \tTraining loss: 0.3817873001098633\n",
      "Step: 29800  \tTraining accuracy: 0.8152921199798584\n",
      "Step: 29800  \tValid loss: 0.4090266823768616\n",
      "Step: 29900  \tTraining loss: 0.3817526698112488\n",
      "Step: 29900  \tTraining accuracy: 0.8153151869773865\n",
      "Step: 29900  \tValid loss: 0.40896716713905334\n",
      "Step: 30000  \tTraining loss: 0.3816763460636139\n",
      "Step: 30000  \tTraining accuracy: 0.815338134765625\n",
      "Step: 30000  \tValid loss: 0.4089158773422241\n",
      "Step: 30100  \tTraining loss: 0.3816152811050415\n",
      "Step: 30100  \tTraining accuracy: 0.8153608441352844\n",
      "Step: 30100  \tValid loss: 0.4089086353778839\n",
      "Step: 30200  \tTraining loss: 0.38156309723854065\n",
      "Step: 30200  \tTraining accuracy: 0.8153834939002991\n",
      "Step: 30200  \tValid loss: 0.4088604748249054\n",
      "Step: 30300  \tTraining loss: 0.38150274753570557\n",
      "Step: 30300  \tTraining accuracy: 0.8154059052467346\n",
      "Step: 30300  \tValid loss: 0.4087960124015808\n",
      "Step: 30400  \tTraining loss: 0.38145142793655396\n",
      "Step: 30400  \tTraining accuracy: 0.8154282569885254\n",
      "Step: 30400  \tValid loss: 0.4087390601634979\n",
      "Step: 30500  \tTraining loss: 0.3813970685005188\n",
      "Step: 30500  \tTraining accuracy: 0.8154504299163818\n",
      "Step: 30500  \tValid loss: 0.4087521433830261\n",
      "Step: 30600  \tTraining loss: 0.38134628534317017\n",
      "Step: 30600  \tTraining accuracy: 0.815472424030304\n",
      "Step: 30600  \tValid loss: 0.40873709321022034\n",
      "Step: 30700  \tTraining loss: 0.38130849599838257\n",
      "Step: 30700  \tTraining accuracy: 0.8154942989349365\n",
      "Step: 30700  \tValid loss: 0.40871700644493103\n",
      "Step: 30800  \tTraining loss: 0.3812565505504608\n",
      "Step: 30800  \tTraining accuracy: 0.8155160546302795\n",
      "Step: 30800  \tValid loss: 0.4087032377719879\n",
      "Step: 30900  \tTraining loss: 0.38119029998779297\n",
      "Step: 30900  \tTraining accuracy: 0.8155376315116882\n",
      "Step: 30900  \tValid loss: 0.4086291790008545\n",
      "Step: 31000  \tTraining loss: 0.3811470866203308\n",
      "Step: 31000  \tTraining accuracy: 0.8155590891838074\n",
      "Step: 31000  \tValid loss: 0.4085794985294342\n",
      "Step: 31100  \tTraining loss: 0.3810889422893524\n",
      "Step: 31100  \tTraining accuracy: 0.8155803680419922\n",
      "Step: 31100  \tValid loss: 0.40855810046195984\n",
      "Step: 31200  \tTraining loss: 0.38102632761001587\n",
      "Step: 31200  \tTraining accuracy: 0.8156015872955322\n",
      "Step: 31200  \tValid loss: 0.4085276424884796\n",
      "Step: 31300  \tTraining loss: 0.38099220395088196\n",
      "Step: 31300  \tTraining accuracy: 0.8156226277351379\n",
      "Step: 31300  \tValid loss: 0.4085119664669037\n",
      "Step: 31400  \tTraining loss: 0.3809269964694977\n",
      "Step: 31400  \tTraining accuracy: 0.8156434893608093\n",
      "Step: 31400  \tValid loss: 0.40854546427726746\n",
      "Step: 31500  \tTraining loss: 0.3808666169643402\n",
      "Step: 31500  \tTraining accuracy: 0.8156625628471375\n",
      "Step: 31500  \tValid loss: 0.40849652886390686\n",
      "Step: 31600  \tTraining loss: 0.3808099329471588\n",
      "Step: 31600  \tTraining accuracy: 0.8156810998916626\n",
      "Step: 31600  \tValid loss: 0.40843474864959717\n",
      "Step: 31700  \tTraining loss: 0.3807471990585327\n",
      "Step: 31700  \tTraining accuracy: 0.8156994581222534\n",
      "Step: 31700  \tValid loss: 0.40843963623046875\n",
      "Step: 31800  \tTraining loss: 0.38069868087768555\n",
      "Step: 31800  \tTraining accuracy: 0.8157172799110413\n",
      "Step: 31800  \tValid loss: 0.4084063172340393\n",
      "Step: 31900  \tTraining loss: 0.38063696026802063\n",
      "Step: 31900  \tTraining accuracy: 0.8157333135604858\n",
      "Step: 31900  \tValid loss: 0.40834760665893555\n",
      "Step: 32000  \tTraining loss: 0.3806042969226837\n",
      "Step: 32000  \tTraining accuracy: 0.8157492876052856\n",
      "Step: 32000  \tValid loss: 0.40834036469459534\n",
      "Step: 32100  \tTraining loss: 0.3805323839187622\n",
      "Step: 32100  \tTraining accuracy: 0.8157651424407959\n",
      "Step: 32100  \tValid loss: 0.40836766362190247\n",
      "Step: 32200  \tTraining loss: 0.3804953992366791\n",
      "Step: 32200  \tTraining accuracy: 0.8157808780670166\n",
      "Step: 32200  \tValid loss: 0.40831732749938965\n",
      "Step: 32300  \tTraining loss: 0.38043519854545593\n",
      "Step: 32300  \tTraining accuracy: 0.8157964944839478\n",
      "Step: 32300  \tValid loss: 0.40825995802879333\n",
      "Step: 32400  \tTraining loss: 0.38039547204971313\n",
      "Step: 32400  \tTraining accuracy: 0.8158120512962341\n",
      "Step: 32400  \tValid loss: 0.4082503318786621\n",
      "Step: 32500  \tTraining loss: 0.3803316354751587\n",
      "Step: 32500  \tTraining accuracy: 0.8158262372016907\n",
      "Step: 32500  \tValid loss: 0.4082085192203522\n",
      "Step: 32600  \tTraining loss: 0.3802734613418579\n",
      "Step: 32600  \tTraining accuracy: 0.8158395290374756\n",
      "Step: 32600  \tValid loss: 0.4082302451133728\n",
      "Step: 32700  \tTraining loss: 0.3802201747894287\n",
      "Step: 32700  \tTraining accuracy: 0.815852701663971\n",
      "Step: 32700  \tValid loss: 0.4081033766269684\n",
      "Step: 32800  \tTraining loss: 0.38016143441200256\n",
      "Step: 32800  \tTraining accuracy: 0.8158658146858215\n",
      "Step: 32800  \tValid loss: 0.4079558253288269\n",
      "Step: 32900  \tTraining loss: 0.38010531663894653\n",
      "Step: 32900  \tTraining accuracy: 0.8158788681030273\n",
      "Step: 32900  \tValid loss: 0.4079166650772095\n",
      "Step: 33000  \tTraining loss: 0.3800489902496338\n",
      "Step: 33000  \tTraining accuracy: 0.8158918023109436\n",
      "Step: 33000  \tValid loss: 0.4079051613807678\n",
      "Step: 33100  \tTraining loss: 0.379986435174942\n",
      "Step: 33100  \tTraining accuracy: 0.8159046769142151\n",
      "Step: 33100  \tValid loss: 0.4078606069087982\n",
      "Step: 33200  \tTraining loss: 0.3799150884151459\n",
      "Step: 33200  \tTraining accuracy: 0.8159174919128418\n",
      "Step: 33200  \tValid loss: 0.40784716606140137\n",
      "Step: 33300  \tTraining loss: 0.3798549473285675\n",
      "Step: 33300  \tTraining accuracy: 0.8159302473068237\n",
      "Step: 33300  \tValid loss: 0.40772154927253723\n",
      "Step: 33400  \tTraining loss: 0.37977132201194763\n",
      "Step: 33400  \tTraining accuracy: 0.8159428834915161\n",
      "Step: 33400  \tValid loss: 0.4077606797218323\n",
      "Step: 33500  \tTraining loss: 0.3797052204608917\n",
      "Step: 33500  \tTraining accuracy: 0.8159550428390503\n",
      "Step: 33500  \tValid loss: 0.4077368378639221\n",
      "Step: 33600  \tTraining loss: 0.3796573281288147\n",
      "Step: 33600  \tTraining accuracy: 0.8159655332565308\n",
      "Step: 33600  \tValid loss: 0.4078240394592285\n",
      "Step: 33700  \tTraining loss: 0.3795810341835022\n",
      "Step: 33700  \tTraining accuracy: 0.8159759640693665\n",
      "Step: 33700  \tValid loss: 0.40774816274642944\n",
      "Step: 33800  \tTraining loss: 0.37952327728271484\n",
      "Step: 33800  \tTraining accuracy: 0.8159863352775574\n",
      "Step: 33800  \tValid loss: 0.40771326422691345\n",
      "Step: 33900  \tTraining loss: 0.37946629524230957\n",
      "Step: 33900  \tTraining accuracy: 0.8159965872764587\n",
      "Step: 33900  \tValid loss: 0.4076983630657196\n",
      "Step: 34000  \tTraining loss: 0.37943148612976074\n",
      "Step: 34000  \tTraining accuracy: 0.8160068392753601\n",
      "Step: 34000  \tValid loss: 0.40762951970100403\n",
      "Step: 34100  \tTraining loss: 0.37935367226600647\n",
      "Step: 34100  \tTraining accuracy: 0.8160170316696167\n",
      "Step: 34100  \tValid loss: 0.4076892137527466\n",
      "Step: 34200  \tTraining loss: 0.37929093837738037\n",
      "Step: 34200  \tTraining accuracy: 0.8160271644592285\n",
      "Step: 34200  \tValid loss: 0.4076601564884186\n",
      "Step: 34300  \tTraining loss: 0.37921464443206787\n",
      "Step: 34300  \tTraining accuracy: 0.8160371780395508\n",
      "Step: 34300  \tValid loss: 0.4076458811759949\n",
      "Step: 34400  \tTraining loss: 0.3791508078575134\n",
      "Step: 34400  \tTraining accuracy: 0.816047191619873\n",
      "Step: 34400  \tValid loss: 0.40751171112060547\n",
      "Step: 34500  \tTraining loss: 0.3790664076805115\n",
      "Step: 34500  \tTraining accuracy: 0.8160571455955505\n",
      "Step: 34500  \tValid loss: 0.4074854552745819\n",
      "Step: 34600  \tTraining loss: 0.3790028691291809\n",
      "Step: 34600  \tTraining accuracy: 0.8160670399665833\n",
      "Step: 34600  \tValid loss: 0.4075022339820862\n",
      "Step: 34700  \tTraining loss: 0.3789568543434143\n",
      "Step: 34700  \tTraining accuracy: 0.8160768747329712\n",
      "Step: 34700  \tValid loss: 0.40748870372772217\n",
      "Step: 34800  \tTraining loss: 0.37888890504837036\n",
      "Step: 34800  \tTraining accuracy: 0.8160866498947144\n",
      "Step: 34800  \tValid loss: 0.40745991468429565\n",
      "Step: 34900  \tTraining loss: 0.3788236081600189\n",
      "Step: 34900  \tTraining accuracy: 0.8160963654518127\n",
      "Step: 34900  \tValid loss: 0.40749138593673706\n",
      "Step: 35000  \tTraining loss: 0.37878531217575073\n",
      "Step: 35000  \tTraining accuracy: 0.8161060214042664\n",
      "Step: 35000  \tValid loss: 0.40749669075012207\n",
      "Step: 35100  \tTraining loss: 0.3787195086479187\n",
      "Step: 35100  \tTraining accuracy: 0.8161156177520752\n",
      "Step: 35100  \tValid loss: 0.4075016975402832\n",
      "Step: 35200  \tTraining loss: 0.3786548972129822\n",
      "Step: 35200  \tTraining accuracy: 0.8161251544952393\n",
      "Step: 35200  \tValid loss: 0.40751776099205017\n",
      "Step: 35300  \tTraining loss: 0.3785942494869232\n",
      "Step: 35300  \tTraining accuracy: 0.8161346912384033\n",
      "Step: 35300  \tValid loss: 0.40749049186706543\n",
      "Step: 35400  \tTraining loss: 0.37852200865745544\n",
      "Step: 35400  \tTraining accuracy: 0.8161441087722778\n",
      "Step: 35400  \tValid loss: 0.4073489308357239\n",
      "Step: 35500  \tTraining loss: 0.37845945358276367\n",
      "Step: 35500  \tTraining accuracy: 0.8161534667015076\n",
      "Step: 35500  \tValid loss: 0.4074529707431793\n",
      "Step: 35600  \tTraining loss: 0.3784027099609375\n",
      "Step: 35600  \tTraining accuracy: 0.816163957118988\n",
      "Step: 35600  \tValid loss: 0.40743890404701233\n",
      "Step: 35700  \tTraining loss: 0.37832385301589966\n",
      "Step: 35700  \tTraining accuracy: 0.8161740303039551\n",
      "Step: 35700  \tValid loss: 0.4074409306049347\n",
      "Step: 35800  \tTraining loss: 0.37824907898902893\n",
      "Step: 35800  \tTraining accuracy: 0.8161832094192505\n",
      "Step: 35800  \tValid loss: 0.4073733389377594\n",
      "Step: 35900  \tTraining loss: 0.378185510635376\n",
      "Step: 35900  \tTraining accuracy: 0.8161923885345459\n",
      "Step: 35900  \tValid loss: 0.40737414360046387\n",
      "Step: 36000  \tTraining loss: 0.3781532347202301\n",
      "Step: 36000  \tTraining accuracy: 0.8162026405334473\n",
      "Step: 36000  \tValid loss: 0.40738940238952637\n",
      "Step: 36100  \tTraining loss: 0.378073126077652\n",
      "Step: 36100  \tTraining accuracy: 0.8162128329277039\n",
      "Step: 36100  \tValid loss: 0.4072660803794861\n",
      "Step: 36200  \tTraining loss: 0.37802639603614807\n",
      "Step: 36200  \tTraining accuracy: 0.8162237405776978\n",
      "Step: 36200  \tValid loss: 0.40730029344558716\n",
      "Step: 36300  \tTraining loss: 0.3779560625553131\n",
      "Step: 36300  \tTraining accuracy: 0.8162345886230469\n",
      "Step: 36300  \tValid loss: 0.40730318427085876\n",
      "Step: 36400  \tTraining loss: 0.3779124617576599\n",
      "Step: 36400  \tTraining accuracy: 0.8162464499473572\n",
      "Step: 36400  \tValid loss: 0.40731385350227356\n",
      "Step: 36500  \tTraining loss: 0.3778368830680847\n",
      "Step: 36500  \tTraining accuracy: 0.816257894039154\n",
      "Step: 36500  \tValid loss: 0.40727537870407104\n",
      "Step: 36600  \tTraining loss: 0.3777799606323242\n",
      "Step: 36600  \tTraining accuracy: 0.8162685632705688\n",
      "Step: 36600  \tValid loss: 0.40727630257606506\n",
      "Step: 36700  \tTraining loss: 0.37772974371910095\n",
      "Step: 36700  \tTraining accuracy: 0.8162791132926941\n",
      "Step: 36700  \tValid loss: 0.40730345249176025\n",
      "Step: 36800  \tTraining loss: 0.3776780664920807\n",
      "Step: 36800  \tTraining accuracy: 0.8162896633148193\n",
      "Step: 36800  \tValid loss: 0.40724021196365356\n",
      "Step: 36900  \tTraining loss: 0.3776218593120575\n",
      "Step: 36900  \tTraining accuracy: 0.8163001537322998\n",
      "Step: 36900  \tValid loss: 0.4072883129119873\n",
      "Step: 37000  \tTraining loss: 0.37757372856140137\n",
      "Step: 37000  \tTraining accuracy: 0.8163105845451355\n",
      "Step: 37000  \tValid loss: 0.40727904438972473\n",
      "Step: 37100  \tTraining loss: 0.37751513719558716\n",
      "Step: 37100  \tTraining accuracy: 0.8163208961486816\n",
      "Step: 37100  \tValid loss: 0.4072939455509186\n",
      "Step: 37200  \tTraining loss: 0.37745970487594604\n",
      "Step: 37200  \tTraining accuracy: 0.8163312077522278\n",
      "Step: 37200  \tValid loss: 0.4072876274585724\n",
      "Step: 37300  \tTraining loss: 0.37741509079933167\n",
      "Step: 37300  \tTraining accuracy: 0.8163418173789978\n",
      "Step: 37300  \tValid loss: 0.40735289454460144\n",
      "Step: 37400  \tTraining loss: 0.3773707449436188\n",
      "Step: 37400  \tTraining accuracy: 0.8163527250289917\n",
      "Step: 37400  \tValid loss: 0.40732601284980774\n",
      "Step: 37500  \tTraining loss: 0.3773000240325928\n",
      "Step: 37500  \tTraining accuracy: 0.8163636326789856\n",
      "Step: 37500  \tValid loss: 0.4074135720729828\n",
      "Step: 37600  \tTraining loss: 0.377261221408844\n",
      "Step: 37600  \tTraining accuracy: 0.8163747787475586\n",
      "Step: 37600  \tValid loss: 0.40736454725265503\n",
      "Step: 37700  \tTraining loss: 0.3772198557853699\n",
      "Step: 37700  \tTraining accuracy: 0.8163865804672241\n",
      "Step: 37700  \tValid loss: 0.4073408544063568\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8163976\n",
      "Precision: 0.8338082\n",
      "Recall: 0.8499516\n",
      "F1 score: 0.82422507\n",
      "AUC: 0.8222763\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.816398   0.833808  0.849952  0.824225  0.822276  0.377153      0.816386   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.407149       0.816386   0.398141      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  37792.0  \n",
      "10\n",
      "(4785, 8)\n",
      "(4785, 1)\n",
      "(2640, 8)\n",
      "(2640, 1)\n",
      "(2145, 8)\n",
      "(2145, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6219577789306641\n",
      "Step: 100  \tTraining accuracy: 0.6664576530456543\n",
      "Step: 100  \tValid loss: 0.6205527186393738\n",
      "Step: 200  \tTraining loss: 0.5332381129264832\n",
      "Step: 200  \tTraining accuracy: 0.7005921006202698\n",
      "Step: 200  \tValid loss: 0.5325019955635071\n",
      "Step: 300  \tTraining loss: 0.42040205001831055\n",
      "Step: 300  \tTraining accuracy: 0.739811897277832\n",
      "Step: 300  \tValid loss: 0.41054216027259827\n",
      "Step: 400  \tTraining loss: 0.37406909465789795\n",
      "Step: 400  \tTraining accuracy: 0.7669204473495483\n",
      "Step: 400  \tValid loss: 0.35604724287986755\n",
      "Step: 500  \tTraining loss: 0.36273396015167236\n",
      "Step: 500  \tTraining accuracy: 0.7837919592857361\n",
      "Step: 500  \tValid loss: 0.3410284221172333\n",
      "Step: 600  \tTraining loss: 0.3587310016155243\n",
      "Step: 600  \tTraining accuracy: 0.7953453063964844\n",
      "Step: 600  \tValid loss: 0.3360253572463989\n",
      "Step: 700  \tTraining loss: 0.35630208253860474\n",
      "Step: 700  \tTraining accuracy: 0.8035849332809448\n",
      "Step: 700  \tValid loss: 0.33342745900154114\n",
      "Step: 800  \tTraining loss: 0.35430607199668884\n",
      "Step: 800  \tTraining accuracy: 0.8095994591712952\n",
      "Step: 800  \tValid loss: 0.33153751492500305\n",
      "Step: 900  \tTraining loss: 0.35248273611068726\n",
      "Step: 900  \tTraining accuracy: 0.8141987919807434\n",
      "Step: 900  \tValid loss: 0.32976433634757996\n",
      "Step: 1000  \tTraining loss: 0.3504810929298401\n",
      "Step: 1000  \tTraining accuracy: 0.8176208734512329\n",
      "Step: 1000  \tValid loss: 0.32818374037742615\n",
      "Step: 1100  \tTraining loss: 0.3482498526573181\n",
      "Step: 1100  \tTraining accuracy: 0.8203214406967163\n",
      "Step: 1100  \tValid loss: 0.32644668221473694\n",
      "Step: 1200  \tTraining loss: 0.3464200496673584\n",
      "Step: 1200  \tTraining accuracy: 0.8227340579032898\n",
      "Step: 1200  \tValid loss: 0.32515081763267517\n",
      "Step: 1300  \tTraining loss: 0.3449428379535675\n",
      "Step: 1300  \tTraining accuracy: 0.8247857689857483\n",
      "Step: 1300  \tValid loss: 0.32429268956184387\n",
      "Step: 1400  \tTraining loss: 0.3436717689037323\n",
      "Step: 1400  \tTraining accuracy: 0.8264716267585754\n",
      "Step: 1400  \tValid loss: 0.32346004247665405\n",
      "Step: 1500  \tTraining loss: 0.3424902856349945\n",
      "Step: 1500  \tTraining accuracy: 0.8279104828834534\n",
      "Step: 1500  \tValid loss: 0.32269808650016785\n",
      "Step: 1600  \tTraining loss: 0.34136027097702026\n",
      "Step: 1600  \tTraining accuracy: 0.8290895819664001\n",
      "Step: 1600  \tValid loss: 0.321919709444046\n",
      "Step: 1700  \tTraining loss: 0.3402753472328186\n",
      "Step: 1700  \tTraining accuracy: 0.8300687074661255\n",
      "Step: 1700  \tValid loss: 0.32113245129585266\n",
      "Step: 1800  \tTraining loss: 0.3392505943775177\n",
      "Step: 1800  \tTraining accuracy: 0.8310016393661499\n",
      "Step: 1800  \tValid loss: 0.3204173743724823\n",
      "Step: 1900  \tTraining loss: 0.33829835057258606\n",
      "Step: 1900  \tTraining accuracy: 0.8318619728088379\n",
      "Step: 1900  \tValid loss: 0.3197053074836731\n",
      "Step: 2000  \tTraining loss: 0.33742210268974304\n",
      "Step: 2000  \tTraining accuracy: 0.832757294178009\n",
      "Step: 2000  \tValid loss: 0.3191060721874237\n",
      "Step: 2100  \tTraining loss: 0.3366110324859619\n",
      "Step: 2100  \tTraining accuracy: 0.8336519002914429\n",
      "Step: 2100  \tValid loss: 0.3184787631034851\n",
      "Step: 2200  \tTraining loss: 0.33583948016166687\n",
      "Step: 2200  \tTraining accuracy: 0.8343806862831116\n",
      "Step: 2200  \tValid loss: 0.31788375973701477\n",
      "Step: 2300  \tTraining loss: 0.33508753776550293\n",
      "Step: 2300  \tTraining accuracy: 0.8350772261619568\n",
      "Step: 2300  \tValid loss: 0.3172557055950165\n",
      "Step: 2400  \tTraining loss: 0.3343302607536316\n",
      "Step: 2400  \tTraining accuracy: 0.83583003282547\n",
      "Step: 2400  \tValid loss: 0.3167206645011902\n",
      "Step: 2500  \tTraining loss: 0.3334462344646454\n",
      "Step: 2500  \tTraining accuracy: 0.8365726470947266\n",
      "Step: 2500  \tValid loss: 0.31639736890792847\n",
      "Step: 2600  \tTraining loss: 0.33252424001693726\n",
      "Step: 2600  \tTraining accuracy: 0.8372528553009033\n",
      "Step: 2600  \tValid loss: 0.3158611059188843\n",
      "Step: 2700  \tTraining loss: 0.3316461443901062\n",
      "Step: 2700  \tTraining accuracy: 0.837952733039856\n",
      "Step: 2700  \tValid loss: 0.31530579924583435\n",
      "Step: 2800  \tTraining loss: 0.33075159788131714\n",
      "Step: 2800  \tTraining accuracy: 0.8386929035186768\n",
      "Step: 2800  \tValid loss: 0.3147898018360138\n",
      "Step: 2900  \tTraining loss: 0.3298427164554596\n",
      "Step: 2900  \tTraining accuracy: 0.8393847942352295\n",
      "Step: 2900  \tValid loss: 0.31417316198349\n",
      "Step: 3000  \tTraining loss: 0.3288596272468567\n",
      "Step: 3000  \tTraining accuracy: 0.8400474786758423\n",
      "Step: 3000  \tValid loss: 0.31341803073883057\n",
      "Step: 3100  \tTraining loss: 0.3278283178806305\n",
      "Step: 3100  \tTraining accuracy: 0.840670108795166\n",
      "Step: 3100  \tValid loss: 0.31248319149017334\n",
      "Step: 3200  \tTraining loss: 0.326709508895874\n",
      "Step: 3200  \tTraining accuracy: 0.8412665128707886\n",
      "Step: 3200  \tValid loss: 0.31152066588401794\n",
      "Step: 3300  \tTraining loss: 0.3255433738231659\n",
      "Step: 3300  \tTraining accuracy: 0.8418165445327759\n",
      "Step: 3300  \tValid loss: 0.31058627367019653\n",
      "Step: 3400  \tTraining loss: 0.3243563175201416\n",
      "Step: 3400  \tTraining accuracy: 0.8423525094985962\n",
      "Step: 3400  \tValid loss: 0.309679239988327\n",
      "Step: 3500  \tTraining loss: 0.3232108950614929\n",
      "Step: 3500  \tTraining accuracy: 0.8428755402565002\n",
      "Step: 3500  \tValid loss: 0.30862653255462646\n",
      "Step: 3600  \tTraining loss: 0.32212188839912415\n",
      "Step: 3600  \tTraining accuracy: 0.843348503112793\n",
      "Step: 3600  \tValid loss: 0.30775055289268494\n",
      "Step: 3700  \tTraining loss: 0.3210866153240204\n",
      "Step: 3700  \tTraining accuracy: 0.84378981590271\n",
      "Step: 3700  \tValid loss: 0.3068735897541046\n",
      "Step: 3800  \tTraining loss: 0.3200341463088989\n",
      "Step: 3800  \tTraining accuracy: 0.8442047834396362\n",
      "Step: 3800  \tValid loss: 0.30643177032470703\n",
      "Step: 3900  \tTraining loss: 0.31911808252334595\n",
      "Step: 3900  \tTraining accuracy: 0.8445873856544495\n",
      "Step: 3900  \tValid loss: 0.3056151866912842\n",
      "Step: 4000  \tTraining loss: 0.3182622194290161\n",
      "Step: 4000  \tTraining accuracy: 0.8449373841285706\n",
      "Step: 4000  \tValid loss: 0.3048297166824341\n",
      "Step: 4100  \tTraining loss: 0.31745296716690063\n",
      "Step: 4100  \tTraining accuracy: 0.8452752232551575\n",
      "Step: 4100  \tValid loss: 0.3041413426399231\n",
      "Step: 4200  \tTraining loss: 0.31667885184288025\n",
      "Step: 4200  \tTraining accuracy: 0.8456270098686218\n",
      "Step: 4200  \tValid loss: 0.30337953567504883\n",
      "Step: 4300  \tTraining loss: 0.3159371614456177\n",
      "Step: 4300  \tTraining accuracy: 0.8459892868995667\n",
      "Step: 4300  \tValid loss: 0.3027579188346863\n",
      "Step: 4400  \tTraining loss: 0.315216064453125\n",
      "Step: 4400  \tTraining accuracy: 0.8463493585586548\n",
      "Step: 4400  \tValid loss: 0.30224838852882385\n",
      "Step: 4500  \tTraining loss: 0.31450578570365906\n",
      "Step: 4500  \tTraining accuracy: 0.8466790914535522\n",
      "Step: 4500  \tValid loss: 0.3017665147781372\n",
      "Step: 4600  \tTraining loss: 0.31380581855773926\n",
      "Step: 4600  \tTraining accuracy: 0.8469760417938232\n",
      "Step: 4600  \tValid loss: 0.30132970213890076\n",
      "Step: 4700  \tTraining loss: 0.313110888004303\n",
      "Step: 4700  \tTraining accuracy: 0.8472601175308228\n",
      "Step: 4700  \tValid loss: 0.300927072763443\n",
      "Step: 4800  \tTraining loss: 0.31243035197257996\n",
      "Step: 4800  \tTraining accuracy: 0.847547709941864\n",
      "Step: 4800  \tValid loss: 0.30052343010902405\n",
      "Step: 4900  \tTraining loss: 0.31176748871803284\n",
      "Step: 4900  \tTraining accuracy: 0.8478255867958069\n",
      "Step: 4900  \tValid loss: 0.30011674761772156\n",
      "Step: 5000  \tTraining loss: 0.3111218512058258\n",
      "Step: 5000  \tTraining accuracy: 0.8480964303016663\n",
      "Step: 5000  \tValid loss: 0.29975438117980957\n",
      "Step: 5100  \tTraining loss: 0.3104917109012604\n",
      "Step: 5100  \tTraining accuracy: 0.8483482599258423\n",
      "Step: 5100  \tValid loss: 0.2994116544723511\n",
      "Step: 5200  \tTraining loss: 0.30987465381622314\n",
      "Step: 5200  \tTraining accuracy: 0.8485944271087646\n",
      "Step: 5200  \tValid loss: 0.29912105202674866\n",
      "Step: 5300  \tTraining loss: 0.30927205085754395\n",
      "Step: 5300  \tTraining accuracy: 0.848857045173645\n",
      "Step: 5300  \tValid loss: 0.2988567352294922\n",
      "Step: 5400  \tTraining loss: 0.3086852729320526\n",
      "Step: 5400  \tTraining accuracy: 0.8491255044937134\n",
      "Step: 5400  \tValid loss: 0.2986011803150177\n",
      "Step: 5500  \tTraining loss: 0.30811160802841187\n",
      "Step: 5500  \tTraining accuracy: 0.8493878841400146\n",
      "Step: 5500  \tValid loss: 0.29835784435272217\n",
      "Step: 5600  \tTraining loss: 0.30754995346069336\n",
      "Step: 5600  \tTraining accuracy: 0.8496370911598206\n",
      "Step: 5600  \tValid loss: 0.2981536090373993\n",
      "Step: 5700  \tTraining loss: 0.30700135231018066\n",
      "Step: 5700  \tTraining accuracy: 0.849892258644104\n",
      "Step: 5700  \tValid loss: 0.2979208827018738\n",
      "Step: 5800  \tTraining loss: 0.30645817518234253\n",
      "Step: 5800  \tTraining accuracy: 0.8501513004302979\n",
      "Step: 5800  \tValid loss: 0.2978692650794983\n",
      "Step: 5900  \tTraining loss: 0.30593177676200867\n",
      "Step: 5900  \tTraining accuracy: 0.8503889441490173\n",
      "Step: 5900  \tValid loss: 0.2976076006889343\n",
      "Step: 6000  \tTraining loss: 0.3054217994213104\n",
      "Step: 6000  \tTraining accuracy: 0.8506203889846802\n",
      "Step: 6000  \tValid loss: 0.2974504828453064\n",
      "Step: 6100  \tTraining loss: 0.3049279451370239\n",
      "Step: 6100  \tTraining accuracy: 0.8508562445640564\n",
      "Step: 6100  \tValid loss: 0.2972490191459656\n",
      "Step: 6200  \tTraining loss: 0.30444902181625366\n",
      "Step: 6200  \tTraining accuracy: 0.8511047959327698\n",
      "Step: 6200  \tValid loss: 0.29713189601898193\n",
      "Step: 6300  \tTraining loss: 0.30398666858673096\n",
      "Step: 6300  \tTraining accuracy: 0.8513604998588562\n",
      "Step: 6300  \tValid loss: 0.29695838689804077\n",
      "Step: 6400  \tTraining loss: 0.3035392463207245\n",
      "Step: 6400  \tTraining accuracy: 0.8516080975532532\n",
      "Step: 6400  \tValid loss: 0.2968183159828186\n",
      "Step: 6500  \tTraining loss: 0.3031044900417328\n",
      "Step: 6500  \tTraining accuracy: 0.8518561720848083\n",
      "Step: 6500  \tValid loss: 0.29663318395614624\n",
      "Step: 6600  \tTraining loss: 0.30268433690071106\n",
      "Step: 6600  \tTraining accuracy: 0.8520886898040771\n",
      "Step: 6600  \tValid loss: 0.2964741885662079\n",
      "Step: 6700  \tTraining loss: 0.3022633492946625\n",
      "Step: 6700  \tTraining accuracy: 0.8523000478744507\n",
      "Step: 6700  \tValid loss: 0.29637956619262695\n",
      "Step: 6800  \tTraining loss: 0.30185002088546753\n",
      "Step: 6800  \tTraining accuracy: 0.8524989485740662\n",
      "Step: 6800  \tValid loss: 0.2962131202220917\n",
      "Step: 6900  \tTraining loss: 0.3014475107192993\n",
      "Step: 6900  \tTraining accuracy: 0.8526920080184937\n",
      "Step: 6900  \tValid loss: 0.29608869552612305\n",
      "Step: 7000  \tTraining loss: 0.30105462670326233\n",
      "Step: 7000  \tTraining accuracy: 0.8528841137886047\n",
      "Step: 7000  \tValid loss: 0.29595816135406494\n",
      "Step: 7100  \tTraining loss: 0.30064988136291504\n",
      "Step: 7100  \tTraining accuracy: 0.8530706763267517\n",
      "Step: 7100  \tValid loss: 0.2958782911300659\n",
      "Step: 7200  \tTraining loss: 0.3002772033214569\n",
      "Step: 7200  \tTraining accuracy: 0.8532578945159912\n",
      "Step: 7200  \tValid loss: 0.295749694108963\n",
      "Step: 7300  \tTraining loss: 0.29991668462753296\n",
      "Step: 7300  \tTraining accuracy: 0.8534414172172546\n",
      "Step: 7300  \tValid loss: 0.29559507966041565\n",
      "Step: 7400  \tTraining loss: 0.2995707392692566\n",
      "Step: 7400  \tTraining accuracy: 0.8536157011985779\n",
      "Step: 7400  \tValid loss: 0.2955256998538971\n",
      "Step: 7500  \tTraining loss: 0.29923713207244873\n",
      "Step: 7500  \tTraining accuracy: 0.8537880778312683\n",
      "Step: 7500  \tValid loss: 0.2954100966453552\n",
      "Step: 7600  \tTraining loss: 0.2989157736301422\n",
      "Step: 7600  \tTraining accuracy: 0.853962779045105\n",
      "Step: 7600  \tValid loss: 0.2953169047832489\n",
      "Step: 7700  \tTraining loss: 0.2986079752445221\n",
      "Step: 7700  \tTraining accuracy: 0.8541370630264282\n",
      "Step: 7700  \tValid loss: 0.2952142059803009\n",
      "Step: 7800  \tTraining loss: 0.29830703139305115\n",
      "Step: 7800  \tTraining accuracy: 0.8543000817298889\n",
      "Step: 7800  \tValid loss: 0.2950395345687866\n",
      "Step: 7900  \tTraining loss: 0.2980135679244995\n",
      "Step: 7900  \tTraining accuracy: 0.8544589281082153\n",
      "Step: 7900  \tValid loss: 0.2948744297027588\n",
      "Step: 8000  \tTraining loss: 0.29772672057151794\n",
      "Step: 8000  \tTraining accuracy: 0.8546098470687866\n",
      "Step: 8000  \tValid loss: 0.2947302758693695\n",
      "Step: 8100  \tTraining loss: 0.29744142293930054\n",
      "Step: 8100  \tTraining accuracy: 0.8547531366348267\n",
      "Step: 8100  \tValid loss: 0.2945394217967987\n",
      "Step: 8200  \tTraining loss: 0.29716724157333374\n",
      "Step: 8200  \tTraining accuracy: 0.8548877835273743\n",
      "Step: 8200  \tValid loss: 0.29445508122444153\n",
      "Step: 8300  \tTraining loss: 0.2968960404396057\n",
      "Step: 8300  \tTraining accuracy: 0.8550191521644592\n",
      "Step: 8300  \tValid loss: 0.2942609488964081\n",
      "Step: 8400  \tTraining loss: 0.29663220047950745\n",
      "Step: 8400  \tTraining accuracy: 0.8551536202430725\n",
      "Step: 8400  \tValid loss: 0.2941088378429413\n",
      "Step: 8500  \tTraining loss: 0.2963714599609375\n",
      "Step: 8500  \tTraining accuracy: 0.8552874326705933\n",
      "Step: 8500  \tValid loss: 0.2939956486225128\n",
      "Step: 8600  \tTraining loss: 0.2961134910583496\n",
      "Step: 8600  \tTraining accuracy: 0.8554290533065796\n",
      "Step: 8600  \tValid loss: 0.29390352964401245\n",
      "Step: 8700  \tTraining loss: 0.29586049914360046\n",
      "Step: 8700  \tTraining accuracy: 0.8555794954299927\n",
      "Step: 8700  \tValid loss: 0.2936832904815674\n",
      "Step: 8800  \tTraining loss: 0.2956123650074005\n",
      "Step: 8800  \tTraining accuracy: 0.8557384610176086\n",
      "Step: 8800  \tValid loss: 0.2935790419578552\n",
      "Step: 8900  \tTraining loss: 0.2953653037548065\n",
      "Step: 8900  \tTraining accuracy: 0.8558902740478516\n",
      "Step: 8900  \tValid loss: 0.2934475839138031\n",
      "Step: 9000  \tTraining loss: 0.2951076030731201\n",
      "Step: 9000  \tTraining accuracy: 0.856042206287384\n",
      "Step: 9000  \tValid loss: 0.2932863235473633\n",
      "Step: 9100  \tTraining loss: 0.29486316442489624\n",
      "Step: 9100  \tTraining accuracy: 0.8561988472938538\n",
      "Step: 9100  \tValid loss: 0.293104887008667\n",
      "Step: 9200  \tTraining loss: 0.2946244180202484\n",
      "Step: 9200  \tTraining accuracy: 0.856353223323822\n",
      "Step: 9200  \tValid loss: 0.29301726818084717\n",
      "Step: 9300  \tTraining loss: 0.294390469789505\n",
      "Step: 9300  \tTraining accuracy: 0.8565019965171814\n",
      "Step: 9300  \tValid loss: 0.29290831089019775\n",
      "Step: 9400  \tTraining loss: 0.29416316747665405\n",
      "Step: 9400  \tTraining accuracy: 0.8566420078277588\n",
      "Step: 9400  \tValid loss: 0.2928446829319\n",
      "Step: 9500  \tTraining loss: 0.29393476247787476\n",
      "Step: 9500  \tTraining accuracy: 0.8567823767662048\n",
      "Step: 9500  \tValid loss: 0.29270848631858826\n",
      "Step: 9600  \tTraining loss: 0.2937139570713043\n",
      "Step: 9600  \tTraining accuracy: 0.8569285273551941\n",
      "Step: 9600  \tValid loss: 0.2926153838634491\n",
      "Step: 9700  \tTraining loss: 0.2934959828853607\n",
      "Step: 9700  \tTraining accuracy: 0.8570727705955505\n",
      "Step: 9700  \tValid loss: 0.29253995418548584\n",
      "Step: 9800  \tTraining loss: 0.2932785451412201\n",
      "Step: 9800  \tTraining accuracy: 0.8572129607200623\n",
      "Step: 9800  \tValid loss: 0.2923634946346283\n",
      "Step: 9900  \tTraining loss: 0.2930712103843689\n",
      "Step: 9900  \tTraining accuracy: 0.8573460578918457\n",
      "Step: 9900  \tValid loss: 0.2923451066017151\n",
      "Step: 10000  \tTraining loss: 0.29285988211631775\n",
      "Step: 10000  \tTraining accuracy: 0.85747230052948\n",
      "Step: 10000  \tValid loss: 0.29221245646476746\n",
      "Step: 10100  \tTraining loss: 0.2926572859287262\n",
      "Step: 10100  \tTraining accuracy: 0.8575929403305054\n",
      "Step: 10100  \tValid loss: 0.29218411445617676\n",
      "Step: 10200  \tTraining loss: 0.29245057702064514\n",
      "Step: 10200  \tTraining accuracy: 0.8577090501785278\n",
      "Step: 10200  \tValid loss: 0.2920193672180176\n",
      "Step: 10300  \tTraining loss: 0.29224634170532227\n",
      "Step: 10300  \tTraining accuracy: 0.8578229546546936\n",
      "Step: 10300  \tValid loss: 0.2919498682022095\n",
      "Step: 10400  \tTraining loss: 0.2920451760292053\n",
      "Step: 10400  \tTraining accuracy: 0.857936680316925\n",
      "Step: 10400  \tValid loss: 0.29186296463012695\n",
      "Step: 10500  \tTraining loss: 0.29184800386428833\n",
      "Step: 10500  \tTraining accuracy: 0.8580512404441833\n",
      "Step: 10500  \tValid loss: 0.2918224036693573\n",
      "Step: 10600  \tTraining loss: 0.29164814949035645\n",
      "Step: 10600  \tTraining accuracy: 0.858163595199585\n",
      "Step: 10600  \tValid loss: 0.29172322154045105\n",
      "Step: 10700  \tTraining loss: 0.2914535105228424\n",
      "Step: 10700  \tTraining accuracy: 0.8582689166069031\n",
      "Step: 10700  \tValid loss: 0.29165542125701904\n",
      "Step: 10800  \tTraining loss: 0.2912587821483612\n",
      "Step: 10800  \tTraining accuracy: 0.8583694100379944\n",
      "Step: 10800  \tValid loss: 0.2915845811367035\n",
      "Step: 10900  \tTraining loss: 0.29106566309928894\n",
      "Step: 10900  \tTraining accuracy: 0.8584709167480469\n",
      "Step: 10900  \tValid loss: 0.29151850938796997\n",
      "Step: 11000  \tTraining loss: 0.2908737063407898\n",
      "Step: 11000  \tTraining accuracy: 0.8585677146911621\n",
      "Step: 11000  \tValid loss: 0.2914581894874573\n",
      "Step: 11100  \tTraining loss: 0.2906827926635742\n",
      "Step: 11100  \tTraining accuracy: 0.8586618304252625\n",
      "Step: 11100  \tValid loss: 0.29137590527534485\n",
      "Step: 11200  \tTraining loss: 0.2904927134513855\n",
      "Step: 11200  \tTraining accuracy: 0.8587533235549927\n",
      "Step: 11200  \tValid loss: 0.291340172290802\n",
      "Step: 11300  \tTraining loss: 0.2903038263320923\n",
      "Step: 11300  \tTraining accuracy: 0.8588486909866333\n",
      "Step: 11300  \tValid loss: 0.29126405715942383\n",
      "Step: 11400  \tTraining loss: 0.2901146113872528\n",
      "Step: 11400  \tTraining accuracy: 0.8589479923248291\n",
      "Step: 11400  \tValid loss: 0.2911987900733948\n",
      "Step: 11500  \tTraining loss: 0.28992965817451477\n",
      "Step: 11500  \tTraining accuracy: 0.8590518832206726\n",
      "Step: 11500  \tValid loss: 0.29119470715522766\n",
      "Step: 11600  \tTraining loss: 0.28974059224128723\n",
      "Step: 11600  \tTraining accuracy: 0.8591621518135071\n",
      "Step: 11600  \tValid loss: 0.29110023379325867\n",
      "Step: 11700  \tTraining loss: 0.2895555794239044\n",
      "Step: 11700  \tTraining accuracy: 0.8592696189880371\n",
      "Step: 11700  \tValid loss: 0.29104700684547424\n",
      "Step: 11800  \tTraining loss: 0.2893710136413574\n",
      "Step: 11800  \tTraining accuracy: 0.8593735098838806\n",
      "Step: 11800  \tValid loss: 0.2909829020500183\n",
      "Step: 11900  \tTraining loss: 0.2891883850097656\n",
      "Step: 11900  \tTraining accuracy: 0.8594764471054077\n",
      "Step: 11900  \tValid loss: 0.29098257422447205\n",
      "Step: 12000  \tTraining loss: 0.2890045642852783\n",
      "Step: 12000  \tTraining accuracy: 0.8595768809318542\n",
      "Step: 12000  \tValid loss: 0.2908855080604553\n",
      "Step: 12100  \tTraining loss: 0.2888229787349701\n",
      "Step: 12100  \tTraining accuracy: 0.8596729636192322\n",
      "Step: 12100  \tValid loss: 0.29085707664489746\n",
      "Step: 12200  \tTraining loss: 0.2886403799057007\n",
      "Step: 12200  \tTraining accuracy: 0.8597658276557922\n",
      "Step: 12200  \tValid loss: 0.2908015251159668\n",
      "Step: 12300  \tTraining loss: 0.2884601652622223\n",
      "Step: 12300  \tTraining accuracy: 0.8598571419715881\n",
      "Step: 12300  \tValid loss: 0.2907866835594177\n",
      "Step: 12400  \tTraining loss: 0.28828054666519165\n",
      "Step: 12400  \tTraining accuracy: 0.8599511981010437\n",
      "Step: 12400  \tValid loss: 0.2907477021217346\n",
      "Step: 12500  \tTraining loss: 0.2881002128124237\n",
      "Step: 12500  \tTraining accuracy: 0.8600428700447083\n",
      "Step: 12500  \tValid loss: 0.29074499011039734\n",
      "Step: 12600  \tTraining loss: 0.28792500495910645\n",
      "Step: 12600  \tTraining accuracy: 0.8601298332214355\n",
      "Step: 12600  \tValid loss: 0.2906891703605652\n",
      "Step: 12700  \tTraining loss: 0.2877422273159027\n",
      "Step: 12700  \tTraining accuracy: 0.8602153658866882\n",
      "Step: 12700  \tValid loss: 0.2906148135662079\n",
      "Step: 12800  \tTraining loss: 0.2875676155090332\n",
      "Step: 12800  \tTraining accuracy: 0.8603011965751648\n",
      "Step: 12800  \tValid loss: 0.29061150550842285\n",
      "Step: 12900  \tTraining loss: 0.28739258646965027\n",
      "Step: 12900  \tTraining accuracy: 0.8603881001472473\n",
      "Step: 12900  \tValid loss: 0.29066792130470276\n",
      "Step: 13000  \tTraining loss: 0.28721487522125244\n",
      "Step: 13000  \tTraining accuracy: 0.8604737520217896\n",
      "Step: 13000  \tValid loss: 0.29060959815979004\n",
      "Step: 13100  \tTraining loss: 0.2870384454727173\n",
      "Step: 13100  \tTraining accuracy: 0.8605524301528931\n",
      "Step: 13100  \tValid loss: 0.2905665636062622\n",
      "Step: 13200  \tTraining loss: 0.2868656516075134\n",
      "Step: 13200  \tTraining accuracy: 0.8606235384941101\n",
      "Step: 13200  \tValid loss: 0.2905491590499878\n",
      "Step: 13300  \tTraining loss: 0.28669241070747375\n",
      "Step: 13300  \tTraining accuracy: 0.8606935739517212\n",
      "Step: 13300  \tValid loss: 0.29056936502456665\n",
      "Step: 13400  \tTraining loss: 0.2865169048309326\n",
      "Step: 13400  \tTraining accuracy: 0.8607633709907532\n",
      "Step: 13400  \tValid loss: 0.29048699140548706\n",
      "Step: 13500  \tTraining loss: 0.2863461971282959\n",
      "Step: 13500  \tTraining accuracy: 0.8608298301696777\n",
      "Step: 13500  \tValid loss: 0.2905077040195465\n",
      "Step: 13600  \tTraining loss: 0.2861725091934204\n",
      "Step: 13600  \tTraining accuracy: 0.8608952760696411\n",
      "Step: 13600  \tValid loss: 0.29044049978256226\n",
      "Step: 13700  \tTraining loss: 0.28599920868873596\n",
      "Step: 13700  \tTraining accuracy: 0.8609681725502014\n",
      "Step: 13700  \tValid loss: 0.2904854416847229\n",
      "Step: 13800  \tTraining loss: 0.2858278155326843\n",
      "Step: 13800  \tTraining accuracy: 0.8610430359840393\n",
      "Step: 13800  \tValid loss: 0.2904636263847351\n",
      "Step: 13900  \tTraining loss: 0.28565263748168945\n",
      "Step: 13900  \tTraining accuracy: 0.8611145615577698\n",
      "Step: 13900  \tValid loss: 0.29044920206069946\n",
      "Step: 14000  \tTraining loss: 0.28547224402427673\n",
      "Step: 14000  \tTraining accuracy: 0.8611873388290405\n",
      "Step: 14000  \tValid loss: 0.29042354226112366\n",
      "Step: 14100  \tTraining loss: 0.2852953374385834\n",
      "Step: 14100  \tTraining accuracy: 0.8612605333328247\n",
      "Step: 14100  \tValid loss: 0.29043275117874146\n",
      "Step: 14200  \tTraining loss: 0.2851199209690094\n",
      "Step: 14200  \tTraining accuracy: 0.8613327145576477\n",
      "Step: 14200  \tValid loss: 0.29036277532577515\n",
      "Step: 14300  \tTraining loss: 0.28494882583618164\n",
      "Step: 14300  \tTraining accuracy: 0.8614038825035095\n",
      "Step: 14300  \tValid loss: 0.2904086112976074\n",
      "Step: 14400  \tTraining loss: 0.28477558493614197\n",
      "Step: 14400  \tTraining accuracy: 0.8614740371704102\n",
      "Step: 14400  \tValid loss: 0.2903539836406708\n",
      "Step: 14500  \tTraining loss: 0.28460317850112915\n",
      "Step: 14500  \tTraining accuracy: 0.8615432381629944\n",
      "Step: 14500  \tValid loss: 0.2903665602207184\n",
      "Step: 14600  \tTraining loss: 0.2844334840774536\n",
      "Step: 14600  \tTraining accuracy: 0.8616100549697876\n",
      "Step: 14600  \tValid loss: 0.2903817296028137\n",
      "Step: 14700  \tTraining loss: 0.28426387906074524\n",
      "Step: 14700  \tTraining accuracy: 0.8616716861724854\n",
      "Step: 14700  \tValid loss: 0.2903898358345032\n",
      "Step: 14800  \tTraining loss: 0.2840912342071533\n",
      "Step: 14800  \tTraining accuracy: 0.8617310523986816\n",
      "Step: 14800  \tValid loss: 0.2903715670108795\n",
      "Step: 14900  \tTraining loss: 0.28392288088798523\n",
      "Step: 14900  \tTraining accuracy: 0.861791729927063\n",
      "Step: 14900  \tValid loss: 0.2903272807598114\n",
      "Step: 15000  \tTraining loss: 0.28375309705734253\n",
      "Step: 15000  \tTraining accuracy: 0.8618530035018921\n",
      "Step: 15000  \tValid loss: 0.2903723120689392\n",
      "Step: 15100  \tTraining loss: 0.28358063101768494\n",
      "Step: 15100  \tTraining accuracy: 0.8619134426116943\n",
      "Step: 15100  \tValid loss: 0.29035165905952454\n",
      "Step: 15200  \tTraining loss: 0.2834143042564392\n",
      "Step: 15200  \tTraining accuracy: 0.8619703054428101\n",
      "Step: 15200  \tValid loss: 0.29038748145103455\n",
      "Step: 15300  \tTraining loss: 0.2832428812980652\n",
      "Step: 15300  \tTraining accuracy: 0.8620251417160034\n",
      "Step: 15300  \tValid loss: 0.2903498709201813\n",
      "Step: 15400  \tTraining loss: 0.28307440876960754\n",
      "Step: 15400  \tTraining accuracy: 0.862076461315155\n",
      "Step: 15400  \tValid loss: 0.29035884141921997\n",
      "Step: 15500  \tTraining loss: 0.28290653228759766\n",
      "Step: 15500  \tTraining accuracy: 0.8621291518211365\n",
      "Step: 15500  \tValid loss: 0.2903577983379364\n",
      "Step: 15600  \tTraining loss: 0.28273719549179077\n",
      "Step: 15600  \tTraining accuracy: 0.862179160118103\n",
      "Step: 15600  \tValid loss: 0.2904198467731476\n",
      "Step: 15700  \tTraining loss: 0.282573401927948\n",
      "Step: 15700  \tTraining accuracy: 0.8622238636016846\n",
      "Step: 15700  \tValid loss: 0.29047369956970215\n",
      "Step: 15800  \tTraining loss: 0.2824002504348755\n",
      "Step: 15800  \tTraining accuracy: 0.8622640371322632\n",
      "Step: 15800  \tValid loss: 0.2903900742530823\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8623004\n",
      "Precision: 0.89590687\n",
      "Recall: 0.89464414\n",
      "F1 score: 0.8616923\n",
      "AUC: 0.8715645\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0    0.8623   0.895907  0.894644  0.861692  0.871565  0.282274      0.862257   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.290261        0.86222   0.345146      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  15875.0  \n",
      "11\n",
      "(3770, 8)\n",
      "(3770, 1)\n",
      "(2080, 8)\n",
      "(2080, 1)\n",
      "(1690, 8)\n",
      "(1690, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4981546103954315\n",
      "Step: 100  \tTraining accuracy: 0.7832891345024109\n",
      "Step: 100  \tValid loss: 0.5028655529022217\n",
      "Step: 200  \tTraining loss: 0.37816283106803894\n",
      "Step: 200  \tTraining accuracy: 0.7989389896392822\n",
      "Step: 200  \tValid loss: 0.396840900182724\n",
      "Step: 300  \tTraining loss: 0.34407365322113037\n",
      "Step: 300  \tTraining accuracy: 0.810769259929657\n",
      "Step: 300  \tValid loss: 0.3704192638397217\n",
      "Step: 400  \tTraining loss: 0.33431586623191833\n",
      "Step: 400  \tTraining accuracy: 0.8176960945129395\n",
      "Step: 400  \tValid loss: 0.3628041446208954\n",
      "Step: 500  \tTraining loss: 0.3287336230278015\n",
      "Step: 500  \tTraining accuracy: 0.8217211961746216\n",
      "Step: 500  \tValid loss: 0.35698625445365906\n",
      "Step: 600  \tTraining loss: 0.3240993022918701\n",
      "Step: 600  \tTraining accuracy: 0.8243308663368225\n",
      "Step: 600  \tValid loss: 0.3513966202735901\n",
      "Step: 700  \tTraining loss: 0.31985947489738464\n",
      "Step: 700  \tTraining accuracy: 0.8263415694236755\n",
      "Step: 700  \tValid loss: 0.34616947174072266\n",
      "Step: 800  \tTraining loss: 0.31578904390335083\n",
      "Step: 800  \tTraining accuracy: 0.8278691172599792\n",
      "Step: 800  \tValid loss: 0.3412318229675293\n",
      "Step: 900  \tTraining loss: 0.31177574396133423\n",
      "Step: 900  \tTraining accuracy: 0.8293025493621826\n",
      "Step: 900  \tValid loss: 0.3365238904953003\n",
      "Step: 1000  \tTraining loss: 0.3078928291797638\n",
      "Step: 1000  \tTraining accuracy: 0.830601692199707\n",
      "Step: 1000  \tValid loss: 0.33200445771217346\n",
      "Step: 1100  \tTraining loss: 0.3042154610157013\n",
      "Step: 1100  \tTraining accuracy: 0.8317670822143555\n",
      "Step: 1100  \tValid loss: 0.327837198972702\n",
      "Step: 1200  \tTraining loss: 0.30076146125793457\n",
      "Step: 1200  \tTraining accuracy: 0.8324875831604004\n",
      "Step: 1200  \tValid loss: 0.3239890933036804\n",
      "Step: 1300  \tTraining loss: 0.2975250482559204\n",
      "Step: 1300  \tTraining accuracy: 0.8331140875816345\n",
      "Step: 1300  \tValid loss: 0.32056522369384766\n",
      "Step: 1400  \tTraining loss: 0.2945494055747986\n",
      "Step: 1400  \tTraining accuracy: 0.8336771726608276\n",
      "Step: 1400  \tValid loss: 0.3174356520175934\n",
      "Step: 1500  \tTraining loss: 0.29183340072631836\n",
      "Step: 1500  \tTraining accuracy: 0.834089457988739\n",
      "Step: 1500  \tValid loss: 0.31467607617378235\n",
      "Step: 1600  \tTraining loss: 0.28937727212905884\n",
      "Step: 1600  \tTraining accuracy: 0.834482729434967\n",
      "Step: 1600  \tValid loss: 0.31229788064956665\n",
      "Step: 1700  \tTraining loss: 0.2871677875518799\n",
      "Step: 1700  \tTraining accuracy: 0.8348042964935303\n",
      "Step: 1700  \tValid loss: 0.31025537848472595\n",
      "Step: 1800  \tTraining loss: 0.2852107584476471\n",
      "Step: 1800  \tTraining accuracy: 0.8351497054100037\n",
      "Step: 1800  \tValid loss: 0.30859920382499695\n",
      "Step: 1900  \tTraining loss: 0.283515065908432\n",
      "Step: 1900  \tTraining accuracy: 0.8355007767677307\n",
      "Step: 1900  \tValid loss: 0.3072889447212219\n",
      "Step: 2000  \tTraining loss: 0.2820836007595062\n",
      "Step: 2000  \tTraining accuracy: 0.8359994292259216\n",
      "Step: 2000  \tValid loss: 0.3063030540943146\n",
      "Step: 2100  \tTraining loss: 0.28090330958366394\n",
      "Step: 2100  \tTraining accuracy: 0.8364365696907043\n",
      "Step: 2100  \tValid loss: 0.3055630624294281\n",
      "Step: 2200  \tTraining loss: 0.279941201210022\n",
      "Step: 2200  \tTraining accuracy: 0.8368515372276306\n",
      "Step: 2200  \tValid loss: 0.3050077557563782\n",
      "Step: 2300  \tTraining loss: 0.27915623784065247\n",
      "Step: 2300  \tTraining accuracy: 0.837282657623291\n",
      "Step: 2300  \tValid loss: 0.30458882451057434\n",
      "Step: 2400  \tTraining loss: 0.27850908041000366\n",
      "Step: 2400  \tTraining accuracy: 0.8377278447151184\n",
      "Step: 2400  \tValid loss: 0.3042842149734497\n",
      "Step: 2500  \tTraining loss: 0.27797165513038635\n",
      "Step: 2500  \tTraining accuracy: 0.8381258845329285\n",
      "Step: 2500  \tValid loss: 0.3040512204170227\n",
      "Step: 2600  \tTraining loss: 0.2775106132030487\n",
      "Step: 2600  \tTraining accuracy: 0.8385135531425476\n",
      "Step: 2600  \tValid loss: 0.3038645386695862\n",
      "Step: 2700  \tTraining loss: 0.2770993411540985\n",
      "Step: 2700  \tTraining accuracy: 0.8389520049095154\n",
      "Step: 2700  \tValid loss: 0.30371296405792236\n",
      "Step: 2800  \tTraining loss: 0.27671176195144653\n",
      "Step: 2800  \tTraining accuracy: 0.8393827080726624\n",
      "Step: 2800  \tValid loss: 0.3035210967063904\n",
      "Step: 2900  \tTraining loss: 0.2763097882270813\n",
      "Step: 2900  \tTraining accuracy: 0.839820384979248\n",
      "Step: 2900  \tValid loss: 0.30316832661628723\n",
      "Step: 3000  \tTraining loss: 0.27591532468795776\n",
      "Step: 3000  \tTraining accuracy: 0.8402193784713745\n",
      "Step: 3000  \tValid loss: 0.3029767572879791\n",
      "Step: 3100  \tTraining loss: 0.2755441665649414\n",
      "Step: 3100  \tTraining accuracy: 0.8406096696853638\n",
      "Step: 3100  \tValid loss: 0.30293238162994385\n",
      "Step: 3200  \tTraining loss: 0.2752177119255066\n",
      "Step: 3200  \tTraining accuracy: 0.840983510017395\n",
      "Step: 3200  \tValid loss: 0.30300503969192505\n",
      "Step: 3300  \tTraining loss: 0.2749268114566803\n",
      "Step: 3300  \tTraining accuracy: 0.8413180708885193\n",
      "Step: 3300  \tValid loss: 0.3031122088432312\n",
      "Step: 3400  \tTraining loss: 0.2746606171131134\n",
      "Step: 3400  \tTraining accuracy: 0.8416247963905334\n",
      "Step: 3400  \tValid loss: 0.3031902313232422\n",
      "Step: 3500  \tTraining loss: 0.2744159400463104\n",
      "Step: 3500  \tTraining accuracy: 0.841913640499115\n",
      "Step: 3500  \tValid loss: 0.30324509739875793\n",
      "Step: 3600  \tTraining loss: 0.27419018745422363\n",
      "Step: 3600  \tTraining accuracy: 0.8421825170516968\n",
      "Step: 3600  \tValid loss: 0.3032900094985962\n",
      "Step: 3700  \tTraining loss: 0.2739817798137665\n",
      "Step: 3700  \tTraining accuracy: 0.8424403071403503\n",
      "Step: 3700  \tValid loss: 0.30331966280937195\n",
      "Step: 3800  \tTraining loss: 0.2737884223461151\n",
      "Step: 3800  \tTraining accuracy: 0.8426914215087891\n",
      "Step: 3800  \tValid loss: 0.30335041880607605\n",
      "Step: 3900  \tTraining loss: 0.27360799908638\n",
      "Step: 3900  \tTraining accuracy: 0.8429432511329651\n",
      "Step: 3900  \tValid loss: 0.3033705949783325\n",
      "Step: 4000  \tTraining loss: 0.2734384536743164\n",
      "Step: 4000  \tTraining accuracy: 0.8431555032730103\n",
      "Step: 4000  \tValid loss: 0.30337318778038025\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8433343\n",
      "Precision: 0.68766403\n",
      "Recall: 0.6405868\n",
      "F1 score: 0.81344396\n",
      "AUC: 0.77998173\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.843334   0.687664  0.640587  0.813444  0.779982  0.273308       0.84314   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.302926       0.843082   0.303892      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  4079.0  \n",
      "12\n",
      "(3770, 8)\n",
      "(3770, 1)\n",
      "(2000, 8)\n",
      "(2000, 1)\n",
      "(1625, 8)\n",
      "(1625, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.524355947971344\n",
      "Step: 100  \tTraining accuracy: 0.7281166911125183\n",
      "Step: 100  \tValid loss: 0.5247493982315063\n",
      "Step: 200  \tTraining loss: 0.46142473816871643\n",
      "Step: 200  \tTraining accuracy: 0.7356919050216675\n",
      "Step: 200  \tValid loss: 0.4707183837890625\n",
      "Step: 300  \tTraining loss: 0.4422151744365692\n",
      "Step: 300  \tTraining accuracy: 0.7454741597175598\n",
      "Step: 300  \tValid loss: 0.4543052315711975\n",
      "Step: 400  \tTraining loss: 0.42840853333473206\n",
      "Step: 400  \tTraining accuracy: 0.7543055415153503\n",
      "Step: 400  \tValid loss: 0.4397284686565399\n",
      "Step: 500  \tTraining loss: 0.4121782183647156\n",
      "Step: 500  \tTraining accuracy: 0.7626386880874634\n",
      "Step: 500  \tValid loss: 0.42203307151794434\n",
      "Step: 600  \tTraining loss: 0.3950425982475281\n",
      "Step: 600  \tTraining accuracy: 0.7710393667221069\n",
      "Step: 600  \tValid loss: 0.4037601351737976\n",
      "Step: 700  \tTraining loss: 0.3806714117527008\n",
      "Step: 700  \tTraining accuracy: 0.7788118124008179\n",
      "Step: 700  \tValid loss: 0.3890446424484253\n",
      "Step: 800  \tTraining loss: 0.37038564682006836\n",
      "Step: 800  \tTraining accuracy: 0.7859548330307007\n",
      "Step: 800  \tValid loss: 0.37903285026550293\n",
      "Step: 900  \tTraining loss: 0.36346885561943054\n",
      "Step: 900  \tTraining accuracy: 0.7920228838920593\n",
      "Step: 900  \tValid loss: 0.37258681654930115\n",
      "Step: 1000  \tTraining loss: 0.35895612835884094\n",
      "Step: 1000  \tTraining accuracy: 0.7970280647277832\n",
      "Step: 1000  \tValid loss: 0.36850255727767944\n",
      "Step: 1100  \tTraining loss: 0.3561003506183624\n",
      "Step: 1100  \tTraining accuracy: 0.8014668226242065\n",
      "Step: 1100  \tValid loss: 0.36594322323799133\n",
      "Step: 1200  \tTraining loss: 0.35431814193725586\n",
      "Step: 1200  \tTraining accuracy: 0.805075466632843\n",
      "Step: 1200  \tValid loss: 0.3643111288547516\n",
      "Step: 1300  \tTraining loss: 0.353179395198822\n",
      "Step: 1300  \tTraining accuracy: 0.8076748251914978\n",
      "Step: 1300  \tValid loss: 0.363205224275589\n",
      "Step: 1400  \tTraining loss: 0.3524008095264435\n",
      "Step: 1400  \tTraining accuracy: 0.8098393678665161\n",
      "Step: 1400  \tValid loss: 0.3623811602592468\n",
      "Step: 1500  \tTraining loss: 0.3518105447292328\n",
      "Step: 1500  \tTraining accuracy: 0.8116682171821594\n",
      "Step: 1500  \tValid loss: 0.36170661449432373\n",
      "Step: 1600  \tTraining loss: 0.35131004452705383\n",
      "Step: 1600  \tTraining accuracy: 0.8133484721183777\n",
      "Step: 1600  \tValid loss: 0.36111292243003845\n",
      "Step: 1700  \tTraining loss: 0.3508411943912506\n",
      "Step: 1700  \tTraining accuracy: 0.8149070143699646\n",
      "Step: 1700  \tValid loss: 0.360580176115036\n",
      "Step: 1800  \tTraining loss: 0.35036683082580566\n",
      "Step: 1800  \tTraining accuracy: 0.8162644505500793\n",
      "Step: 1800  \tValid loss: 0.36008885502815247\n",
      "Step: 1900  \tTraining loss: 0.34985777735710144\n",
      "Step: 1900  \tTraining accuracy: 0.8174605369567871\n",
      "Step: 1900  \tValid loss: 0.3596401512622833\n",
      "Step: 2000  \tTraining loss: 0.3493044674396515\n",
      "Step: 2000  \tTraining accuracy: 0.8184993863105774\n",
      "Step: 2000  \tValid loss: 0.35926905274391174\n",
      "Step: 2100  \tTraining loss: 0.34871652722358704\n",
      "Step: 2100  \tTraining accuracy: 0.8194303512573242\n",
      "Step: 2100  \tValid loss: 0.35908061265945435\n",
      "Step: 2200  \tTraining loss: 0.34812045097351074\n",
      "Step: 2200  \tTraining accuracy: 0.8202621340751648\n",
      "Step: 2200  \tValid loss: 0.3589709401130676\n",
      "Step: 2300  \tTraining loss: 0.3475140631198883\n",
      "Step: 2300  \tTraining accuracy: 0.8210321068763733\n",
      "Step: 2300  \tValid loss: 0.3588548004627228\n",
      "Step: 2400  \tTraining loss: 0.3468885123729706\n",
      "Step: 2400  \tTraining accuracy: 0.8217422366142273\n",
      "Step: 2400  \tValid loss: 0.3583538830280304\n",
      "Step: 2500  \tTraining loss: 0.3462390601634979\n",
      "Step: 2500  \tTraining accuracy: 0.8223613500595093\n",
      "Step: 2500  \tValid loss: 0.3579992651939392\n",
      "Step: 2600  \tTraining loss: 0.3455764651298523\n",
      "Step: 2600  \tTraining accuracy: 0.8229107856750488\n",
      "Step: 2600  \tValid loss: 0.357667475938797\n",
      "Step: 2700  \tTraining loss: 0.3448947072029114\n",
      "Step: 2700  \tTraining accuracy: 0.8235207200050354\n",
      "Step: 2700  \tValid loss: 0.35731247067451477\n",
      "Step: 2800  \tTraining loss: 0.34419506788253784\n",
      "Step: 2800  \tTraining accuracy: 0.8240814208984375\n",
      "Step: 2800  \tValid loss: 0.35685765743255615\n",
      "Step: 2900  \tTraining loss: 0.34347912669181824\n",
      "Step: 2900  \tTraining accuracy: 0.8246217370033264\n",
      "Step: 2900  \tValid loss: 0.3564155101776123\n",
      "Step: 3000  \tTraining loss: 0.3427644968032837\n",
      "Step: 3000  \tTraining accuracy: 0.8251254558563232\n",
      "Step: 3000  \tValid loss: 0.3559384346008301\n",
      "Step: 3100  \tTraining loss: 0.3420588970184326\n",
      "Step: 3100  \tTraining accuracy: 0.8256227374076843\n",
      "Step: 3100  \tValid loss: 0.3554131090641022\n",
      "Step: 3200  \tTraining loss: 0.34135881066322327\n",
      "Step: 3200  \tTraining accuracy: 0.8260583877563477\n",
      "Step: 3200  \tValid loss: 0.35497787594795227\n",
      "Step: 3300  \tTraining loss: 0.3406708240509033\n",
      "Step: 3300  \tTraining accuracy: 0.8264880776405334\n",
      "Step: 3300  \tValid loss: 0.35449475049972534\n",
      "Step: 3400  \tTraining loss: 0.33999741077423096\n",
      "Step: 3400  \tTraining accuracy: 0.8269001841545105\n",
      "Step: 3400  \tValid loss: 0.3539223372936249\n",
      "Step: 3500  \tTraining loss: 0.3393392264842987\n",
      "Step: 3500  \tTraining accuracy: 0.8273236751556396\n",
      "Step: 3500  \tValid loss: 0.35341793298721313\n",
      "Step: 3600  \tTraining loss: 0.33870095014572144\n",
      "Step: 3600  \tTraining accuracy: 0.827742338180542\n",
      "Step: 3600  \tValid loss: 0.3529146611690521\n",
      "Step: 3700  \tTraining loss: 0.3380856513977051\n",
      "Step: 3700  \tTraining accuracy: 0.8281232714653015\n",
      "Step: 3700  \tValid loss: 0.3524659276008606\n",
      "Step: 3800  \tTraining loss: 0.33749663829803467\n",
      "Step: 3800  \tTraining accuracy: 0.8284730315208435\n",
      "Step: 3800  \tValid loss: 0.352021187543869\n",
      "Step: 3900  \tTraining loss: 0.33693549036979675\n",
      "Step: 3900  \tTraining accuracy: 0.8287941813468933\n",
      "Step: 3900  \tValid loss: 0.35155004262924194\n",
      "Step: 4000  \tTraining loss: 0.33640289306640625\n",
      "Step: 4000  \tTraining accuracy: 0.8291229605674744\n",
      "Step: 4000  \tValid loss: 0.35114675760269165\n",
      "Step: 4100  \tTraining loss: 0.3358986973762512\n",
      "Step: 4100  \tTraining accuracy: 0.829455554485321\n",
      "Step: 4100  \tValid loss: 0.3508166968822479\n",
      "Step: 4200  \tTraining loss: 0.33542317152023315\n",
      "Step: 4200  \tTraining accuracy: 0.829749345779419\n",
      "Step: 4200  \tValid loss: 0.35046520829200745\n",
      "Step: 4300  \tTraining loss: 0.33497411012649536\n",
      "Step: 4300  \tTraining accuracy: 0.8300387859344482\n",
      "Step: 4300  \tValid loss: 0.3503599166870117\n",
      "Step: 4400  \tTraining loss: 0.33455225825309753\n",
      "Step: 4400  \tTraining accuracy: 0.8302963376045227\n",
      "Step: 4400  \tValid loss: 0.3502143621444702\n",
      "Step: 4500  \tTraining loss: 0.3341551721096039\n",
      "Step: 4500  \tTraining accuracy: 0.830527126789093\n",
      "Step: 4500  \tValid loss: 0.3499755263328552\n",
      "Step: 4600  \tTraining loss: 0.3337787687778473\n",
      "Step: 4600  \tTraining accuracy: 0.8307477235794067\n",
      "Step: 4600  \tValid loss: 0.3499891459941864\n",
      "Step: 4700  \tTraining loss: 0.3334224820137024\n",
      "Step: 4700  \tTraining accuracy: 0.8309705257415771\n",
      "Step: 4700  \tValid loss: 0.34978652000427246\n",
      "Step: 4800  \tTraining loss: 0.3330831527709961\n",
      "Step: 4800  \tTraining accuracy: 0.8311640024185181\n",
      "Step: 4800  \tValid loss: 0.3498024642467499\n",
      "Step: 4900  \tTraining loss: 0.33275946974754333\n",
      "Step: 4900  \tTraining accuracy: 0.8313438892364502\n",
      "Step: 4900  \tValid loss: 0.3497351109981537\n",
      "Step: 5000  \tTraining loss: 0.3324495553970337\n",
      "Step: 5000  \tTraining accuracy: 0.8315165638923645\n",
      "Step: 5000  \tValid loss: 0.3496851623058319\n",
      "Step: 5100  \tTraining loss: 0.33215221762657166\n",
      "Step: 5100  \tTraining accuracy: 0.8316930532455444\n",
      "Step: 5100  \tValid loss: 0.3496599495410919\n",
      "Step: 5200  \tTraining loss: 0.33186596632003784\n",
      "Step: 5200  \tTraining accuracy: 0.8318575024604797\n",
      "Step: 5200  \tValid loss: 0.3496928811073303\n",
      "Step: 5300  \tTraining loss: 0.3315903842449188\n",
      "Step: 5300  \tTraining accuracy: 0.8320233821868896\n",
      "Step: 5300  \tValid loss: 0.34968826174736023\n",
      "Step: 5400  \tTraining loss: 0.33132416009902954\n",
      "Step: 5400  \tTraining accuracy: 0.8321805596351624\n",
      "Step: 5400  \tValid loss: 0.3496464490890503\n",
      "Step: 5500  \tTraining loss: 0.3310665488243103\n",
      "Step: 5500  \tTraining accuracy: 0.8323517441749573\n",
      "Step: 5500  \tValid loss: 0.3496638238430023\n",
      "Step: 5600  \tTraining loss: 0.33081692457199097\n",
      "Step: 5600  \tTraining accuracy: 0.8324973583221436\n",
      "Step: 5600  \tValid loss: 0.3496553599834442\n",
      "Step: 5700  \tTraining loss: 0.33057430386543274\n",
      "Step: 5700  \tTraining accuracy: 0.8326210379600525\n",
      "Step: 5700  \tValid loss: 0.3497394919395447\n",
      "Step: 5800  \tTraining loss: 0.3303396999835968\n",
      "Step: 5800  \tTraining accuracy: 0.8327662348747253\n",
      "Step: 5800  \tValid loss: 0.3496280908584595\n",
      "Step: 5900  \tTraining loss: 0.33010968565940857\n",
      "Step: 5900  \tTraining accuracy: 0.8329088687896729\n",
      "Step: 5900  \tValid loss: 0.34993958473205566\n",
      "Step: 6000  \tTraining loss: 0.32988741993904114\n",
      "Step: 6000  \tTraining accuracy: 0.8330512046813965\n",
      "Step: 6000  \tValid loss: 0.35002267360687256\n",
      "Step: 6100  \tTraining loss: 0.32967159152030945\n",
      "Step: 6100  \tTraining accuracy: 0.833204448223114\n",
      "Step: 6100  \tValid loss: 0.349966824054718\n",
      "Step: 6200  \tTraining loss: 0.32946181297302246\n",
      "Step: 6200  \tTraining accuracy: 0.8333527445793152\n",
      "Step: 6200  \tValid loss: 0.3499898910522461\n",
      "Step: 6300  \tTraining loss: 0.3292563557624817\n",
      "Step: 6300  \tTraining accuracy: 0.8335027694702148\n",
      "Step: 6300  \tValid loss: 0.35026276111602783\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8336566\n",
      "Precision: 0.87153137\n",
      "Recall: 0.92677593\n",
      "F1 score: 0.8592668\n",
      "AUC: 0.78046113\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.833657   0.871531  0.926776  0.859267  0.780461  0.329102      0.833547   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.349432       0.833532   0.339927      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  6377.0  \n",
      "13\n",
      "(4785, 8)\n",
      "(4785, 1)\n",
      "(2640, 8)\n",
      "(2640, 1)\n",
      "(2145, 8)\n",
      "(2145, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6395780444145203\n",
      "Step: 100  \tTraining accuracy: 0.6639498472213745\n",
      "Step: 100  \tValid loss: 0.6427602767944336\n",
      "Step: 200  \tTraining loss: 0.5767392516136169\n",
      "Step: 200  \tTraining accuracy: 0.6707767248153687\n",
      "Step: 200  \tValid loss: 0.5790836215019226\n",
      "Step: 300  \tTraining loss: 0.44923165440559387\n",
      "Step: 300  \tTraining accuracy: 0.7015256285667419\n",
      "Step: 300  \tValid loss: 0.44971367716789246\n",
      "Step: 400  \tTraining loss: 0.3723485469818115\n",
      "Step: 400  \tTraining accuracy: 0.7379608750343323\n",
      "Step: 400  \tValid loss: 0.37368783354759216\n",
      "Step: 500  \tTraining loss: 0.33472275733947754\n",
      "Step: 500  \tTraining accuracy: 0.7624985575675964\n",
      "Step: 500  \tValid loss: 0.338251531124115\n",
      "Step: 600  \tTraining loss: 0.30945494771003723\n",
      "Step: 600  \tTraining accuracy: 0.7805072665214539\n",
      "Step: 600  \tValid loss: 0.3171212077140808\n",
      "Step: 700  \tTraining loss: 0.30316418409347534\n",
      "Step: 700  \tTraining accuracy: 0.7945020198822021\n",
      "Step: 700  \tValid loss: 0.31360405683517456\n",
      "Step: 800  \tTraining loss: 0.30066758394241333\n",
      "Step: 800  \tTraining accuracy: 0.8050992488861084\n",
      "Step: 800  \tValid loss: 0.31229305267333984\n",
      "Step: 900  \tTraining loss: 0.29902374744415283\n",
      "Step: 900  \tTraining accuracy: 0.8133505582809448\n",
      "Step: 900  \tValid loss: 0.31117454171180725\n",
      "Step: 1000  \tTraining loss: 0.29789435863494873\n",
      "Step: 1000  \tTraining accuracy: 0.8202056884765625\n",
      "Step: 1000  \tValid loss: 0.3104837238788605\n",
      "Step: 1100  \tTraining loss: 0.2970637381076813\n",
      "Step: 1100  \tTraining accuracy: 0.8260735273361206\n",
      "Step: 1100  \tValid loss: 0.3100762963294983\n",
      "Step: 1200  \tTraining loss: 0.2964479327201843\n",
      "Step: 1200  \tTraining accuracy: 0.8310117721557617\n",
      "Step: 1200  \tValid loss: 0.30982327461242676\n",
      "Step: 1300  \tTraining loss: 0.29599201679229736\n",
      "Step: 1300  \tTraining accuracy: 0.8352685570716858\n",
      "Step: 1300  \tValid loss: 0.3096334636211395\n",
      "Step: 1400  \tTraining loss: 0.29564177989959717\n",
      "Step: 1400  \tTraining accuracy: 0.8389489054679871\n",
      "Step: 1400  \tValid loss: 0.30945882201194763\n",
      "Step: 1500  \tTraining loss: 0.29534459114074707\n",
      "Step: 1500  \tTraining accuracy: 0.8421143889427185\n",
      "Step: 1500  \tValid loss: 0.30927562713623047\n",
      "Step: 1600  \tTraining loss: 0.2950659394264221\n",
      "Step: 1600  \tTraining accuracy: 0.8448444604873657\n",
      "Step: 1600  \tValid loss: 0.3090641498565674\n",
      "Step: 1700  \tTraining loss: 0.2947738468647003\n",
      "Step: 1700  \tTraining accuracy: 0.84724360704422\n",
      "Step: 1700  \tValid loss: 0.30878692865371704\n",
      "Step: 1800  \tTraining loss: 0.2944847345352173\n",
      "Step: 1800  \tTraining accuracy: 0.8493446707725525\n",
      "Step: 1800  \tValid loss: 0.3085358142852783\n",
      "Step: 1900  \tTraining loss: 0.29420146346092224\n",
      "Step: 1900  \tTraining accuracy: 0.8512186408042908\n",
      "Step: 1900  \tValid loss: 0.3083246052265167\n",
      "Step: 2000  \tTraining loss: 0.2939220070838928\n",
      "Step: 2000  \tTraining accuracy: 0.8528896570205688\n",
      "Step: 2000  \tValid loss: 0.3081147074699402\n",
      "Step: 2100  \tTraining loss: 0.2936515808105469\n",
      "Step: 2100  \tTraining accuracy: 0.8544231057167053\n",
      "Step: 2100  \tValid loss: 0.3079487979412079\n",
      "Step: 2200  \tTraining loss: 0.29337915778160095\n",
      "Step: 2200  \tTraining accuracy: 0.8558090925216675\n",
      "Step: 2200  \tValid loss: 0.30774354934692383\n",
      "Step: 2300  \tTraining loss: 0.2931126058101654\n",
      "Step: 2300  \tTraining accuracy: 0.8570114970207214\n",
      "Step: 2300  \tValid loss: 0.30757060647010803\n",
      "Step: 2400  \tTraining loss: 0.29285159707069397\n",
      "Step: 2400  \tTraining accuracy: 0.8580893278121948\n",
      "Step: 2400  \tValid loss: 0.30740341544151306\n",
      "Step: 2500  \tTraining loss: 0.29259049892425537\n",
      "Step: 2500  \tTraining accuracy: 0.8590450882911682\n",
      "Step: 2500  \tValid loss: 0.3072356879711151\n",
      "Step: 2600  \tTraining loss: 0.2923242449760437\n",
      "Step: 2600  \tTraining accuracy: 0.8599217534065247\n",
      "Step: 2600  \tValid loss: 0.3070736527442932\n",
      "Step: 2700  \tTraining loss: 0.2920432388782501\n",
      "Step: 2700  \tTraining accuracy: 0.8607479929924011\n",
      "Step: 2700  \tValid loss: 0.30682119727134705\n",
      "Step: 2800  \tTraining loss: 0.2917468845844269\n",
      "Step: 2800  \tTraining accuracy: 0.8614913821220398\n",
      "Step: 2800  \tValid loss: 0.3065868318080902\n",
      "Step: 2900  \tTraining loss: 0.29143527150154114\n",
      "Step: 2900  \tTraining accuracy: 0.8621899485588074\n",
      "Step: 2900  \tValid loss: 0.30633705854415894\n",
      "Step: 3000  \tTraining loss: 0.2911016643047333\n",
      "Step: 3000  \tTraining accuracy: 0.8628517985343933\n",
      "Step: 3000  \tValid loss: 0.30606958270072937\n",
      "Step: 3100  \tTraining loss: 0.2907373607158661\n",
      "Step: 3100  \tTraining accuracy: 0.8634667992591858\n",
      "Step: 3100  \tValid loss: 0.305771142244339\n",
      "Step: 3200  \tTraining loss: 0.2903146743774414\n",
      "Step: 3200  \tTraining accuracy: 0.8640526533126831\n",
      "Step: 3200  \tValid loss: 0.30536842346191406\n",
      "Step: 3300  \tTraining loss: 0.2896142601966858\n",
      "Step: 3300  \tTraining accuracy: 0.8645928502082825\n",
      "Step: 3300  \tValid loss: 0.30427390336990356\n",
      "Step: 3400  \tTraining loss: 0.2888641655445099\n",
      "Step: 3400  \tTraining accuracy: 0.8650540113449097\n",
      "Step: 3400  \tValid loss: 0.30313998460769653\n",
      "Step: 3500  \tTraining loss: 0.2882985770702362\n",
      "Step: 3500  \tTraining accuracy: 0.8654975295066833\n",
      "Step: 3500  \tValid loss: 0.30260565876960754\n",
      "Step: 3600  \tTraining loss: 0.28779542446136475\n",
      "Step: 3600  \tTraining accuracy: 0.8659248948097229\n",
      "Step: 3600  \tValid loss: 0.30214470624923706\n",
      "Step: 3700  \tTraining loss: 0.2873346507549286\n",
      "Step: 3700  \tTraining accuracy: 0.8663231134414673\n",
      "Step: 3700  \tValid loss: 0.3017255365848541\n",
      "Step: 3800  \tTraining loss: 0.2869028151035309\n",
      "Step: 3800  \tTraining accuracy: 0.8667029142379761\n",
      "Step: 3800  \tValid loss: 0.30132144689559937\n",
      "Step: 3900  \tTraining loss: 0.28648751974105835\n",
      "Step: 3900  \tTraining accuracy: 0.8670520782470703\n",
      "Step: 3900  \tValid loss: 0.3009246289730072\n",
      "Step: 4000  \tTraining loss: 0.28609129786491394\n",
      "Step: 4000  \tTraining accuracy: 0.867378294467926\n",
      "Step: 4000  \tValid loss: 0.30055949091911316\n",
      "Step: 4100  \tTraining loss: 0.2857058048248291\n",
      "Step: 4100  \tTraining accuracy: 0.8676832318305969\n",
      "Step: 4100  \tValid loss: 0.30022096633911133\n",
      "Step: 4200  \tTraining loss: 0.28533029556274414\n",
      "Step: 4200  \tTraining accuracy: 0.8679759502410889\n",
      "Step: 4200  \tValid loss: 0.2999171018600464\n",
      "Step: 4300  \tTraining loss: 0.284959614276886\n",
      "Step: 4300  \tTraining accuracy: 0.8682549595832825\n",
      "Step: 4300  \tValid loss: 0.2996145486831665\n",
      "Step: 4400  \tTraining loss: 0.284595251083374\n",
      "Step: 4400  \tTraining accuracy: 0.8685091137886047\n",
      "Step: 4400  \tValid loss: 0.2993122935295105\n",
      "Step: 4500  \tTraining loss: 0.2842385768890381\n",
      "Step: 4500  \tTraining accuracy: 0.8687635660171509\n",
      "Step: 4500  \tValid loss: 0.2989922761917114\n",
      "Step: 4600  \tTraining loss: 0.2838820815086365\n",
      "Step: 4600  \tTraining accuracy: 0.8690091371536255\n",
      "Step: 4600  \tValid loss: 0.29866284132003784\n",
      "Step: 4700  \tTraining loss: 0.2835409939289093\n",
      "Step: 4700  \tTraining accuracy: 0.8692486882209778\n",
      "Step: 4700  \tValid loss: 0.2982962727546692\n",
      "Step: 4800  \tTraining loss: 0.28305694460868835\n",
      "Step: 4800  \tTraining accuracy: 0.8694824576377869\n",
      "Step: 4800  \tValid loss: 0.2977065443992615\n",
      "Step: 4900  \tTraining loss: 0.2826673984527588\n",
      "Step: 4900  \tTraining accuracy: 0.8697023391723633\n",
      "Step: 4900  \tValid loss: 0.2970282733440399\n",
      "Step: 5000  \tTraining loss: 0.2823364734649658\n",
      "Step: 5000  \tTraining accuracy: 0.8699218034744263\n",
      "Step: 5000  \tValid loss: 0.29646921157836914\n",
      "Step: 5100  \tTraining loss: 0.28202253580093384\n",
      "Step: 5100  \tTraining accuracy: 0.8701635599136353\n",
      "Step: 5100  \tValid loss: 0.2959965169429779\n",
      "Step: 5200  \tTraining loss: 0.2817212641239166\n",
      "Step: 5200  \tTraining accuracy: 0.8703979849815369\n",
      "Step: 5200  \tValid loss: 0.2955823838710785\n",
      "Step: 5300  \tTraining loss: 0.2814137041568756\n",
      "Step: 5300  \tTraining accuracy: 0.8706274628639221\n",
      "Step: 5300  \tValid loss: 0.2952277660369873\n",
      "Step: 5400  \tTraining loss: 0.28108030557632446\n",
      "Step: 5400  \tTraining accuracy: 0.8708463907241821\n",
      "Step: 5400  \tValid loss: 0.2948816418647766\n",
      "Step: 5500  \tTraining loss: 0.28072336316108704\n",
      "Step: 5500  \tTraining accuracy: 0.8710553646087646\n",
      "Step: 5500  \tValid loss: 0.2945443391799927\n",
      "Step: 5600  \tTraining loss: 0.2803671061992645\n",
      "Step: 5600  \tTraining accuracy: 0.8712511658668518\n",
      "Step: 5600  \tValid loss: 0.2942236363887787\n",
      "Step: 5700  \tTraining loss: 0.28000500798225403\n",
      "Step: 5700  \tTraining accuracy: 0.8714492917060852\n",
      "Step: 5700  \tValid loss: 0.29397040605545044\n",
      "Step: 5800  \tTraining loss: 0.27966052293777466\n",
      "Step: 5800  \tTraining accuracy: 0.8716441988945007\n",
      "Step: 5800  \tValid loss: 0.2936815321445465\n",
      "Step: 5900  \tTraining loss: 0.2793305516242981\n",
      "Step: 5900  \tTraining accuracy: 0.8718252182006836\n",
      "Step: 5900  \tValid loss: 0.2933882772922516\n",
      "Step: 6000  \tTraining loss: 0.27900436520576477\n",
      "Step: 6000  \tTraining accuracy: 0.8719949126243591\n",
      "Step: 6000  \tValid loss: 0.29310742020606995\n",
      "Step: 6100  \tTraining loss: 0.27869728207588196\n",
      "Step: 6100  \tTraining accuracy: 0.8721538782119751\n",
      "Step: 6100  \tValid loss: 0.2928720712661743\n",
      "Step: 6200  \tTraining loss: 0.2783992290496826\n",
      "Step: 6200  \tTraining accuracy: 0.8723075985908508\n",
      "Step: 6200  \tValid loss: 0.29261741042137146\n",
      "Step: 6300  \tTraining loss: 0.27810853719711304\n",
      "Step: 6300  \tTraining accuracy: 0.8724614381790161\n",
      "Step: 6300  \tValid loss: 0.29236194491386414\n",
      "Step: 6400  \tTraining loss: 0.27782687544822693\n",
      "Step: 6400  \tTraining accuracy: 0.8726120591163635\n",
      "Step: 6400  \tValid loss: 0.2921127676963806\n",
      "Step: 6500  \tTraining loss: 0.2775525152683258\n",
      "Step: 6500  \tTraining accuracy: 0.8727677464485168\n",
      "Step: 6500  \tValid loss: 0.2918635904788971\n",
      "Step: 6600  \tTraining loss: 0.2772843539714813\n",
      "Step: 6600  \tTraining accuracy: 0.872917115688324\n",
      "Step: 6600  \tValid loss: 0.291610985994339\n",
      "Step: 6700  \tTraining loss: 0.2770197093486786\n",
      "Step: 6700  \tTraining accuracy: 0.8730619549751282\n",
      "Step: 6700  \tValid loss: 0.2913798689842224\n",
      "Step: 6800  \tTraining loss: 0.27676182985305786\n",
      "Step: 6800  \tTraining accuracy: 0.8732056021690369\n",
      "Step: 6800  \tValid loss: 0.2911396324634552\n",
      "Step: 6900  \tTraining loss: 0.27650654315948486\n",
      "Step: 6900  \tTraining accuracy: 0.8733481168746948\n",
      "Step: 6900  \tValid loss: 0.29090383648872375\n",
      "Step: 7000  \tTraining loss: 0.2762546241283417\n",
      "Step: 7000  \tTraining accuracy: 0.8734865188598633\n",
      "Step: 7000  \tValid loss: 0.2906695306301117\n",
      "Step: 7100  \tTraining loss: 0.27600550651550293\n",
      "Step: 7100  \tTraining accuracy: 0.8736150860786438\n",
      "Step: 7100  \tValid loss: 0.2904384732246399\n",
      "Step: 7200  \tTraining loss: 0.27575695514678955\n",
      "Step: 7200  \tTraining accuracy: 0.873731255531311\n",
      "Step: 7200  \tValid loss: 0.2902095317840576\n",
      "Step: 7300  \tTraining loss: 0.2755093574523926\n",
      "Step: 7300  \tTraining accuracy: 0.8738442659378052\n",
      "Step: 7300  \tValid loss: 0.2899855077266693\n",
      "Step: 7400  \tTraining loss: 0.27526330947875977\n",
      "Step: 7400  \tTraining accuracy: 0.873954176902771\n",
      "Step: 7400  \tValid loss: 0.2897630035877228\n",
      "Step: 7500  \tTraining loss: 0.27501755952835083\n",
      "Step: 7500  \tTraining accuracy: 0.8740653395652771\n",
      "Step: 7500  \tValid loss: 0.2895520329475403\n",
      "Step: 7600  \tTraining loss: 0.2747719883918762\n",
      "Step: 7600  \tTraining accuracy: 0.8741722106933594\n",
      "Step: 7600  \tValid loss: 0.28933775424957275\n",
      "Step: 7700  \tTraining loss: 0.27452749013900757\n",
      "Step: 7700  \tTraining accuracy: 0.8742817044258118\n",
      "Step: 7700  \tValid loss: 0.2891187369823456\n",
      "Step: 7800  \tTraining loss: 0.27428168058395386\n",
      "Step: 7800  \tTraining accuracy: 0.8743924498558044\n",
      "Step: 7800  \tValid loss: 0.28890755772590637\n",
      "Step: 7900  \tTraining loss: 0.27403536438941956\n",
      "Step: 7900  \tTraining accuracy: 0.8745003342628479\n",
      "Step: 7900  \tValid loss: 0.28870460391044617\n",
      "Step: 8000  \tTraining loss: 0.2737865746021271\n",
      "Step: 8000  \tTraining accuracy: 0.8746120929718018\n",
      "Step: 8000  \tValid loss: 0.28850290179252625\n",
      "Step: 8100  \tTraining loss: 0.2735360264778137\n",
      "Step: 8100  \tTraining accuracy: 0.8747301697731018\n",
      "Step: 8100  \tValid loss: 0.2883053719997406\n",
      "Step: 8200  \tTraining loss: 0.2732841372489929\n",
      "Step: 8200  \tTraining accuracy: 0.8748440742492676\n",
      "Step: 8200  \tValid loss: 0.28810620307922363\n",
      "Step: 8300  \tTraining loss: 0.27303028106689453\n",
      "Step: 8300  \tTraining accuracy: 0.8749628067016602\n",
      "Step: 8300  \tValid loss: 0.28791189193725586\n",
      "Step: 8400  \tTraining loss: 0.2727715075016022\n",
      "Step: 8400  \tTraining accuracy: 0.8750836849212646\n",
      "Step: 8400  \tValid loss: 0.2876967191696167\n",
      "Step: 8500  \tTraining loss: 0.27251115441322327\n",
      "Step: 8500  \tTraining accuracy: 0.8752017021179199\n",
      "Step: 8500  \tValid loss: 0.2874808609485626\n",
      "Step: 8600  \tTraining loss: 0.272247850894928\n",
      "Step: 8600  \tTraining accuracy: 0.8753133416175842\n",
      "Step: 8600  \tValid loss: 0.28726571798324585\n",
      "Step: 8700  \tTraining loss: 0.2719804346561432\n",
      "Step: 8700  \tTraining accuracy: 0.8754150867462158\n",
      "Step: 8700  \tValid loss: 0.28704380989074707\n",
      "Step: 8800  \tTraining loss: 0.2717108726501465\n",
      "Step: 8800  \tTraining accuracy: 0.8755133748054504\n",
      "Step: 8800  \tValid loss: 0.2868193984031677\n",
      "Step: 8900  \tTraining loss: 0.271438866853714\n",
      "Step: 8900  \tTraining accuracy: 0.8756093978881836\n",
      "Step: 8900  \tValid loss: 0.2865966856479645\n",
      "Step: 9000  \tTraining loss: 0.271165132522583\n",
      "Step: 9000  \tTraining accuracy: 0.8756939172744751\n",
      "Step: 9000  \tValid loss: 0.28636765480041504\n",
      "Step: 9100  \tTraining loss: 0.27088984847068787\n",
      "Step: 9100  \tTraining accuracy: 0.8757743239402771\n",
      "Step: 9100  \tValid loss: 0.2861239016056061\n",
      "Step: 9200  \tTraining loss: 0.2706104815006256\n",
      "Step: 9200  \tTraining accuracy: 0.8758620619773865\n",
      "Step: 9200  \tValid loss: 0.2858346104621887\n",
      "Step: 9300  \tTraining loss: 0.27033162117004395\n",
      "Step: 9300  \tTraining accuracy: 0.8759546875953674\n",
      "Step: 9300  \tValid loss: 0.28553399443626404\n",
      "Step: 9400  \tTraining loss: 0.27005258202552795\n",
      "Step: 9400  \tTraining accuracy: 0.8760464787483215\n",
      "Step: 9400  \tValid loss: 0.28524690866470337\n",
      "Step: 9500  \tTraining loss: 0.26977303624153137\n",
      "Step: 9500  \tTraining accuracy: 0.876136302947998\n",
      "Step: 9500  \tValid loss: 0.28496599197387695\n",
      "Step: 9600  \tTraining loss: 0.2694931924343109\n",
      "Step: 9600  \tTraining accuracy: 0.8762242197990417\n",
      "Step: 9600  \tValid loss: 0.2846832871437073\n",
      "Step: 9700  \tTraining loss: 0.2692127227783203\n",
      "Step: 9700  \tTraining accuracy: 0.8763146996498108\n",
      "Step: 9700  \tValid loss: 0.2843974232673645\n",
      "Step: 9800  \tTraining loss: 0.26893123984336853\n",
      "Step: 9800  \tTraining accuracy: 0.8764097094535828\n",
      "Step: 9800  \tValid loss: 0.2841126620769501\n",
      "Step: 9900  \tTraining loss: 0.2686486840248108\n",
      "Step: 9900  \tTraining accuracy: 0.8764985799789429\n",
      "Step: 9900  \tValid loss: 0.2838173806667328\n",
      "Step: 10000  \tTraining loss: 0.2683635652065277\n",
      "Step: 10000  \tTraining accuracy: 0.8765856623649597\n",
      "Step: 10000  \tValid loss: 0.283521443605423\n",
      "Step: 10100  \tTraining loss: 0.2680772840976715\n",
      "Step: 10100  \tTraining accuracy: 0.8766751289367676\n",
      "Step: 10100  \tValid loss: 0.2832258641719818\n",
      "Step: 10200  \tTraining loss: 0.2677890360355377\n",
      "Step: 10200  \tTraining accuracy: 0.8767690658569336\n",
      "Step: 10200  \tValid loss: 0.28293174505233765\n",
      "Step: 10300  \tTraining loss: 0.2674999237060547\n",
      "Step: 10300  \tTraining accuracy: 0.8768652081489563\n",
      "Step: 10300  \tValid loss: 0.28263407945632935\n",
      "Step: 10400  \tTraining loss: 0.26720982789993286\n",
      "Step: 10400  \tTraining accuracy: 0.876959502696991\n",
      "Step: 10400  \tValid loss: 0.28234022855758667\n",
      "Step: 10500  \tTraining loss: 0.2669226825237274\n",
      "Step: 10500  \tTraining accuracy: 0.8770520091056824\n",
      "Step: 10500  \tValid loss: 0.28204938769340515\n",
      "Step: 10600  \tTraining loss: 0.26663854718208313\n",
      "Step: 10600  \tTraining accuracy: 0.8771427273750305\n",
      "Step: 10600  \tValid loss: 0.2817500829696655\n",
      "Step: 10700  \tTraining loss: 0.2663598656654358\n",
      "Step: 10700  \tTraining accuracy: 0.8772337436676025\n",
      "Step: 10700  \tValid loss: 0.28146690130233765\n",
      "Step: 10800  \tTraining loss: 0.26608580350875854\n",
      "Step: 10800  \tTraining accuracy: 0.8773298263549805\n",
      "Step: 10800  \tValid loss: 0.28117749094963074\n",
      "Step: 10900  \tTraining loss: 0.26578810811042786\n",
      "Step: 10900  \tTraining accuracy: 0.8774357438087463\n",
      "Step: 10900  \tValid loss: 0.2808205485343933\n",
      "Step: 11000  \tTraining loss: 0.26544857025146484\n",
      "Step: 11000  \tTraining accuracy: 0.8775454163551331\n",
      "Step: 11000  \tValid loss: 0.2803056240081787\n",
      "Step: 11100  \tTraining loss: 0.26513662934303284\n",
      "Step: 11100  \tTraining accuracy: 0.8776597380638123\n",
      "Step: 11100  \tValid loss: 0.27991217374801636\n",
      "Step: 11200  \tTraining loss: 0.2648474872112274\n",
      "Step: 11200  \tTraining accuracy: 0.8777766823768616\n",
      "Step: 11200  \tValid loss: 0.27955809235572815\n",
      "Step: 11300  \tTraining loss: 0.26456761360168457\n",
      "Step: 11300  \tTraining accuracy: 0.8778878450393677\n",
      "Step: 11300  \tValid loss: 0.2792145609855652\n",
      "Step: 11400  \tTraining loss: 0.26429498195648193\n",
      "Step: 11400  \tTraining accuracy: 0.8779961466789246\n",
      "Step: 11400  \tValid loss: 0.2788778245449066\n",
      "Step: 11500  \tTraining loss: 0.2640281319618225\n",
      "Step: 11500  \tTraining accuracy: 0.8781043291091919\n",
      "Step: 11500  \tValid loss: 0.2785433232784271\n",
      "Step: 11600  \tTraining loss: 0.26376673579216003\n",
      "Step: 11600  \tTraining accuracy: 0.8782115578651428\n",
      "Step: 11600  \tValid loss: 0.2782151997089386\n",
      "Step: 11700  \tTraining loss: 0.2635103762149811\n",
      "Step: 11700  \tTraining accuracy: 0.8783196806907654\n",
      "Step: 11700  \tValid loss: 0.2778984308242798\n",
      "Step: 11800  \tTraining loss: 0.26325786113739014\n",
      "Step: 11800  \tTraining accuracy: 0.8784205913543701\n",
      "Step: 11800  \tValid loss: 0.27758365869522095\n",
      "Step: 11900  \tTraining loss: 0.26300790905952454\n",
      "Step: 11900  \tTraining accuracy: 0.8785136342048645\n",
      "Step: 11900  \tValid loss: 0.27727240324020386\n",
      "Step: 12000  \tTraining loss: 0.26275986433029175\n",
      "Step: 12000  \tTraining accuracy: 0.8785989880561829\n",
      "Step: 12000  \tValid loss: 0.27696701884269714\n",
      "Step: 12100  \tTraining loss: 0.26251310110092163\n",
      "Step: 12100  \tTraining accuracy: 0.8786829710006714\n",
      "Step: 12100  \tValid loss: 0.2766673266887665\n",
      "Step: 12200  \tTraining loss: 0.2622673809528351\n",
      "Step: 12200  \tTraining accuracy: 0.8787680864334106\n",
      "Step: 12200  \tValid loss: 0.27636703848838806\n",
      "Step: 12300  \tTraining loss: 0.2620222866535187\n",
      "Step: 12300  \tTraining accuracy: 0.8788509964942932\n",
      "Step: 12300  \tValid loss: 0.2760704755783081\n",
      "Step: 12400  \tTraining loss: 0.26177820563316345\n",
      "Step: 12400  \tTraining accuracy: 0.8789308667182922\n",
      "Step: 12400  \tValid loss: 0.2757803201675415\n",
      "Step: 12500  \tTraining loss: 0.26153451204299927\n",
      "Step: 12500  \tTraining accuracy: 0.8790094256401062\n",
      "Step: 12500  \tValid loss: 0.27549460530281067\n",
      "Step: 12600  \tTraining loss: 0.26129165291786194\n",
      "Step: 12600  \tTraining accuracy: 0.8790867924690247\n",
      "Step: 12600  \tValid loss: 0.27521398663520813\n",
      "Step: 12700  \tTraining loss: 0.26104965806007385\n",
      "Step: 12700  \tTraining accuracy: 0.8791629076004028\n",
      "Step: 12700  \tValid loss: 0.27494192123413086\n",
      "Step: 12800  \tTraining loss: 0.2608104348182678\n",
      "Step: 12800  \tTraining accuracy: 0.8792378306388855\n",
      "Step: 12800  \tValid loss: 0.2746727764606476\n",
      "Step: 12900  \tTraining loss: 0.26057130098342896\n",
      "Step: 12900  \tTraining accuracy: 0.8793107271194458\n",
      "Step: 12900  \tValid loss: 0.27440187335014343\n",
      "Step: 13000  \tTraining loss: 0.2603328824043274\n",
      "Step: 13000  \tTraining accuracy: 0.8793793320655823\n",
      "Step: 13000  \tValid loss: 0.2741408944129944\n",
      "Step: 13100  \tTraining loss: 0.2600971460342407\n",
      "Step: 13100  \tTraining accuracy: 0.8794452548027039\n",
      "Step: 13100  \tValid loss: 0.2738782465457916\n",
      "Step: 13200  \tTraining loss: 0.25986430048942566\n",
      "Step: 13200  \tTraining accuracy: 0.8795077800750732\n",
      "Step: 13200  \tValid loss: 0.2736210823059082\n",
      "Step: 13300  \tTraining loss: 0.25963327288627625\n",
      "Step: 13300  \tTraining accuracy: 0.8795725703239441\n",
      "Step: 13300  \tValid loss: 0.27336764335632324\n",
      "Step: 13400  \tTraining loss: 0.25940465927124023\n",
      "Step: 13400  \tTraining accuracy: 0.8796355724334717\n",
      "Step: 13400  \tValid loss: 0.2731194496154785\n",
      "Step: 13500  \tTraining loss: 0.2591799795627594\n",
      "Step: 13500  \tTraining accuracy: 0.8796968460083008\n",
      "Step: 13500  \tValid loss: 0.27288198471069336\n",
      "Step: 13600  \tTraining loss: 0.25895482301712036\n",
      "Step: 13600  \tTraining accuracy: 0.8797610998153687\n",
      "Step: 13600  \tValid loss: 0.27264970541000366\n",
      "Step: 13700  \tTraining loss: 0.25873202085494995\n",
      "Step: 13700  \tTraining accuracy: 0.8798274397850037\n",
      "Step: 13700  \tValid loss: 0.2724141776561737\n",
      "Step: 13800  \tTraining loss: 0.25850731134414673\n",
      "Step: 13800  \tTraining accuracy: 0.8798936009407043\n",
      "Step: 13800  \tValid loss: 0.27222609519958496\n",
      "Step: 13900  \tTraining loss: 0.2582835555076599\n",
      "Step: 13900  \tTraining accuracy: 0.8799610733985901\n",
      "Step: 13900  \tValid loss: 0.2720136344432831\n",
      "Step: 14000  \tTraining loss: 0.2580684721469879\n",
      "Step: 14000  \tTraining accuracy: 0.8800275921821594\n",
      "Step: 14000  \tValid loss: 0.2717738449573517\n",
      "Step: 14100  \tTraining loss: 0.2578611671924591\n",
      "Step: 14100  \tTraining accuracy: 0.8800901174545288\n",
      "Step: 14100  \tValid loss: 0.2715259790420532\n",
      "Step: 14200  \tTraining loss: 0.257650226354599\n",
      "Step: 14200  \tTraining accuracy: 0.8801481127738953\n",
      "Step: 14200  \tValid loss: 0.271261602640152\n",
      "Step: 14300  \tTraining loss: 0.25744229555130005\n",
      "Step: 14300  \tTraining accuracy: 0.8802045583724976\n",
      "Step: 14300  \tValid loss: 0.2709704339504242\n",
      "Step: 14400  \tTraining loss: 0.2572401165962219\n",
      "Step: 14400  \tTraining accuracy: 0.8802617192268372\n",
      "Step: 14400  \tValid loss: 0.2706679701805115\n",
      "Step: 14500  \tTraining loss: 0.2570488154888153\n",
      "Step: 14500  \tTraining accuracy: 0.8803201913833618\n",
      "Step: 14500  \tValid loss: 0.2704240381717682\n",
      "Step: 14600  \tTraining loss: 0.2568672001361847\n",
      "Step: 14600  \tTraining accuracy: 0.8803757429122925\n",
      "Step: 14600  \tValid loss: 0.2702224552631378\n",
      "Step: 14700  \tTraining loss: 0.2566896677017212\n",
      "Step: 14700  \tTraining accuracy: 0.880432665348053\n",
      "Step: 14700  \tValid loss: 0.27002081274986267\n",
      "Step: 14800  \tTraining loss: 0.2562828063964844\n",
      "Step: 14800  \tTraining accuracy: 0.8804909586906433\n",
      "Step: 14800  \tValid loss: 0.26992255449295044\n",
      "Step: 14900  \tTraining loss: 0.2560752034187317\n",
      "Step: 14900  \tTraining accuracy: 0.8805526494979858\n",
      "Step: 14900  \tValid loss: 0.2697916328907013\n",
      "Step: 15000  \tTraining loss: 0.25588494539260864\n",
      "Step: 15000  \tTraining accuracy: 0.8806142210960388\n",
      "Step: 15000  \tValid loss: 0.2696225345134735\n",
      "Step: 15100  \tTraining loss: 0.2557019293308258\n",
      "Step: 15100  \tTraining accuracy: 0.8806750178337097\n",
      "Step: 15100  \tValid loss: 0.26943439245224\n",
      "Step: 15200  \tTraining loss: 0.2555241882801056\n",
      "Step: 15200  \tTraining accuracy: 0.8807328939437866\n",
      "Step: 15200  \tValid loss: 0.2692267596721649\n",
      "Step: 15300  \tTraining loss: 0.255353718996048\n",
      "Step: 15300  \tTraining accuracy: 0.8807886838912964\n",
      "Step: 15300  \tValid loss: 0.2690125107765198\n",
      "Step: 15400  \tTraining loss: 0.25518742203712463\n",
      "Step: 15400  \tTraining accuracy: 0.8808450698852539\n",
      "Step: 15400  \tValid loss: 0.2687964141368866\n",
      "Step: 15500  \tTraining loss: 0.2550284266471863\n",
      "Step: 15500  \tTraining accuracy: 0.8809007406234741\n",
      "Step: 15500  \tValid loss: 0.26860249042510986\n",
      "Step: 15600  \tTraining loss: 0.25487297773361206\n",
      "Step: 15600  \tTraining accuracy: 0.8809550404548645\n",
      "Step: 15600  \tValid loss: 0.2684105634689331\n",
      "Step: 15700  \tTraining loss: 0.254716694355011\n",
      "Step: 15700  \tTraining accuracy: 0.8810032606124878\n",
      "Step: 15700  \tValid loss: 0.26821649074554443\n",
      "Step: 15800  \tTraining loss: 0.25456586480140686\n",
      "Step: 15800  \tTraining accuracy: 0.8810489177703857\n",
      "Step: 15800  \tValid loss: 0.26803314685821533\n",
      "Step: 15900  \tTraining loss: 0.25441569089889526\n",
      "Step: 15900  \tTraining accuracy: 0.8810939788818359\n",
      "Step: 15900  \tValid loss: 0.26784181594848633\n",
      "Step: 16000  \tTraining loss: 0.25426915287971497\n",
      "Step: 16000  \tTraining accuracy: 0.8811365365982056\n",
      "Step: 16000  \tValid loss: 0.26766660809516907\n",
      "Step: 16100  \tTraining loss: 0.2541269063949585\n",
      "Step: 16100  \tTraining accuracy: 0.8811752796173096\n",
      "Step: 16100  \tValid loss: 0.2674931287765503\n",
      "Step: 16200  \tTraining loss: 0.2539859712123871\n",
      "Step: 16200  \tTraining accuracy: 0.88121098279953\n",
      "Step: 16200  \tValid loss: 0.26732054352760315\n",
      "Step: 16300  \tTraining loss: 0.2538450360298157\n",
      "Step: 16300  \tTraining accuracy: 0.8812474608421326\n",
      "Step: 16300  \tValid loss: 0.26715144515037537\n",
      "Step: 16400  \tTraining loss: 0.25370460748672485\n",
      "Step: 16400  \tTraining accuracy: 0.8812835812568665\n",
      "Step: 16400  \tValid loss: 0.26697659492492676\n",
      "Step: 16500  \tTraining loss: 0.2535668909549713\n",
      "Step: 16500  \tTraining accuracy: 0.8813192248344421\n",
      "Step: 16500  \tValid loss: 0.2668074369430542\n",
      "Step: 16600  \tTraining loss: 0.2534300982952118\n",
      "Step: 16600  \tTraining accuracy: 0.881356954574585\n",
      "Step: 16600  \tValid loss: 0.2666381001472473\n",
      "Step: 16700  \tTraining loss: 0.25329530239105225\n",
      "Step: 16700  \tTraining accuracy: 0.8813948631286621\n",
      "Step: 16700  \tValid loss: 0.2664716839790344\n",
      "Step: 16800  \tTraining loss: 0.25316429138183594\n",
      "Step: 16800  \tTraining accuracy: 0.8814323544502258\n",
      "Step: 16800  \tValid loss: 0.26630955934524536\n",
      "Step: 16900  \tTraining loss: 0.2530321776866913\n",
      "Step: 16900  \tTraining accuracy: 0.8814693689346313\n",
      "Step: 16900  \tValid loss: 0.26614266633987427\n",
      "Step: 17000  \tTraining loss: 0.2529042661190033\n",
      "Step: 17000  \tTraining accuracy: 0.8815065622329712\n",
      "Step: 17000  \tValid loss: 0.26598113775253296\n",
      "Step: 17100  \tTraining loss: 0.2527753412723541\n",
      "Step: 17100  \tTraining accuracy: 0.8815426826477051\n",
      "Step: 17100  \tValid loss: 0.2658204138278961\n",
      "Step: 17200  \tTraining loss: 0.2526487708091736\n",
      "Step: 17200  \tTraining accuracy: 0.8815784454345703\n",
      "Step: 17200  \tValid loss: 0.26565876603126526\n",
      "Step: 17300  \tTraining loss: 0.25253209471702576\n",
      "Step: 17300  \tTraining accuracy: 0.8816161751747131\n",
      "Step: 17300  \tValid loss: 0.26552891731262207\n",
      "Step: 17400  \tTraining loss: 0.2524186372756958\n",
      "Step: 17400  \tTraining accuracy: 0.8816558718681335\n",
      "Step: 17400  \tValid loss: 0.2654058635234833\n",
      "Step: 17500  \tTraining loss: 0.25229522585868835\n",
      "Step: 17500  \tTraining accuracy: 0.881696343421936\n",
      "Step: 17500  \tValid loss: 0.26531967520713806\n",
      "Step: 17600  \tTraining loss: 0.25217074155807495\n",
      "Step: 17600  \tTraining accuracy: 0.8817363381385803\n",
      "Step: 17600  \tValid loss: 0.26525676250457764\n",
      "Step: 17700  \tTraining loss: 0.25205644965171814\n",
      "Step: 17700  \tTraining accuracy: 0.8817746639251709\n",
      "Step: 17700  \tValid loss: 0.26515212655067444\n",
      "Step: 17800  \tTraining loss: 0.25194546580314636\n",
      "Step: 17800  \tTraining accuracy: 0.8818108439445496\n",
      "Step: 17800  \tValid loss: 0.2650415003299713\n",
      "Step: 17900  \tTraining loss: 0.25183823704719543\n",
      "Step: 17900  \tTraining accuracy: 0.8818471431732178\n",
      "Step: 17900  \tValid loss: 0.2649361491203308\n",
      "Step: 18000  \tTraining loss: 0.25173133611679077\n",
      "Step: 18000  \tTraining accuracy: 0.8818854093551636\n",
      "Step: 18000  \tValid loss: 0.2648303508758545\n",
      "Step: 18100  \tTraining loss: 0.25162607431411743\n",
      "Step: 18100  \tTraining accuracy: 0.881923258304596\n",
      "Step: 18100  \tValid loss: 0.26471859216690063\n",
      "Step: 18200  \tTraining loss: 0.25152382254600525\n",
      "Step: 18200  \tTraining accuracy: 0.8819606900215149\n",
      "Step: 18200  \tValid loss: 0.2646147608757019\n",
      "Step: 18300  \tTraining loss: 0.2514234185218811\n",
      "Step: 18300  \tTraining accuracy: 0.8819977045059204\n",
      "Step: 18300  \tValid loss: 0.26450884342193604\n",
      "Step: 18400  \tTraining loss: 0.25132495164871216\n",
      "Step: 18400  \tTraining accuracy: 0.8820343017578125\n",
      "Step: 18400  \tValid loss: 0.2644073963165283\n",
      "Step: 18500  \tTraining loss: 0.2512269914150238\n",
      "Step: 18500  \tTraining accuracy: 0.8820704817771912\n",
      "Step: 18500  \tValid loss: 0.26430508494377136\n",
      "Step: 18600  \tTraining loss: 0.2511318624019623\n",
      "Step: 18600  \tTraining accuracy: 0.8821063041687012\n",
      "Step: 18600  \tValid loss: 0.26421573758125305\n",
      "Step: 18700  \tTraining loss: 0.2510358691215515\n",
      "Step: 18700  \tTraining accuracy: 0.8821417689323425\n",
      "Step: 18700  \tValid loss: 0.2641352415084839\n",
      "Step: 18800  \tTraining loss: 0.2509424686431885\n",
      "Step: 18800  \tTraining accuracy: 0.8821768164634705\n",
      "Step: 18800  \tValid loss: 0.26402905583381653\n",
      "Step: 18900  \tTraining loss: 0.250851035118103\n",
      "Step: 18900  \tTraining accuracy: 0.8822115063667297\n",
      "Step: 18900  \tValid loss: 0.2639361321926117\n",
      "Step: 19000  \tTraining loss: 0.2507595121860504\n",
      "Step: 19000  \tTraining accuracy: 0.8822457790374756\n",
      "Step: 19000  \tValid loss: 0.26385563611984253\n",
      "Step: 19100  \tTraining loss: 0.25066572427749634\n",
      "Step: 19100  \tTraining accuracy: 0.8822797536849976\n",
      "Step: 19100  \tValid loss: 0.2637637257575989\n",
      "Step: 19200  \tTraining loss: 0.2505796253681183\n",
      "Step: 19200  \tTraining accuracy: 0.8823133707046509\n",
      "Step: 19200  \tValid loss: 0.26366737484931946\n",
      "Step: 19300  \tTraining loss: 0.2504945993423462\n",
      "Step: 19300  \tTraining accuracy: 0.8823466300964355\n",
      "Step: 19300  \tValid loss: 0.26357966661453247\n",
      "Step: 19400  \tTraining loss: 0.2504121959209442\n",
      "Step: 19400  \tTraining accuracy: 0.8823811411857605\n",
      "Step: 19400  \tValid loss: 0.2634882628917694\n",
      "Step: 19500  \tTraining loss: 0.25033000111579895\n",
      "Step: 19500  \tTraining accuracy: 0.8824164271354675\n",
      "Step: 19500  \tValid loss: 0.26340654492378235\n",
      "Step: 19600  \tTraining loss: 0.2502480745315552\n",
      "Step: 19600  \tTraining accuracy: 0.8824529051780701\n",
      "Step: 19600  \tValid loss: 0.2633204460144043\n",
      "Step: 19700  \tTraining loss: 0.2501690983772278\n",
      "Step: 19700  \tTraining accuracy: 0.8824900984764099\n",
      "Step: 19700  \tValid loss: 0.26324474811553955\n",
      "Step: 19800  \tTraining loss: 0.2500884532928467\n",
      "Step: 19800  \tTraining accuracy: 0.8825268745422363\n",
      "Step: 19800  \tValid loss: 0.26317036151885986\n",
      "Step: 19900  \tTraining loss: 0.25000834465026855\n",
      "Step: 19900  \tTraining accuracy: 0.8825632929801941\n",
      "Step: 19900  \tValid loss: 0.2631029188632965\n",
      "Step: 20000  \tTraining loss: 0.24993014335632324\n",
      "Step: 20000  \tTraining accuracy: 0.882599413394928\n",
      "Step: 20000  \tValid loss: 0.26303207874298096\n",
      "Step: 20100  \tTraining loss: 0.2498522400856018\n",
      "Step: 20100  \tTraining accuracy: 0.8826372027397156\n",
      "Step: 20100  \tValid loss: 0.2629629075527191\n",
      "Step: 20200  \tTraining loss: 0.2497778981924057\n",
      "Step: 20200  \tTraining accuracy: 0.8826751112937927\n",
      "Step: 20200  \tValid loss: 0.26289626955986023\n",
      "Step: 20300  \tTraining loss: 0.24970290064811707\n",
      "Step: 20300  \tTraining accuracy: 0.882712721824646\n",
      "Step: 20300  \tValid loss: 0.26283392310142517\n",
      "Step: 20400  \tTraining loss: 0.249628946185112\n",
      "Step: 20400  \tTraining accuracy: 0.8827499151229858\n",
      "Step: 20400  \tValid loss: 0.2627691924571991\n",
      "Step: 20500  \tTraining loss: 0.2495557963848114\n",
      "Step: 20500  \tTraining accuracy: 0.882786750793457\n",
      "Step: 20500  \tValid loss: 0.2627110481262207\n",
      "Step: 20600  \tTraining loss: 0.24948322772979736\n",
      "Step: 20600  \tTraining accuracy: 0.8828231692314148\n",
      "Step: 20600  \tValid loss: 0.26264920830726624\n",
      "Step: 20700  \tTraining loss: 0.2494092732667923\n",
      "Step: 20700  \tTraining accuracy: 0.8828592896461487\n",
      "Step: 20700  \tValid loss: 0.2625877261161804\n",
      "Step: 20800  \tTraining loss: 0.24933691322803497\n",
      "Step: 20800  \tTraining accuracy: 0.8828951120376587\n",
      "Step: 20800  \tValid loss: 0.2625396251678467\n",
      "Step: 20900  \tTraining loss: 0.24926245212554932\n",
      "Step: 20900  \tTraining accuracy: 0.8829305171966553\n",
      "Step: 20900  \tValid loss: 0.2624834179878235\n",
      "Step: 21000  \tTraining loss: 0.24918729066848755\n",
      "Step: 21000  \tTraining accuracy: 0.882965624332428\n",
      "Step: 21000  \tValid loss: 0.26242688298225403\n",
      "Step: 21100  \tTraining loss: 0.24911601841449738\n",
      "Step: 21100  \tTraining accuracy: 0.883000373840332\n",
      "Step: 21100  \tValid loss: 0.2623731195926666\n",
      "Step: 21200  \tTraining loss: 0.24904698133468628\n",
      "Step: 21200  \tTraining accuracy: 0.883033812046051\n",
      "Step: 21200  \tValid loss: 0.2623155117034912\n",
      "Step: 21300  \tTraining loss: 0.24898073077201843\n",
      "Step: 21300  \tTraining accuracy: 0.8830679059028625\n",
      "Step: 21300  \tValid loss: 0.2622598707675934\n",
      "Step: 21400  \tTraining loss: 0.24891312420368195\n",
      "Step: 21400  \tTraining accuracy: 0.8831031918525696\n",
      "Step: 21400  \tValid loss: 0.2621985375881195\n",
      "Step: 21500  \tTraining loss: 0.24884629249572754\n",
      "Step: 21500  \tTraining accuracy: 0.8831390738487244\n",
      "Step: 21500  \tValid loss: 0.26213476061820984\n",
      "Step: 21600  \tTraining loss: 0.2487819939851761\n",
      "Step: 21600  \tTraining accuracy: 0.8831746578216553\n",
      "Step: 21600  \tValid loss: 0.2620680034160614\n",
      "Step: 21700  \tTraining loss: 0.24871644377708435\n",
      "Step: 21700  \tTraining accuracy: 0.8832098841667175\n",
      "Step: 21700  \tValid loss: 0.26199862360954285\n",
      "Step: 21800  \tTraining loss: 0.2486533671617508\n",
      "Step: 21800  \tTraining accuracy: 0.8832448124885559\n",
      "Step: 21800  \tValid loss: 0.26193150877952576\n",
      "Step: 21900  \tTraining loss: 0.24858775734901428\n",
      "Step: 21900  \tTraining accuracy: 0.8832793831825256\n",
      "Step: 21900  \tValid loss: 0.26186880469322205\n",
      "Step: 22000  \tTraining loss: 0.24852395057678223\n",
      "Step: 22000  \tTraining accuracy: 0.8833151459693909\n",
      "Step: 22000  \tValid loss: 0.26180386543273926\n",
      "Step: 22100  \tTraining loss: 0.2484610229730606\n",
      "Step: 22100  \tTraining accuracy: 0.8833514451980591\n",
      "Step: 22100  \tValid loss: 0.26174601912498474\n",
      "Step: 22200  \tTraining loss: 0.248396635055542\n",
      "Step: 22200  \tTraining accuracy: 0.883388876914978\n",
      "Step: 22200  \tValid loss: 0.2616806626319885\n",
      "Step: 22300  \tTraining loss: 0.2483324408531189\n",
      "Step: 22300  \tTraining accuracy: 0.8834269046783447\n",
      "Step: 22300  \tValid loss: 0.26162734627723694\n",
      "Step: 22400  \tTraining loss: 0.24826915562152863\n",
      "Step: 22400  \tTraining accuracy: 0.8834645748138428\n",
      "Step: 22400  \tValid loss: 0.26155441999435425\n",
      "Step: 22500  \tTraining loss: 0.24820734560489655\n",
      "Step: 22500  \tTraining accuracy: 0.8835019469261169\n",
      "Step: 22500  \tValid loss: 0.2614956796169281\n",
      "Step: 22600  \tTraining loss: 0.24815014004707336\n",
      "Step: 22600  \tTraining accuracy: 0.8835399150848389\n",
      "Step: 22600  \tValid loss: 0.26144301891326904\n",
      "Step: 22700  \tTraining loss: 0.24809424579143524\n",
      "Step: 22700  \tTraining accuracy: 0.8835779428482056\n",
      "Step: 22700  \tValid loss: 0.2613829970359802\n",
      "Step: 22800  \tTraining loss: 0.2480398565530777\n",
      "Step: 22800  \tTraining accuracy: 0.883615255355835\n",
      "Step: 22800  \tValid loss: 0.261324405670166\n",
      "Step: 22900  \tTraining loss: 0.24797965586185455\n",
      "Step: 22900  \tTraining accuracy: 0.8836535811424255\n",
      "Step: 22900  \tValid loss: 0.2612655758857727\n",
      "Step: 23000  \tTraining loss: 0.24792508780956268\n",
      "Step: 23000  \tTraining accuracy: 0.8836915493011475\n",
      "Step: 23000  \tValid loss: 0.2611983120441437\n",
      "Step: 23100  \tTraining loss: 0.2478693127632141\n",
      "Step: 23100  \tTraining accuracy: 0.8837273716926575\n",
      "Step: 23100  \tValid loss: 0.2611441910266876\n",
      "Step: 23200  \tTraining loss: 0.24781297147274017\n",
      "Step: 23200  \tTraining accuracy: 0.8837611079216003\n",
      "Step: 23200  \tValid loss: 0.2610797882080078\n",
      "Step: 23300  \tTraining loss: 0.24776124954223633\n",
      "Step: 23300  \tTraining accuracy: 0.8837950229644775\n",
      "Step: 23300  \tValid loss: 0.26102522015571594\n",
      "Step: 23400  \tTraining loss: 0.24770867824554443\n",
      "Step: 23400  \tTraining accuracy: 0.8838263750076294\n",
      "Step: 23400  \tValid loss: 0.2609651982784271\n",
      "Step: 23500  \tTraining loss: 0.24765267968177795\n",
      "Step: 23500  \tTraining accuracy: 0.8838561177253723\n",
      "Step: 23500  \tValid loss: 0.26090243458747864\n",
      "Step: 23600  \tTraining loss: 0.2476007342338562\n",
      "Step: 23600  \tTraining accuracy: 0.8838865160942078\n",
      "Step: 23600  \tValid loss: 0.26084399223327637\n",
      "Step: 23700  \tTraining loss: 0.24754716455936432\n",
      "Step: 23700  \tTraining accuracy: 0.8839179873466492\n",
      "Step: 23700  \tValid loss: 0.2607874572277069\n",
      "Step: 23800  \tTraining loss: 0.24749331176280975\n",
      "Step: 23800  \tTraining accuracy: 0.8839483261108398\n",
      "Step: 23800  \tValid loss: 0.26072657108306885\n",
      "Step: 23900  \tTraining loss: 0.24744053184986115\n",
      "Step: 23900  \tTraining accuracy: 0.8839770555496216\n",
      "Step: 23900  \tValid loss: 0.260667085647583\n",
      "Step: 24000  \tTraining loss: 0.24738669395446777\n",
      "Step: 24000  \tTraining accuracy: 0.8840055465698242\n",
      "Step: 24000  \tValid loss: 0.26060912013053894\n",
      "Step: 24100  \tTraining loss: 0.24733799695968628\n",
      "Step: 24100  \tTraining accuracy: 0.8840337991714478\n",
      "Step: 24100  \tValid loss: 0.26055341958999634\n",
      "Step: 24200  \tTraining loss: 0.247286856174469\n",
      "Step: 24200  \tTraining accuracy: 0.884061872959137\n",
      "Step: 24200  \tValid loss: 0.26049813628196716\n",
      "Step: 24300  \tTraining loss: 0.2472321093082428\n",
      "Step: 24300  \tTraining accuracy: 0.8840896487236023\n",
      "Step: 24300  \tValid loss: 0.2604389488697052\n",
      "Step: 24400  \tTraining loss: 0.2471812665462494\n",
      "Step: 24400  \tTraining accuracy: 0.8841172456741333\n",
      "Step: 24400  \tValid loss: 0.26038527488708496\n",
      "Step: 24500  \tTraining loss: 0.24713069200515747\n",
      "Step: 24500  \tTraining accuracy: 0.8841446042060852\n",
      "Step: 24500  \tValid loss: 0.260331928730011\n",
      "Step: 24600  \tTraining loss: 0.2470809817314148\n",
      "Step: 24600  \tTraining accuracy: 0.884171724319458\n",
      "Step: 24600  \tValid loss: 0.26027536392211914\n",
      "Step: 24700  \tTraining loss: 0.24703165888786316\n",
      "Step: 24700  \tTraining accuracy: 0.8841986060142517\n",
      "Step: 24700  \tValid loss: 0.26022252440452576\n",
      "Step: 24800  \tTraining loss: 0.24697938561439514\n",
      "Step: 24800  \tTraining accuracy: 0.8842253088951111\n",
      "Step: 24800  \tValid loss: 0.26016706228256226\n",
      "Step: 24900  \tTraining loss: 0.24693118035793304\n",
      "Step: 24900  \tTraining accuracy: 0.8842530846595764\n",
      "Step: 24900  \tValid loss: 0.2601175904273987\n",
      "Step: 25000  \tTraining loss: 0.2468823790550232\n",
      "Step: 25000  \tTraining accuracy: 0.8842813968658447\n",
      "Step: 25000  \tValid loss: 0.26006561517715454\n",
      "Step: 25100  \tTraining loss: 0.24683313071727753\n",
      "Step: 25100  \tTraining accuracy: 0.8843095302581787\n",
      "Step: 25100  \tValid loss: 0.26000523567199707\n",
      "Step: 25200  \tTraining loss: 0.24678173661231995\n",
      "Step: 25200  \tTraining accuracy: 0.8843374252319336\n",
      "Step: 25200  \tValid loss: 0.2599521577358246\n",
      "Step: 25300  \tTraining loss: 0.24673351645469666\n",
      "Step: 25300  \tTraining accuracy: 0.8843651413917542\n",
      "Step: 25300  \tValid loss: 0.2599068582057953\n",
      "Step: 25400  \tTraining loss: 0.24668535590171814\n",
      "Step: 25400  \tTraining accuracy: 0.8843926191329956\n",
      "Step: 25400  \tValid loss: 0.2598450183868408\n",
      "Step: 25500  \tTraining loss: 0.24663831293582916\n",
      "Step: 25500  \tTraining accuracy: 0.884419858455658\n",
      "Step: 25500  \tValid loss: 0.2598054111003876\n",
      "Step: 25600  \tTraining loss: 0.24658697843551636\n",
      "Step: 25600  \tTraining accuracy: 0.8844468593597412\n",
      "Step: 25600  \tValid loss: 0.25974124670028687\n",
      "Step: 25700  \tTraining loss: 0.2465379536151886\n",
      "Step: 25700  \tTraining accuracy: 0.8844736814498901\n",
      "Step: 25700  \tValid loss: 0.25968262553215027\n",
      "Step: 25800  \tTraining loss: 0.2464887946844101\n",
      "Step: 25800  \tTraining accuracy: 0.8845003247261047\n",
      "Step: 25800  \tValid loss: 0.25962594151496887\n",
      "Step: 25900  \tTraining loss: 0.24644121527671814\n",
      "Step: 25900  \tTraining accuracy: 0.8845267295837402\n",
      "Step: 25900  \tValid loss: 0.2595769166946411\n",
      "Step: 26000  \tTraining loss: 0.2463892698287964\n",
      "Step: 26000  \tTraining accuracy: 0.8845528960227966\n",
      "Step: 26000  \tValid loss: 0.2595199942588806\n",
      "Step: 26100  \tTraining loss: 0.2463381439447403\n",
      "Step: 26100  \tTraining accuracy: 0.8845789432525635\n",
      "Step: 26100  \tValid loss: 0.25947028398513794\n",
      "Step: 26200  \tTraining loss: 0.24627898633480072\n",
      "Step: 26200  \tTraining accuracy: 0.8846047520637512\n",
      "Step: 26200  \tValid loss: 0.25941580533981323\n",
      "Step: 26300  \tTraining loss: 0.24621808528900146\n",
      "Step: 26300  \tTraining accuracy: 0.8846303224563599\n",
      "Step: 26300  \tValid loss: 0.2593371868133545\n",
      "Step: 26400  \tTraining loss: 0.24616767466068268\n",
      "Step: 26400  \tTraining accuracy: 0.884655773639679\n",
      "Step: 26400  \tValid loss: 0.2592889368534088\n",
      "Step: 26500  \tTraining loss: 0.24611617624759674\n",
      "Step: 26500  \tTraining accuracy: 0.884680986404419\n",
      "Step: 26500  \tValid loss: 0.2592433989048004\n",
      "Step: 26600  \tTraining loss: 0.2460654377937317\n",
      "Step: 26600  \tTraining accuracy: 0.8847060203552246\n",
      "Step: 26600  \tValid loss: 0.25920069217681885\n",
      "Step: 26700  \tTraining loss: 0.24601471424102783\n",
      "Step: 26700  \tTraining accuracy: 0.884730875492096\n",
      "Step: 26700  \tValid loss: 0.2591635584831238\n",
      "Step: 26800  \tTraining loss: 0.24596139788627625\n",
      "Step: 26800  \tTraining accuracy: 0.8847554922103882\n",
      "Step: 26800  \tValid loss: 0.2591157853603363\n",
      "Step: 26900  \tTraining loss: 0.24591420590877533\n",
      "Step: 26900  \tTraining accuracy: 0.8847799897193909\n",
      "Step: 26900  \tValid loss: 0.25907468795776367\n",
      "Step: 27000  \tTraining loss: 0.24586522579193115\n",
      "Step: 27000  \tTraining accuracy: 0.8848035335540771\n",
      "Step: 27000  \tValid loss: 0.25901898741722107\n",
      "Step: 27100  \tTraining loss: 0.24581879377365112\n",
      "Step: 27100  \tTraining accuracy: 0.8848257064819336\n",
      "Step: 27100  \tValid loss: 0.25898057222366333\n",
      "Step: 27200  \tTraining loss: 0.2457721084356308\n",
      "Step: 27200  \tTraining accuracy: 0.8848477005958557\n",
      "Step: 27200  \tValid loss: 0.2589336633682251\n",
      "Step: 27300  \tTraining loss: 0.2457275092601776\n",
      "Step: 27300  \tTraining accuracy: 0.8848695755004883\n",
      "Step: 27300  \tValid loss: 0.25889864563941956\n",
      "Step: 27400  \tTraining loss: 0.24568231403827667\n",
      "Step: 27400  \tTraining accuracy: 0.8848912715911865\n",
      "Step: 27400  \tValid loss: 0.25885647535324097\n",
      "Step: 27500  \tTraining loss: 0.24563686549663544\n",
      "Step: 27500  \tTraining accuracy: 0.8849127888679504\n",
      "Step: 27500  \tValid loss: 0.2588193416595459\n",
      "Step: 27600  \tTraining loss: 0.24558934569358826\n",
      "Step: 27600  \tTraining accuracy: 0.8849341869354248\n",
      "Step: 27600  \tValid loss: 0.25876784324645996\n",
      "Step: 27700  \tTraining loss: 0.24554403126239777\n",
      "Step: 27700  \tTraining accuracy: 0.8849554061889648\n",
      "Step: 27700  \tValid loss: 0.2587308883666992\n",
      "Step: 27800  \tTraining loss: 0.2454971969127655\n",
      "Step: 27800  \tTraining accuracy: 0.8849757313728333\n",
      "Step: 27800  \tValid loss: 0.25868672132492065\n",
      "Step: 27900  \tTraining loss: 0.24544642865657806\n",
      "Step: 27900  \tTraining accuracy: 0.8849948048591614\n",
      "Step: 27900  \tValid loss: 0.2586565315723419\n",
      "Step: 28000  \tTraining loss: 0.24539658427238464\n",
      "Step: 28000  \tTraining accuracy: 0.8850137591362\n",
      "Step: 28000  \tValid loss: 0.2586091160774231\n",
      "Step: 28100  \tTraining loss: 0.24535056948661804\n",
      "Step: 28100  \tTraining accuracy: 0.8850325345993042\n",
      "Step: 28100  \tValid loss: 0.25855088233947754\n",
      "Step: 28200  \tTraining loss: 0.245303213596344\n",
      "Step: 28200  \tTraining accuracy: 0.8850511312484741\n",
      "Step: 28200  \tValid loss: 0.25849395990371704\n",
      "Step: 28300  \tTraining loss: 0.24525997042655945\n",
      "Step: 28300  \tTraining accuracy: 0.8850696682929993\n",
      "Step: 28300  \tValid loss: 0.2584426999092102\n",
      "Step: 28400  \tTraining loss: 0.2452147901058197\n",
      "Step: 28400  \tTraining accuracy: 0.8850880861282349\n",
      "Step: 28400  \tValid loss: 0.25838443636894226\n",
      "Step: 28500  \tTraining loss: 0.2451712191104889\n",
      "Step: 28500  \tTraining accuracy: 0.8851063251495361\n",
      "Step: 28500  \tValid loss: 0.258331298828125\n",
      "Step: 28600  \tTraining loss: 0.24512653052806854\n",
      "Step: 28600  \tTraining accuracy: 0.8851262927055359\n",
      "Step: 28600  \tValid loss: 0.25827160477638245\n",
      "Step: 28700  \tTraining loss: 0.24509117007255554\n",
      "Step: 28700  \tTraining accuracy: 0.8851460814476013\n",
      "Step: 28700  \tValid loss: 0.2582169473171234\n",
      "Step: 28800  \tTraining loss: 0.24504335224628448\n",
      "Step: 28800  \tTraining accuracy: 0.8851657509803772\n",
      "Step: 28800  \tValid loss: 0.2581706643104553\n",
      "Step: 28900  \tTraining loss: 0.24500198662281036\n",
      "Step: 28900  \tTraining accuracy: 0.8851860761642456\n",
      "Step: 28900  \tValid loss: 0.25811144709587097\n",
      "Step: 29000  \tTraining loss: 0.24495917558670044\n",
      "Step: 29000  \tTraining accuracy: 0.8852065205574036\n",
      "Step: 29000  \tValid loss: 0.2580583393573761\n",
      "Step: 29100  \tTraining loss: 0.2449069321155548\n",
      "Step: 29100  \tTraining accuracy: 0.8852272629737854\n",
      "Step: 29100  \tValid loss: 0.2581039071083069\n",
      "Step: 29200  \tTraining loss: 0.2448483258485794\n",
      "Step: 29200  \tTraining accuracy: 0.8852481842041016\n",
      "Step: 29200  \tValid loss: 0.25813207030296326\n",
      "Step: 29300  \tTraining loss: 0.24478386342525482\n",
      "Step: 29300  \tTraining accuracy: 0.8852689862251282\n",
      "Step: 29300  \tValid loss: 0.258147269487381\n",
      "Step: 29400  \tTraining loss: 0.24472299218177795\n",
      "Step: 29400  \tTraining accuracy: 0.8852896094322205\n",
      "Step: 29400  \tValid loss: 0.2580910921096802\n",
      "Step: 29500  \tTraining loss: 0.2446761578321457\n",
      "Step: 29500  \tTraining accuracy: 0.8853108286857605\n",
      "Step: 29500  \tValid loss: 0.25801151990890503\n",
      "Step: 29600  \tTraining loss: 0.24462753534317017\n",
      "Step: 29600  \tTraining accuracy: 0.8853329420089722\n",
      "Step: 29600  \tValid loss: 0.2579175531864166\n",
      "Step: 29700  \tTraining loss: 0.24458125233650208\n",
      "Step: 29700  \tTraining accuracy: 0.8853549361228943\n",
      "Step: 29700  \tValid loss: 0.2578562796115875\n",
      "Step: 29800  \tTraining loss: 0.24453754723072052\n",
      "Step: 29800  \tTraining accuracy: 0.8853767514228821\n",
      "Step: 29800  \tValid loss: 0.2577863037586212\n",
      "Step: 29900  \tTraining loss: 0.2444913536310196\n",
      "Step: 29900  \tTraining accuracy: 0.885397732257843\n",
      "Step: 29900  \tValid loss: 0.25773027539253235\n",
      "Step: 30000  \tTraining loss: 0.24444718658924103\n",
      "Step: 30000  \tTraining accuracy: 0.8854175209999084\n",
      "Step: 30000  \tValid loss: 0.25766974687576294\n",
      "Step: 30100  \tTraining loss: 0.2444036900997162\n",
      "Step: 30100  \tTraining accuracy: 0.8854379057884216\n",
      "Step: 30100  \tValid loss: 0.257612019777298\n",
      "Step: 30200  \tTraining loss: 0.24435925483703613\n",
      "Step: 30200  \tTraining accuracy: 0.8854591846466064\n",
      "Step: 30200  \tValid loss: 0.2575591504573822\n",
      "Step: 30300  \tTraining loss: 0.24432019889354706\n",
      "Step: 30300  \tTraining accuracy: 0.8854802846908569\n",
      "Step: 30300  \tValid loss: 0.257507860660553\n",
      "Step: 30400  \tTraining loss: 0.24427425861358643\n",
      "Step: 30400  \tTraining accuracy: 0.8855012655258179\n",
      "Step: 30400  \tValid loss: 0.25745657086372375\n",
      "Step: 30500  \tTraining loss: 0.24423041939735413\n",
      "Step: 30500  \tTraining accuracy: 0.8855221271514893\n",
      "Step: 30500  \tValid loss: 0.25738993287086487\n",
      "Step: 30600  \tTraining loss: 0.24419257044792175\n",
      "Step: 30600  \tTraining accuracy: 0.8855428099632263\n",
      "Step: 30600  \tValid loss: 0.2573476731777191\n",
      "Step: 30700  \tTraining loss: 0.24414511024951935\n",
      "Step: 30700  \tTraining accuracy: 0.8855637311935425\n",
      "Step: 30700  \tValid loss: 0.2572929263114929\n",
      "Step: 30800  \tTraining loss: 0.2441035509109497\n",
      "Step: 30800  \tTraining accuracy: 0.8855859041213989\n",
      "Step: 30800  \tValid loss: 0.257245808839798\n",
      "Step: 30900  \tTraining loss: 0.24406138062477112\n",
      "Step: 30900  \tTraining accuracy: 0.885607898235321\n",
      "Step: 30900  \tValid loss: 0.25718531012535095\n",
      "Step: 31000  \tTraining loss: 0.24401801824569702\n",
      "Step: 31000  \tTraining accuracy: 0.8856304287910461\n",
      "Step: 31000  \tValid loss: 0.25713828206062317\n",
      "Step: 31100  \tTraining loss: 0.2439766377210617\n",
      "Step: 31100  \tTraining accuracy: 0.8856537938117981\n",
      "Step: 31100  \tValid loss: 0.25708651542663574\n",
      "Step: 31200  \tTraining loss: 0.243935689330101\n",
      "Step: 31200  \tTraining accuracy: 0.8856770396232605\n",
      "Step: 31200  \tValid loss: 0.25703734159469604\n",
      "Step: 31300  \tTraining loss: 0.24389469623565674\n",
      "Step: 31300  \tTraining accuracy: 0.8857001662254333\n",
      "Step: 31300  \tValid loss: 0.2569836676120758\n",
      "Step: 31400  \tTraining loss: 0.2438490390777588\n",
      "Step: 31400  \tTraining accuracy: 0.8857231140136719\n",
      "Step: 31400  \tValid loss: 0.25693845748901367\n",
      "Step: 31500  \tTraining loss: 0.24380387365818024\n",
      "Step: 31500  \tTraining accuracy: 0.8857458829879761\n",
      "Step: 31500  \tValid loss: 0.2568906843662262\n",
      "Step: 31600  \tTraining loss: 0.2437574565410614\n",
      "Step: 31600  \tTraining accuracy: 0.8857685327529907\n",
      "Step: 31600  \tValid loss: 0.2568380534648895\n",
      "Step: 31700  \tTraining loss: 0.24371053278446198\n",
      "Step: 31700  \tTraining accuracy: 0.8857910633087158\n",
      "Step: 31700  \tValid loss: 0.2567821145057678\n",
      "Step: 31800  \tTraining loss: 0.24366745352745056\n",
      "Step: 31800  \tTraining accuracy: 0.8858127593994141\n",
      "Step: 31800  \tValid loss: 0.256735235452652\n",
      "Step: 31900  \tTraining loss: 0.24362386763095856\n",
      "Step: 31900  \tTraining accuracy: 0.8858327269554138\n",
      "Step: 31900  \tValid loss: 0.2566843330860138\n",
      "Step: 32000  \tTraining loss: 0.24357552826404572\n",
      "Step: 32000  \tTraining accuracy: 0.8858515620231628\n",
      "Step: 32000  \tValid loss: 0.2566390633583069\n",
      "Step: 32100  \tTraining loss: 0.24352869391441345\n",
      "Step: 32100  \tTraining accuracy: 0.8858702778816223\n",
      "Step: 32100  \tValid loss: 0.2565944194793701\n",
      "Step: 32200  \tTraining loss: 0.24348126351833344\n",
      "Step: 32200  \tTraining accuracy: 0.8858895301818848\n",
      "Step: 32200  \tValid loss: 0.25654804706573486\n",
      "Step: 32300  \tTraining loss: 0.24343422055244446\n",
      "Step: 32300  \tTraining accuracy: 0.8859096169471741\n",
      "Step: 32300  \tValid loss: 0.25650250911712646\n",
      "Step: 32400  \tTraining loss: 0.24338868260383606\n",
      "Step: 32400  \tTraining accuracy: 0.8859295845031738\n",
      "Step: 32400  \tValid loss: 0.25645703077316284\n",
      "Step: 32500  \tTraining loss: 0.2433481514453888\n",
      "Step: 32500  \tTraining accuracy: 0.885949432849884\n",
      "Step: 32500  \tValid loss: 0.25643956661224365\n",
      "Step: 32600  \tTraining loss: 0.24330534040927887\n",
      "Step: 32600  \tTraining accuracy: 0.8859691619873047\n",
      "Step: 32600  \tValid loss: 0.2564227879047394\n",
      "Step: 32700  \tTraining loss: 0.2432648241519928\n",
      "Step: 32700  \tTraining accuracy: 0.8859887719154358\n",
      "Step: 32700  \tValid loss: 0.2563987076282501\n",
      "Step: 32800  \tTraining loss: 0.243222177028656\n",
      "Step: 32800  \tTraining accuracy: 0.8860082626342773\n",
      "Step: 32800  \tValid loss: 0.2563599646091461\n",
      "Step: 32900  \tTraining loss: 0.24318166077136993\n",
      "Step: 32900  \tTraining accuracy: 0.8860276341438293\n",
      "Step: 32900  \tValid loss: 0.25632244348526\n",
      "Step: 33000  \tTraining loss: 0.24314329028129578\n",
      "Step: 33000  \tTraining accuracy: 0.8860468864440918\n",
      "Step: 33000  \tValid loss: 0.2562941908836365\n",
      "Step: 33100  \tTraining loss: 0.24310217797756195\n",
      "Step: 33100  \tTraining accuracy: 0.8860660195350647\n",
      "Step: 33100  \tValid loss: 0.2562515437602997\n",
      "Step: 33200  \tTraining loss: 0.2430626004934311\n",
      "Step: 33200  \tTraining accuracy: 0.8860850930213928\n",
      "Step: 33200  \tValid loss: 0.256206214427948\n",
      "Step: 33300  \tTraining loss: 0.2430230677127838\n",
      "Step: 33300  \tTraining accuracy: 0.8861039876937866\n",
      "Step: 33300  \tValid loss: 0.25617316365242004\n",
      "Step: 33400  \tTraining loss: 0.24298273026943207\n",
      "Step: 33400  \tTraining accuracy: 0.8861227631568909\n",
      "Step: 33400  \tValid loss: 0.25612470507621765\n",
      "Step: 33500  \tTraining loss: 0.24294379353523254\n",
      "Step: 33500  \tTraining accuracy: 0.8861414790153503\n",
      "Step: 33500  \tValid loss: 0.2560900151729584\n",
      "Step: 33600  \tTraining loss: 0.24290543794631958\n",
      "Step: 33600  \tTraining accuracy: 0.8861600160598755\n",
      "Step: 33600  \tValid loss: 0.2560414969921112\n",
      "Step: 33700  \tTraining loss: 0.24286584556102753\n",
      "Step: 33700  \tTraining accuracy: 0.8861784934997559\n",
      "Step: 33700  \tValid loss: 0.2560049891471863\n",
      "Step: 33800  \tTraining loss: 0.24282854795455933\n",
      "Step: 33800  \tTraining accuracy: 0.8861956000328064\n",
      "Step: 33800  \tValid loss: 0.25596415996551514\n",
      "Step: 33900  \tTraining loss: 0.24279017746448517\n",
      "Step: 33900  \tTraining accuracy: 0.8862135410308838\n",
      "Step: 33900  \tValid loss: 0.2559291124343872\n",
      "Step: 34000  \tTraining loss: 0.24275535345077515\n",
      "Step: 34000  \tTraining accuracy: 0.88623046875\n",
      "Step: 34000  \tValid loss: 0.255892276763916\n",
      "Step: 34100  \tTraining loss: 0.2427203208208084\n",
      "Step: 34100  \tTraining accuracy: 0.886246919631958\n",
      "Step: 34100  \tValid loss: 0.25584644079208374\n",
      "Step: 34200  \tTraining loss: 0.24268114566802979\n",
      "Step: 34200  \tTraining accuracy: 0.886263370513916\n",
      "Step: 34200  \tValid loss: 0.2558121681213379\n",
      "Step: 34300  \tTraining loss: 0.24264422059059143\n",
      "Step: 34300  \tTraining accuracy: 0.8862793445587158\n",
      "Step: 34300  \tValid loss: 0.25577887892723083\n",
      "Step: 34400  \tTraining loss: 0.24260841310024261\n",
      "Step: 34400  \tTraining accuracy: 0.8862955570220947\n",
      "Step: 34400  \tValid loss: 0.2557394206523895\n",
      "Step: 34500  \tTraining loss: 0.24257352948188782\n",
      "Step: 34500  \tTraining accuracy: 0.8863111138343811\n",
      "Step: 34500  \tValid loss: 0.2556930482387543\n",
      "Step: 34600  \tTraining loss: 0.24253705143928528\n",
      "Step: 34600  \tTraining accuracy: 0.8863261938095093\n",
      "Step: 34600  \tValid loss: 0.2556561231613159\n",
      "Step: 34700  \tTraining loss: 0.24250026047229767\n",
      "Step: 34700  \tTraining accuracy: 0.8863421678543091\n",
      "Step: 34700  \tValid loss: 0.25562700629234314\n",
      "Step: 34800  \tTraining loss: 0.24246317148208618\n",
      "Step: 34800  \tTraining accuracy: 0.8863574266433716\n",
      "Step: 34800  \tValid loss: 0.25558632612228394\n",
      "Step: 34900  \tTraining loss: 0.24242593348026276\n",
      "Step: 34900  \tTraining accuracy: 0.8863722681999207\n",
      "Step: 34900  \tValid loss: 0.2555568218231201\n",
      "Step: 35000  \tTraining loss: 0.24238541722297668\n",
      "Step: 35000  \tTraining accuracy: 0.8863879442214966\n",
      "Step: 35000  \tValid loss: 0.25552046298980713\n",
      "Step: 35100  \tTraining loss: 0.24234852194786072\n",
      "Step: 35100  \tTraining accuracy: 0.886403501033783\n",
      "Step: 35100  \tValid loss: 0.25548306107521057\n",
      "Step: 35200  \tTraining loss: 0.2423097938299179\n",
      "Step: 35200  \tTraining accuracy: 0.8864195942878723\n",
      "Step: 35200  \tValid loss: 0.25544798374176025\n",
      "Step: 35300  \tTraining loss: 0.24227258563041687\n",
      "Step: 35300  \tTraining accuracy: 0.886435866355896\n",
      "Step: 35300  \tValid loss: 0.25540730357170105\n",
      "Step: 35400  \tTraining loss: 0.24224041402339935\n",
      "Step: 35400  \tTraining accuracy: 0.8864511847496033\n",
      "Step: 35400  \tValid loss: 0.255361944437027\n",
      "Step: 35500  \tTraining loss: 0.24219878017902374\n",
      "Step: 35500  \tTraining accuracy: 0.8864664435386658\n",
      "Step: 35500  \tValid loss: 0.255329430103302\n",
      "Step: 35600  \tTraining loss: 0.24216359853744507\n",
      "Step: 35600  \tTraining accuracy: 0.8864815831184387\n",
      "Step: 35600  \tValid loss: 0.25528764724731445\n",
      "Step: 35700  \tTraining loss: 0.24212591350078583\n",
      "Step: 35700  \tTraining accuracy: 0.8864966630935669\n",
      "Step: 35700  \tValid loss: 0.2552593946456909\n",
      "Step: 35800  \tTraining loss: 0.2420904040336609\n",
      "Step: 35800  \tTraining accuracy: 0.8865116238594055\n",
      "Step: 35800  \tValid loss: 0.2552129626274109\n",
      "Step: 35900  \tTraining loss: 0.24205371737480164\n",
      "Step: 35900  \tTraining accuracy: 0.8865256309509277\n",
      "Step: 35900  \tValid loss: 0.2551831901073456\n",
      "Step: 36000  \tTraining loss: 0.24201767146587372\n",
      "Step: 36000  \tTraining accuracy: 0.8865389823913574\n",
      "Step: 36000  \tValid loss: 0.2551420032978058\n",
      "Step: 36100  \tTraining loss: 0.2419825941324234\n",
      "Step: 36100  \tTraining accuracy: 0.8865522742271423\n",
      "Step: 36100  \tValid loss: 0.25510743260383606\n",
      "Step: 36200  \tTraining loss: 0.24194668233394623\n",
      "Step: 36200  \tTraining accuracy: 0.8865654468536377\n",
      "Step: 36200  \tValid loss: 0.2550729215145111\n",
      "Step: 36300  \tTraining loss: 0.2419138103723526\n",
      "Step: 36300  \tTraining accuracy: 0.8865786194801331\n",
      "Step: 36300  \tValid loss: 0.25503769516944885\n",
      "Step: 36400  \tTraining loss: 0.24187639355659485\n",
      "Step: 36400  \tTraining accuracy: 0.8865916728973389\n",
      "Step: 36400  \tValid loss: 0.2550027072429657\n",
      "Step: 36500  \tTraining loss: 0.24184271693229675\n",
      "Step: 36500  \tTraining accuracy: 0.8866046667098999\n",
      "Step: 36500  \tValid loss: 0.2549823224544525\n",
      "Step: 36600  \tTraining loss: 0.24180641770362854\n",
      "Step: 36600  \tTraining accuracy: 0.8866176009178162\n",
      "Step: 36600  \tValid loss: 0.25494080781936646\n",
      "Step: 36700  \tTraining loss: 0.24176962673664093\n",
      "Step: 36700  \tTraining accuracy: 0.8866304159164429\n",
      "Step: 36700  \tValid loss: 0.2549130618572235\n",
      "Step: 36800  \tTraining loss: 0.24173390865325928\n",
      "Step: 36800  \tTraining accuracy: 0.8866432309150696\n",
      "Step: 36800  \tValid loss: 0.25488513708114624\n",
      "Step: 36900  \tTraining loss: 0.24170033633708954\n",
      "Step: 36900  \tTraining accuracy: 0.8866559267044067\n",
      "Step: 36900  \tValid loss: 0.25485143065452576\n",
      "Step: 37000  \tTraining loss: 0.241664320230484\n",
      "Step: 37000  \tTraining accuracy: 0.8866685628890991\n",
      "Step: 37000  \tValid loss: 0.2548237144947052\n",
      "Step: 37100  \tTraining loss: 0.2416311800479889\n",
      "Step: 37100  \tTraining accuracy: 0.8866811394691467\n",
      "Step: 37100  \tValid loss: 0.2547916769981384\n",
      "Step: 37200  \tTraining loss: 0.2415967434644699\n",
      "Step: 37200  \tTraining accuracy: 0.8866936564445496\n",
      "Step: 37200  \tValid loss: 0.25475913286209106\n",
      "Step: 37300  \tTraining loss: 0.24156400561332703\n",
      "Step: 37300  \tTraining accuracy: 0.8867060542106628\n",
      "Step: 37300  \tValid loss: 0.25473299622535706\n",
      "Step: 37400  \tTraining loss: 0.24153025448322296\n",
      "Step: 37400  \tTraining accuracy: 0.886719286441803\n",
      "Step: 37400  \tValid loss: 0.2547065019607544\n",
      "Step: 37500  \tTraining loss: 0.24149523675441742\n",
      "Step: 37500  \tTraining accuracy: 0.8867329955101013\n",
      "Step: 37500  \tValid loss: 0.2546699643135071\n",
      "Step: 37600  \tTraining loss: 0.24146242439746857\n",
      "Step: 37600  \tTraining accuracy: 0.8867466449737549\n",
      "Step: 37600  \tValid loss: 0.25463584065437317\n",
      "Step: 37700  \tTraining loss: 0.24142961204051971\n",
      "Step: 37700  \tTraining accuracy: 0.8867601752281189\n",
      "Step: 37700  \tValid loss: 0.2546076774597168\n",
      "Step: 37800  \tTraining loss: 0.24139650166034698\n",
      "Step: 37800  \tTraining accuracy: 0.8867736458778381\n",
      "Step: 37800  \tValid loss: 0.2545855939388275\n",
      "Step: 37900  \tTraining loss: 0.2413647621870041\n",
      "Step: 37900  \tTraining accuracy: 0.8867870569229126\n",
      "Step: 37900  \tValid loss: 0.254562109708786\n",
      "Step: 38000  \tTraining loss: 0.24133354425430298\n",
      "Step: 38000  \tTraining accuracy: 0.8868004083633423\n",
      "Step: 38000  \tValid loss: 0.2545336186885834\n",
      "Step: 38100  \tTraining loss: 0.24129609763622284\n",
      "Step: 38100  \tTraining accuracy: 0.8868137001991272\n",
      "Step: 38100  \tValid loss: 0.25449138879776\n",
      "Step: 38200  \tTraining loss: 0.24126262962818146\n",
      "Step: 38200  \tTraining accuracy: 0.8868263363838196\n",
      "Step: 38200  \tValid loss: 0.2544616162776947\n",
      "Step: 38300  \tTraining loss: 0.24123375117778778\n",
      "Step: 38300  \tTraining accuracy: 0.8868380784988403\n",
      "Step: 38300  \tValid loss: 0.2544287145137787\n",
      "Step: 38400  \tTraining loss: 0.24120044708251953\n",
      "Step: 38400  \tTraining accuracy: 0.8868498206138611\n",
      "Step: 38400  \tValid loss: 0.25440526008605957\n",
      "Step: 38500  \tTraining loss: 0.24116872251033783\n",
      "Step: 38500  \tTraining accuracy: 0.8868614435195923\n",
      "Step: 38500  \tValid loss: 0.25437867641448975\n",
      "Step: 38600  \tTraining loss: 0.24113701283931732\n",
      "Step: 38600  \tTraining accuracy: 0.8868730068206787\n",
      "Step: 38600  \tValid loss: 0.25434622168540955\n",
      "Step: 38700  \tTraining loss: 0.24110843241214752\n",
      "Step: 38700  \tTraining accuracy: 0.8868845701217651\n",
      "Step: 38700  \tValid loss: 0.2543269693851471\n",
      "Step: 38800  \tTraining loss: 0.2410752922296524\n",
      "Step: 38800  \tTraining accuracy: 0.886896014213562\n",
      "Step: 38800  \tValid loss: 0.2542940676212311\n",
      "Step: 38900  \tTraining loss: 0.24104389548301697\n",
      "Step: 38900  \tTraining accuracy: 0.8869073987007141\n",
      "Step: 38900  \tValid loss: 0.2542683780193329\n",
      "Step: 39000  \tTraining loss: 0.24101506173610687\n",
      "Step: 39000  \tTraining accuracy: 0.8869177103042603\n",
      "Step: 39000  \tValid loss: 0.25424623489379883\n",
      "Step: 39100  \tTraining loss: 0.2409834861755371\n",
      "Step: 39100  \tTraining accuracy: 0.8869268298149109\n",
      "Step: 39100  \tValid loss: 0.25422030687332153\n",
      "Step: 39200  \tTraining loss: 0.24095399677753448\n",
      "Step: 39200  \tTraining accuracy: 0.8869367241859436\n",
      "Step: 39200  \tValid loss: 0.25419411063194275\n",
      "Step: 39300  \tTraining loss: 0.24092219769954681\n",
      "Step: 39300  \tTraining accuracy: 0.8869466185569763\n",
      "Step: 39300  \tValid loss: 0.2541663348674774\n",
      "Step: 39400  \tTraining loss: 0.24089358747005463\n",
      "Step: 39400  \tTraining accuracy: 0.8869563937187195\n",
      "Step: 39400  \tValid loss: 0.25414612889289856\n",
      "Step: 39500  \tTraining loss: 0.2408652901649475\n",
      "Step: 39500  \tTraining accuracy: 0.8869656324386597\n",
      "Step: 39500  \tValid loss: 0.25411084294319153\n",
      "Step: 39600  \tTraining loss: 0.24083462357521057\n",
      "Step: 39600  \tTraining accuracy: 0.886974573135376\n",
      "Step: 39600  \tValid loss: 0.25408899784088135\n",
      "Step: 39700  \tTraining loss: 0.24080632627010345\n",
      "Step: 39700  \tTraining accuracy: 0.8869842290878296\n",
      "Step: 39700  \tValid loss: 0.254060298204422\n",
      "Step: 39800  \tTraining loss: 0.24077507853507996\n",
      "Step: 39800  \tTraining accuracy: 0.8869938254356384\n",
      "Step: 39800  \tValid loss: 0.25402867794036865\n",
      "Step: 39900  \tTraining loss: 0.24074715375900269\n",
      "Step: 39900  \tTraining accuracy: 0.8870033621788025\n",
      "Step: 39900  \tValid loss: 0.2540023922920227\n",
      "Step: 40000  \tTraining loss: 0.24071668088436127\n",
      "Step: 40000  \tTraining accuracy: 0.8870128989219666\n",
      "Step: 40000  \tValid loss: 0.253971666097641\n",
      "Step: 40100  \tTraining loss: 0.2406884729862213\n",
      "Step: 40100  \tTraining accuracy: 0.8870223760604858\n",
      "Step: 40100  \tValid loss: 0.2539418339729309\n",
      "Step: 40200  \tTraining loss: 0.24065902829170227\n",
      "Step: 40200  \tTraining accuracy: 0.8870317935943604\n",
      "Step: 40200  \tValid loss: 0.2539120614528656\n",
      "Step: 40300  \tTraining loss: 0.24063168466091156\n",
      "Step: 40300  \tTraining accuracy: 0.8870406150817871\n",
      "Step: 40300  \tValid loss: 0.25388869643211365\n",
      "Step: 40400  \tTraining loss: 0.24060428142547607\n",
      "Step: 40400  \tTraining accuracy: 0.8870491981506348\n",
      "Step: 40400  \tValid loss: 0.2538621723651886\n",
      "Step: 40500  \tTraining loss: 0.24057450890541077\n",
      "Step: 40500  \tTraining accuracy: 0.8870584964752197\n",
      "Step: 40500  \tValid loss: 0.25384220480918884\n",
      "Step: 40600  \tTraining loss: 0.24054810404777527\n",
      "Step: 40600  \tTraining accuracy: 0.8870677351951599\n",
      "Step: 40600  \tValid loss: 0.25382813811302185\n",
      "Step: 40700  \tTraining loss: 0.24052093923091888\n",
      "Step: 40700  \tTraining accuracy: 0.8870763778686523\n",
      "Step: 40700  \tValid loss: 0.25379297137260437\n",
      "Step: 40800  \tTraining loss: 0.24048969149589539\n",
      "Step: 40800  \tTraining accuracy: 0.8870840072631836\n",
      "Step: 40800  \tValid loss: 0.2537737190723419\n",
      "Step: 40900  \tTraining loss: 0.24046310782432556\n",
      "Step: 40900  \tTraining accuracy: 0.887091338634491\n",
      "Step: 40900  \tValid loss: 0.2537477910518646\n",
      "Step: 41000  \tTraining loss: 0.24043504893779755\n",
      "Step: 41000  \tTraining accuracy: 0.8870983719825745\n",
      "Step: 41000  \tValid loss: 0.25372442603111267\n",
      "Step: 41100  \tTraining loss: 0.2404070794582367\n",
      "Step: 41100  \tTraining accuracy: 0.8871061205863953\n",
      "Step: 41100  \tValid loss: 0.25369128584861755\n",
      "Step: 41200  \tTraining loss: 0.24038216471672058\n",
      "Step: 41200  \tTraining accuracy: 0.8871133327484131\n",
      "Step: 41200  \tValid loss: 0.2536703944206238\n",
      "Step: 41300  \tTraining loss: 0.2403530776500702\n",
      "Step: 41300  \tTraining accuracy: 0.8871207237243652\n",
      "Step: 41300  \tValid loss: 0.2536512613296509\n",
      "Step: 41400  \tTraining loss: 0.24032659828662872\n",
      "Step: 41400  \tTraining accuracy: 0.8871291279792786\n",
      "Step: 41400  \tValid loss: 0.2536398768424988\n",
      "Step: 41500  \tTraining loss: 0.2403026968240738\n",
      "Step: 41500  \tTraining accuracy: 0.8871367573738098\n",
      "Step: 41500  \tValid loss: 0.2536064088344574\n",
      "Step: 41600  \tTraining loss: 0.24027259647846222\n",
      "Step: 41600  \tTraining accuracy: 0.8871448040008545\n",
      "Step: 41600  \tValid loss: 0.25358808040618896\n",
      "Step: 41700  \tTraining loss: 0.240244522690773\n",
      "Step: 41700  \tTraining accuracy: 0.8871536254882812\n",
      "Step: 41700  \tValid loss: 0.25356829166412354\n",
      "Step: 41800  \tTraining loss: 0.24021980166435242\n",
      "Step: 41800  \tTraining accuracy: 0.8871618509292603\n",
      "Step: 41800  \tValid loss: 0.25354576110839844\n",
      "Step: 41900  \tTraining loss: 0.24018947780132294\n",
      "Step: 41900  \tTraining accuracy: 0.8871697783470154\n",
      "Step: 41900  \tValid loss: 0.2535300552845001\n",
      "Step: 42000  \tTraining loss: 0.24016261100769043\n",
      "Step: 42000  \tTraining accuracy: 0.8871784806251526\n",
      "Step: 42000  \tValid loss: 0.2535060942173004\n",
      "Step: 42100  \tTraining loss: 0.24013549089431763\n",
      "Step: 42100  \tTraining accuracy: 0.887186586856842\n",
      "Step: 42100  \tValid loss: 0.25348782539367676\n",
      "Step: 42200  \tTraining loss: 0.24010835587978363\n",
      "Step: 42200  \tTraining accuracy: 0.8871939182281494\n",
      "Step: 42200  \tValid loss: 0.2534649968147278\n",
      "Step: 42300  \tTraining loss: 0.2400835007429123\n",
      "Step: 42300  \tTraining accuracy: 0.8872012495994568\n",
      "Step: 42300  \tValid loss: 0.2534429728984833\n",
      "Step: 42400  \tTraining loss: 0.24005529284477234\n",
      "Step: 42400  \tTraining accuracy: 0.8872085213661194\n",
      "Step: 42400  \tValid loss: 0.2534257471561432\n",
      "Step: 42500  \tTraining loss: 0.24002860486507416\n",
      "Step: 42500  \tTraining accuracy: 0.887215793132782\n",
      "Step: 42500  \tValid loss: 0.25340622663497925\n",
      "Step: 42600  \tTraining loss: 0.2400021255016327\n",
      "Step: 42600  \tTraining accuracy: 0.887222945690155\n",
      "Step: 42600  \tValid loss: 0.25338485836982727\n",
      "Step: 42700  \tTraining loss: 0.23997855186462402\n",
      "Step: 42700  \tTraining accuracy: 0.8872301578521729\n",
      "Step: 42700  \tValid loss: 0.2533572018146515\n",
      "Step: 42800  \tTraining loss: 0.23994971811771393\n",
      "Step: 42800  \tTraining accuracy: 0.8872373104095459\n",
      "Step: 42800  \tValid loss: 0.25334081053733826\n",
      "Step: 42900  \tTraining loss: 0.23992538452148438\n",
      "Step: 42900  \tTraining accuracy: 0.8872448801994324\n",
      "Step: 42900  \tValid loss: 0.2533259689807892\n",
      "Step: 43000  \tTraining loss: 0.23989783227443695\n",
      "Step: 43000  \tTraining accuracy: 0.8872531652450562\n",
      "Step: 43000  \tValid loss: 0.25330471992492676\n",
      "Step: 43100  \tTraining loss: 0.23987184464931488\n",
      "Step: 43100  \tTraining accuracy: 0.8872601985931396\n",
      "Step: 43100  \tValid loss: 0.2532821595668793\n",
      "Step: 43200  \tTraining loss: 0.23984742164611816\n",
      "Step: 43200  \tTraining accuracy: 0.8872659802436829\n",
      "Step: 43200  \tValid loss: 0.2532619833946228\n",
      "Step: 43300  \tTraining loss: 0.2398225963115692\n",
      "Step: 43300  \tTraining accuracy: 0.8872717618942261\n",
      "Step: 43300  \tValid loss: 0.25324028730392456\n",
      "Step: 43400  \tTraining loss: 0.2397940307855606\n",
      "Step: 43400  \tTraining accuracy: 0.8872774839401245\n",
      "Step: 43400  \tValid loss: 0.2532353699207306\n",
      "Step: 43500  \tTraining loss: 0.23976752161979675\n",
      "Step: 43500  \tTraining accuracy: 0.887283205986023\n",
      "Step: 43500  \tValid loss: 0.25320377945899963\n",
      "Step: 43600  \tTraining loss: 0.23974254727363586\n",
      "Step: 43600  \tTraining accuracy: 0.8872891664505005\n",
      "Step: 43600  \tValid loss: 0.2531769871711731\n",
      "Step: 43700  \tTraining loss: 0.23971520364284515\n",
      "Step: 43700  \tTraining accuracy: 0.8872947692871094\n",
      "Step: 43700  \tValid loss: 0.2531580924987793\n",
      "Step: 43800  \tTraining loss: 0.2396901696920395\n",
      "Step: 43800  \tTraining accuracy: 0.8872992396354675\n",
      "Step: 43800  \tValid loss: 0.25313055515289307\n",
      "Step: 43900  \tTraining loss: 0.2396642565727234\n",
      "Step: 43900  \tTraining accuracy: 0.8873036503791809\n",
      "Step: 43900  \tValid loss: 0.2531106173992157\n",
      "Step: 44000  \tTraining loss: 0.23963885009288788\n",
      "Step: 44000  \tTraining accuracy: 0.8873082995414734\n",
      "Step: 44000  \tValid loss: 0.2530881464481354\n",
      "Step: 44100  \tTraining loss: 0.2396133840084076\n",
      "Step: 44100  \tTraining accuracy: 0.8873138427734375\n",
      "Step: 44100  \tValid loss: 0.2530533969402313\n",
      "Step: 44200  \tTraining loss: 0.23958861827850342\n",
      "Step: 44200  \tTraining accuracy: 0.8873193860054016\n",
      "Step: 44200  \tValid loss: 0.25303784012794495\n",
      "Step: 44300  \tTraining loss: 0.23956353962421417\n",
      "Step: 44300  \tTraining accuracy: 0.8873249292373657\n",
      "Step: 44300  \tValid loss: 0.25301432609558105\n",
      "Step: 44400  \tTraining loss: 0.23953787982463837\n",
      "Step: 44400  \tTraining accuracy: 0.8873304128646851\n",
      "Step: 44400  \tValid loss: 0.252993106842041\n",
      "Step: 44500  \tTraining loss: 0.2395118772983551\n",
      "Step: 44500  \tTraining accuracy: 0.8873358964920044\n",
      "Step: 44500  \tValid loss: 0.25297069549560547\n",
      "Step: 44600  \tTraining loss: 0.23948560655117035\n",
      "Step: 44600  \tTraining accuracy: 0.887341320514679\n",
      "Step: 44600  \tValid loss: 0.252946138381958\n",
      "Step: 44700  \tTraining loss: 0.23946024477481842\n",
      "Step: 44700  \tTraining accuracy: 0.8873467445373535\n",
      "Step: 44700  \tValid loss: 0.25292542576789856\n",
      "Step: 44800  \tTraining loss: 0.2394338846206665\n",
      "Step: 44800  \tTraining accuracy: 0.8873521089553833\n",
      "Step: 44800  \tValid loss: 0.25289982557296753\n",
      "Step: 44900  \tTraining loss: 0.23940907418727875\n",
      "Step: 44900  \tTraining accuracy: 0.8873574733734131\n",
      "Step: 44900  \tValid loss: 0.25287869572639465\n",
      "Step: 45000  \tTraining loss: 0.23938332498073578\n",
      "Step: 45000  \tTraining accuracy: 0.8873628377914429\n",
      "Step: 45000  \tValid loss: 0.2528505325317383\n",
      "Step: 45100  \tTraining loss: 0.239358052611351\n",
      "Step: 45100  \tTraining accuracy: 0.8873681426048279\n",
      "Step: 45100  \tValid loss: 0.2528340220451355\n",
      "Step: 45200  \tTraining loss: 0.23933297395706177\n",
      "Step: 45200  \tTraining accuracy: 0.8873734474182129\n",
      "Step: 45200  \tValid loss: 0.25280290842056274\n",
      "Step: 45300  \tTraining loss: 0.23931020498275757\n",
      "Step: 45300  \tTraining accuracy: 0.8873791694641113\n",
      "Step: 45300  \tValid loss: 0.2528001666069031\n",
      "Step: 45400  \tTraining loss: 0.2392832189798355\n",
      "Step: 45400  \tTraining accuracy: 0.8873851299285889\n",
      "Step: 45400  \tValid loss: 0.25276291370391846\n",
      "Step: 45500  \tTraining loss: 0.23925867676734924\n",
      "Step: 45500  \tTraining accuracy: 0.8873903751373291\n",
      "Step: 45500  \tValid loss: 0.25274190306663513\n",
      "Step: 45600  \tTraining loss: 0.23922008275985718\n",
      "Step: 45600  \tTraining accuracy: 0.8873955607414246\n",
      "Step: 45600  \tValid loss: 0.2527328133583069\n",
      "Step: 45700  \tTraining loss: 0.23919765651226044\n",
      "Step: 45700  \tTraining accuracy: 0.8874002695083618\n",
      "Step: 45700  \tValid loss: 0.252686470746994\n",
      "Step: 45800  \tTraining loss: 0.23916608095169067\n",
      "Step: 45800  \tTraining accuracy: 0.88740473985672\n",
      "Step: 45800  \tValid loss: 0.25265660881996155\n",
      "Step: 45900  \tTraining loss: 0.23913975059986115\n",
      "Step: 45900  \tTraining accuracy: 0.8874094486236572\n",
      "Step: 45900  \tValid loss: 0.25263530015945435\n",
      "Step: 46000  \tTraining loss: 0.2391134649515152\n",
      "Step: 46000  \tTraining accuracy: 0.8874133825302124\n",
      "Step: 46000  \tValid loss: 0.2526124119758606\n",
      "Step: 46100  \tTraining loss: 0.23908928036689758\n",
      "Step: 46100  \tTraining accuracy: 0.8874177932739258\n",
      "Step: 46100  \tValid loss: 0.2525995373725891\n",
      "Step: 46200  \tTraining loss: 0.23906128108501434\n",
      "Step: 46200  \tTraining accuracy: 0.8874229192733765\n",
      "Step: 46200  \tValid loss: 0.2525629699230194\n",
      "Step: 46300  \tTraining loss: 0.239035964012146\n",
      "Step: 46300  \tTraining accuracy: 0.8874279260635376\n",
      "Step: 46300  \tValid loss: 0.2525448799133301\n",
      "Step: 46400  \tTraining loss: 0.23901183903217316\n",
      "Step: 46400  \tTraining accuracy: 0.8874329924583435\n",
      "Step: 46400  \tValid loss: 0.25252702832221985\n",
      "Step: 46500  \tTraining loss: 0.23898793756961823\n",
      "Step: 46500  \tTraining accuracy: 0.8874379992485046\n",
      "Step: 46500  \tValid loss: 0.25250762701034546\n",
      "Step: 46600  \tTraining loss: 0.23896491527557373\n",
      "Step: 46600  \tTraining accuracy: 0.887442946434021\n",
      "Step: 46600  \tValid loss: 0.2524849772453308\n",
      "Step: 46700  \tTraining loss: 0.23894397914409637\n",
      "Step: 46700  \tTraining accuracy: 0.8874479532241821\n",
      "Step: 46700  \tValid loss: 0.2524741291999817\n",
      "Step: 46800  \tTraining loss: 0.23891785740852356\n",
      "Step: 46800  \tTraining accuracy: 0.8874529004096985\n",
      "Step: 46800  \tValid loss: 0.2524488568305969\n",
      "Step: 46900  \tTraining loss: 0.23889537155628204\n",
      "Step: 46900  \tTraining accuracy: 0.8874577879905701\n",
      "Step: 46900  \tValid loss: 0.2524303197860718\n",
      "Step: 47000  \tTraining loss: 0.2388717085123062\n",
      "Step: 47000  \tTraining accuracy: 0.8874627351760864\n",
      "Step: 47000  \tValid loss: 0.25240200757980347\n",
      "Step: 47100  \tTraining loss: 0.2388484627008438\n",
      "Step: 47100  \tTraining accuracy: 0.8874675631523132\n",
      "Step: 47100  \tValid loss: 0.2523824870586395\n",
      "Step: 47200  \tTraining loss: 0.23882661759853363\n",
      "Step: 47200  \tTraining accuracy: 0.8874720335006714\n",
      "Step: 47200  \tValid loss: 0.2523576319217682\n",
      "Step: 47300  \tTraining loss: 0.23880314826965332\n",
      "Step: 47300  \tTraining accuracy: 0.8874762058258057\n",
      "Step: 47300  \tValid loss: 0.25233349204063416\n",
      "Step: 47400  \tTraining loss: 0.23878024518489838\n",
      "Step: 47400  \tTraining accuracy: 0.8874810338020325\n",
      "Step: 47400  \tValid loss: 0.25231578946113586\n",
      "Step: 47500  \tTraining loss: 0.23875820636749268\n",
      "Step: 47500  \tTraining accuracy: 0.8874858021736145\n",
      "Step: 47500  \tValid loss: 0.2522961497306824\n",
      "Step: 47600  \tTraining loss: 0.2387365698814392\n",
      "Step: 47600  \tTraining accuracy: 0.8874905705451965\n",
      "Step: 47600  \tValid loss: 0.2522776424884796\n",
      "Step: 47700  \tTraining loss: 0.23871473968029022\n",
      "Step: 47700  \tTraining accuracy: 0.8874949216842651\n",
      "Step: 47700  \tValid loss: 0.2522592842578888\n",
      "Step: 47800  \tTraining loss: 0.23869259655475616\n",
      "Step: 47800  \tTraining accuracy: 0.8874989748001099\n",
      "Step: 47800  \tValid loss: 0.2522428631782532\n",
      "Step: 47900  \tTraining loss: 0.23867018520832062\n",
      "Step: 47900  \tTraining accuracy: 0.8875041604042053\n",
      "Step: 47900  \tValid loss: 0.2522229254245758\n",
      "Step: 48000  \tTraining loss: 0.23864784836769104\n",
      "Step: 48000  \tTraining accuracy: 0.8875108361244202\n",
      "Step: 48000  \tValid loss: 0.2521999478340149\n",
      "Step: 48100  \tTraining loss: 0.23862679302692413\n",
      "Step: 48100  \tTraining accuracy: 0.8875163793563843\n",
      "Step: 48100  \tValid loss: 0.252174973487854\n",
      "Step: 48200  \tTraining loss: 0.23860636353492737\n",
      "Step: 48200  \tTraining accuracy: 0.8875219225883484\n",
      "Step: 48200  \tValid loss: 0.25215667486190796\n",
      "Step: 48300  \tTraining loss: 0.23858317732810974\n",
      "Step: 48300  \tTraining accuracy: 0.8875280618667603\n",
      "Step: 48300  \tValid loss: 0.2521388530731201\n",
      "Step: 48400  \tTraining loss: 0.23856337368488312\n",
      "Step: 48400  \tTraining accuracy: 0.8875343799591064\n",
      "Step: 48400  \tValid loss: 0.2521202862262726\n",
      "Step: 48500  \tTraining loss: 0.23854126036167145\n",
      "Step: 48500  \tTraining accuracy: 0.8875401020050049\n",
      "Step: 48500  \tValid loss: 0.25210511684417725\n",
      "Step: 48600  \tTraining loss: 0.23851962387561798\n",
      "Step: 48600  \tTraining accuracy: 0.8875457048416138\n",
      "Step: 48600  \tValid loss: 0.25207918882369995\n",
      "Step: 48700  \tTraining loss: 0.2384985089302063\n",
      "Step: 48700  \tTraining accuracy: 0.8875513672828674\n",
      "Step: 48700  \tValid loss: 0.2520610988140106\n",
      "Step: 48800  \tTraining loss: 0.2384769767522812\n",
      "Step: 48800  \tTraining accuracy: 0.8875569701194763\n",
      "Step: 48800  \tValid loss: 0.2520451843738556\n",
      "Step: 48900  \tTraining loss: 0.2384558469057083\n",
      "Step: 48900  \tTraining accuracy: 0.8875634074211121\n",
      "Step: 48900  \tValid loss: 0.252025842666626\n",
      "Step: 49000  \tTraining loss: 0.23843564093112946\n",
      "Step: 49000  \tTraining accuracy: 0.8875706195831299\n",
      "Step: 49000  \tValid loss: 0.2519989013671875\n",
      "Step: 49100  \tTraining loss: 0.23841418325901031\n",
      "Step: 49100  \tTraining accuracy: 0.8875776529312134\n",
      "Step: 49100  \tValid loss: 0.2519911229610443\n",
      "Step: 49200  \tTraining loss: 0.23839548230171204\n",
      "Step: 49200  \tTraining accuracy: 0.8875848650932312\n",
      "Step: 49200  \tValid loss: 0.25197508931159973\n",
      "Step: 49300  \tTraining loss: 0.23837333917617798\n",
      "Step: 49300  \tTraining accuracy: 0.8875914216041565\n",
      "Step: 49300  \tValid loss: 0.2519635856151581\n",
      "Step: 49400  \tTraining loss: 0.2383531928062439\n",
      "Step: 49400  \tTraining accuracy: 0.887597918510437\n",
      "Step: 49400  \tValid loss: 0.25194740295410156\n",
      "Step: 49500  \tTraining loss: 0.2383318692445755\n",
      "Step: 49500  \tTraining accuracy: 0.8876044154167175\n",
      "Step: 49500  \tValid loss: 0.25193122029304504\n",
      "Step: 49600  \tTraining loss: 0.23831120133399963\n",
      "Step: 49600  \tTraining accuracy: 0.8876108527183533\n",
      "Step: 49600  \tValid loss: 0.25191330909729004\n",
      "Step: 49700  \tTraining loss: 0.23829135298728943\n",
      "Step: 49700  \tTraining accuracy: 0.887617290019989\n",
      "Step: 49700  \tValid loss: 0.251899778842926\n",
      "Step: 49800  \tTraining loss: 0.23827093839645386\n",
      "Step: 49800  \tTraining accuracy: 0.8876237273216248\n",
      "Step: 49800  \tValid loss: 0.25187987089157104\n",
      "Step: 49900  \tTraining loss: 0.23825158178806305\n",
      "Step: 49900  \tTraining accuracy: 0.8876301050186157\n",
      "Step: 49900  \tValid loss: 0.2518704831600189\n",
      "Step: 50000  \tTraining loss: 0.23823073506355286\n",
      "Step: 50000  \tTraining accuracy: 0.8876364231109619\n",
      "Step: 50000  \tValid loss: 0.2518502175807953\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.88764274\n",
      "Precision: 0.9121891\n",
      "Recall: 0.93515897\n",
      "F1 score: 0.8986797\n",
      "AUC: 0.8786492\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.887643   0.912189  0.935159   0.89868  0.878649  0.238231      0.887633   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.251846        0.88763   0.314608      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  49999.0  \n",
      "14\n",
      "(8990, 8)\n",
      "(8990, 1)\n",
      "(4960, 8)\n",
      "(4960, 1)\n",
      "(4030, 8)\n",
      "(4030, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5901938676834106\n",
      "Step: 100  \tTraining accuracy: 0.680088996887207\n",
      "Step: 100  \tValid loss: 0.5881088376045227\n",
      "Step: 200  \tTraining loss: 0.5523455142974854\n",
      "Step: 200  \tTraining accuracy: 0.6946607232093811\n",
      "Step: 200  \tValid loss: 0.5502246618270874\n",
      "Step: 300  \tTraining loss: 0.5032894611358643\n",
      "Step: 300  \tTraining accuracy: 0.7108564972877502\n",
      "Step: 300  \tValid loss: 0.4991684854030609\n",
      "Step: 400  \tTraining loss: 0.4495752453804016\n",
      "Step: 400  \tTraining accuracy: 0.7288892269134521\n",
      "Step: 400  \tValid loss: 0.4450036883354187\n",
      "Step: 500  \tTraining loss: 0.42663729190826416\n",
      "Step: 500  \tTraining accuracy: 0.7449511885643005\n",
      "Step: 500  \tValid loss: 0.4213870167732239\n",
      "Step: 600  \tTraining loss: 0.4166095554828644\n",
      "Step: 600  \tTraining accuracy: 0.7572049498558044\n",
      "Step: 600  \tValid loss: 0.41053506731987\n",
      "Step: 700  \tTraining loss: 0.4105238616466522\n",
      "Step: 700  \tTraining accuracy: 0.7667322754859924\n",
      "Step: 700  \tValid loss: 0.40399661660194397\n",
      "Step: 800  \tTraining loss: 0.4067371189594269\n",
      "Step: 800  \tTraining accuracy: 0.7742232084274292\n",
      "Step: 800  \tValid loss: 0.39981138706207275\n",
      "Step: 900  \tTraining loss: 0.4041145145893097\n",
      "Step: 900  \tTraining accuracy: 0.7803310751914978\n",
      "Step: 900  \tValid loss: 0.39695942401885986\n",
      "Step: 1000  \tTraining loss: 0.4020691514015198\n",
      "Step: 1000  \tTraining accuracy: 0.7854750752449036\n",
      "Step: 1000  \tValid loss: 0.3947362005710602\n",
      "Step: 1100  \tTraining loss: 0.4003356993198395\n",
      "Step: 1100  \tTraining accuracy: 0.7897769808769226\n",
      "Step: 1100  \tValid loss: 0.3928462266921997\n",
      "Step: 1200  \tTraining loss: 0.39890363812446594\n",
      "Step: 1200  \tTraining accuracy: 0.7932968735694885\n",
      "Step: 1200  \tValid loss: 0.39124929904937744\n",
      "Step: 1300  \tTraining loss: 0.3976099193096161\n",
      "Step: 1300  \tTraining accuracy: 0.7962847352027893\n",
      "Step: 1300  \tValid loss: 0.3898182511329651\n",
      "Step: 1400  \tTraining loss: 0.39632660150527954\n",
      "Step: 1400  \tTraining accuracy: 0.7988464832305908\n",
      "Step: 1400  \tValid loss: 0.3882726728916168\n",
      "Step: 1500  \tTraining loss: 0.3951723575592041\n",
      "Step: 1500  \tTraining accuracy: 0.8011008501052856\n",
      "Step: 1500  \tValid loss: 0.3870091736316681\n",
      "Step: 1600  \tTraining loss: 0.3941693902015686\n",
      "Step: 1600  \tTraining accuracy: 0.8030391931533813\n",
      "Step: 1600  \tValid loss: 0.38593292236328125\n",
      "Step: 1700  \tTraining loss: 0.393211305141449\n",
      "Step: 1700  \tTraining accuracy: 0.8047628402709961\n",
      "Step: 1700  \tValid loss: 0.38480648398399353\n",
      "Step: 1800  \tTraining loss: 0.39232054352760315\n",
      "Step: 1800  \tTraining accuracy: 0.806299090385437\n",
      "Step: 1800  \tValid loss: 0.38403356075286865\n",
      "Step: 1900  \tTraining loss: 0.39149370789527893\n",
      "Step: 1900  \tTraining accuracy: 0.8076481223106384\n",
      "Step: 1900  \tValid loss: 0.38318267464637756\n",
      "Step: 2000  \tTraining loss: 0.3906814455986023\n",
      "Step: 2000  \tTraining accuracy: 0.8088588714599609\n",
      "Step: 2000  \tValid loss: 0.382305771112442\n",
      "Step: 2100  \tTraining loss: 0.38990989327430725\n",
      "Step: 2100  \tTraining accuracy: 0.809924304485321\n",
      "Step: 2100  \tValid loss: 0.38165438175201416\n",
      "Step: 2200  \tTraining loss: 0.3891630470752716\n",
      "Step: 2200  \tTraining accuracy: 0.8108699321746826\n",
      "Step: 2200  \tValid loss: 0.3809799253940582\n",
      "Step: 2300  \tTraining loss: 0.3884483277797699\n",
      "Step: 2300  \tTraining accuracy: 0.8117538094520569\n",
      "Step: 2300  \tValid loss: 0.3802127540111542\n",
      "Step: 2400  \tTraining loss: 0.38782554864883423\n",
      "Step: 2400  \tTraining accuracy: 0.8125789761543274\n",
      "Step: 2400  \tValid loss: 0.3796514570713043\n",
      "Step: 2500  \tTraining loss: 0.38725408911705017\n",
      "Step: 2500  \tTraining accuracy: 0.8133459091186523\n",
      "Step: 2500  \tValid loss: 0.37911275029182434\n",
      "Step: 2600  \tTraining loss: 0.3866960406303406\n",
      "Step: 2600  \tTraining accuracy: 0.8140308260917664\n",
      "Step: 2600  \tValid loss: 0.37847900390625\n",
      "Step: 2700  \tTraining loss: 0.3861715793609619\n",
      "Step: 2700  \tTraining accuracy: 0.8147018551826477\n",
      "Step: 2700  \tValid loss: 0.377949059009552\n",
      "Step: 2800  \tTraining loss: 0.3856618106365204\n",
      "Step: 2800  \tTraining accuracy: 0.8153079152107239\n",
      "Step: 2800  \tValid loss: 0.3772564232349396\n",
      "Step: 2900  \tTraining loss: 0.38515618443489075\n",
      "Step: 2900  \tTraining accuracy: 0.8158811926841736\n",
      "Step: 2900  \tValid loss: 0.37660592794418335\n",
      "Step: 3000  \tTraining loss: 0.3847138583660126\n",
      "Step: 3000  \tTraining accuracy: 0.8164287805557251\n",
      "Step: 3000  \tValid loss: 0.37610551714897156\n",
      "Step: 3100  \tTraining loss: 0.38431671261787415\n",
      "Step: 3100  \tTraining accuracy: 0.8169550895690918\n",
      "Step: 3100  \tValid loss: 0.3756929039955139\n",
      "Step: 3200  \tTraining loss: 0.38394516706466675\n",
      "Step: 3200  \tTraining accuracy: 0.817454993724823\n",
      "Step: 3200  \tValid loss: 0.3752850890159607\n",
      "Step: 3300  \tTraining loss: 0.38358429074287415\n",
      "Step: 3300  \tTraining accuracy: 0.8179242014884949\n",
      "Step: 3300  \tValid loss: 0.3749082386493683\n",
      "Step: 3400  \tTraining loss: 0.38324153423309326\n",
      "Step: 3400  \tTraining accuracy: 0.8183620572090149\n",
      "Step: 3400  \tValid loss: 0.3745344579219818\n",
      "Step: 3500  \tTraining loss: 0.3829180896282196\n",
      "Step: 3500  \tTraining accuracy: 0.8187922239303589\n",
      "Step: 3500  \tValid loss: 0.3742155432701111\n",
      "Step: 3600  \tTraining loss: 0.3826008439064026\n",
      "Step: 3600  \tTraining accuracy: 0.8192185163497925\n",
      "Step: 3600  \tValid loss: 0.37391600012779236\n",
      "Step: 3700  \tTraining loss: 0.382295161485672\n",
      "Step: 3700  \tTraining accuracy: 0.8196382522583008\n",
      "Step: 3700  \tValid loss: 0.37366169691085815\n",
      "Step: 3800  \tTraining loss: 0.38199204206466675\n",
      "Step: 3800  \tTraining accuracy: 0.8200400471687317\n",
      "Step: 3800  \tValid loss: 0.37342503666877747\n",
      "Step: 3900  \tTraining loss: 0.38168588280677795\n",
      "Step: 3900  \tTraining accuracy: 0.8204166293144226\n",
      "Step: 3900  \tValid loss: 0.37319833040237427\n",
      "Step: 4000  \tTraining loss: 0.38135677576065063\n",
      "Step: 4000  \tTraining accuracy: 0.8207671046257019\n",
      "Step: 4000  \tValid loss: 0.37291842699050903\n",
      "Step: 4100  \tTraining loss: 0.3810168206691742\n",
      "Step: 4100  \tTraining accuracy: 0.8211002349853516\n",
      "Step: 4100  \tValid loss: 0.37268510460853577\n",
      "Step: 4200  \tTraining loss: 0.3806909918785095\n",
      "Step: 4200  \tTraining accuracy: 0.8214187026023865\n",
      "Step: 4200  \tValid loss: 0.37244194746017456\n",
      "Step: 4300  \tTraining loss: 0.3803657591342926\n",
      "Step: 4300  \tTraining accuracy: 0.8217418193817139\n",
      "Step: 4300  \tValid loss: 0.3722213804721832\n",
      "Step: 4400  \tTraining loss: 0.3800382912158966\n",
      "Step: 4400  \tTraining accuracy: 0.8220615386962891\n",
      "Step: 4400  \tValid loss: 0.37200498580932617\n",
      "Step: 4500  \tTraining loss: 0.37971317768096924\n",
      "Step: 4500  \tTraining accuracy: 0.8223719000816345\n",
      "Step: 4500  \tValid loss: 0.3717968463897705\n",
      "Step: 4600  \tTraining loss: 0.379354864358902\n",
      "Step: 4600  \tTraining accuracy: 0.8226796388626099\n",
      "Step: 4600  \tValid loss: 0.3715307414531708\n",
      "Step: 4700  \tTraining loss: 0.3789912462234497\n",
      "Step: 4700  \tTraining accuracy: 0.822995662689209\n",
      "Step: 4700  \tValid loss: 0.3712712228298187\n",
      "Step: 4800  \tTraining loss: 0.37864038348197937\n",
      "Step: 4800  \tTraining accuracy: 0.8233054280281067\n",
      "Step: 4800  \tValid loss: 0.37101104855537415\n",
      "Step: 4900  \tTraining loss: 0.37827903032302856\n",
      "Step: 4900  \tTraining accuracy: 0.8235966563224792\n",
      "Step: 4900  \tValid loss: 0.3707265853881836\n",
      "Step: 5000  \tTraining loss: 0.3779323101043701\n",
      "Step: 5000  \tTraining accuracy: 0.8238738775253296\n",
      "Step: 5000  \tValid loss: 0.3704639673233032\n",
      "Step: 5100  \tTraining loss: 0.37759190797805786\n",
      "Step: 5100  \tTraining accuracy: 0.824141263961792\n",
      "Step: 5100  \tValid loss: 0.37021204829216003\n",
      "Step: 5200  \tTraining loss: 0.3772668242454529\n",
      "Step: 5200  \tTraining accuracy: 0.824409008026123\n",
      "Step: 5200  \tValid loss: 0.3699391484260559\n",
      "Step: 5300  \tTraining loss: 0.3769487142562866\n",
      "Step: 5300  \tTraining accuracy: 0.8246697187423706\n",
      "Step: 5300  \tValid loss: 0.3696633577346802\n",
      "Step: 5400  \tTraining loss: 0.37663811445236206\n",
      "Step: 5400  \tTraining accuracy: 0.8249166011810303\n",
      "Step: 5400  \tValid loss: 0.36939457058906555\n",
      "Step: 5500  \tTraining loss: 0.37632232904434204\n",
      "Step: 5500  \tTraining accuracy: 0.825158417224884\n",
      "Step: 5500  \tValid loss: 0.369128555059433\n",
      "Step: 5600  \tTraining loss: 0.3759957551956177\n",
      "Step: 5600  \tTraining accuracy: 0.8254005908966064\n",
      "Step: 5600  \tValid loss: 0.36886897683143616\n",
      "Step: 5700  \tTraining loss: 0.3756723403930664\n",
      "Step: 5700  \tTraining accuracy: 0.8256469964981079\n",
      "Step: 5700  \tValid loss: 0.36862272024154663\n",
      "Step: 5800  \tTraining loss: 0.3753376007080078\n",
      "Step: 5800  \tTraining accuracy: 0.8258906006813049\n",
      "Step: 5800  \tValid loss: 0.3684016168117523\n",
      "Step: 5900  \tTraining loss: 0.37499895691871643\n",
      "Step: 5900  \tTraining accuracy: 0.8261277675628662\n",
      "Step: 5900  \tValid loss: 0.36817213892936707\n",
      "Step: 6000  \tTraining loss: 0.3746590316295624\n",
      "Step: 6000  \tTraining accuracy: 0.8263504505157471\n",
      "Step: 6000  \tValid loss: 0.3679414987564087\n",
      "Step: 6100  \tTraining loss: 0.37431663274765015\n",
      "Step: 6100  \tTraining accuracy: 0.8265694379806519\n",
      "Step: 6100  \tValid loss: 0.36769986152648926\n",
      "Step: 6200  \tTraining loss: 0.3739573359489441\n",
      "Step: 6200  \tTraining accuracy: 0.8267776966094971\n",
      "Step: 6200  \tValid loss: 0.36745601892471313\n",
      "Step: 6300  \tTraining loss: 0.37360110878944397\n",
      "Step: 6300  \tTraining accuracy: 0.826977550983429\n",
      "Step: 6300  \tValid loss: 0.3672075867652893\n",
      "Step: 6400  \tTraining loss: 0.37325090169906616\n",
      "Step: 6400  \tTraining accuracy: 0.8271728157997131\n",
      "Step: 6400  \tValid loss: 0.3669682741165161\n",
      "Step: 6500  \tTraining loss: 0.37289685010910034\n",
      "Step: 6500  \tTraining accuracy: 0.8273628950119019\n",
      "Step: 6500  \tValid loss: 0.36671364307403564\n",
      "Step: 6600  \tTraining loss: 0.37254175543785095\n",
      "Step: 6600  \tTraining accuracy: 0.8275420665740967\n",
      "Step: 6600  \tValid loss: 0.366459459066391\n",
      "Step: 6700  \tTraining loss: 0.3721872568130493\n",
      "Step: 6700  \tTraining accuracy: 0.8277183771133423\n",
      "Step: 6700  \tValid loss: 0.36620235443115234\n",
      "Step: 6800  \tTraining loss: 0.3718315362930298\n",
      "Step: 6800  \tTraining accuracy: 0.8278902769088745\n",
      "Step: 6800  \tValid loss: 0.36594218015670776\n",
      "Step: 6900  \tTraining loss: 0.37147602438926697\n",
      "Step: 6900  \tTraining accuracy: 0.8280587792396545\n",
      "Step: 6900  \tValid loss: 0.36567768454551697\n",
      "Step: 7000  \tTraining loss: 0.37112298607826233\n",
      "Step: 7000  \tTraining accuracy: 0.828220009803772\n",
      "Step: 7000  \tValid loss: 0.3654302656650543\n",
      "Step: 7100  \tTraining loss: 0.3707740604877472\n",
      "Step: 7100  \tTraining accuracy: 0.8283727169036865\n",
      "Step: 7100  \tValid loss: 0.36513790488243103\n",
      "Step: 7200  \tTraining loss: 0.3704291582107544\n",
      "Step: 7200  \tTraining accuracy: 0.8285118937492371\n",
      "Step: 7200  \tValid loss: 0.36488449573516846\n",
      "Step: 7300  \tTraining loss: 0.3700912296772003\n",
      "Step: 7300  \tTraining accuracy: 0.8286409974098206\n",
      "Step: 7300  \tValid loss: 0.3646087944507599\n",
      "Step: 7400  \tTraining loss: 0.3697599470615387\n",
      "Step: 7400  \tTraining accuracy: 0.8287674188613892\n",
      "Step: 7400  \tValid loss: 0.36433786153793335\n",
      "Step: 7500  \tTraining loss: 0.3694358468055725\n",
      "Step: 7500  \tTraining accuracy: 0.8288896679878235\n",
      "Step: 7500  \tValid loss: 0.36406058073043823\n",
      "Step: 7600  \tTraining loss: 0.36911943554878235\n",
      "Step: 7600  \tTraining accuracy: 0.8289968967437744\n",
      "Step: 7600  \tValid loss: 0.36378100514411926\n",
      "Step: 7700  \tTraining loss: 0.3688122034072876\n",
      "Step: 7700  \tTraining accuracy: 0.8290984034538269\n",
      "Step: 7700  \tValid loss: 0.36352795362472534\n",
      "Step: 7800  \tTraining loss: 0.36851435899734497\n",
      "Step: 7800  \tTraining accuracy: 0.8291929960250854\n",
      "Step: 7800  \tValid loss: 0.36326485872268677\n",
      "Step: 7900  \tTraining loss: 0.3682275414466858\n",
      "Step: 7900  \tTraining accuracy: 0.8292866349220276\n",
      "Step: 7900  \tValid loss: 0.3630066514015198\n",
      "Step: 8000  \tTraining loss: 0.3679514527320862\n",
      "Step: 8000  \tTraining accuracy: 0.8293792605400085\n",
      "Step: 8000  \tValid loss: 0.3627551198005676\n",
      "Step: 8100  \tTraining loss: 0.3676873445510864\n",
      "Step: 8100  \tTraining accuracy: 0.8294689059257507\n",
      "Step: 8100  \tValid loss: 0.3625071942806244\n",
      "Step: 8200  \tTraining loss: 0.3674337565898895\n",
      "Step: 8200  \tTraining accuracy: 0.8295556902885437\n",
      "Step: 8200  \tValid loss: 0.3622499406337738\n",
      "Step: 8300  \tTraining loss: 0.36718806624412537\n",
      "Step: 8300  \tTraining accuracy: 0.829643726348877\n",
      "Step: 8300  \tValid loss: 0.36200833320617676\n",
      "Step: 8400  \tTraining loss: 0.3669428825378418\n",
      "Step: 8400  \tTraining accuracy: 0.8297316431999207\n",
      "Step: 8400  \tValid loss: 0.3617768883705139\n",
      "Step: 8500  \tTraining loss: 0.36671218276023865\n",
      "Step: 8500  \tTraining accuracy: 0.8298174738883972\n",
      "Step: 8500  \tValid loss: 0.36153116822242737\n",
      "Step: 8600  \tTraining loss: 0.36649131774902344\n",
      "Step: 8600  \tTraining accuracy: 0.8299000263214111\n",
      "Step: 8600  \tValid loss: 0.3612980544567108\n",
      "Step: 8700  \tTraining loss: 0.3662780225276947\n",
      "Step: 8700  \tTraining accuracy: 0.8299793601036072\n",
      "Step: 8700  \tValid loss: 0.36105895042419434\n",
      "Step: 8800  \tTraining loss: 0.36607199907302856\n",
      "Step: 8800  \tTraining accuracy: 0.8300575017929077\n",
      "Step: 8800  \tValid loss: 0.36083948612213135\n",
      "Step: 8900  \tTraining loss: 0.3657868504524231\n",
      "Step: 8900  \tTraining accuracy: 0.8301314115524292\n",
      "Step: 8900  \tValid loss: 0.36056965589523315\n",
      "Step: 9000  \tTraining loss: 0.3655441701412201\n",
      "Step: 9000  \tTraining accuracy: 0.8302018046379089\n",
      "Step: 9000  \tValid loss: 0.3602980375289917\n",
      "Step: 9100  \tTraining loss: 0.3653199076652527\n",
      "Step: 9100  \tTraining accuracy: 0.8302724361419678\n",
      "Step: 9100  \tValid loss: 0.3600746989250183\n",
      "Step: 9200  \tTraining loss: 0.3650999069213867\n",
      "Step: 9200  \tTraining accuracy: 0.8303409218788147\n",
      "Step: 9200  \tValid loss: 0.35984349250793457\n",
      "Step: 9300  \tTraining loss: 0.3648902475833893\n",
      "Step: 9300  \tTraining accuracy: 0.830410361289978\n",
      "Step: 9300  \tValid loss: 0.3596310615539551\n",
      "Step: 9400  \tTraining loss: 0.3646893799304962\n",
      "Step: 9400  \tTraining accuracy: 0.8304800987243652\n",
      "Step: 9400  \tValid loss: 0.35944241285324097\n",
      "Step: 9500  \tTraining loss: 0.3644978404045105\n",
      "Step: 9500  \tTraining accuracy: 0.8305495381355286\n",
      "Step: 9500  \tValid loss: 0.35925090312957764\n",
      "Step: 9600  \tTraining loss: 0.36431363224983215\n",
      "Step: 9600  \tTraining accuracy: 0.8306145668029785\n",
      "Step: 9600  \tValid loss: 0.3590734601020813\n",
      "Step: 9700  \tTraining loss: 0.3641306757926941\n",
      "Step: 9700  \tTraining accuracy: 0.8306754231452942\n",
      "Step: 9700  \tValid loss: 0.35888567566871643\n",
      "Step: 9800  \tTraining loss: 0.3639523684978485\n",
      "Step: 9800  \tTraining accuracy: 0.8307315707206726\n",
      "Step: 9800  \tValid loss: 0.35871759057044983\n",
      "Step: 9900  \tTraining loss: 0.3637797236442566\n",
      "Step: 9900  \tTraining accuracy: 0.8307899832725525\n",
      "Step: 9900  \tValid loss: 0.3585747480392456\n",
      "Step: 10000  \tTraining loss: 0.3636121153831482\n",
      "Step: 10000  \tTraining accuracy: 0.8308466672897339\n",
      "Step: 10000  \tValid loss: 0.3584163188934326\n",
      "Step: 10100  \tTraining loss: 0.36344805359840393\n",
      "Step: 10100  \tTraining accuracy: 0.8308994770050049\n",
      "Step: 10100  \tValid loss: 0.35827675461769104\n",
      "Step: 10200  \tTraining loss: 0.3632887601852417\n",
      "Step: 10200  \tTraining accuracy: 0.8309484720230103\n",
      "Step: 10200  \tValid loss: 0.3581392467021942\n",
      "Step: 10300  \tTraining loss: 0.36313173174858093\n",
      "Step: 10300  \tTraining accuracy: 0.8309975862503052\n",
      "Step: 10300  \tValid loss: 0.3580080270767212\n",
      "Step: 10400  \tTraining loss: 0.3629782199859619\n",
      "Step: 10400  \tTraining accuracy: 0.8310457468032837\n",
      "Step: 10400  \tValid loss: 0.35787585377693176\n",
      "Step: 10500  \tTraining loss: 0.3628278374671936\n",
      "Step: 10500  \tTraining accuracy: 0.8310919404029846\n",
      "Step: 10500  \tValid loss: 0.35774022340774536\n",
      "Step: 10600  \tTraining loss: 0.3626782298088074\n",
      "Step: 10600  \tTraining accuracy: 0.8311388492584229\n",
      "Step: 10600  \tValid loss: 0.3576105833053589\n",
      "Step: 10700  \tTraining loss: 0.36253198981285095\n",
      "Step: 10700  \tTraining accuracy: 0.8311880230903625\n",
      "Step: 10700  \tValid loss: 0.3574780821800232\n",
      "Step: 10800  \tTraining loss: 0.3623793125152588\n",
      "Step: 10800  \tTraining accuracy: 0.83123779296875\n",
      "Step: 10800  \tValid loss: 0.3573608994483948\n",
      "Step: 10900  \tTraining loss: 0.3622291386127472\n",
      "Step: 10900  \tTraining accuracy: 0.8312897682189941\n",
      "Step: 10900  \tValid loss: 0.35719814896583557\n",
      "Step: 11000  \tTraining loss: 0.36208242177963257\n",
      "Step: 11000  \tTraining accuracy: 0.8313432931900024\n",
      "Step: 11000  \tValid loss: 0.35704857110977173\n",
      "Step: 11100  \tTraining loss: 0.36193758249282837\n",
      "Step: 11100  \tTraining accuracy: 0.8313968777656555\n",
      "Step: 11100  \tValid loss: 0.3569168150424957\n",
      "Step: 11200  \tTraining loss: 0.3617972731590271\n",
      "Step: 11200  \tTraining accuracy: 0.8314470052719116\n",
      "Step: 11200  \tValid loss: 0.35677486658096313\n",
      "Step: 11300  \tTraining loss: 0.36166083812713623\n",
      "Step: 11300  \tTraining accuracy: 0.8314947485923767\n",
      "Step: 11300  \tValid loss: 0.3566216826438904\n",
      "Step: 11400  \tTraining loss: 0.36152204871177673\n",
      "Step: 11400  \tTraining accuracy: 0.8315441012382507\n",
      "Step: 11400  \tValid loss: 0.3565163016319275\n",
      "Step: 11500  \tTraining loss: 0.36138853430747986\n",
      "Step: 11500  \tTraining accuracy: 0.8315935730934143\n",
      "Step: 11500  \tValid loss: 0.3563457727432251\n",
      "Step: 11600  \tTraining loss: 0.3612547814846039\n",
      "Step: 11600  \tTraining accuracy: 0.831639289855957\n",
      "Step: 11600  \tValid loss: 0.3561975657939911\n",
      "Step: 11700  \tTraining loss: 0.36112287640571594\n",
      "Step: 11700  \tTraining accuracy: 0.8316847085952759\n",
      "Step: 11700  \tValid loss: 0.3560514748096466\n",
      "Step: 11800  \tTraining loss: 0.3609675467014313\n",
      "Step: 11800  \tTraining accuracy: 0.8317359685897827\n",
      "Step: 11800  \tValid loss: 0.35593315958976746\n",
      "Step: 11900  \tTraining loss: 0.36083120107650757\n",
      "Step: 11900  \tTraining accuracy: 0.8317919969558716\n",
      "Step: 11900  \tValid loss: 0.355785071849823\n",
      "Step: 12000  \tTraining loss: 0.3606988489627838\n",
      "Step: 12000  \tTraining accuracy: 0.8318549990653992\n",
      "Step: 12000  \tValid loss: 0.355623722076416\n",
      "Step: 12100  \tTraining loss: 0.3605673909187317\n",
      "Step: 12100  \tTraining accuracy: 0.8319183588027954\n",
      "Step: 12100  \tValid loss: 0.355482816696167\n",
      "Step: 12200  \tTraining loss: 0.360422819852829\n",
      "Step: 12200  \tTraining accuracy: 0.8319761157035828\n",
      "Step: 12200  \tValid loss: 0.3553263545036316\n",
      "Step: 12300  \tTraining loss: 0.36028826236724854\n",
      "Step: 12300  \tTraining accuracy: 0.8320314884185791\n",
      "Step: 12300  \tValid loss: 0.35516998171806335\n",
      "Step: 12400  \tTraining loss: 0.36015164852142334\n",
      "Step: 12400  \tTraining accuracy: 0.832085132598877\n",
      "Step: 12400  \tValid loss: 0.35503628849983215\n",
      "Step: 12500  \tTraining loss: 0.36002129316329956\n",
      "Step: 12500  \tTraining accuracy: 0.8321365714073181\n",
      "Step: 12500  \tValid loss: 0.3548624813556671\n",
      "Step: 12600  \tTraining loss: 0.3598928451538086\n",
      "Step: 12600  \tTraining accuracy: 0.8321849703788757\n",
      "Step: 12600  \tValid loss: 0.3546997010707855\n",
      "Step: 12700  \tTraining loss: 0.3597692847251892\n",
      "Step: 12700  \tTraining accuracy: 0.8322339057922363\n",
      "Step: 12700  \tValid loss: 0.3545476496219635\n",
      "Step: 12800  \tTraining loss: 0.35965070128440857\n",
      "Step: 12800  \tTraining accuracy: 0.8322798609733582\n",
      "Step: 12800  \tValid loss: 0.3543701171875\n",
      "Step: 12900  \tTraining loss: 0.3595326840877533\n",
      "Step: 12900  \tTraining accuracy: 0.8323264718055725\n",
      "Step: 12900  \tValid loss: 0.35423800349235535\n",
      "Step: 13000  \tTraining loss: 0.359418123960495\n",
      "Step: 13000  \tTraining accuracy: 0.8323701620101929\n",
      "Step: 13000  \tValid loss: 0.35407811403274536\n",
      "Step: 13100  \tTraining loss: 0.35930463671684265\n",
      "Step: 13100  \tTraining accuracy: 0.8324140310287476\n",
      "Step: 13100  \tValid loss: 0.3539328873157501\n",
      "Step: 13200  \tTraining loss: 0.359192430973053\n",
      "Step: 13200  \tTraining accuracy: 0.8324572443962097\n",
      "Step: 13200  \tValid loss: 0.35378220677375793\n",
      "Step: 13300  \tTraining loss: 0.3590831756591797\n",
      "Step: 13300  \tTraining accuracy: 0.8324977159500122\n",
      "Step: 13300  \tValid loss: 0.3536132872104645\n",
      "Step: 13400  \tTraining loss: 0.3589743673801422\n",
      "Step: 13400  \tTraining accuracy: 0.8325359225273132\n",
      "Step: 13400  \tValid loss: 0.35344937443733215\n",
      "Step: 13500  \tTraining loss: 0.35886624455451965\n",
      "Step: 13500  \tTraining accuracy: 0.832572340965271\n",
      "Step: 13500  \tValid loss: 0.35330039262771606\n",
      "Step: 13600  \tTraining loss: 0.3587574064731598\n",
      "Step: 13600  \tTraining accuracy: 0.8326089978218079\n",
      "Step: 13600  \tValid loss: 0.3531489372253418\n",
      "Step: 13700  \tTraining loss: 0.3586515188217163\n",
      "Step: 13700  \tTraining accuracy: 0.8326455354690552\n",
      "Step: 13700  \tValid loss: 0.352990061044693\n",
      "Step: 13800  \tTraining loss: 0.35854604840278625\n",
      "Step: 13800  \tTraining accuracy: 0.8326832056045532\n",
      "Step: 13800  \tValid loss: 0.3528345227241516\n",
      "Step: 13900  \tTraining loss: 0.35844138264656067\n",
      "Step: 13900  \tTraining accuracy: 0.832721471786499\n",
      "Step: 13900  \tValid loss: 0.3526891767978668\n",
      "Step: 14000  \tTraining loss: 0.3583386242389679\n",
      "Step: 14000  \tTraining accuracy: 0.8327608108520508\n",
      "Step: 14000  \tValid loss: 0.35252538323402405\n",
      "Step: 14100  \tTraining loss: 0.3582362234592438\n",
      "Step: 14100  \tTraining accuracy: 0.8328003883361816\n",
      "Step: 14100  \tValid loss: 0.35237500071525574\n",
      "Step: 14200  \tTraining loss: 0.3581348657608032\n",
      "Step: 14200  \tTraining accuracy: 0.8328409790992737\n",
      "Step: 14200  \tValid loss: 0.3522295355796814\n",
      "Step: 14300  \tTraining loss: 0.35803213715553284\n",
      "Step: 14300  \tTraining accuracy: 0.8328794240951538\n",
      "Step: 14300  \tValid loss: 0.35208016633987427\n",
      "Step: 14400  \tTraining loss: 0.3579273223876953\n",
      "Step: 14400  \tTraining accuracy: 0.832918107509613\n",
      "Step: 14400  \tValid loss: 0.35192257165908813\n",
      "Step: 14500  \tTraining loss: 0.35782405734062195\n",
      "Step: 14500  \tTraining accuracy: 0.8329566717147827\n",
      "Step: 14500  \tValid loss: 0.3517718017101288\n",
      "Step: 14600  \tTraining loss: 0.3577222228050232\n",
      "Step: 14600  \tTraining accuracy: 0.8329931497573853\n",
      "Step: 14600  \tValid loss: 0.3516136407852173\n",
      "Step: 14700  \tTraining loss: 0.3576194643974304\n",
      "Step: 14700  \tTraining accuracy: 0.8330298662185669\n",
      "Step: 14700  \tValid loss: 0.35148149728775024\n",
      "Step: 14800  \tTraining loss: 0.3575187623500824\n",
      "Step: 14800  \tTraining accuracy: 0.8330672383308411\n",
      "Step: 14800  \tValid loss: 0.35132384300231934\n",
      "Step: 14900  \tTraining loss: 0.35741907358169556\n",
      "Step: 14900  \tTraining accuracy: 0.8331082463264465\n",
      "Step: 14900  \tValid loss: 0.3511773645877838\n",
      "Step: 15000  \tTraining loss: 0.35731983184814453\n",
      "Step: 15000  \tTraining accuracy: 0.8331509232521057\n",
      "Step: 15000  \tValid loss: 0.35105401277542114\n",
      "Step: 15100  \tTraining loss: 0.35722193121910095\n",
      "Step: 15100  \tTraining accuracy: 0.8331937789916992\n",
      "Step: 15100  \tValid loss: 0.3509056866168976\n",
      "Step: 15200  \tTraining loss: 0.3571244478225708\n",
      "Step: 15200  \tTraining accuracy: 0.8332346081733704\n",
      "Step: 15200  \tValid loss: 0.3507607877254486\n",
      "Step: 15300  \tTraining loss: 0.35702717304229736\n",
      "Step: 15300  \tTraining accuracy: 0.8332741260528564\n",
      "Step: 15300  \tValid loss: 0.350626140832901\n",
      "Step: 15400  \tTraining loss: 0.3569319248199463\n",
      "Step: 15400  \tTraining accuracy: 0.8333120942115784\n",
      "Step: 15400  \tValid loss: 0.3504817485809326\n",
      "Step: 15500  \tTraining loss: 0.3568360209465027\n",
      "Step: 15500  \tTraining accuracy: 0.8333513140678406\n",
      "Step: 15500  \tValid loss: 0.35035622119903564\n",
      "Step: 15600  \tTraining loss: 0.35674360394477844\n",
      "Step: 15600  \tTraining accuracy: 0.8333882689476013\n",
      "Step: 15600  \tValid loss: 0.3502264618873596\n",
      "Step: 15700  \tTraining loss: 0.3566480875015259\n",
      "Step: 15700  \tTraining accuracy: 0.8334248065948486\n",
      "Step: 15700  \tValid loss: 0.35014501214027405\n",
      "Step: 15800  \tTraining loss: 0.35655704140663147\n",
      "Step: 15800  \tTraining accuracy: 0.8334583640098572\n",
      "Step: 15800  \tValid loss: 0.3500048816204071\n",
      "Step: 15900  \tTraining loss: 0.3564658761024475\n",
      "Step: 15900  \tTraining accuracy: 0.8334932327270508\n",
      "Step: 15900  \tValid loss: 0.34990301728248596\n",
      "Step: 16000  \tTraining loss: 0.35637643933296204\n",
      "Step: 16000  \tTraining accuracy: 0.833527684211731\n",
      "Step: 16000  \tValid loss: 0.3497893214225769\n",
      "Step: 16100  \tTraining loss: 0.3562867045402527\n",
      "Step: 16100  \tTraining accuracy: 0.8335634469985962\n",
      "Step: 16100  \tValid loss: 0.3496933877468109\n",
      "Step: 16200  \tTraining loss: 0.3561989367008209\n",
      "Step: 16200  \tTraining accuracy: 0.8336014747619629\n",
      "Step: 16200  \tValid loss: 0.34958091378211975\n",
      "Step: 16300  \tTraining loss: 0.3561096787452698\n",
      "Step: 16300  \tTraining accuracy: 0.8336373567581177\n",
      "Step: 16300  \tValid loss: 0.34949830174446106\n",
      "Step: 16400  \tTraining loss: 0.3560214936733246\n",
      "Step: 16400  \tTraining accuracy: 0.8336758613586426\n",
      "Step: 16400  \tValid loss: 0.34940624237060547\n",
      "Step: 16500  \tTraining loss: 0.35593417286872864\n",
      "Step: 16500  \tTraining accuracy: 0.833713948726654\n",
      "Step: 16500  \tValid loss: 0.34929755330085754\n",
      "Step: 16600  \tTraining loss: 0.35584524273872375\n",
      "Step: 16600  \tTraining accuracy: 0.8337525129318237\n",
      "Step: 16600  \tValid loss: 0.3492271304130554\n",
      "Step: 16700  \tTraining loss: 0.35575708746910095\n",
      "Step: 16700  \tTraining accuracy: 0.8337913155555725\n",
      "Step: 16700  \tValid loss: 0.3491387665271759\n",
      "Step: 16800  \tTraining loss: 0.3556680679321289\n",
      "Step: 16800  \tTraining accuracy: 0.8338276147842407\n",
      "Step: 16800  \tValid loss: 0.34906142950057983\n",
      "Step: 16900  \tTraining loss: 0.3555786609649658\n",
      "Step: 16900  \tTraining accuracy: 0.8338635563850403\n",
      "Step: 16900  \tValid loss: 0.3489883840084076\n",
      "Step: 17000  \tTraining loss: 0.35548943281173706\n",
      "Step: 17000  \tTraining accuracy: 0.8339003324508667\n",
      "Step: 17000  \tValid loss: 0.3488942086696625\n",
      "Step: 17100  \tTraining loss: 0.35539907217025757\n",
      "Step: 17100  \tTraining accuracy: 0.8339363932609558\n",
      "Step: 17100  \tValid loss: 0.3488229215145111\n",
      "Step: 17200  \tTraining loss: 0.3553107976913452\n",
      "Step: 17200  \tTraining accuracy: 0.8339710235595703\n",
      "Step: 17200  \tValid loss: 0.3487280011177063\n",
      "Step: 17300  \tTraining loss: 0.3552201986312866\n",
      "Step: 17300  \tTraining accuracy: 0.8340029716491699\n",
      "Step: 17300  \tValid loss: 0.3486692011356354\n",
      "Step: 17400  \tTraining loss: 0.3551309108734131\n",
      "Step: 17400  \tTraining accuracy: 0.8340330123901367\n",
      "Step: 17400  \tValid loss: 0.34863370656967163\n",
      "Step: 17500  \tTraining loss: 0.35504385828971863\n",
      "Step: 17500  \tTraining accuracy: 0.8340613842010498\n",
      "Step: 17500  \tValid loss: 0.34853309392929077\n",
      "Step: 17600  \tTraining loss: 0.3549577295780182\n",
      "Step: 17600  \tTraining accuracy: 0.834089457988739\n",
      "Step: 17600  \tValid loss: 0.34845075011253357\n",
      "Step: 17700  \tTraining loss: 0.35487231612205505\n",
      "Step: 17700  \tTraining accuracy: 0.8341162800788879\n",
      "Step: 17700  \tValid loss: 0.3483683168888092\n",
      "Step: 17800  \tTraining loss: 0.35478758811950684\n",
      "Step: 17800  \tTraining accuracy: 0.8341431021690369\n",
      "Step: 17800  \tValid loss: 0.34828412532806396\n",
      "Step: 17900  \tTraining loss: 0.3547031283378601\n",
      "Step: 17900  \tTraining accuracy: 0.8341705799102783\n",
      "Step: 17900  \tValid loss: 0.34820252656936646\n",
      "Step: 18000  \tTraining loss: 0.3546190559864044\n",
      "Step: 18000  \tTraining accuracy: 0.8341977000236511\n",
      "Step: 18000  \tValid loss: 0.348120778799057\n",
      "Step: 18100  \tTraining loss: 0.35453617572784424\n",
      "Step: 18100  \tTraining accuracy: 0.8342257738113403\n",
      "Step: 18100  \tValid loss: 0.34803223609924316\n",
      "Step: 18200  \tTraining loss: 0.35445258021354675\n",
      "Step: 18200  \tTraining accuracy: 0.8342544436454773\n",
      "Step: 18200  \tValid loss: 0.34795236587524414\n",
      "Step: 18300  \tTraining loss: 0.3543696701526642\n",
      "Step: 18300  \tTraining accuracy: 0.8342819213867188\n",
      "Step: 18300  \tValid loss: 0.34787726402282715\n",
      "Step: 18400  \tTraining loss: 0.35428720712661743\n",
      "Step: 18400  \tTraining accuracy: 0.834309995174408\n",
      "Step: 18400  \tValid loss: 0.3478015959262848\n",
      "Step: 18500  \tTraining loss: 0.3542039394378662\n",
      "Step: 18500  \tTraining accuracy: 0.8343377709388733\n",
      "Step: 18500  \tValid loss: 0.3477289080619812\n",
      "Step: 18600  \tTraining loss: 0.35412195324897766\n",
      "Step: 18600  \tTraining accuracy: 0.8343652486801147\n",
      "Step: 18600  \tValid loss: 0.3476583659648895\n",
      "Step: 18700  \tTraining loss: 0.35403814911842346\n",
      "Step: 18700  \tTraining accuracy: 0.834391176700592\n",
      "Step: 18700  \tValid loss: 0.34759408235549927\n",
      "Step: 18800  \tTraining loss: 0.35395705699920654\n",
      "Step: 18800  \tTraining accuracy: 0.8344172239303589\n",
      "Step: 18800  \tValid loss: 0.34749674797058105\n",
      "Step: 18900  \tTraining loss: 0.35387367010116577\n",
      "Step: 18900  \tTraining accuracy: 0.8344441056251526\n",
      "Step: 18900  \tValid loss: 0.3474295735359192\n",
      "Step: 19000  \tTraining loss: 0.3537900745868683\n",
      "Step: 19000  \tTraining accuracy: 0.8344689607620239\n",
      "Step: 19000  \tValid loss: 0.34734901785850525\n",
      "Step: 19100  \tTraining loss: 0.35370755195617676\n",
      "Step: 19100  \tTraining accuracy: 0.8344950079917908\n",
      "Step: 19100  \tValid loss: 0.34726986289024353\n",
      "Step: 19200  \tTraining loss: 0.3536243736743927\n",
      "Step: 19200  \tTraining accuracy: 0.8345202207565308\n",
      "Step: 19200  \tValid loss: 0.34720703959465027\n",
      "Step: 19300  \tTraining loss: 0.3535382151603699\n",
      "Step: 19300  \tTraining accuracy: 0.8345434069633484\n",
      "Step: 19300  \tValid loss: 0.347160279750824\n",
      "Step: 19400  \tTraining loss: 0.3534514605998993\n",
      "Step: 19400  \tTraining accuracy: 0.8345658183097839\n",
      "Step: 19400  \tValid loss: 0.3470810353755951\n",
      "Step: 19500  \tTraining loss: 0.35335758328437805\n",
      "Step: 19500  \tTraining accuracy: 0.8345888257026672\n",
      "Step: 19500  \tValid loss: 0.34701213240623474\n",
      "Step: 19600  \tTraining loss: 0.3532596230506897\n",
      "Step: 19600  \tTraining accuracy: 0.8346139192581177\n",
      "Step: 19600  \tValid loss: 0.3469667136669159\n",
      "Step: 19700  \tTraining loss: 0.35314926505088806\n",
      "Step: 19700  \tTraining accuracy: 0.8346356153488159\n",
      "Step: 19700  \tValid loss: 0.3469419777393341\n",
      "Step: 19800  \tTraining loss: 0.3530362546443939\n",
      "Step: 19800  \tTraining accuracy: 0.8346554040908813\n",
      "Step: 19800  \tValid loss: 0.3469141721725464\n",
      "Step: 19900  \tTraining loss: 0.3529300391674042\n",
      "Step: 19900  \tTraining accuracy: 0.8346752524375916\n",
      "Step: 19900  \tValid loss: 0.3468714952468872\n",
      "Step: 20000  \tTraining loss: 0.3528297543525696\n",
      "Step: 20000  \tTraining accuracy: 0.8346946239471436\n",
      "Step: 20000  \tValid loss: 0.3468059003353119\n",
      "Step: 20100  \tTraining loss: 0.35273441672325134\n",
      "Step: 20100  \tTraining accuracy: 0.8347168564796448\n",
      "Step: 20100  \tValid loss: 0.346754252910614\n",
      "Step: 20200  \tTraining loss: 0.35264286398887634\n",
      "Step: 20200  \tTraining accuracy: 0.8347394466400146\n",
      "Step: 20200  \tValid loss: 0.34668174386024475\n",
      "Step: 20300  \tTraining loss: 0.35255295038223267\n",
      "Step: 20300  \tTraining accuracy: 0.834764301776886\n",
      "Step: 20300  \tValid loss: 0.346623033285141\n",
      "Step: 20400  \tTraining loss: 0.3524664640426636\n",
      "Step: 20400  \tTraining accuracy: 0.8347877860069275\n",
      "Step: 20400  \tValid loss: 0.3465510606765747\n",
      "Step: 20500  \tTraining loss: 0.35238105058670044\n",
      "Step: 20500  \tTraining accuracy: 0.8348091244697571\n",
      "Step: 20500  \tValid loss: 0.34649163484573364\n",
      "Step: 20600  \tTraining loss: 0.35229751467704773\n",
      "Step: 20600  \tTraining accuracy: 0.8348286151885986\n",
      "Step: 20600  \tValid loss: 0.3464109003543854\n",
      "Step: 20700  \tTraining loss: 0.352213978767395\n",
      "Step: 20700  \tTraining accuracy: 0.8348498344421387\n",
      "Step: 20700  \tValid loss: 0.34636586904525757\n",
      "Step: 20800  \tTraining loss: 0.35213255882263184\n",
      "Step: 20800  \tTraining accuracy: 0.8348695039749146\n",
      "Step: 20800  \tValid loss: 0.3463001847267151\n",
      "Step: 20900  \tTraining loss: 0.35205215215682983\n",
      "Step: 20900  \tTraining accuracy: 0.834889829158783\n",
      "Step: 20900  \tValid loss: 0.34624728560447693\n",
      "Step: 21000  \tTraining loss: 0.35196414589881897\n",
      "Step: 21000  \tTraining accuracy: 0.8349130749702454\n",
      "Step: 21000  \tValid loss: 0.3461301028728485\n",
      "Step: 21100  \tTraining loss: 0.3518705666065216\n",
      "Step: 21100  \tTraining accuracy: 0.8349356055259705\n",
      "Step: 21100  \tValid loss: 0.3460308015346527\n",
      "Step: 21200  \tTraining loss: 0.35177919268608093\n",
      "Step: 21200  \tTraining accuracy: 0.8349581956863403\n",
      "Step: 21200  \tValid loss: 0.3459266424179077\n",
      "Step: 21300  \tTraining loss: 0.35166406631469727\n",
      "Step: 21300  \tTraining accuracy: 0.8349787592887878\n",
      "Step: 21300  \tValid loss: 0.3458271026611328\n",
      "Step: 21400  \tTraining loss: 0.35152143239974976\n",
      "Step: 21400  \tTraining accuracy: 0.8349990844726562\n",
      "Step: 21400  \tValid loss: 0.345670223236084\n",
      "Step: 21500  \tTraining loss: 0.3513985276222229\n",
      "Step: 21500  \tTraining accuracy: 0.8350194692611694\n",
      "Step: 21500  \tValid loss: 0.3455027937889099\n",
      "Step: 21600  \tTraining loss: 0.3512793481349945\n",
      "Step: 21600  \tTraining accuracy: 0.8350417613983154\n",
      "Step: 21600  \tValid loss: 0.34531623125076294\n",
      "Step: 21700  \tTraining loss: 0.35117894411087036\n",
      "Step: 21700  \tTraining accuracy: 0.8350676894187927\n",
      "Step: 21700  \tValid loss: 0.34523528814315796\n",
      "Step: 21800  \tTraining loss: 0.3510800898075104\n",
      "Step: 21800  \tTraining accuracy: 0.8350915908813477\n",
      "Step: 21800  \tValid loss: 0.34518811106681824\n",
      "Step: 21900  \tTraining loss: 0.3509417176246643\n",
      "Step: 21900  \tTraining accuracy: 0.8351143002510071\n",
      "Step: 21900  \tValid loss: 0.34509626030921936\n",
      "Step: 22000  \tTraining loss: 0.35082873702049255\n",
      "Step: 22000  \tTraining accuracy: 0.8351370096206665\n",
      "Step: 22000  \tValid loss: 0.3450165390968323\n",
      "Step: 22100  \tTraining loss: 0.35072779655456543\n",
      "Step: 22100  \tTraining accuracy: 0.8351594805717468\n",
      "Step: 22100  \tValid loss: 0.3449919521808624\n",
      "Step: 22200  \tTraining loss: 0.35063475370407104\n",
      "Step: 22200  \tTraining accuracy: 0.8351810574531555\n",
      "Step: 22200  \tValid loss: 0.34493693709373474\n",
      "Step: 22300  \tTraining loss: 0.3505462110042572\n",
      "Step: 22300  \tTraining accuracy: 0.8352019190788269\n",
      "Step: 22300  \tValid loss: 0.3449096381664276\n",
      "Step: 22400  \tTraining loss: 0.35046032071113586\n",
      "Step: 22400  \tTraining accuracy: 0.835222601890564\n",
      "Step: 22400  \tValid loss: 0.3448878526687622\n",
      "Step: 22500  \tTraining loss: 0.35037705302238464\n",
      "Step: 22500  \tTraining accuracy: 0.8352433443069458\n",
      "Step: 22500  \tValid loss: 0.3448646664619446\n",
      "Step: 22600  \tTraining loss: 0.3502886891365051\n",
      "Step: 22600  \tTraining accuracy: 0.83526211977005\n",
      "Step: 22600  \tValid loss: 0.344854474067688\n",
      "Step: 22700  \tTraining loss: 0.3502022624015808\n",
      "Step: 22700  \tTraining accuracy: 0.8352800607681274\n",
      "Step: 22700  \tValid loss: 0.34484362602233887\n",
      "Step: 22800  \tTraining loss: 0.35011962056159973\n",
      "Step: 22800  \tTraining accuracy: 0.8352973461151123\n",
      "Step: 22800  \tValid loss: 0.3448372781276703\n",
      "Step: 22900  \tTraining loss: 0.35003817081451416\n",
      "Step: 22900  \tTraining accuracy: 0.8353159427642822\n",
      "Step: 22900  \tValid loss: 0.34478047490119934\n",
      "Step: 23000  \tTraining loss: 0.3499530255794525\n",
      "Step: 23000  \tTraining accuracy: 0.8353345990180969\n",
      "Step: 23000  \tValid loss: 0.3447502851486206\n",
      "Step: 23100  \tTraining loss: 0.34984228014945984\n",
      "Step: 23100  \tTraining accuracy: 0.8353525996208191\n",
      "Step: 23100  \tValid loss: 0.3447253108024597\n",
      "Step: 23200  \tTraining loss: 0.3497297763824463\n",
      "Step: 23200  \tTraining accuracy: 0.8353719115257263\n",
      "Step: 23200  \tValid loss: 0.34471848607063293\n",
      "Step: 23300  \tTraining loss: 0.3496318757534027\n",
      "Step: 23300  \tTraining accuracy: 0.8353939056396484\n",
      "Step: 23300  \tValid loss: 0.3447350263595581\n",
      "Step: 23400  \tTraining loss: 0.34954604506492615\n",
      "Step: 23400  \tTraining accuracy: 0.8354161977767944\n",
      "Step: 23400  \tValid loss: 0.3446940779685974\n",
      "Step: 23500  \tTraining loss: 0.3494599461555481\n",
      "Step: 23500  \tTraining accuracy: 0.8354390263557434\n",
      "Step: 23500  \tValid loss: 0.3446701467037201\n",
      "Step: 23600  \tTraining loss: 0.349378377199173\n",
      "Step: 23600  \tTraining accuracy: 0.8354626297950745\n",
      "Step: 23600  \tValid loss: 0.344635009765625\n",
      "Step: 23700  \tTraining loss: 0.3492971360683441\n",
      "Step: 23700  \tTraining accuracy: 0.8354857563972473\n",
      "Step: 23700  \tValid loss: 0.3445984423160553\n",
      "Step: 23800  \tTraining loss: 0.3492150604724884\n",
      "Step: 23800  \tTraining accuracy: 0.8355103135108948\n",
      "Step: 23800  \tValid loss: 0.3445669114589691\n",
      "Step: 23900  \tTraining loss: 0.34913769364356995\n",
      "Step: 23900  \tTraining accuracy: 0.8355342149734497\n",
      "Step: 23900  \tValid loss: 0.3445107340812683\n",
      "Step: 24000  \tTraining loss: 0.3490588963031769\n",
      "Step: 24000  \tTraining accuracy: 0.8355572819709778\n",
      "Step: 24000  \tValid loss: 0.3444922864437103\n",
      "Step: 24100  \tTraining loss: 0.34898266196250916\n",
      "Step: 24100  \tTraining accuracy: 0.8355796337127686\n",
      "Step: 24100  \tValid loss: 0.3444564938545227\n",
      "Step: 24200  \tTraining loss: 0.34890681505203247\n",
      "Step: 24200  \tTraining accuracy: 0.8356019854545593\n",
      "Step: 24200  \tValid loss: 0.3444347381591797\n",
      "Step: 24300  \tTraining loss: 0.34883251786231995\n",
      "Step: 24300  \tTraining accuracy: 0.8356269598007202\n",
      "Step: 24300  \tValid loss: 0.3443892300128937\n",
      "Step: 24400  \tTraining loss: 0.34876105189323425\n",
      "Step: 24400  \tTraining accuracy: 0.8356533646583557\n",
      "Step: 24400  \tValid loss: 0.34433624148368835\n",
      "Step: 24500  \tTraining loss: 0.34868669509887695\n",
      "Step: 24500  \tTraining accuracy: 0.8356810808181763\n",
      "Step: 24500  \tValid loss: 0.3443243205547333\n",
      "Step: 24600  \tTraining loss: 0.3486148416996002\n",
      "Step: 24600  \tTraining accuracy: 0.835709273815155\n",
      "Step: 24600  \tValid loss: 0.3442970812320709\n",
      "Step: 24700  \tTraining loss: 0.34854286909103394\n",
      "Step: 24700  \tTraining accuracy: 0.835737943649292\n",
      "Step: 24700  \tValid loss: 0.344291627407074\n",
      "Step: 24800  \tTraining loss: 0.34847256541252136\n",
      "Step: 24800  \tTraining accuracy: 0.8357676863670349\n",
      "Step: 24800  \tValid loss: 0.3442598283290863\n",
      "Step: 24900  \tTraining loss: 0.34840402007102966\n",
      "Step: 24900  \tTraining accuracy: 0.835797905921936\n",
      "Step: 24900  \tValid loss: 0.3442140519618988\n",
      "Step: 25000  \tTraining loss: 0.34833478927612305\n",
      "Step: 25000  \tTraining accuracy: 0.8358284831047058\n",
      "Step: 25000  \tValid loss: 0.3441896438598633\n",
      "Step: 25100  \tTraining loss: 0.3482666015625\n",
      "Step: 25100  \tTraining accuracy: 0.8358586430549622\n",
      "Step: 25100  \tValid loss: 0.34417808055877686\n",
      "Step: 25200  \tTraining loss: 0.3481999933719635\n",
      "Step: 25200  \tTraining accuracy: 0.8358888030052185\n",
      "Step: 25200  \tValid loss: 0.34414711594581604\n",
      "Step: 25300  \tTraining loss: 0.34813249111175537\n",
      "Step: 25300  \tTraining accuracy: 0.8359202146530151\n",
      "Step: 25300  \tValid loss: 0.3441234827041626\n",
      "Step: 25400  \tTraining loss: 0.3480656147003174\n",
      "Step: 25400  \tTraining accuracy: 0.8359513878822327\n",
      "Step: 25400  \tValid loss: 0.34409815073013306\n",
      "Step: 25500  \tTraining loss: 0.34799930453300476\n",
      "Step: 25500  \tTraining accuracy: 0.8359812498092651\n",
      "Step: 25500  \tValid loss: 0.34409090876579285\n",
      "Step: 25600  \tTraining loss: 0.34793519973754883\n",
      "Step: 25600  \tTraining accuracy: 0.8360117673873901\n",
      "Step: 25600  \tValid loss: 0.3440678119659424\n",
      "Step: 25700  \tTraining loss: 0.3478712737560272\n",
      "Step: 25700  \tTraining accuracy: 0.8360430598258972\n",
      "Step: 25700  \tValid loss: 0.34406372904777527\n",
      "Step: 25800  \tTraining loss: 0.34780940413475037\n",
      "Step: 25800  \tTraining accuracy: 0.8360745906829834\n",
      "Step: 25800  \tValid loss: 0.34403324127197266\n",
      "Step: 25900  \tTraining loss: 0.3477497100830078\n",
      "Step: 25900  \tTraining accuracy: 0.8361058831214905\n",
      "Step: 25900  \tValid loss: 0.3440011441707611\n",
      "Step: 26000  \tTraining loss: 0.3476894795894623\n",
      "Step: 26000  \tTraining accuracy: 0.8361375331878662\n",
      "Step: 26000  \tValid loss: 0.34398284554481506\n",
      "Step: 26100  \tTraining loss: 0.3476305305957794\n",
      "Step: 26100  \tTraining accuracy: 0.8361698389053345\n",
      "Step: 26100  \tValid loss: 0.34396156668663025\n",
      "Step: 26200  \tTraining loss: 0.3475714325904846\n",
      "Step: 26200  \tTraining accuracy: 0.836202085018158\n",
      "Step: 26200  \tValid loss: 0.3439481556415558\n",
      "Step: 26300  \tTraining loss: 0.34751713275909424\n",
      "Step: 26300  \tTraining accuracy: 0.8362343311309814\n",
      "Step: 26300  \tValid loss: 0.3438946306705475\n",
      "Step: 26400  \tTraining loss: 0.34746068716049194\n",
      "Step: 26400  \tTraining accuracy: 0.8362652659416199\n",
      "Step: 26400  \tValid loss: 0.3438824415206909\n",
      "Step: 26500  \tTraining loss: 0.34740564227104187\n",
      "Step: 26500  \tTraining accuracy: 0.8362961411476135\n",
      "Step: 26500  \tValid loss: 0.34387439489364624\n",
      "Step: 26600  \tTraining loss: 0.3473537862300873\n",
      "Step: 26600  \tTraining accuracy: 0.8363276720046997\n",
      "Step: 26600  \tValid loss: 0.34383371472358704\n",
      "Step: 26700  \tTraining loss: 0.3473007380962372\n",
      "Step: 26700  \tTraining accuracy: 0.8363593220710754\n",
      "Step: 26700  \tValid loss: 0.3438290059566498\n",
      "Step: 26800  \tTraining loss: 0.34724828600883484\n",
      "Step: 26800  \tTraining accuracy: 0.8363903760910034\n",
      "Step: 26800  \tValid loss: 0.3438117206096649\n",
      "Step: 26900  \tTraining loss: 0.3471980094909668\n",
      "Step: 26900  \tTraining accuracy: 0.8364211916923523\n",
      "Step: 26900  \tValid loss: 0.34379497170448303\n",
      "Step: 27000  \tTraining loss: 0.3471483588218689\n",
      "Step: 27000  \tTraining accuracy: 0.8364517688751221\n",
      "Step: 27000  \tValid loss: 0.34377098083496094\n",
      "Step: 27100  \tTraining loss: 0.34709882736206055\n",
      "Step: 27100  \tTraining accuracy: 0.8364816904067993\n",
      "Step: 27100  \tValid loss: 0.3437511920928955\n",
      "Step: 27200  \tTraining loss: 0.34705016016960144\n",
      "Step: 27200  \tTraining accuracy: 0.8365116119384766\n",
      "Step: 27200  \tValid loss: 0.3437348008155823\n",
      "Step: 27300  \tTraining loss: 0.3470015227794647\n",
      "Step: 27300  \tTraining accuracy: 0.8365408778190613\n",
      "Step: 27300  \tValid loss: 0.3437258005142212\n",
      "Step: 27400  \tTraining loss: 0.3469539284706116\n",
      "Step: 27400  \tTraining accuracy: 0.8365693688392639\n",
      "Step: 27400  \tValid loss: 0.3437170386314392\n",
      "Step: 27500  \tTraining loss: 0.34690791368484497\n",
      "Step: 27500  \tTraining accuracy: 0.8365976214408875\n",
      "Step: 27500  \tValid loss: 0.343697726726532\n",
      "Step: 27600  \tTraining loss: 0.3468612730503082\n",
      "Step: 27600  \tTraining accuracy: 0.8366256952285767\n",
      "Step: 27600  \tValid loss: 0.34368571639060974\n",
      "Step: 27700  \tTraining loss: 0.3468157649040222\n",
      "Step: 27700  \tTraining accuracy: 0.8366535305976868\n",
      "Step: 27700  \tValid loss: 0.34366917610168457\n",
      "Step: 27800  \tTraining loss: 0.3467704951763153\n",
      "Step: 27800  \tTraining accuracy: 0.8366811871528625\n",
      "Step: 27800  \tValid loss: 0.343658983707428\n",
      "Step: 27900  \tTraining loss: 0.3467264473438263\n",
      "Step: 27900  \tTraining accuracy: 0.836708664894104\n",
      "Step: 27900  \tValid loss: 0.34363889694213867\n",
      "Step: 28000  \tTraining loss: 0.34668195247650146\n",
      "Step: 28000  \tTraining accuracy: 0.8367359042167664\n",
      "Step: 28000  \tValid loss: 0.3436376750469208\n",
      "Step: 28100  \tTraining loss: 0.3466382324695587\n",
      "Step: 28100  \tTraining accuracy: 0.8367629647254944\n",
      "Step: 28100  \tValid loss: 0.34362611174583435\n",
      "Step: 28200  \tTraining loss: 0.3465959131717682\n",
      "Step: 28200  \tTraining accuracy: 0.8367904424667358\n",
      "Step: 28200  \tValid loss: 0.3436061143875122\n",
      "Step: 28300  \tTraining loss: 0.34655359387397766\n",
      "Step: 28300  \tTraining accuracy: 0.8368173241615295\n",
      "Step: 28300  \tValid loss: 0.34359458088874817\n",
      "Step: 28400  \tTraining loss: 0.3465111255645752\n",
      "Step: 28400  \tTraining accuracy: 0.8368428349494934\n",
      "Step: 28400  \tValid loss: 0.34358251094818115\n",
      "Step: 28500  \tTraining loss: 0.34646934270858765\n",
      "Step: 28500  \tTraining accuracy: 0.8368675708770752\n",
      "Step: 28500  \tValid loss: 0.34358271956443787\n",
      "Step: 28600  \tTraining loss: 0.3464282155036926\n",
      "Step: 28600  \tTraining accuracy: 0.8368925452232361\n",
      "Step: 28600  \tValid loss: 0.34356388449668884\n",
      "Step: 28700  \tTraining loss: 0.3463872969150543\n",
      "Step: 28700  \tTraining accuracy: 0.8369172811508179\n",
      "Step: 28700  \tValid loss: 0.3435591161251068\n",
      "Step: 28800  \tTraining loss: 0.34634798765182495\n",
      "Step: 28800  \tTraining accuracy: 0.8369415402412415\n",
      "Step: 28800  \tValid loss: 0.34353333711624146\n",
      "Step: 28900  \tTraining loss: 0.3463080823421478\n",
      "Step: 28900  \tTraining accuracy: 0.8369659781455994\n",
      "Step: 28900  \tValid loss: 0.34352895617485046\n",
      "Step: 29000  \tTraining loss: 0.34626859426498413\n",
      "Step: 29000  \tTraining accuracy: 0.836990237236023\n",
      "Step: 29000  \tValid loss: 0.34351733326911926\n",
      "Step: 29100  \tTraining loss: 0.3462289571762085\n",
      "Step: 29100  \tTraining accuracy: 0.837014377117157\n",
      "Step: 29100  \tValid loss: 0.3435104191303253\n",
      "Step: 29200  \tTraining loss: 0.34619036316871643\n",
      "Step: 29200  \tTraining accuracy: 0.8370382785797119\n",
      "Step: 29200  \tValid loss: 0.34351077675819397\n",
      "Step: 29300  \tTraining loss: 0.34615179896354675\n",
      "Step: 29300  \tTraining accuracy: 0.8370617032051086\n",
      "Step: 29300  \tValid loss: 0.343502014875412\n",
      "Step: 29400  \tTraining loss: 0.34611400961875916\n",
      "Step: 29400  \tTraining accuracy: 0.8370847702026367\n",
      "Step: 29400  \tValid loss: 0.34349575638771057\n",
      "Step: 29500  \tTraining loss: 0.346076637506485\n",
      "Step: 29500  \tTraining accuracy: 0.8371068835258484\n",
      "Step: 29500  \tValid loss: 0.3434823453426361\n",
      "Step: 29600  \tTraining loss: 0.34604009985923767\n",
      "Step: 29600  \tTraining accuracy: 0.8371288776397705\n",
      "Step: 29600  \tValid loss: 0.3434615731239319\n",
      "Step: 29700  \tTraining loss: 0.34600168466567993\n",
      "Step: 29700  \tTraining accuracy: 0.8371501564979553\n",
      "Step: 29700  \tValid loss: 0.34346696734428406\n",
      "Step: 29800  \tTraining loss: 0.34595996141433716\n",
      "Step: 29800  \tTraining accuracy: 0.8371712565422058\n",
      "Step: 29800  \tValid loss: 0.34342578053474426\n",
      "Step: 29900  \tTraining loss: 0.3459133207798004\n",
      "Step: 29900  \tTraining accuracy: 0.8371918797492981\n",
      "Step: 29900  \tValid loss: 0.3434927761554718\n",
      "Step: 30000  \tTraining loss: 0.3458712100982666\n",
      "Step: 30000  \tTraining accuracy: 0.8372120261192322\n",
      "Step: 30000  \tValid loss: 0.34353864192962646\n",
      "Step: 30100  \tTraining loss: 0.3458322286605835\n",
      "Step: 30100  \tTraining accuracy: 0.8372316360473633\n",
      "Step: 30100  \tValid loss: 0.3435364365577698\n",
      "Step: 30200  \tTraining loss: 0.34579503536224365\n",
      "Step: 30200  \tTraining accuracy: 0.8372510671615601\n",
      "Step: 30200  \tValid loss: 0.3435439169406891\n",
      "Step: 30300  \tTraining loss: 0.3457583487033844\n",
      "Step: 30300  \tTraining accuracy: 0.8372704386711121\n",
      "Step: 30300  \tValid loss: 0.3435397148132324\n",
      "Step: 30400  \tTraining loss: 0.3457220196723938\n",
      "Step: 30400  \tTraining accuracy: 0.8372896313667297\n",
      "Step: 30400  \tValid loss: 0.3435414135456085\n",
      "Step: 30500  \tTraining loss: 0.34568649530410767\n",
      "Step: 30500  \tTraining accuracy: 0.8373083472251892\n",
      "Step: 30500  \tValid loss: 0.3435295820236206\n",
      "Step: 30600  \tTraining loss: 0.3456510603427887\n",
      "Step: 30600  \tTraining accuracy: 0.8373270034790039\n",
      "Step: 30600  \tValid loss: 0.34352487325668335\n",
      "Step: 30700  \tTraining loss: 0.3456156551837921\n",
      "Step: 30700  \tTraining accuracy: 0.8373454809188843\n",
      "Step: 30700  \tValid loss: 0.34352099895477295\n",
      "Step: 30800  \tTraining loss: 0.3455808162689209\n",
      "Step: 30800  \tTraining accuracy: 0.8373638391494751\n",
      "Step: 30800  \tValid loss: 0.34351634979248047\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8373817\n",
      "Precision: 0.87125295\n",
      "Recall: 0.8425443\n",
      "F1 score: 0.82335436\n",
      "AUC: 0.850116\n",
      "   accuracy  precision    recall  f1_score       auc     loss  accuracy_val  \\\n",
      "0  0.837382   0.871253  0.842544  0.823354  0.850116  0.34558      0.837362   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.343426       0.837351   0.411147      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  30800.0  \n",
      "15\n",
      "(4350, 8)\n",
      "(4350, 1)\n",
      "(2400, 8)\n",
      "(2400, 1)\n",
      "(1950, 8)\n",
      "(1950, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5739508271217346\n",
      "Step: 100  \tTraining accuracy: 0.7085057497024536\n",
      "Step: 100  \tValid loss: 0.5640411376953125\n",
      "Step: 200  \tTraining loss: 0.5404407978057861\n",
      "Step: 200  \tTraining accuracy: 0.7116475105285645\n",
      "Step: 200  \tValid loss: 0.5392428636550903\n",
      "Step: 300  \tTraining loss: 0.5283986330032349\n",
      "Step: 300  \tTraining accuracy: 0.7149885296821594\n",
      "Step: 300  \tValid loss: 0.5317522287368774\n",
      "Step: 400  \tTraining loss: 0.5102857351303101\n",
      "Step: 400  \tTraining accuracy: 0.7220689654350281\n",
      "Step: 400  \tValid loss: 0.5150642991065979\n",
      "Step: 500  \tTraining loss: 0.4811779260635376\n",
      "Step: 500  \tTraining accuracy: 0.7300894260406494\n",
      "Step: 500  \tValid loss: 0.48197585344314575\n",
      "Step: 600  \tTraining loss: 0.4558405876159668\n",
      "Step: 600  \tTraining accuracy: 0.7374921441078186\n",
      "Step: 600  \tValid loss: 0.4522576332092285\n",
      "Step: 700  \tTraining loss: 0.4391366243362427\n",
      "Step: 700  \tTraining accuracy: 0.7447568774223328\n",
      "Step: 700  \tValid loss: 0.4312981069087982\n",
      "Step: 800  \tTraining loss: 0.4304303526878357\n",
      "Step: 800  \tTraining accuracy: 0.7508965730667114\n",
      "Step: 800  \tValid loss: 0.4193468987941742\n",
      "Step: 900  \tTraining loss: 0.42478427290916443\n",
      "Step: 900  \tTraining accuracy: 0.7559026479721069\n",
      "Step: 900  \tValid loss: 0.4119400680065155\n",
      "Step: 1000  \tTraining loss: 0.41980570554733276\n",
      "Step: 1000  \tTraining accuracy: 0.7600241899490356\n",
      "Step: 1000  \tValid loss: 0.40638282895088196\n",
      "Step: 1100  \tTraining loss: 0.4159799814224243\n",
      "Step: 1100  \tTraining accuracy: 0.7631417512893677\n",
      "Step: 1100  \tValid loss: 0.4024009108543396\n",
      "Step: 1200  \tTraining loss: 0.412833034992218\n",
      "Step: 1200  \tTraining accuracy: 0.7656771540641785\n",
      "Step: 1200  \tValid loss: 0.39940235018730164\n",
      "Step: 1300  \tTraining loss: 0.4097841680049896\n",
      "Step: 1300  \tTraining accuracy: 0.7680827379226685\n",
      "Step: 1300  \tValid loss: 0.3969340920448303\n",
      "Step: 1400  \tTraining loss: 0.4064350724220276\n",
      "Step: 1400  \tTraining accuracy: 0.7703533172607422\n",
      "Step: 1400  \tValid loss: 0.3938112258911133\n",
      "Step: 1500  \tTraining loss: 0.40383511781692505\n",
      "Step: 1500  \tTraining accuracy: 0.772437572479248\n",
      "Step: 1500  \tValid loss: 0.3910441994667053\n",
      "Step: 1600  \tTraining loss: 0.4020182192325592\n",
      "Step: 1600  \tTraining accuracy: 0.7743566632270813\n",
      "Step: 1600  \tValid loss: 0.38911694288253784\n",
      "Step: 1700  \tTraining loss: 0.39959877729415894\n",
      "Step: 1700  \tTraining accuracy: 0.7761616110801697\n",
      "Step: 1700  \tValid loss: 0.3882216811180115\n",
      "Step: 1800  \tTraining loss: 0.3983807861804962\n",
      "Step: 1800  \tTraining accuracy: 0.777642011642456\n",
      "Step: 1800  \tValid loss: 0.3867328464984894\n",
      "Step: 1900  \tTraining loss: 0.39746609330177307\n",
      "Step: 1900  \tTraining accuracy: 0.7789686322212219\n",
      "Step: 1900  \tValid loss: 0.38607004284858704\n",
      "Step: 2000  \tTraining loss: 0.3966940641403198\n",
      "Step: 2000  \tTraining accuracy: 0.780259370803833\n",
      "Step: 2000  \tValid loss: 0.3855153024196625\n",
      "Step: 2100  \tTraining loss: 0.39601948857307434\n",
      "Step: 2100  \tTraining accuracy: 0.7814017534255981\n",
      "Step: 2100  \tValid loss: 0.38508597016334534\n",
      "Step: 2200  \tTraining loss: 0.3954189121723175\n",
      "Step: 2200  \tTraining accuracy: 0.782432496547699\n",
      "Step: 2200  \tValid loss: 0.3846868872642517\n",
      "Step: 2300  \tTraining loss: 0.3948584496974945\n",
      "Step: 2300  \tTraining accuracy: 0.7834023237228394\n",
      "Step: 2300  \tValid loss: 0.3842799961566925\n",
      "Step: 2400  \tTraining loss: 0.394320547580719\n",
      "Step: 2400  \tTraining accuracy: 0.7842455506324768\n",
      "Step: 2400  \tValid loss: 0.3839176297187805\n",
      "Step: 2500  \tTraining loss: 0.3937784731388092\n",
      "Step: 2500  \tTraining accuracy: 0.7850340008735657\n",
      "Step: 2500  \tValid loss: 0.38367024064064026\n",
      "Step: 2600  \tTraining loss: 0.39326053857803345\n",
      "Step: 2600  \tTraining accuracy: 0.7857561707496643\n",
      "Step: 2600  \tValid loss: 0.3834506571292877\n",
      "Step: 2700  \tTraining loss: 0.39274436235427856\n",
      "Step: 2700  \tTraining accuracy: 0.7863543629646301\n",
      "Step: 2700  \tValid loss: 0.3833024203777313\n",
      "Step: 2800  \tTraining loss: 0.39226052165031433\n",
      "Step: 2800  \tTraining accuracy: 0.7869174480438232\n",
      "Step: 2800  \tValid loss: 0.3830837905406952\n",
      "Step: 2900  \tTraining loss: 0.39178943634033203\n",
      "Step: 2900  \tTraining accuracy: 0.7874531149864197\n",
      "Step: 2900  \tValid loss: 0.38284623622894287\n",
      "Step: 3000  \tTraining loss: 0.39131712913513184\n",
      "Step: 3000  \tTraining accuracy: 0.7879524827003479\n",
      "Step: 3000  \tValid loss: 0.3825082778930664\n",
      "Step: 3100  \tTraining loss: 0.39081141352653503\n",
      "Step: 3100  \tTraining accuracy: 0.7884831428527832\n",
      "Step: 3100  \tValid loss: 0.38209450244903564\n",
      "Step: 3200  \tTraining loss: 0.3902323544025421\n",
      "Step: 3200  \tTraining accuracy: 0.7889983654022217\n",
      "Step: 3200  \tValid loss: 0.3815925121307373\n",
      "Step: 3300  \tTraining loss: 0.38958945870399475\n",
      "Step: 3300  \tTraining accuracy: 0.7894606590270996\n",
      "Step: 3300  \tValid loss: 0.38102760910987854\n",
      "Step: 3400  \tTraining loss: 0.3889465034008026\n",
      "Step: 3400  \tTraining accuracy: 0.7898438572883606\n",
      "Step: 3400  \tValid loss: 0.38039737939834595\n",
      "Step: 3500  \tTraining loss: 0.3883112370967865\n",
      "Step: 3500  \tTraining accuracy: 0.7901949286460876\n",
      "Step: 3500  \tValid loss: 0.3797610402107239\n",
      "Step: 3600  \tTraining loss: 0.38767632842063904\n",
      "Step: 3600  \tTraining accuracy: 0.7905261516571045\n",
      "Step: 3600  \tValid loss: 0.3791782557964325\n",
      "Step: 3700  \tTraining loss: 0.38702821731567383\n",
      "Step: 3700  \tTraining accuracy: 0.7908549904823303\n",
      "Step: 3700  \tValid loss: 0.37866827845573425\n",
      "Step: 3800  \tTraining loss: 0.3864099681377411\n",
      "Step: 3800  \tTraining accuracy: 0.7911478877067566\n",
      "Step: 3800  \tValid loss: 0.37839943170547485\n",
      "Step: 3900  \tTraining loss: 0.38581642508506775\n",
      "Step: 3900  \tTraining accuracy: 0.7914375066757202\n",
      "Step: 3900  \tValid loss: 0.3781411647796631\n",
      "Step: 4000  \tTraining loss: 0.38523709774017334\n",
      "Step: 4000  \tTraining accuracy: 0.7917125225067139\n",
      "Step: 4000  \tValid loss: 0.3779016137123108\n",
      "Step: 4100  \tTraining loss: 0.3846692144870758\n",
      "Step: 4100  \tTraining accuracy: 0.7919653654098511\n",
      "Step: 4100  \tValid loss: 0.377640038728714\n",
      "Step: 4200  \tTraining loss: 0.38411152362823486\n",
      "Step: 4200  \tTraining accuracy: 0.7922254800796509\n",
      "Step: 4200  \tValid loss: 0.37739554047584534\n",
      "Step: 4300  \tTraining loss: 0.3835684359073639\n",
      "Step: 4300  \tTraining accuracy: 0.7924976348876953\n",
      "Step: 4300  \tValid loss: 0.37710821628570557\n",
      "Step: 4400  \tTraining loss: 0.3830357491970062\n",
      "Step: 4400  \tTraining accuracy: 0.792765200138092\n",
      "Step: 4400  \tValid loss: 0.37693172693252563\n",
      "Step: 4500  \tTraining loss: 0.3824983239173889\n",
      "Step: 4500  \tTraining accuracy: 0.793041467666626\n",
      "Step: 4500  \tValid loss: 0.3766057789325714\n",
      "Step: 4600  \tTraining loss: 0.38196712732315063\n",
      "Step: 4600  \tTraining accuracy: 0.793305516242981\n",
      "Step: 4600  \tValid loss: 0.37630993127822876\n",
      "Step: 4700  \tTraining loss: 0.38142094016075134\n",
      "Step: 4700  \tTraining accuracy: 0.793585479259491\n",
      "Step: 4700  \tValid loss: 0.3761095702648163\n",
      "Step: 4800  \tTraining loss: 0.38092494010925293\n",
      "Step: 4800  \tTraining accuracy: 0.7938608527183533\n",
      "Step: 4800  \tValid loss: 0.3760366439819336\n",
      "Step: 4900  \tTraining loss: 0.3804492652416229\n",
      "Step: 4900  \tTraining accuracy: 0.7940964698791504\n",
      "Step: 4900  \tValid loss: 0.37590697407722473\n",
      "Step: 5000  \tTraining loss: 0.37998899817466736\n",
      "Step: 5000  \tTraining accuracy: 0.794317901134491\n",
      "Step: 5000  \tValid loss: 0.3757835924625397\n",
      "Step: 5100  \tTraining loss: 0.379528671503067\n",
      "Step: 5100  \tTraining accuracy: 0.794532835483551\n",
      "Step: 5100  \tValid loss: 0.3756796717643738\n",
      "Step: 5200  \tTraining loss: 0.3790867030620575\n",
      "Step: 5200  \tTraining accuracy: 0.794752836227417\n",
      "Step: 5200  \tValid loss: 0.37556058168411255\n",
      "Step: 5300  \tTraining loss: 0.37866178154945374\n",
      "Step: 5300  \tTraining accuracy: 0.794953465461731\n",
      "Step: 5300  \tValid loss: 0.3755158483982086\n",
      "Step: 5400  \tTraining loss: 0.37824746966362\n",
      "Step: 5400  \tTraining accuracy: 0.7951766848564148\n",
      "Step: 5400  \tValid loss: 0.3753921687602997\n",
      "Step: 5500  \tTraining loss: 0.37785106897354126\n",
      "Step: 5500  \tTraining accuracy: 0.7953980565071106\n",
      "Step: 5500  \tValid loss: 0.37519922852516174\n",
      "Step: 5600  \tTraining loss: 0.3774595260620117\n",
      "Step: 5600  \tTraining accuracy: 0.7956115007400513\n",
      "Step: 5600  \tValid loss: 0.3750692307949066\n",
      "Step: 5700  \tTraining loss: 0.37708547711372375\n",
      "Step: 5700  \tTraining accuracy: 0.7958295345306396\n",
      "Step: 5700  \tValid loss: 0.3749525845050812\n",
      "Step: 5800  \tTraining loss: 0.37672677636146545\n",
      "Step: 5800  \tTraining accuracy: 0.7960219979286194\n",
      "Step: 5800  \tValid loss: 0.37485620379447937\n",
      "Step: 5900  \tTraining loss: 0.3763405382633209\n",
      "Step: 5900  \tTraining accuracy: 0.7962255477905273\n",
      "Step: 5900  \tValid loss: 0.37457937002182007\n",
      "Step: 6000  \tTraining loss: 0.3758205473423004\n",
      "Step: 6000  \tTraining accuracy: 0.7964299917221069\n",
      "Step: 6000  \tValid loss: 0.37422439455986023\n",
      "Step: 6100  \tTraining loss: 0.3753250241279602\n",
      "Step: 6100  \tTraining accuracy: 0.7966296076774597\n",
      "Step: 6100  \tValid loss: 0.37417611479759216\n",
      "Step: 6200  \tTraining loss: 0.37469860911369324\n",
      "Step: 6200  \tTraining accuracy: 0.7968526482582092\n",
      "Step: 6200  \tValid loss: 0.3739832937717438\n",
      "Step: 6300  \tTraining loss: 0.3741576373577118\n",
      "Step: 6300  \tTraining accuracy: 0.7970629930496216\n",
      "Step: 6300  \tValid loss: 0.37385275959968567\n",
      "Step: 6400  \tTraining loss: 0.3736743927001953\n",
      "Step: 6400  \tTraining accuracy: 0.7972703576087952\n",
      "Step: 6400  \tValid loss: 0.37398311495780945\n",
      "Step: 6500  \tTraining loss: 0.37324175238609314\n",
      "Step: 6500  \tTraining accuracy: 0.7974801659584045\n",
      "Step: 6500  \tValid loss: 0.3740948438644409\n",
      "Step: 6600  \tTraining loss: 0.3727583885192871\n",
      "Step: 6600  \tTraining accuracy: 0.7976765632629395\n",
      "Step: 6600  \tValid loss: 0.37443456053733826\n",
      "Step: 6700  \tTraining loss: 0.37233951687812805\n",
      "Step: 6700  \tTraining accuracy: 0.797886073589325\n",
      "Step: 6700  \tValid loss: 0.37443339824676514\n",
      "Step: 6800  \tTraining loss: 0.37194952368736267\n",
      "Step: 6800  \tTraining accuracy: 0.7980911135673523\n",
      "Step: 6800  \tValid loss: 0.3744659423828125\n",
      "Step: 6900  \tTraining loss: 0.37153196334838867\n",
      "Step: 6900  \tTraining accuracy: 0.7982985377311707\n",
      "Step: 6900  \tValid loss: 0.37461429834365845\n",
      "Step: 7000  \tTraining loss: 0.37114980816841125\n",
      "Step: 7000  \tTraining accuracy: 0.7985065579414368\n",
      "Step: 7000  \tValid loss: 0.3745112717151642\n",
      "Step: 7100  \tTraining loss: 0.3707863986492157\n",
      "Step: 7100  \tTraining accuracy: 0.798707127571106\n",
      "Step: 7100  \tValid loss: 0.37448978424072266\n",
      "Step: 7200  \tTraining loss: 0.3704356253147125\n",
      "Step: 7200  \tTraining accuracy: 0.7989036440849304\n",
      "Step: 7200  \tValid loss: 0.37451979517936707\n",
      "Step: 7300  \tTraining loss: 0.37010061740875244\n",
      "Step: 7300  \tTraining accuracy: 0.7991042137145996\n",
      "Step: 7300  \tValid loss: 0.37456390261650085\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.79928064\n",
      "Precision: 0.85246915\n",
      "Recall: 0.89617133\n",
      "F1 score: 0.8192563\n",
      "AUC: 0.75959986\n",
      "   accuracy  precision    recall  f1_score     auc      loss  accuracy_val  \\\n",
      "0  0.799281   0.852469  0.896171  0.819256  0.7596  0.369933      0.799162   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.373795       0.799116   0.420426      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  7351.0  \n",
      "16\n",
      "(8120, 8)\n",
      "(8120, 1)\n",
      "(4400, 8)\n",
      "(4400, 1)\n",
      "(3575, 8)\n",
      "(3575, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6075899004936218\n",
      "Step: 100  \tTraining accuracy: 0.6854679584503174\n",
      "Step: 100  \tValid loss: 0.601101279258728\n",
      "Step: 200  \tTraining loss: 0.461857408285141\n",
      "Step: 200  \tTraining accuracy: 0.736981213092804\n",
      "Step: 200  \tValid loss: 0.4502737820148468\n",
      "Step: 300  \tTraining loss: 0.3867042064666748\n",
      "Step: 300  \tTraining accuracy: 0.7785413265228271\n",
      "Step: 300  \tValid loss: 0.36825212836265564\n",
      "Step: 400  \tTraining loss: 0.36431390047073364\n",
      "Step: 400  \tTraining accuracy: 0.7990426421165466\n",
      "Step: 400  \tValid loss: 0.34113436937332153\n",
      "Step: 500  \tTraining loss: 0.3540555536746979\n",
      "Step: 500  \tTraining accuracy: 0.8111310601234436\n",
      "Step: 500  \tValid loss: 0.3294643759727478\n",
      "Step: 600  \tTraining loss: 0.34662652015686035\n",
      "Step: 600  \tTraining accuracy: 0.8195383548736572\n",
      "Step: 600  \tValid loss: 0.322433203458786\n",
      "Step: 700  \tTraining loss: 0.3421327471733093\n",
      "Step: 700  \tTraining accuracy: 0.8260961174964905\n",
      "Step: 700  \tValid loss: 0.3183344304561615\n",
      "Step: 800  \tTraining loss: 0.3395501971244812\n",
      "Step: 800  \tTraining accuracy: 0.830980658531189\n",
      "Step: 800  \tValid loss: 0.31605401635169983\n",
      "Step: 900  \tTraining loss: 0.3378874361515045\n",
      "Step: 900  \tTraining accuracy: 0.8346654176712036\n",
      "Step: 900  \tValid loss: 0.3147444427013397\n",
      "Step: 1000  \tTraining loss: 0.3366946280002594\n",
      "Step: 1000  \tTraining accuracy: 0.8376270532608032\n",
      "Step: 1000  \tValid loss: 0.3140140175819397\n",
      "Step: 1100  \tTraining loss: 0.33585312962532043\n",
      "Step: 1100  \tTraining accuracy: 0.8399657011032104\n",
      "Step: 1100  \tValid loss: 0.31341439485549927\n",
      "Step: 1200  \tTraining loss: 0.33519256114959717\n",
      "Step: 1200  \tTraining accuracy: 0.8419625759124756\n",
      "Step: 1200  \tValid loss: 0.3129318058490753\n",
      "Step: 1300  \tTraining loss: 0.3346336781978607\n",
      "Step: 1300  \tTraining accuracy: 0.843635082244873\n",
      "Step: 1300  \tValid loss: 0.3125442862510681\n",
      "Step: 1400  \tTraining loss: 0.3341406583786011\n",
      "Step: 1400  \tTraining accuracy: 0.8450783491134644\n",
      "Step: 1400  \tValid loss: 0.3122294843196869\n",
      "Step: 1500  \tTraining loss: 0.333689421415329\n",
      "Step: 1500  \tTraining accuracy: 0.846314013004303\n",
      "Step: 1500  \tValid loss: 0.3119919002056122\n",
      "Step: 1600  \tTraining loss: 0.3332553803920746\n",
      "Step: 1600  \tTraining accuracy: 0.847410261631012\n",
      "Step: 1600  \tValid loss: 0.3117924928665161\n",
      "Step: 1700  \tTraining loss: 0.3328098952770233\n",
      "Step: 1700  \tTraining accuracy: 0.848358690738678\n",
      "Step: 1700  \tValid loss: 0.3116365969181061\n",
      "Step: 1800  \tTraining loss: 0.33235156536102295\n",
      "Step: 1800  \tTraining accuracy: 0.8491561412811279\n",
      "Step: 1800  \tValid loss: 0.3114936351776123\n",
      "Step: 1900  \tTraining loss: 0.33192285895347595\n",
      "Step: 1900  \tTraining accuracy: 0.8498237133026123\n",
      "Step: 1900  \tValid loss: 0.31133323907852173\n",
      "Step: 2000  \tTraining loss: 0.3315492272377014\n",
      "Step: 2000  \tTraining accuracy: 0.8503655195236206\n",
      "Step: 2000  \tValid loss: 0.31119850277900696\n",
      "Step: 2100  \tTraining loss: 0.3312290608882904\n",
      "Step: 2100  \tTraining accuracy: 0.8508332967758179\n",
      "Step: 2100  \tValid loss: 0.31111007928848267\n",
      "Step: 2200  \tTraining loss: 0.33095091581344604\n",
      "Step: 2200  \tTraining accuracy: 0.8512372970581055\n",
      "Step: 2200  \tValid loss: 0.3110596835613251\n",
      "Step: 2300  \tTraining loss: 0.33070108294487\n",
      "Step: 2300  \tTraining accuracy: 0.8516219854354858\n",
      "Step: 2300  \tValid loss: 0.31102484464645386\n",
      "Step: 2400  \tTraining loss: 0.3304692804813385\n",
      "Step: 2400  \tTraining accuracy: 0.8519739508628845\n",
      "Step: 2400  \tValid loss: 0.3109872043132782\n",
      "Step: 2500  \tTraining loss: 0.3302503228187561\n",
      "Step: 2500  \tTraining accuracy: 0.8522869944572449\n",
      "Step: 2500  \tValid loss: 0.31095951795578003\n",
      "Step: 2600  \tTraining loss: 0.3300407826900482\n",
      "Step: 2600  \tTraining accuracy: 0.8525901436805725\n",
      "Step: 2600  \tValid loss: 0.31092557311058044\n",
      "Step: 2700  \tTraining loss: 0.3298359811306\n",
      "Step: 2700  \tTraining accuracy: 0.8528774976730347\n",
      "Step: 2700  \tValid loss: 0.31089699268341064\n",
      "Step: 2800  \tTraining loss: 0.32963332533836365\n",
      "Step: 2800  \tTraining accuracy: 0.8531145453453064\n",
      "Step: 2800  \tValid loss: 0.3108650743961334\n",
      "Step: 2900  \tTraining loss: 0.3294302821159363\n",
      "Step: 2900  \tTraining accuracy: 0.8533284068107605\n",
      "Step: 2900  \tValid loss: 0.3108333647251129\n",
      "Step: 3000  \tTraining loss: 0.3292255103588104\n",
      "Step: 3000  \tTraining accuracy: 0.8535193204879761\n",
      "Step: 3000  \tValid loss: 0.3108013868331909\n",
      "Step: 3100  \tTraining loss: 0.3290194272994995\n",
      "Step: 3100  \tTraining accuracy: 0.8537140488624573\n",
      "Step: 3100  \tValid loss: 0.3107750415802002\n",
      "Step: 3200  \tTraining loss: 0.32881107926368713\n",
      "Step: 3200  \tTraining accuracy: 0.8538846373558044\n",
      "Step: 3200  \tValid loss: 0.3107629418373108\n",
      "Step: 3300  \tTraining loss: 0.32859939336776733\n",
      "Step: 3300  \tTraining accuracy: 0.8540427684783936\n",
      "Step: 3300  \tValid loss: 0.3107643127441406\n",
      "Step: 3400  \tTraining loss: 0.3283850848674774\n",
      "Step: 3400  \tTraining accuracy: 0.8542118072509766\n",
      "Step: 3400  \tValid loss: 0.31078284978866577\n",
      "Step: 3500  \tTraining loss: 0.3281678259372711\n",
      "Step: 3500  \tTraining accuracy: 0.8543657064437866\n",
      "Step: 3500  \tValid loss: 0.31080853939056396\n",
      "Step: 3600  \tTraining loss: 0.327945739030838\n",
      "Step: 3600  \tTraining accuracy: 0.8545214533805847\n",
      "Step: 3600  \tValid loss: 0.3108460605144501\n",
      "Step: 3700  \tTraining loss: 0.32771605253219604\n",
      "Step: 3700  \tTraining accuracy: 0.854682207107544\n",
      "Step: 3700  \tValid loss: 0.3108932673931122\n",
      "Step: 3800  \tTraining loss: 0.3274761736392975\n",
      "Step: 3800  \tTraining accuracy: 0.8548576831817627\n",
      "Step: 3800  \tValid loss: 0.31094399094581604\n",
      "Step: 3900  \tTraining loss: 0.32722774147987366\n",
      "Step: 3900  \tTraining accuracy: 0.8550207614898682\n",
      "Step: 3900  \tValid loss: 0.3109864890575409\n",
      "Step: 4000  \tTraining loss: 0.3269771635532379\n",
      "Step: 4000  \tTraining accuracy: 0.8551708459854126\n",
      "Step: 4000  \tValid loss: 0.3110020160675049\n",
      "Step: 4100  \tTraining loss: 0.3267306685447693\n",
      "Step: 4100  \tTraining accuracy: 0.8553012609481812\n",
      "Step: 4100  \tValid loss: 0.310997873544693\n",
      "Step: 4200  \tTraining loss: 0.3264915347099304\n",
      "Step: 4200  \tTraining accuracy: 0.855419397354126\n",
      "Step: 4200  \tValid loss: 0.310981422662735\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.85552907\n",
      "Precision: 0.8480285\n",
      "Recall: 0.8402062\n",
      "F1 score: 0.851565\n",
      "AUC: 0.8575185\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.855529   0.848028  0.840206  0.851565  0.857518  0.326363      0.855486   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0   0.31076       0.855433   0.316829      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  4254.0  \n",
      "17\n",
      "(3915, 8)\n",
      "(3915, 1)\n",
      "(2160, 8)\n",
      "(2160, 1)\n",
      "(1755, 8)\n",
      "(1755, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4387962818145752\n",
      "Step: 100  \tTraining accuracy: 0.8122605085372925\n",
      "Step: 100  \tValid loss: 0.4732010066509247\n",
      "Step: 200  \tTraining loss: 0.42077308893203735\n",
      "Step: 200  \tTraining accuracy: 0.8090251088142395\n",
      "Step: 200  \tValid loss: 0.45926156640052795\n",
      "Step: 300  \tTraining loss: 0.41304048895835876\n",
      "Step: 300  \tTraining accuracy: 0.806845486164093\n",
      "Step: 300  \tValid loss: 0.45044511556625366\n",
      "Step: 400  \tTraining loss: 0.4049280881881714\n",
      "Step: 400  \tTraining accuracy: 0.8068965673446655\n",
      "Step: 400  \tValid loss: 0.4386370778083801\n",
      "Step: 500  \tTraining loss: 0.39563077688217163\n",
      "Step: 500  \tTraining accuracy: 0.8083439469337463\n",
      "Step: 500  \tValid loss: 0.4242612421512604\n",
      "Step: 600  \tTraining loss: 0.3887588083744049\n",
      "Step: 600  \tTraining accuracy: 0.81075119972229\n",
      "Step: 600  \tValid loss: 0.4144155979156494\n",
      "Step: 700  \tTraining loss: 0.3843134045600891\n",
      "Step: 700  \tTraining accuracy: 0.8134590983390808\n",
      "Step: 700  \tValid loss: 0.4093368947505951\n",
      "Step: 800  \tTraining loss: 0.38126543164253235\n",
      "Step: 800  \tTraining accuracy: 0.8159556984901428\n",
      "Step: 800  \tValid loss: 0.4058612287044525\n",
      "Step: 900  \tTraining loss: 0.37897610664367676\n",
      "Step: 900  \tTraining accuracy: 0.8177897930145264\n",
      "Step: 900  \tValid loss: 0.4030967950820923\n",
      "Step: 1000  \tTraining loss: 0.37706977128982544\n",
      "Step: 1000  \tTraining accuracy: 0.8194528222084045\n",
      "Step: 1000  \tValid loss: 0.4008502960205078\n",
      "Step: 1100  \tTraining loss: 0.37536606192588806\n",
      "Step: 1100  \tTraining accuracy: 0.8210910558700562\n",
      "Step: 1100  \tValid loss: 0.3988148868083954\n",
      "Step: 1200  \tTraining loss: 0.3737538456916809\n",
      "Step: 1200  \tTraining accuracy: 0.8224443197250366\n",
      "Step: 1200  \tValid loss: 0.397011935710907\n",
      "Step: 1300  \tTraining loss: 0.3721826672554016\n",
      "Step: 1300  \tTraining accuracy: 0.8238058686256409\n",
      "Step: 1300  \tValid loss: 0.395405650138855\n",
      "Step: 1400  \tTraining loss: 0.3706471621990204\n",
      "Step: 1400  \tTraining accuracy: 0.8249373435974121\n",
      "Step: 1400  \tValid loss: 0.3938365578651428\n",
      "Step: 1500  \tTraining loss: 0.36929312348365784\n",
      "Step: 1500  \tTraining accuracy: 0.8259655833244324\n",
      "Step: 1500  \tValid loss: 0.3925181031227112\n",
      "Step: 1600  \tTraining loss: 0.3680892288684845\n",
      "Step: 1600  \tTraining accuracy: 0.8269105553627014\n",
      "Step: 1600  \tValid loss: 0.39136430621147156\n",
      "Step: 1700  \tTraining loss: 0.3670041859149933\n",
      "Step: 1700  \tTraining accuracy: 0.8278571367263794\n",
      "Step: 1700  \tValid loss: 0.39053621888160706\n",
      "Step: 1800  \tTraining loss: 0.3660160303115845\n",
      "Step: 1800  \tTraining accuracy: 0.8287392854690552\n",
      "Step: 1800  \tValid loss: 0.3898639976978302\n",
      "Step: 1900  \tTraining loss: 0.36510640382766724\n",
      "Step: 1900  \tTraining accuracy: 0.8294915556907654\n",
      "Step: 1900  \tValid loss: 0.38916677236557007\n",
      "Step: 2000  \tTraining loss: 0.36424922943115234\n",
      "Step: 2000  \tTraining accuracy: 0.8301798105239868\n",
      "Step: 2000  \tValid loss: 0.3886173963546753\n",
      "Step: 2100  \tTraining loss: 0.3634692132472992\n",
      "Step: 2100  \tTraining accuracy: 0.8308195471763611\n",
      "Step: 2100  \tValid loss: 0.38815951347351074\n",
      "Step: 2200  \tTraining loss: 0.3627437949180603\n",
      "Step: 2200  \tTraining accuracy: 0.831405758857727\n",
      "Step: 2200  \tValid loss: 0.3877725601196289\n",
      "Step: 2300  \tTraining loss: 0.36207085847854614\n",
      "Step: 2300  \tTraining accuracy: 0.8319512009620667\n",
      "Step: 2300  \tValid loss: 0.38741329312324524\n",
      "Step: 2400  \tTraining loss: 0.36138826608657837\n",
      "Step: 2400  \tTraining accuracy: 0.8324610590934753\n",
      "Step: 2400  \tValid loss: 0.38701653480529785\n",
      "Step: 2500  \tTraining loss: 0.360720157623291\n",
      "Step: 2500  \tTraining accuracy: 0.8329606056213379\n",
      "Step: 2500  \tValid loss: 0.38675040006637573\n",
      "Step: 2600  \tTraining loss: 0.360102117061615\n",
      "Step: 2600  \tTraining accuracy: 0.8334360122680664\n",
      "Step: 2600  \tValid loss: 0.38647952675819397\n",
      "Step: 2700  \tTraining loss: 0.3595268428325653\n",
      "Step: 2700  \tTraining accuracy: 0.8339044451713562\n",
      "Step: 2700  \tValid loss: 0.38622498512268066\n",
      "Step: 2800  \tTraining loss: 0.3589862883090973\n",
      "Step: 2800  \tTraining accuracy: 0.8343573808670044\n",
      "Step: 2800  \tValid loss: 0.3860498368740082\n",
      "Step: 2900  \tTraining loss: 0.35844552516937256\n",
      "Step: 2900  \tTraining accuracy: 0.8348008990287781\n",
      "Step: 2900  \tValid loss: 0.3858198821544647\n",
      "Step: 3000  \tTraining loss: 0.35792651772499084\n",
      "Step: 3000  \tTraining accuracy: 0.8352317214012146\n",
      "Step: 3000  \tValid loss: 0.38564932346343994\n",
      "Step: 3100  \tTraining loss: 0.35741642117500305\n",
      "Step: 3100  \tTraining accuracy: 0.8356384634971619\n",
      "Step: 3100  \tValid loss: 0.3854595720767975\n",
      "Step: 3200  \tTraining loss: 0.35686081647872925\n",
      "Step: 3200  \tTraining accuracy: 0.8360356092453003\n",
      "Step: 3200  \tValid loss: 0.385508269071579\n",
      "Step: 3300  \tTraining loss: 0.3563452959060669\n",
      "Step: 3300  \tTraining accuracy: 0.8364357948303223\n",
      "Step: 3300  \tValid loss: 0.38534995913505554\n",
      "Step: 3400  \tTraining loss: 0.3558398485183716\n",
      "Step: 3400  \tTraining accuracy: 0.8368311524391174\n",
      "Step: 3400  \tValid loss: 0.3851846158504486\n",
      "Step: 3500  \tTraining loss: 0.355325847864151\n",
      "Step: 3500  \tTraining accuracy: 0.837211012840271\n",
      "Step: 3500  \tValid loss: 0.38504135608673096\n",
      "Step: 3600  \tTraining loss: 0.3547992408275604\n",
      "Step: 3600  \tTraining accuracy: 0.8375694751739502\n",
      "Step: 3600  \tValid loss: 0.38493210077285767\n",
      "Step: 3700  \tTraining loss: 0.3542468249797821\n",
      "Step: 3700  \tTraining accuracy: 0.8379048109054565\n",
      "Step: 3700  \tValid loss: 0.38486629724502563\n",
      "Step: 3800  \tTraining loss: 0.3536669909954071\n",
      "Step: 3800  \tTraining accuracy: 0.8382086157798767\n",
      "Step: 3800  \tValid loss: 0.3847837746143341\n",
      "Step: 3900  \tTraining loss: 0.3530508875846863\n",
      "Step: 3900  \tTraining accuracy: 0.8384866714477539\n",
      "Step: 3900  \tValid loss: 0.3846672475337982\n",
      "Step: 4000  \tTraining loss: 0.35240575671195984\n",
      "Step: 4000  \tTraining accuracy: 0.8387377262115479\n",
      "Step: 4000  \tValid loss: 0.38452476263046265\n",
      "Step: 4100  \tTraining loss: 0.351735919713974\n",
      "Step: 4100  \tTraining accuracy: 0.8389700651168823\n",
      "Step: 4100  \tValid loss: 0.38434863090515137\n",
      "Step: 4200  \tTraining loss: 0.3510344624519348\n",
      "Step: 4200  \tTraining accuracy: 0.8392097353935242\n",
      "Step: 4200  \tValid loss: 0.38411033153533936\n",
      "Step: 4300  \tTraining loss: 0.3503093123435974\n",
      "Step: 4300  \tTraining accuracy: 0.8394591212272644\n",
      "Step: 4300  \tValid loss: 0.3838493227958679\n",
      "Step: 4400  \tTraining loss: 0.34954172372817993\n",
      "Step: 4400  \tTraining accuracy: 0.8396823406219482\n",
      "Step: 4400  \tValid loss: 0.383686363697052\n",
      "Step: 4500  \tTraining loss: 0.34864509105682373\n",
      "Step: 4500  \tTraining accuracy: 0.8398955464363098\n",
      "Step: 4500  \tValid loss: 0.3829921782016754\n",
      "Step: 4600  \tTraining loss: 0.34782642126083374\n",
      "Step: 4600  \tTraining accuracy: 0.8401134014129639\n",
      "Step: 4600  \tValid loss: 0.38287851214408875\n",
      "Step: 4700  \tTraining loss: 0.34702587127685547\n",
      "Step: 4700  \tTraining accuracy: 0.8403164148330688\n",
      "Step: 4700  \tValid loss: 0.38276174664497375\n",
      "Step: 4800  \tTraining loss: 0.3462764322757721\n",
      "Step: 4800  \tTraining accuracy: 0.8405108451843262\n",
      "Step: 4800  \tValid loss: 0.3828040659427643\n",
      "Step: 4900  \tTraining loss: 0.3455667495727539\n",
      "Step: 4900  \tTraining accuracy: 0.8407130837440491\n",
      "Step: 4900  \tValid loss: 0.38274940848350525\n",
      "Step: 5000  \tTraining loss: 0.3448792099952698\n",
      "Step: 5000  \tTraining accuracy: 0.8409252166748047\n",
      "Step: 5000  \tValid loss: 0.3826029598712921\n",
      "Step: 5100  \tTraining loss: 0.3442225754261017\n",
      "Step: 5100  \tTraining accuracy: 0.8411365151405334\n",
      "Step: 5100  \tValid loss: 0.38249319791793823\n",
      "Step: 5200  \tTraining loss: 0.3435840904712677\n",
      "Step: 5200  \tTraining accuracy: 0.8413569927215576\n",
      "Step: 5200  \tValid loss: 0.3823038637638092\n",
      "Step: 5300  \tTraining loss: 0.34297022223472595\n",
      "Step: 5300  \tTraining accuracy: 0.8416031002998352\n",
      "Step: 5300  \tValid loss: 0.3820863962173462\n",
      "Step: 5400  \tTraining loss: 0.3423764705657959\n",
      "Step: 5400  \tTraining accuracy: 0.8418400287628174\n",
      "Step: 5400  \tValid loss: 0.38182172179222107\n",
      "Step: 5500  \tTraining loss: 0.3417993187904358\n",
      "Step: 5500  \tTraining accuracy: 0.8420659303665161\n",
      "Step: 5500  \tValid loss: 0.38144001364707947\n",
      "Step: 5600  \tTraining loss: 0.3412296772003174\n",
      "Step: 5600  \tTraining accuracy: 0.8422882556915283\n",
      "Step: 5600  \tValid loss: 0.38099613785743713\n",
      "Step: 5700  \tTraining loss: 0.3406611382961273\n",
      "Step: 5700  \tTraining accuracy: 0.8425027132034302\n",
      "Step: 5700  \tValid loss: 0.3805554211139679\n",
      "Step: 5800  \tTraining loss: 0.34008434414863586\n",
      "Step: 5800  \tTraining accuracy: 0.8427097797393799\n",
      "Step: 5800  \tValid loss: 0.38009029626846313\n",
      "Step: 5900  \tTraining loss: 0.33949241042137146\n",
      "Step: 5900  \tTraining accuracy: 0.8429184556007385\n",
      "Step: 5900  \tValid loss: 0.37963223457336426\n",
      "Step: 6000  \tTraining loss: 0.33887720108032227\n",
      "Step: 6000  \tTraining accuracy: 0.8431243896484375\n",
      "Step: 6000  \tValid loss: 0.37916824221611023\n",
      "Step: 6100  \tTraining loss: 0.3382408618927002\n",
      "Step: 6100  \tTraining accuracy: 0.8433235287666321\n",
      "Step: 6100  \tValid loss: 0.37869757413864136\n",
      "Step: 6200  \tTraining loss: 0.33758434653282166\n",
      "Step: 6200  \tTraining accuracy: 0.8435286283493042\n",
      "Step: 6200  \tValid loss: 0.3782415986061096\n",
      "Step: 6300  \tTraining loss: 0.3369244635105133\n",
      "Step: 6300  \tTraining accuracy: 0.8437333106994629\n",
      "Step: 6300  \tValid loss: 0.37779897451400757\n",
      "Step: 6400  \tTraining loss: 0.33625465631484985\n",
      "Step: 6400  \tTraining accuracy: 0.8439255356788635\n",
      "Step: 6400  \tValid loss: 0.377400666475296\n",
      "Step: 6500  \tTraining loss: 0.3355848491191864\n",
      "Step: 6500  \tTraining accuracy: 0.8441078066825867\n",
      "Step: 6500  \tValid loss: 0.37703704833984375\n",
      "Step: 6600  \tTraining loss: 0.33493202924728394\n",
      "Step: 6600  \tTraining accuracy: 0.8442904353141785\n",
      "Step: 6600  \tValid loss: 0.37665805220603943\n",
      "Step: 6700  \tTraining loss: 0.3343029320240021\n",
      "Step: 6700  \tTraining accuracy: 0.8444809317588806\n",
      "Step: 6700  \tValid loss: 0.3763321340084076\n",
      "Step: 6800  \tTraining loss: 0.33369961380958557\n",
      "Step: 6800  \tTraining accuracy: 0.8446809649467468\n",
      "Step: 6800  \tValid loss: 0.37598633766174316\n",
      "Step: 6900  \tTraining loss: 0.33312490582466125\n",
      "Step: 6900  \tTraining accuracy: 0.8448900580406189\n",
      "Step: 6900  \tValid loss: 0.37571388483047485\n",
      "Step: 7000  \tTraining loss: 0.33252230286598206\n",
      "Step: 7000  \tTraining accuracy: 0.8451077938079834\n",
      "Step: 7000  \tValid loss: 0.3753640651702881\n",
      "Step: 7100  \tTraining loss: 0.33200058341026306\n",
      "Step: 7100  \tTraining accuracy: 0.8453085422515869\n",
      "Step: 7100  \tValid loss: 0.37511953711509705\n",
      "Step: 7200  \tTraining loss: 0.3315103352069855\n",
      "Step: 7200  \tTraining accuracy: 0.8455018997192383\n",
      "Step: 7200  \tValid loss: 0.3749224841594696\n",
      "Step: 7300  \tTraining loss: 0.3310486972332001\n",
      "Step: 7300  \tTraining accuracy: 0.8456881046295166\n",
      "Step: 7300  \tValid loss: 0.3747114837169647\n",
      "Step: 7400  \tTraining loss: 0.3306080996990204\n",
      "Step: 7400  \tTraining accuracy: 0.8458675146102905\n",
      "Step: 7400  \tValid loss: 0.3744948208332062\n",
      "Step: 7500  \tTraining loss: 0.3301888108253479\n",
      "Step: 7500  \tTraining accuracy: 0.8460353016853333\n",
      "Step: 7500  \tValid loss: 0.3743346631526947\n",
      "Step: 7600  \tTraining loss: 0.3297877311706543\n",
      "Step: 7600  \tTraining accuracy: 0.8461952209472656\n",
      "Step: 7600  \tValid loss: 0.37417343258857727\n",
      "Step: 7700  \tTraining loss: 0.3294004797935486\n",
      "Step: 7700  \tTraining accuracy: 0.8463442921638489\n",
      "Step: 7700  \tValid loss: 0.3739777207374573\n",
      "Step: 7800  \tTraining loss: 0.32902324199676514\n",
      "Step: 7800  \tTraining accuracy: 0.8464928269386292\n",
      "Step: 7800  \tValid loss: 0.3738233745098114\n",
      "Step: 7900  \tTraining loss: 0.3286605477333069\n",
      "Step: 7900  \tTraining accuracy: 0.8466391563415527\n",
      "Step: 7900  \tValid loss: 0.3736538290977478\n",
      "Step: 8000  \tTraining loss: 0.3283073604106903\n",
      "Step: 8000  \tTraining accuracy: 0.8467850685119629\n",
      "Step: 8000  \tValid loss: 0.3735194802284241\n",
      "Step: 8100  \tTraining loss: 0.3279638886451721\n",
      "Step: 8100  \tTraining accuracy: 0.8469288945198059\n",
      "Step: 8100  \tValid loss: 0.3733634650707245\n",
      "Step: 8200  \tTraining loss: 0.32762783765792847\n",
      "Step: 8200  \tTraining accuracy: 0.847069263458252\n",
      "Step: 8200  \tValid loss: 0.3732362985610962\n",
      "Step: 8300  \tTraining loss: 0.3272993266582489\n",
      "Step: 8300  \tTraining accuracy: 0.8472077250480652\n",
      "Step: 8300  \tValid loss: 0.3730841279029846\n",
      "Step: 8400  \tTraining loss: 0.32697534561157227\n",
      "Step: 8400  \tTraining accuracy: 0.8473398089408875\n",
      "Step: 8400  \tValid loss: 0.3729562759399414\n",
      "Step: 8500  \tTraining loss: 0.3266356289386749\n",
      "Step: 8500  \tTraining accuracy: 0.8474687933921814\n",
      "Step: 8500  \tValid loss: 0.37307655811309814\n",
      "Step: 8600  \tTraining loss: 0.32631635665893555\n",
      "Step: 8600  \tTraining accuracy: 0.847591757774353\n",
      "Step: 8600  \tValid loss: 0.3730657994747162\n",
      "Step: 8700  \tTraining loss: 0.32600921392440796\n",
      "Step: 8700  \tTraining accuracy: 0.8477118611335754\n",
      "Step: 8700  \tValid loss: 0.372958779335022\n",
      "Step: 8800  \tTraining loss: 0.32570940256118774\n",
      "Step: 8800  \tTraining accuracy: 0.8478175401687622\n",
      "Step: 8800  \tValid loss: 0.37284523248672485\n",
      "Step: 8900  \tTraining loss: 0.325415700674057\n",
      "Step: 8900  \tTraining accuracy: 0.8479179739952087\n",
      "Step: 8900  \tValid loss: 0.37276554107666016\n",
      "Step: 9000  \tTraining loss: 0.3251131474971771\n",
      "Step: 9000  \tTraining accuracy: 0.848019003868103\n",
      "Step: 9000  \tValid loss: 0.37258610129356384\n",
      "Step: 9100  \tTraining loss: 0.3248211741447449\n",
      "Step: 9100  \tTraining accuracy: 0.8481149673461914\n",
      "Step: 9100  \tValid loss: 0.37246641516685486\n",
      "Step: 9200  \tTraining loss: 0.3245324194431305\n",
      "Step: 9200  \tTraining accuracy: 0.8482186198234558\n",
      "Step: 9200  \tValid loss: 0.37232497334480286\n",
      "Step: 9300  \tTraining loss: 0.3242517113685608\n",
      "Step: 9300  \tTraining accuracy: 0.8483200669288635\n",
      "Step: 9300  \tValid loss: 0.37224915623664856\n",
      "Step: 9400  \tTraining loss: 0.32397621870040894\n",
      "Step: 9400  \tTraining accuracy: 0.8484193086624146\n",
      "Step: 9400  \tValid loss: 0.3721783757209778\n",
      "Step: 9500  \tTraining loss: 0.3237079381942749\n",
      "Step: 9500  \tTraining accuracy: 0.8485191464424133\n",
      "Step: 9500  \tValid loss: 0.3720930814743042\n",
      "Step: 9600  \tTraining loss: 0.32344135642051697\n",
      "Step: 9600  \tTraining accuracy: 0.848616898059845\n",
      "Step: 9600  \tValid loss: 0.37199899554252625\n",
      "Step: 9700  \tTraining loss: 0.32318243384361267\n",
      "Step: 9700  \tTraining accuracy: 0.8487205505371094\n",
      "Step: 9700  \tValid loss: 0.3719038665294647\n",
      "Step: 9800  \tTraining loss: 0.3229236900806427\n",
      "Step: 9800  \tTraining accuracy: 0.848824679851532\n",
      "Step: 9800  \tValid loss: 0.3718852996826172\n",
      "Step: 9900  \tTraining loss: 0.32267171144485474\n",
      "Step: 9900  \tTraining accuracy: 0.8489267230033875\n",
      "Step: 9900  \tValid loss: 0.37176313996315\n",
      "Step: 10000  \tTraining loss: 0.3224196135997772\n",
      "Step: 10000  \tTraining accuracy: 0.8490216135978699\n",
      "Step: 10000  \tValid loss: 0.371681809425354\n",
      "Step: 10100  \tTraining loss: 0.32217302918434143\n",
      "Step: 10100  \tTraining accuracy: 0.8491094708442688\n",
      "Step: 10100  \tValid loss: 0.3715934753417969\n",
      "Step: 10200  \tTraining loss: 0.32192814350128174\n",
      "Step: 10200  \tTraining accuracy: 0.8491931557655334\n",
      "Step: 10200  \tValid loss: 0.37146392464637756\n",
      "Step: 10300  \tTraining loss: 0.32168638706207275\n",
      "Step: 10300  \tTraining accuracy: 0.8492751717567444\n",
      "Step: 10300  \tValid loss: 0.37141209840774536\n",
      "Step: 10400  \tTraining loss: 0.32144778966903687\n",
      "Step: 10400  \tTraining accuracy: 0.8493592739105225\n",
      "Step: 10400  \tValid loss: 0.3713226318359375\n",
      "Step: 10500  \tTraining loss: 0.32121360301971436\n",
      "Step: 10500  \tTraining accuracy: 0.8494393229484558\n",
      "Step: 10500  \tValid loss: 0.37124285101890564\n",
      "Step: 10600  \tTraining loss: 0.32098284363746643\n",
      "Step: 10600  \tTraining accuracy: 0.8495215177536011\n",
      "Step: 10600  \tValid loss: 0.3711361289024353\n",
      "Step: 10700  \tTraining loss: 0.32075396180152893\n",
      "Step: 10700  \tTraining accuracy: 0.8495985865592957\n",
      "Step: 10700  \tValid loss: 0.37103191018104553\n",
      "Step: 10800  \tTraining loss: 0.32052773237228394\n",
      "Step: 10800  \tTraining accuracy: 0.8496670722961426\n",
      "Step: 10800  \tValid loss: 0.3709777891635895\n",
      "Step: 10900  \tTraining loss: 0.32026952505111694\n",
      "Step: 10900  \tTraining accuracy: 0.8497307300567627\n",
      "Step: 10900  \tValid loss: 0.3710944354534149\n",
      "Step: 11000  \tTraining loss: 0.32000797986984253\n",
      "Step: 11000  \tTraining accuracy: 0.8497932553291321\n",
      "Step: 11000  \tValid loss: 0.3710046410560608\n",
      "Step: 11100  \tTraining loss: 0.3197557032108307\n",
      "Step: 11100  \tTraining accuracy: 0.849853515625\n",
      "Step: 11100  \tValid loss: 0.37092676758766174\n",
      "Step: 11200  \tTraining loss: 0.31950438022613525\n",
      "Step: 11200  \tTraining accuracy: 0.8499126434326172\n",
      "Step: 11200  \tValid loss: 0.37075158953666687\n",
      "Step: 11300  \tTraining loss: 0.3192553222179413\n",
      "Step: 11300  \tTraining accuracy: 0.8499696254730225\n",
      "Step: 11300  \tValid loss: 0.3706617057323456\n",
      "Step: 11400  \tTraining loss: 0.3190139830112457\n",
      "Step: 11400  \tTraining accuracy: 0.8500255942344666\n",
      "Step: 11400  \tValid loss: 0.3704926669597626\n",
      "Step: 11500  \tTraining loss: 0.3187732398509979\n",
      "Step: 11500  \tTraining accuracy: 0.8500850200653076\n",
      "Step: 11500  \tValid loss: 0.37035173177719116\n",
      "Step: 11600  \tTraining loss: 0.31854259967803955\n",
      "Step: 11600  \tTraining accuracy: 0.8501445651054382\n",
      "Step: 11600  \tValid loss: 0.37024110555648804\n",
      "Step: 11700  \tTraining loss: 0.31831151247024536\n",
      "Step: 11700  \tTraining accuracy: 0.8502030968666077\n",
      "Step: 11700  \tValid loss: 0.3700239956378937\n",
      "Step: 11800  \tTraining loss: 0.318087100982666\n",
      "Step: 11800  \tTraining accuracy: 0.8502606153488159\n",
      "Step: 11800  \tValid loss: 0.3698594272136688\n",
      "Step: 11900  \tTraining loss: 0.3178107738494873\n",
      "Step: 11900  \tTraining accuracy: 0.8503214120864868\n",
      "Step: 11900  \tValid loss: 0.36983421444892883\n",
      "Step: 12000  \tTraining loss: 0.3175029456615448\n",
      "Step: 12000  \tTraining accuracy: 0.8503812551498413\n",
      "Step: 12000  \tValid loss: 0.3695370852947235\n",
      "Step: 12100  \tTraining loss: 0.3172200918197632\n",
      "Step: 12100  \tTraining accuracy: 0.8504358530044556\n",
      "Step: 12100  \tValid loss: 0.3691609501838684\n",
      "Step: 12200  \tTraining loss: 0.3169591426849365\n",
      "Step: 12200  \tTraining accuracy: 0.8504895567893982\n",
      "Step: 12200  \tValid loss: 0.368884414434433\n",
      "Step: 12300  \tTraining loss: 0.31670454144477844\n",
      "Step: 12300  \tTraining accuracy: 0.8505423665046692\n",
      "Step: 12300  \tValid loss: 0.36872947216033936\n",
      "Step: 12400  \tTraining loss: 0.31645435094833374\n",
      "Step: 12400  \tTraining accuracy: 0.8505892157554626\n",
      "Step: 12400  \tValid loss: 0.36850032210350037\n",
      "Step: 12500  \tTraining loss: 0.31621044874191284\n",
      "Step: 12500  \tTraining accuracy: 0.8506383299827576\n",
      "Step: 12500  \tValid loss: 0.3683517575263977\n",
      "Step: 12600  \tTraining loss: 0.31596463918685913\n",
      "Step: 12600  \tTraining accuracy: 0.8506886959075928\n",
      "Step: 12600  \tValid loss: 0.36818012595176697\n",
      "Step: 12700  \tTraining loss: 0.3157254457473755\n",
      "Step: 12700  \tTraining accuracy: 0.8507382869720459\n",
      "Step: 12700  \tValid loss: 0.3680463433265686\n",
      "Step: 12800  \tTraining loss: 0.3154888153076172\n",
      "Step: 12800  \tTraining accuracy: 0.8507870435714722\n",
      "Step: 12800  \tValid loss: 0.36792728304862976\n",
      "Step: 12900  \tTraining loss: 0.3152550756931305\n",
      "Step: 12900  \tTraining accuracy: 0.8508350849151611\n",
      "Step: 12900  \tValid loss: 0.3677746653556824\n",
      "Step: 13000  \tTraining loss: 0.315022736787796\n",
      "Step: 13000  \tTraining accuracy: 0.8508824110031128\n",
      "Step: 13000  \tValid loss: 0.3676629066467285\n",
      "Step: 13100  \tTraining loss: 0.31479379534721375\n",
      "Step: 13100  \tTraining accuracy: 0.8509289622306824\n",
      "Step: 13100  \tValid loss: 0.3675291836261749\n",
      "Step: 13200  \tTraining loss: 0.3145653307437897\n",
      "Step: 13200  \tTraining accuracy: 0.8509787321090698\n",
      "Step: 13200  \tValid loss: 0.3673536479473114\n",
      "Step: 13300  \tTraining loss: 0.3143385946750641\n",
      "Step: 13300  \tTraining accuracy: 0.851032555103302\n",
      "Step: 13300  \tValid loss: 0.36727502942085266\n",
      "Step: 13400  \tTraining loss: 0.31411024928092957\n",
      "Step: 13400  \tTraining accuracy: 0.8510817289352417\n",
      "Step: 13400  \tValid loss: 0.36711111664772034\n",
      "Step: 13500  \tTraining loss: 0.31387874484062195\n",
      "Step: 13500  \tTraining accuracy: 0.8511292338371277\n",
      "Step: 13500  \tValid loss: 0.36687350273132324\n",
      "Step: 13600  \tTraining loss: 0.3136506974697113\n",
      "Step: 13600  \tTraining accuracy: 0.8511769771575928\n",
      "Step: 13600  \tValid loss: 0.366696834564209\n",
      "Step: 13700  \tTraining loss: 0.31342482566833496\n",
      "Step: 13700  \tTraining accuracy: 0.8512221574783325\n",
      "Step: 13700  \tValid loss: 0.3665432035923004\n",
      "Step: 13800  \tTraining loss: 0.31314536929130554\n",
      "Step: 13800  \tTraining accuracy: 0.8512611389160156\n",
      "Step: 13800  \tValid loss: 0.3663693964481354\n",
      "Step: 13900  \tTraining loss: 0.3128693103790283\n",
      "Step: 13900  \tTraining accuracy: 0.8512976765632629\n",
      "Step: 13900  \tValid loss: 0.36622461676597595\n",
      "Step: 14000  \tTraining loss: 0.3126099705696106\n",
      "Step: 14000  \tTraining accuracy: 0.8513428568840027\n",
      "Step: 14000  \tValid loss: 0.3660697638988495\n",
      "Step: 14100  \tTraining loss: 0.3123593330383301\n",
      "Step: 14100  \tTraining accuracy: 0.8513946533203125\n",
      "Step: 14100  \tValid loss: 0.3658641278743744\n",
      "Step: 14200  \tTraining loss: 0.31212055683135986\n",
      "Step: 14200  \tTraining accuracy: 0.8514475226402283\n",
      "Step: 14200  \tValid loss: 0.3656671643257141\n",
      "Step: 14300  \tTraining loss: 0.3118855655193329\n",
      "Step: 14300  \tTraining accuracy: 0.851499617099762\n",
      "Step: 14300  \tValid loss: 0.3654642105102539\n",
      "Step: 14400  \tTraining loss: 0.3116416037082672\n",
      "Step: 14400  \tTraining accuracy: 0.8515554666519165\n",
      "Step: 14400  \tValid loss: 0.3651353716850281\n",
      "Step: 14500  \tTraining loss: 0.3113972246646881\n",
      "Step: 14500  \tTraining accuracy: 0.8516132235527039\n",
      "Step: 14500  \tValid loss: 0.3648889362812042\n",
      "Step: 14600  \tTraining loss: 0.31115609407424927\n",
      "Step: 14600  \tTraining accuracy: 0.8516675233840942\n",
      "Step: 14600  \tValid loss: 0.3646983504295349\n",
      "Step: 14700  \tTraining loss: 0.3109195828437805\n",
      "Step: 14700  \tTraining accuracy: 0.851719319820404\n",
      "Step: 14700  \tValid loss: 0.3645312786102295\n",
      "Step: 14800  \tTraining loss: 0.3105347454547882\n",
      "Step: 14800  \tTraining accuracy: 0.8517730832099915\n",
      "Step: 14800  \tValid loss: 0.36422696709632874\n",
      "Step: 14900  \tTraining loss: 0.31020185351371765\n",
      "Step: 14900  \tTraining accuracy: 0.8518251776695251\n",
      "Step: 14900  \tValid loss: 0.36421671509742737\n",
      "Step: 15000  \tTraining loss: 0.30991560220718384\n",
      "Step: 15000  \tTraining accuracy: 0.8518766164779663\n",
      "Step: 15000  \tValid loss: 0.36405643820762634\n",
      "Step: 15100  \tTraining loss: 0.3096540868282318\n",
      "Step: 15100  \tTraining accuracy: 0.8519256711006165\n",
      "Step: 15100  \tValid loss: 0.3637757897377014\n",
      "Step: 15200  \tTraining loss: 0.3093988597393036\n",
      "Step: 15200  \tTraining accuracy: 0.8519673347473145\n",
      "Step: 15200  \tValid loss: 0.36358124017715454\n",
      "Step: 15300  \tTraining loss: 0.3091493546962738\n",
      "Step: 15300  \tTraining accuracy: 0.8520076274871826\n",
      "Step: 15300  \tValid loss: 0.36335307359695435\n",
      "Step: 15400  \tTraining loss: 0.30890607833862305\n",
      "Step: 15400  \tTraining accuracy: 0.8520440459251404\n",
      "Step: 15400  \tValid loss: 0.3631710410118103\n",
      "Step: 15500  \tTraining loss: 0.3086674213409424\n",
      "Step: 15500  \tTraining accuracy: 0.8520833253860474\n",
      "Step: 15500  \tValid loss: 0.3629955053329468\n",
      "Step: 15600  \tTraining loss: 0.30842745304107666\n",
      "Step: 15600  \tTraining accuracy: 0.8521294593811035\n",
      "Step: 15600  \tValid loss: 0.36272966861724854\n",
      "Step: 15700  \tTraining loss: 0.30818238854408264\n",
      "Step: 15700  \tTraining accuracy: 0.8521774411201477\n",
      "Step: 15700  \tValid loss: 0.3624281585216522\n",
      "Step: 15800  \tTraining loss: 0.30794110894203186\n",
      "Step: 15800  \tTraining accuracy: 0.8522264957427979\n",
      "Step: 15800  \tValid loss: 0.3622465431690216\n",
      "Step: 15900  \tTraining loss: 0.30770498514175415\n",
      "Step: 15900  \tTraining accuracy: 0.8522692322731018\n",
      "Step: 15900  \tValid loss: 0.36203569173812866\n",
      "Step: 16000  \tTraining loss: 0.3074706196784973\n",
      "Step: 16000  \tTraining accuracy: 0.8523074388504028\n",
      "Step: 16000  \tValid loss: 0.36184772849082947\n",
      "Step: 16100  \tTraining loss: 0.30723950266838074\n",
      "Step: 16100  \tTraining accuracy: 0.8523460030555725\n",
      "Step: 16100  \tValid loss: 0.36167484521865845\n",
      "Step: 16200  \tTraining loss: 0.307007759809494\n",
      "Step: 16200  \tTraining accuracy: 0.8523848652839661\n",
      "Step: 16200  \tValid loss: 0.36147627234458923\n",
      "Step: 16300  \tTraining loss: 0.306779682636261\n",
      "Step: 16300  \tTraining accuracy: 0.8524208664894104\n",
      "Step: 16300  \tValid loss: 0.36128848791122437\n",
      "Step: 16400  \tTraining loss: 0.3065531551837921\n",
      "Step: 16400  \tTraining accuracy: 0.8524564504623413\n",
      "Step: 16400  \tValid loss: 0.36108189821243286\n",
      "Step: 16500  \tTraining loss: 0.30632930994033813\n",
      "Step: 16500  \tTraining accuracy: 0.8524884581565857\n",
      "Step: 16500  \tValid loss: 0.3609601557254791\n",
      "Step: 16600  \tTraining loss: 0.3061060607433319\n",
      "Step: 16600  \tTraining accuracy: 0.85251784324646\n",
      "Step: 16600  \tValid loss: 0.360766738653183\n",
      "Step: 16700  \tTraining loss: 0.30588164925575256\n",
      "Step: 16700  \tTraining accuracy: 0.852548360824585\n",
      "Step: 16700  \tValid loss: 0.360638827085495\n",
      "Step: 16800  \tTraining loss: 0.3056602478027344\n",
      "Step: 16800  \tTraining accuracy: 0.8525830507278442\n",
      "Step: 16800  \tValid loss: 0.36043575406074524\n",
      "Step: 16900  \tTraining loss: 0.3054351508617401\n",
      "Step: 16900  \tTraining accuracy: 0.8526173830032349\n",
      "Step: 16900  \tValid loss: 0.36024776101112366\n",
      "Step: 17000  \tTraining loss: 0.30521249771118164\n",
      "Step: 17000  \tTraining accuracy: 0.8526512980461121\n",
      "Step: 17000  \tValid loss: 0.36002880334854126\n",
      "Step: 17100  \tTraining loss: 0.30498987436294556\n",
      "Step: 17100  \tTraining accuracy: 0.8526915311813354\n",
      "Step: 17100  \tValid loss: 0.35986796021461487\n",
      "Step: 17200  \tTraining loss: 0.30476775765419006\n",
      "Step: 17200  \tTraining accuracy: 0.8527350425720215\n",
      "Step: 17200  \tValid loss: 0.35962942242622375\n",
      "Step: 17300  \tTraining loss: 0.30454346537590027\n",
      "Step: 17300  \tTraining accuracy: 0.8527780771255493\n",
      "Step: 17300  \tValid loss: 0.35942214727401733\n",
      "Step: 17400  \tTraining loss: 0.3043200373649597\n",
      "Step: 17400  \tTraining accuracy: 0.8528190851211548\n",
      "Step: 17400  \tValid loss: 0.35922595858573914\n",
      "Step: 17500  \tTraining loss: 0.30409759283065796\n",
      "Step: 17500  \tTraining accuracy: 0.8528574705123901\n",
      "Step: 17500  \tValid loss: 0.35900071263313293\n",
      "Step: 17600  \tTraining loss: 0.30387577414512634\n",
      "Step: 17600  \tTraining accuracy: 0.8528953790664673\n",
      "Step: 17600  \tValid loss: 0.3587755858898163\n",
      "Step: 17700  \tTraining loss: 0.30365321040153503\n",
      "Step: 17700  \tTraining accuracy: 0.852932870388031\n",
      "Step: 17700  \tValid loss: 0.35852330923080444\n",
      "Step: 17800  \tTraining loss: 0.3034253418445587\n",
      "Step: 17800  \tTraining accuracy: 0.8529700040817261\n",
      "Step: 17800  \tValid loss: 0.35826683044433594\n",
      "Step: 17900  \tTraining loss: 0.3031890392303467\n",
      "Step: 17900  \tTraining accuracy: 0.8530066609382629\n",
      "Step: 17900  \tValid loss: 0.3580184876918793\n",
      "Step: 18000  \tTraining loss: 0.30295488238334656\n",
      "Step: 18000  \tTraining accuracy: 0.853044331073761\n",
      "Step: 18000  \tValid loss: 0.3577752709388733\n",
      "Step: 18100  \tTraining loss: 0.3027222156524658\n",
      "Step: 18100  \tTraining accuracy: 0.8530837297439575\n",
      "Step: 18100  \tValid loss: 0.3575442135334015\n",
      "Step: 18200  \tTraining loss: 0.3024911880493164\n",
      "Step: 18200  \tTraining accuracy: 0.8531226515769958\n",
      "Step: 18200  \tValid loss: 0.3573750853538513\n",
      "Step: 18300  \tTraining loss: 0.3022577166557312\n",
      "Step: 18300  \tTraining accuracy: 0.8531611561775208\n",
      "Step: 18300  \tValid loss: 0.3571656048297882\n",
      "Step: 18400  \tTraining loss: 0.3020259141921997\n",
      "Step: 18400  \tTraining accuracy: 0.8531985878944397\n",
      "Step: 18400  \tValid loss: 0.35698339343070984\n",
      "Step: 18500  \tTraining loss: 0.3017849028110504\n",
      "Step: 18500  \tTraining accuracy: 0.8532328009605408\n",
      "Step: 18500  \tValid loss: 0.356777161359787\n",
      "Step: 18600  \tTraining loss: 0.3015446066856384\n",
      "Step: 18600  \tTraining accuracy: 0.8532646298408508\n",
      "Step: 18600  \tValid loss: 0.35662969946861267\n",
      "Step: 18700  \tTraining loss: 0.3011596202850342\n",
      "Step: 18700  \tTraining accuracy: 0.8533015847206116\n",
      "Step: 18700  \tValid loss: 0.3567262887954712\n",
      "Step: 18800  \tTraining loss: 0.30087611079216003\n",
      "Step: 18800  \tTraining accuracy: 0.8533401489257812\n",
      "Step: 18800  \tValid loss: 0.35682472586631775\n",
      "Step: 18900  \tTraining loss: 0.30061307549476624\n",
      "Step: 18900  \tTraining accuracy: 0.8533810377120972\n",
      "Step: 18900  \tValid loss: 0.35678207874298096\n",
      "Step: 19000  \tTraining loss: 0.3003493547439575\n",
      "Step: 19000  \tTraining accuracy: 0.8534228205680847\n",
      "Step: 19000  \tValid loss: 0.3567640483379364\n",
      "Step: 19100  \tTraining loss: 0.30009374022483826\n",
      "Step: 19100  \tTraining accuracy: 0.8534648418426514\n",
      "Step: 19100  \tValid loss: 0.35669219493865967\n",
      "Step: 19200  \tTraining loss: 0.2998451888561249\n",
      "Step: 19200  \tTraining accuracy: 0.8535064458847046\n",
      "Step: 19200  \tValid loss: 0.3566499650478363\n",
      "Step: 19300  \tTraining loss: 0.2996053397655487\n",
      "Step: 19300  \tTraining accuracy: 0.8535502552986145\n",
      "Step: 19300  \tValid loss: 0.356570839881897\n",
      "Step: 19400  \tTraining loss: 0.29937344789505005\n",
      "Step: 19400  \tTraining accuracy: 0.8535943031311035\n",
      "Step: 19400  \tValid loss: 0.356469988822937\n",
      "Step: 19500  \tTraining loss: 0.2991489768028259\n",
      "Step: 19500  \tTraining accuracy: 0.8536385297775269\n",
      "Step: 19500  \tValid loss: 0.35638830065727234\n",
      "Step: 19600  \tTraining loss: 0.29893141984939575\n",
      "Step: 19600  \tTraining accuracy: 0.8536816835403442\n",
      "Step: 19600  \tValid loss: 0.35628461837768555\n",
      "Step: 19700  \tTraining loss: 0.29872047901153564\n",
      "Step: 19700  \tTraining accuracy: 0.8537243604660034\n",
      "Step: 19700  \tValid loss: 0.35619452595710754\n",
      "Step: 19800  \tTraining loss: 0.29851412773132324\n",
      "Step: 19800  \tTraining accuracy: 0.8537666201591492\n",
      "Step: 19800  \tValid loss: 0.35611069202423096\n",
      "Step: 19900  \tTraining loss: 0.2983153462409973\n",
      "Step: 19900  \tTraining accuracy: 0.8538103699684143\n",
      "Step: 19900  \tValid loss: 0.3560112416744232\n",
      "Step: 20000  \tTraining loss: 0.29812091588974\n",
      "Step: 20000  \tTraining accuracy: 0.8538530468940735\n",
      "Step: 20000  \tValid loss: 0.3559780716896057\n",
      "Step: 20100  \tTraining loss: 0.2979310154914856\n",
      "Step: 20100  \tTraining accuracy: 0.8538939952850342\n",
      "Step: 20100  \tValid loss: 0.35585474967956543\n",
      "Step: 20200  \tTraining loss: 0.29774799942970276\n",
      "Step: 20200  \tTraining accuracy: 0.8539358377456665\n",
      "Step: 20200  \tValid loss: 0.3557771146297455\n",
      "Step: 20300  \tTraining loss: 0.29756852984428406\n",
      "Step: 20300  \tTraining accuracy: 0.8539785146713257\n",
      "Step: 20300  \tValid loss: 0.3556913733482361\n",
      "Step: 20400  \tTraining loss: 0.2973938584327698\n",
      "Step: 20400  \tTraining accuracy: 0.8540151715278625\n",
      "Step: 20400  \tValid loss: 0.35561153292655945\n",
      "Step: 20500  \tTraining loss: 0.29722288250923157\n",
      "Step: 20500  \tTraining accuracy: 0.854051411151886\n",
      "Step: 20500  \tValid loss: 0.35554400086402893\n",
      "Step: 20600  \tTraining loss: 0.29703953862190247\n",
      "Step: 20600  \tTraining accuracy: 0.8540872931480408\n",
      "Step: 20600  \tValid loss: 0.35545864701271057\n",
      "Step: 20700  \tTraining loss: 0.2968503534793854\n",
      "Step: 20700  \tTraining accuracy: 0.8541228771209717\n",
      "Step: 20700  \tValid loss: 0.35544976592063904\n",
      "Step: 20800  \tTraining loss: 0.296669602394104\n",
      "Step: 20800  \tTraining accuracy: 0.8541586995124817\n",
      "Step: 20800  \tValid loss: 0.35535332560539246\n",
      "Step: 20900  \tTraining loss: 0.29649636149406433\n",
      "Step: 20900  \tTraining accuracy: 0.8541948199272156\n",
      "Step: 20900  \tValid loss: 0.3552684187889099\n",
      "Step: 21000  \tTraining loss: 0.2963307797908783\n",
      "Step: 21000  \tTraining accuracy: 0.8542293310165405\n",
      "Step: 21000  \tValid loss: 0.35516712069511414\n",
      "Step: 21100  \tTraining loss: 0.2961706221103668\n",
      "Step: 21100  \tTraining accuracy: 0.8542647361755371\n",
      "Step: 21100  \tValid loss: 0.35511770844459534\n",
      "Step: 21200  \tTraining loss: 0.29601627588272095\n",
      "Step: 21200  \tTraining accuracy: 0.8543016910552979\n",
      "Step: 21200  \tValid loss: 0.35503971576690674\n",
      "Step: 21300  \tTraining loss: 0.29586756229400635\n",
      "Step: 21300  \tTraining accuracy: 0.8543400168418884\n",
      "Step: 21300  \tValid loss: 0.3550090491771698\n",
      "Step: 21400  \tTraining loss: 0.2957209050655365\n",
      "Step: 21400  \tTraining accuracy: 0.8543792366981506\n",
      "Step: 21400  \tValid loss: 0.3549262285232544\n",
      "Step: 21500  \tTraining loss: 0.2955797016620636\n",
      "Step: 21500  \tTraining accuracy: 0.8544180393218994\n",
      "Step: 21500  \tValid loss: 0.35484039783477783\n",
      "Step: 21600  \tTraining loss: 0.295441597700119\n",
      "Step: 21600  \tTraining accuracy: 0.854457676410675\n",
      "Step: 21600  \tValid loss: 0.3547973036766052\n",
      "Step: 21700  \tTraining loss: 0.2953057289123535\n",
      "Step: 21700  \tTraining accuracy: 0.8544987440109253\n",
      "Step: 21700  \tValid loss: 0.3547177314758301\n",
      "Step: 21800  \tTraining loss: 0.2951737940311432\n",
      "Step: 21800  \tTraining accuracy: 0.8545393943786621\n",
      "Step: 21800  \tValid loss: 0.35466665029525757\n",
      "Step: 21900  \tTraining loss: 0.2950448989868164\n",
      "Step: 21900  \tTraining accuracy: 0.8545814752578735\n",
      "Step: 21900  \tValid loss: 0.35458406805992126\n",
      "Step: 22000  \tTraining loss: 0.2949175536632538\n",
      "Step: 22000  \tTraining accuracy: 0.854624330997467\n",
      "Step: 22000  \tValid loss: 0.3545283377170563\n",
      "Step: 22100  \tTraining loss: 0.29479122161865234\n",
      "Step: 22100  \tTraining accuracy: 0.8546679019927979\n",
      "Step: 22100  \tValid loss: 0.3544721305370331\n",
      "Step: 22200  \tTraining loss: 0.2946663796901703\n",
      "Step: 22200  \tTraining accuracy: 0.8547129034996033\n",
      "Step: 22200  \tValid loss: 0.35440900921821594\n",
      "Step: 22300  \tTraining loss: 0.29454362392425537\n",
      "Step: 22300  \tTraining accuracy: 0.8547574281692505\n",
      "Step: 22300  \tValid loss: 0.3543570041656494\n",
      "Step: 22400  \tTraining loss: 0.2944236099720001\n",
      "Step: 22400  \tTraining accuracy: 0.8548015356063843\n",
      "Step: 22400  \tValid loss: 0.3542872369289398\n",
      "Step: 22500  \tTraining loss: 0.2943083643913269\n",
      "Step: 22500  \tTraining accuracy: 0.8548436164855957\n",
      "Step: 22500  \tValid loss: 0.3542158603668213\n",
      "Step: 22600  \tTraining loss: 0.29419437050819397\n",
      "Step: 22600  \tTraining accuracy: 0.854884147644043\n",
      "Step: 22600  \tValid loss: 0.35418954491615295\n",
      "Step: 22700  \tTraining loss: 0.2940834164619446\n",
      "Step: 22700  \tTraining accuracy: 0.8549254536628723\n",
      "Step: 22700  \tValid loss: 0.3541298806667328\n",
      "Step: 22800  \tTraining loss: 0.29397404193878174\n",
      "Step: 22800  \tTraining accuracy: 0.8549658060073853\n",
      "Step: 22800  \tValid loss: 0.3541071116924286\n",
      "Step: 22900  \tTraining loss: 0.2938666343688965\n",
      "Step: 22900  \tTraining accuracy: 0.8550041913986206\n",
      "Step: 22900  \tValid loss: 0.3540448248386383\n",
      "Step: 23000  \tTraining loss: 0.293760746717453\n",
      "Step: 23000  \tTraining accuracy: 0.8550405502319336\n",
      "Step: 23000  \tValid loss: 0.3540172278881073\n",
      "Step: 23100  \tTraining loss: 0.2936566472053528\n",
      "Step: 23100  \tTraining accuracy: 0.8550765514373779\n",
      "Step: 23100  \tValid loss: 0.3539591431617737\n",
      "Step: 23200  \tTraining loss: 0.2935542166233063\n",
      "Step: 23200  \tTraining accuracy: 0.8551122546195984\n",
      "Step: 23200  \tValid loss: 0.35393741726875305\n",
      "Step: 23300  \tTraining loss: 0.29345256090164185\n",
      "Step: 23300  \tTraining accuracy: 0.8551493287086487\n",
      "Step: 23300  \tValid loss: 0.35393062233924866\n",
      "Step: 23400  \tTraining loss: 0.29335325956344604\n",
      "Step: 23400  \tTraining accuracy: 0.855187177658081\n",
      "Step: 23400  \tValid loss: 0.35388290882110596\n",
      "Step: 23500  \tTraining loss: 0.2932545244693756\n",
      "Step: 23500  \tTraining accuracy: 0.8552246689796448\n",
      "Step: 23500  \tValid loss: 0.35385093092918396\n",
      "Step: 23600  \tTraining loss: 0.293157696723938\n",
      "Step: 23600  \tTraining accuracy: 0.8552629947662354\n",
      "Step: 23600  \tValid loss: 0.35382747650146484\n",
      "Step: 23700  \tTraining loss: 0.2930624186992645\n",
      "Step: 23700  \tTraining accuracy: 0.8553025722503662\n",
      "Step: 23700  \tValid loss: 0.35379165410995483\n",
      "Step: 23800  \tTraining loss: 0.2929677963256836\n",
      "Step: 23800  \tTraining accuracy: 0.8553417921066284\n",
      "Step: 23800  \tValid loss: 0.3537614047527313\n",
      "Step: 23900  \tTraining loss: 0.29287418723106384\n",
      "Step: 23900  \tTraining accuracy: 0.8553807139396667\n",
      "Step: 23900  \tValid loss: 0.3537278175354004\n",
      "Step: 24000  \tTraining loss: 0.29278287291526794\n",
      "Step: 24000  \tTraining accuracy: 0.8554193377494812\n",
      "Step: 24000  \tValid loss: 0.3537028133869171\n",
      "Step: 24100  \tTraining loss: 0.29269203543663025\n",
      "Step: 24100  \tTraining accuracy: 0.8554602265357971\n",
      "Step: 24100  \tValid loss: 0.3537036180496216\n",
      "Step: 24200  \tTraining loss: 0.29260337352752686\n",
      "Step: 24200  \tTraining accuracy: 0.8555008172988892\n",
      "Step: 24200  \tValid loss: 0.35366711020469666\n",
      "Step: 24300  \tTraining loss: 0.292514443397522\n",
      "Step: 24300  \tTraining accuracy: 0.8555410504341125\n",
      "Step: 24300  \tValid loss: 0.3536457419395447\n",
      "Step: 24400  \tTraining loss: 0.29242831468582153\n",
      "Step: 24400  \tTraining accuracy: 0.8555809855461121\n",
      "Step: 24400  \tValid loss: 0.35360389947891235\n",
      "Step: 24500  \tTraining loss: 0.29234278202056885\n",
      "Step: 24500  \tTraining accuracy: 0.8556205630302429\n",
      "Step: 24500  \tValid loss: 0.35360807180404663\n",
      "Step: 24600  \tTraining loss: 0.2922576069831848\n",
      "Step: 24600  \tTraining accuracy: 0.8556598424911499\n",
      "Step: 24600  \tValid loss: 0.3535926640033722\n",
      "Step: 24700  \tTraining loss: 0.2921735346317291\n",
      "Step: 24700  \tTraining accuracy: 0.855698823928833\n",
      "Step: 24700  \tValid loss: 0.35356950759887695\n",
      "Step: 24800  \tTraining loss: 0.29208993911743164\n",
      "Step: 24800  \tTraining accuracy: 0.8557374477386475\n",
      "Step: 24800  \tValid loss: 0.35353824496269226\n",
      "Step: 24900  \tTraining loss: 0.29200616478919983\n",
      "Step: 24900  \tTraining accuracy: 0.855775773525238\n",
      "Step: 24900  \tValid loss: 0.3534878194332123\n",
      "Step: 25000  \tTraining loss: 0.29188093543052673\n",
      "Step: 25000  \tTraining accuracy: 0.8558138012886047\n",
      "Step: 25000  \tValid loss: 0.3535223603248596\n",
      "Step: 25100  \tTraining loss: 0.2917773425579071\n",
      "Step: 25100  \tTraining accuracy: 0.8558515310287476\n",
      "Step: 25100  \tValid loss: 0.35359328985214233\n",
      "Step: 25200  \tTraining loss: 0.2916845679283142\n",
      "Step: 25200  \tTraining accuracy: 0.8558904528617859\n",
      "Step: 25200  \tValid loss: 0.3536492586135864\n",
      "Step: 25300  \tTraining loss: 0.29158687591552734\n",
      "Step: 25300  \tTraining accuracy: 0.8559300899505615\n",
      "Step: 25300  \tValid loss: 0.35366520285606384\n",
      "Step: 25400  \tTraining loss: 0.2914903461933136\n",
      "Step: 25400  \tTraining accuracy: 0.8559694290161133\n",
      "Step: 25400  \tValid loss: 0.35368582606315613\n",
      "Step: 25500  \tTraining loss: 0.2914007306098938\n",
      "Step: 25500  \tTraining accuracy: 0.8560059666633606\n",
      "Step: 25500  \tValid loss: 0.35366514325141907\n",
      "Step: 25600  \tTraining loss: 0.2913140058517456\n",
      "Step: 25600  \tTraining accuracy: 0.8560421466827393\n",
      "Step: 25600  \tValid loss: 0.3536711633205414\n",
      "Step: 25700  \tTraining loss: 0.2912287712097168\n",
      "Step: 25700  \tTraining accuracy: 0.8560780882835388\n",
      "Step: 25700  \tValid loss: 0.3536880910396576\n",
      "Step: 25800  \tTraining loss: 0.2911446988582611\n",
      "Step: 25800  \tTraining accuracy: 0.8561137914657593\n",
      "Step: 25800  \tValid loss: 0.3536861538887024\n",
      "Step: 25900  \tTraining loss: 0.2910613715648651\n",
      "Step: 25900  \tTraining accuracy: 0.8561491370201111\n",
      "Step: 25900  \tValid loss: 0.3537159562110901\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8561843\n",
      "Precision: 0.88637024\n",
      "Recall: 0.9664132\n",
      "F1 score: 0.8931727\n",
      "AUC: 0.7256309\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.856184    0.88637  0.966413  0.893173  0.725631  0.291036      0.856152   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.353459       0.856173   0.310068      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  25929.0  \n",
      "18\n",
      "(5945, 8)\n",
      "(5945, 1)\n",
      "(3200, 8)\n",
      "(3200, 1)\n",
      "(2600, 8)\n",
      "(2600, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.49792909622192383\n",
      "Step: 100  \tTraining accuracy: 0.7820016741752625\n",
      "Step: 100  \tValid loss: 0.5630130171775818\n",
      "Step: 200  \tTraining loss: 0.45736363530158997\n",
      "Step: 200  \tTraining accuracy: 0.7678914666175842\n",
      "Step: 200  \tValid loss: 0.5183535814285278\n",
      "Step: 300  \tTraining loss: 0.44424566626548767\n",
      "Step: 300  \tTraining accuracy: 0.7712587118148804\n",
      "Step: 300  \tValid loss: 0.5014468431472778\n",
      "Step: 400  \tTraining loss: 0.44173282384872437\n",
      "Step: 400  \tTraining accuracy: 0.7783875465393066\n",
      "Step: 400  \tValid loss: 0.49768248200416565\n",
      "Step: 500  \tTraining loss: 0.43894895911216736\n",
      "Step: 500  \tTraining accuracy: 0.7825602293014526\n",
      "Step: 500  \tValid loss: 0.49379339814186096\n",
      "Step: 600  \tTraining loss: 0.433692067861557\n",
      "Step: 600  \tTraining accuracy: 0.7856502532958984\n",
      "Step: 600  \tValid loss: 0.48733848333358765\n",
      "Step: 700  \tTraining loss: 0.424146831035614\n",
      "Step: 700  \tTraining accuracy: 0.7884577512741089\n",
      "Step: 700  \tValid loss: 0.47685784101486206\n",
      "Step: 800  \tTraining loss: 0.41703686118125916\n",
      "Step: 800  \tTraining accuracy: 0.7910390496253967\n",
      "Step: 800  \tValid loss: 0.47037452459335327\n",
      "Step: 900  \tTraining loss: 0.4130186438560486\n",
      "Step: 900  \tTraining accuracy: 0.7935038208961487\n",
      "Step: 900  \tValid loss: 0.467085063457489\n",
      "Step: 1000  \tTraining loss: 0.41050824522972107\n",
      "Step: 1000  \tTraining accuracy: 0.7957277297973633\n",
      "Step: 1000  \tValid loss: 0.4648516774177551\n",
      "Step: 1100  \tTraining loss: 0.40860265493392944\n",
      "Step: 1100  \tTraining accuracy: 0.7977146506309509\n",
      "Step: 1100  \tValid loss: 0.46288084983825684\n",
      "Step: 1200  \tTraining loss: 0.40689417719841003\n",
      "Step: 1200  \tTraining accuracy: 0.79935622215271\n",
      "Step: 1200  \tValid loss: 0.46091192960739136\n",
      "Step: 1300  \tTraining loss: 0.40544062852859497\n",
      "Step: 1300  \tTraining accuracy: 0.8007080554962158\n",
      "Step: 1300  \tValid loss: 0.4591864049434662\n",
      "Step: 1400  \tTraining loss: 0.404062956571579\n",
      "Step: 1400  \tTraining accuracy: 0.8017650842666626\n",
      "Step: 1400  \tValid loss: 0.4574616253376007\n",
      "Step: 1500  \tTraining loss: 0.4028151035308838\n",
      "Step: 1500  \tTraining accuracy: 0.8026353716850281\n",
      "Step: 1500  \tValid loss: 0.4558781087398529\n",
      "Step: 1600  \tTraining loss: 0.40171414613723755\n",
      "Step: 1600  \tTraining accuracy: 0.8034757375717163\n",
      "Step: 1600  \tValid loss: 0.4546210467815399\n",
      "Step: 1700  \tTraining loss: 0.4007261395454407\n",
      "Step: 1700  \tTraining accuracy: 0.8042813539505005\n",
      "Step: 1700  \tValid loss: 0.4535650610923767\n",
      "Step: 1800  \tTraining loss: 0.39990484714508057\n",
      "Step: 1800  \tTraining accuracy: 0.8049900531768799\n",
      "Step: 1800  \tValid loss: 0.452677458524704\n",
      "Step: 1900  \tTraining loss: 0.3991750180721283\n",
      "Step: 1900  \tTraining accuracy: 0.8056267499923706\n",
      "Step: 1900  \tValid loss: 0.45196449756622314\n",
      "Step: 2000  \tTraining loss: 0.3985004127025604\n",
      "Step: 2000  \tTraining accuracy: 0.8062942028045654\n",
      "Step: 2000  \tValid loss: 0.4513823390007019\n",
      "Step: 2100  \tTraining loss: 0.39786458015441895\n",
      "Step: 2100  \tTraining accuracy: 0.8069173097610474\n",
      "Step: 2100  \tValid loss: 0.4509657621383667\n",
      "Step: 2200  \tTraining loss: 0.39727503061294556\n",
      "Step: 2200  \tTraining accuracy: 0.8075101971626282\n",
      "Step: 2200  \tValid loss: 0.4506545960903168\n",
      "Step: 2300  \tTraining loss: 0.39670875668525696\n",
      "Step: 2300  \tTraining accuracy: 0.808073103427887\n",
      "Step: 2300  \tValid loss: 0.4504709243774414\n",
      "Step: 2400  \tTraining loss: 0.396182656288147\n",
      "Step: 2400  \tTraining accuracy: 0.8086025714874268\n",
      "Step: 2400  \tValid loss: 0.45027464628219604\n",
      "Step: 2500  \tTraining loss: 0.395663857460022\n",
      "Step: 2500  \tTraining accuracy: 0.8091305494308472\n",
      "Step: 2500  \tValid loss: 0.45006638765335083\n",
      "Step: 2600  \tTraining loss: 0.39512327313423157\n",
      "Step: 2600  \tTraining accuracy: 0.8096104264259338\n",
      "Step: 2600  \tValid loss: 0.4497494399547577\n",
      "Step: 2700  \tTraining loss: 0.39456409215927124\n",
      "Step: 2700  \tTraining accuracy: 0.8100765943527222\n",
      "Step: 2700  \tValid loss: 0.44936829805374146\n",
      "Step: 2800  \tTraining loss: 0.3939748704433441\n",
      "Step: 2800  \tTraining accuracy: 0.8105460405349731\n",
      "Step: 2800  \tValid loss: 0.44899827241897583\n",
      "Step: 2900  \tTraining loss: 0.3934020698070526\n",
      "Step: 2900  \tTraining accuracy: 0.8109645843505859\n",
      "Step: 2900  \tValid loss: 0.4486485421657562\n",
      "Step: 3000  \tTraining loss: 0.39286312460899353\n",
      "Step: 3000  \tTraining accuracy: 0.8113749623298645\n",
      "Step: 3000  \tValid loss: 0.4483911991119385\n",
      "Step: 3100  \tTraining loss: 0.39237669110298157\n",
      "Step: 3100  \tTraining accuracy: 0.8117780089378357\n",
      "Step: 3100  \tValid loss: 0.44816628098487854\n",
      "Step: 3200  \tTraining loss: 0.39191383123397827\n",
      "Step: 3200  \tTraining accuracy: 0.812160849571228\n",
      "Step: 3200  \tValid loss: 0.44790634512901306\n",
      "Step: 3300  \tTraining loss: 0.39145800471305847\n",
      "Step: 3300  \tTraining accuracy: 0.8125253915786743\n",
      "Step: 3300  \tValid loss: 0.4476349949836731\n",
      "Step: 3400  \tTraining loss: 0.3910478949546814\n",
      "Step: 3400  \tTraining accuracy: 0.8128605484962463\n",
      "Step: 3400  \tValid loss: 0.44735032320022583\n",
      "Step: 3500  \tTraining loss: 0.3906845450401306\n",
      "Step: 3500  \tTraining accuracy: 0.8131811618804932\n",
      "Step: 3500  \tValid loss: 0.4472864866256714\n",
      "Step: 3600  \tTraining loss: 0.3903520703315735\n",
      "Step: 3600  \tTraining accuracy: 0.8134861588478088\n",
      "Step: 3600  \tValid loss: 0.4471830725669861\n",
      "Step: 3700  \tTraining loss: 0.39004674553871155\n",
      "Step: 3700  \tTraining accuracy: 0.8137814402580261\n",
      "Step: 3700  \tValid loss: 0.44715845584869385\n",
      "Step: 3800  \tTraining loss: 0.3897095322608948\n",
      "Step: 3800  \tTraining accuracy: 0.8140655159950256\n",
      "Step: 3800  \tValid loss: 0.44703781604766846\n",
      "Step: 3900  \tTraining loss: 0.389328271150589\n",
      "Step: 3900  \tTraining accuracy: 0.8143525123596191\n",
      "Step: 3900  \tValid loss: 0.4470706284046173\n",
      "Step: 4000  \tTraining loss: 0.3889673948287964\n",
      "Step: 4000  \tTraining accuracy: 0.814629316329956\n",
      "Step: 4000  \tValid loss: 0.4469470679759979\n",
      "Step: 4100  \tTraining loss: 0.388596773147583\n",
      "Step: 4100  \tTraining accuracy: 0.8148735165596008\n",
      "Step: 4100  \tValid loss: 0.44675594568252563\n",
      "Step: 4200  \tTraining loss: 0.38824138045310974\n",
      "Step: 4200  \tTraining accuracy: 0.8151223659515381\n",
      "Step: 4200  \tValid loss: 0.44660484790802\n",
      "Step: 4300  \tTraining loss: 0.3879213035106659\n",
      "Step: 4300  \tTraining accuracy: 0.815361499786377\n",
      "Step: 4300  \tValid loss: 0.4464443325996399\n",
      "Step: 4400  \tTraining loss: 0.387612521648407\n",
      "Step: 4400  \tTraining accuracy: 0.8155994415283203\n",
      "Step: 4400  \tValid loss: 0.4463467001914978\n",
      "Step: 4500  \tTraining loss: 0.38730689883232117\n",
      "Step: 4500  \tTraining accuracy: 0.8158285617828369\n",
      "Step: 4500  \tValid loss: 0.4462701380252838\n",
      "Step: 4600  \tTraining loss: 0.38700926303863525\n",
      "Step: 4600  \tTraining accuracy: 0.8160495162010193\n",
      "Step: 4600  \tValid loss: 0.4462248980998993\n",
      "Step: 4700  \tTraining loss: 0.38661542534828186\n",
      "Step: 4700  \tTraining accuracy: 0.8162591457366943\n",
      "Step: 4700  \tValid loss: 0.4459434449672699\n",
      "Step: 4800  \tTraining loss: 0.38599613308906555\n",
      "Step: 4800  \tTraining accuracy: 0.8164688944816589\n",
      "Step: 4800  \tValid loss: 0.44545993208885193\n",
      "Step: 4900  \tTraining loss: 0.38557901978492737\n",
      "Step: 4900  \tTraining accuracy: 0.8166753053665161\n",
      "Step: 4900  \tValid loss: 0.44538700580596924\n",
      "Step: 5000  \tTraining loss: 0.38525572419166565\n",
      "Step: 5000  \tTraining accuracy: 0.8168681859970093\n",
      "Step: 5000  \tValid loss: 0.44524911046028137\n",
      "Step: 5100  \tTraining loss: 0.3849535584449768\n",
      "Step: 5100  \tTraining accuracy: 0.8170416355133057\n",
      "Step: 5100  \tValid loss: 0.44510406255722046\n",
      "Step: 5200  \tTraining loss: 0.3846702575683594\n",
      "Step: 5200  \tTraining accuracy: 0.8172017335891724\n",
      "Step: 5200  \tValid loss: 0.4449499547481537\n",
      "Step: 5300  \tTraining loss: 0.3844071626663208\n",
      "Step: 5300  \tTraining accuracy: 0.8173556923866272\n",
      "Step: 5300  \tValid loss: 0.4448207914829254\n",
      "Step: 5400  \tTraining loss: 0.3841543197631836\n",
      "Step: 5400  \tTraining accuracy: 0.8174943923950195\n",
      "Step: 5400  \tValid loss: 0.44475993514060974\n",
      "Step: 5500  \tTraining loss: 0.3839167654514313\n",
      "Step: 5500  \tTraining accuracy: 0.8176357746124268\n",
      "Step: 5500  \tValid loss: 0.4446773827075958\n",
      "Step: 5600  \tTraining loss: 0.38368746638298035\n",
      "Step: 5600  \tTraining accuracy: 0.8177644610404968\n",
      "Step: 5600  \tValid loss: 0.4445981979370117\n",
      "Step: 5700  \tTraining loss: 0.38347145915031433\n",
      "Step: 5700  \tTraining accuracy: 0.8178794980049133\n",
      "Step: 5700  \tValid loss: 0.44455578923225403\n",
      "Step: 5800  \tTraining loss: 0.383263498544693\n",
      "Step: 5800  \tTraining accuracy: 0.8179905414581299\n",
      "Step: 5800  \tValid loss: 0.4444694519042969\n",
      "Step: 5900  \tTraining loss: 0.38306644558906555\n",
      "Step: 5900  \tTraining accuracy: 0.8181006908416748\n",
      "Step: 5900  \tValid loss: 0.4443861246109009\n",
      "Step: 6000  \tTraining loss: 0.3828810751438141\n",
      "Step: 6000  \tTraining accuracy: 0.8182029128074646\n",
      "Step: 6000  \tValid loss: 0.44435256719589233\n",
      "Step: 6100  \tTraining loss: 0.3827058970928192\n",
      "Step: 6100  \tTraining accuracy: 0.8183059096336365\n",
      "Step: 6100  \tValid loss: 0.44427749514579773\n",
      "Step: 6200  \tTraining loss: 0.3825375735759735\n",
      "Step: 6200  \tTraining accuracy: 0.8184041976928711\n",
      "Step: 6200  \tValid loss: 0.44421809911727905\n",
      "Step: 6300  \tTraining loss: 0.38237470388412476\n",
      "Step: 6300  \tTraining accuracy: 0.8184979557991028\n",
      "Step: 6300  \tValid loss: 0.4441300928592682\n",
      "Step: 6400  \tTraining loss: 0.38221439719200134\n",
      "Step: 6400  \tTraining accuracy: 0.8185887932777405\n",
      "Step: 6400  \tValid loss: 0.44411879777908325\n",
      "Step: 6500  \tTraining loss: 0.382057249546051\n",
      "Step: 6500  \tTraining accuracy: 0.818676769733429\n",
      "Step: 6500  \tValid loss: 0.4440726339817047\n",
      "Step: 6600  \tTraining loss: 0.3819043040275574\n",
      "Step: 6600  \tTraining accuracy: 0.8187581896781921\n",
      "Step: 6600  \tValid loss: 0.4440234303474426\n",
      "Step: 6700  \tTraining loss: 0.3817543089389801\n",
      "Step: 6700  \tTraining accuracy: 0.8188384771347046\n",
      "Step: 6700  \tValid loss: 0.44399893283843994\n",
      "Step: 6800  \tTraining loss: 0.3816070258617401\n",
      "Step: 6800  \tTraining accuracy: 0.8189289569854736\n",
      "Step: 6800  \tValid loss: 0.4439421594142914\n",
      "Step: 6900  \tTraining loss: 0.38146132230758667\n",
      "Step: 6900  \tTraining accuracy: 0.8190205097198486\n",
      "Step: 6900  \tValid loss: 0.44393062591552734\n",
      "Step: 7000  \tTraining loss: 0.3813112676143646\n",
      "Step: 7000  \tTraining accuracy: 0.8191069960594177\n",
      "Step: 7000  \tValid loss: 0.44377246499061584\n",
      "Step: 7100  \tTraining loss: 0.3811657428741455\n",
      "Step: 7100  \tTraining accuracy: 0.8191946744918823\n",
      "Step: 7100  \tValid loss: 0.44368991255760193\n",
      "Step: 7200  \tTraining loss: 0.3809988498687744\n",
      "Step: 7200  \tTraining accuracy: 0.8192762732505798\n",
      "Step: 7200  \tValid loss: 0.44364356994628906\n",
      "Step: 7300  \tTraining loss: 0.38084930181503296\n",
      "Step: 7300  \tTraining accuracy: 0.8193591833114624\n",
      "Step: 7300  \tValid loss: 0.4435821771621704\n",
      "Step: 7400  \tTraining loss: 0.3807061016559601\n",
      "Step: 7400  \tTraining accuracy: 0.8194456100463867\n",
      "Step: 7400  \tValid loss: 0.443548321723938\n",
      "Step: 7500  \tTraining loss: 0.38056254386901855\n",
      "Step: 7500  \tTraining accuracy: 0.8195263147354126\n",
      "Step: 7500  \tValid loss: 0.44348305463790894\n",
      "Step: 7600  \tTraining loss: 0.38042566180229187\n",
      "Step: 7600  \tTraining accuracy: 0.8196048736572266\n",
      "Step: 7600  \tValid loss: 0.44342857599258423\n",
      "Step: 7700  \tTraining loss: 0.3802899420261383\n",
      "Step: 7700  \tTraining accuracy: 0.8196847438812256\n",
      "Step: 7700  \tValid loss: 0.4434330463409424\n",
      "Step: 7800  \tTraining loss: 0.38015520572662354\n",
      "Step: 7800  \tTraining accuracy: 0.8197668790817261\n",
      "Step: 7800  \tValid loss: 0.4434014558792114\n",
      "Step: 7900  \tTraining loss: 0.3800197243690491\n",
      "Step: 7900  \tTraining accuracy: 0.8198469877243042\n",
      "Step: 7900  \tValid loss: 0.44337350130081177\n",
      "Step: 8000  \tTraining loss: 0.3798874020576477\n",
      "Step: 8000  \tTraining accuracy: 0.8199250102043152\n",
      "Step: 8000  \tValid loss: 0.4433518350124359\n",
      "Step: 8100  \tTraining loss: 0.3797592520713806\n",
      "Step: 8100  \tTraining accuracy: 0.8200011849403381\n",
      "Step: 8100  \tValid loss: 0.44336608052253723\n",
      "Step: 8200  \tTraining loss: 0.3796200752258301\n",
      "Step: 8200  \tTraining accuracy: 0.8200733065605164\n",
      "Step: 8200  \tValid loss: 0.44316184520721436\n",
      "Step: 8300  \tTraining loss: 0.3794916272163391\n",
      "Step: 8300  \tTraining accuracy: 0.8201478719711304\n",
      "Step: 8300  \tValid loss: 0.4431988596916199\n",
      "Step: 8400  \tTraining loss: 0.37936365604400635\n",
      "Step: 8400  \tTraining accuracy: 0.8202216625213623\n",
      "Step: 8400  \tValid loss: 0.44319164752960205\n",
      "Step: 8500  \tTraining loss: 0.3792354464530945\n",
      "Step: 8500  \tTraining accuracy: 0.8202967047691345\n",
      "Step: 8500  \tValid loss: 0.44324687123298645\n",
      "Step: 8600  \tTraining loss: 0.3791077136993408\n",
      "Step: 8600  \tTraining accuracy: 0.8203700184822083\n",
      "Step: 8600  \tValid loss: 0.4433017373085022\n",
      "Step: 8700  \tTraining loss: 0.37898197770118713\n",
      "Step: 8700  \tTraining accuracy: 0.8204435706138611\n",
      "Step: 8700  \tValid loss: 0.44333404302597046\n",
      "Step: 8800  \tTraining loss: 0.37885722517967224\n",
      "Step: 8800  \tTraining accuracy: 0.820518434047699\n",
      "Step: 8800  \tValid loss: 0.4433593451976776\n",
      "Step: 8900  \tTraining loss: 0.378733366727829\n",
      "Step: 8900  \tTraining accuracy: 0.8205915093421936\n",
      "Step: 8900  \tValid loss: 0.44338828325271606\n",
      "Step: 9000  \tTraining loss: 0.3786117136478424\n",
      "Step: 9000  \tTraining accuracy: 0.8206630349159241\n",
      "Step: 9000  \tValid loss: 0.44340232014656067\n",
      "Step: 9100  \tTraining loss: 0.3784914016723633\n",
      "Step: 9100  \tTraining accuracy: 0.8207329511642456\n",
      "Step: 9100  \tValid loss: 0.4433823823928833\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8208013\n",
      "Precision: 0.8566467\n",
      "Recall: 0.9550441\n",
      "F1 score: 0.86538106\n",
      "AUC: 0.69087076\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.820801   0.856647  0.955044  0.865381  0.690871  0.378423      0.820695   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0   0.44311       0.820759   0.388325      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  9156.0  \n",
      "19\n",
      "(4495, 8)\n",
      "(4495, 1)\n",
      "(2400, 8)\n",
      "(2400, 1)\n",
      "(1950, 8)\n",
      "(1950, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4391610324382782\n",
      "Step: 100  \tTraining accuracy: 0.8335928916931152\n",
      "Step: 100  \tValid loss: 0.452732652425766\n",
      "Step: 200  \tTraining loss: 0.43458986282348633\n",
      "Step: 200  \tTraining accuracy: 0.8318590521812439\n",
      "Step: 200  \tValid loss: 0.44755634665489197\n",
      "Step: 300  \tTraining loss: 0.4321216344833374\n",
      "Step: 300  \tTraining accuracy: 0.8315078020095825\n",
      "Step: 300  \tValid loss: 0.4455445408821106\n",
      "Step: 400  \tTraining loss: 0.42903873324394226\n",
      "Step: 400  \tTraining accuracy: 0.8313567638397217\n",
      "Step: 400  \tValid loss: 0.44373422861099243\n",
      "Step: 500  \tTraining loss: 0.42561081051826477\n",
      "Step: 500  \tTraining accuracy: 0.8313479423522949\n",
      "Step: 500  \tValid loss: 0.442055344581604\n",
      "Step: 600  \tTraining loss: 0.42261719703674316\n",
      "Step: 600  \tTraining accuracy: 0.831691324710846\n",
      "Step: 600  \tValid loss: 0.44070759415626526\n",
      "Step: 700  \tTraining loss: 0.42008623480796814\n",
      "Step: 700  \tTraining accuracy: 0.8319464921951294\n",
      "Step: 700  \tValid loss: 0.43938854336738586\n",
      "Step: 800  \tTraining loss: 0.41790497303009033\n",
      "Step: 800  \tTraining accuracy: 0.8320132493972778\n",
      "Step: 800  \tValid loss: 0.43792593479156494\n",
      "Step: 900  \tTraining loss: 0.41617801785469055\n",
      "Step: 900  \tTraining accuracy: 0.8320111632347107\n",
      "Step: 900  \tValid loss: 0.43678081035614014\n",
      "Step: 1000  \tTraining loss: 0.4147716164588928\n",
      "Step: 1000  \tTraining accuracy: 0.8319738507270813\n",
      "Step: 1000  \tValid loss: 0.43581482768058777\n",
      "Step: 1100  \tTraining loss: 0.4135039150714874\n",
      "Step: 1100  \tTraining accuracy: 0.8318682909011841\n",
      "Step: 1100  \tValid loss: 0.4349191188812256\n",
      "Step: 1200  \tTraining loss: 0.4123072922229767\n",
      "Step: 1200  \tTraining accuracy: 0.8317614793777466\n",
      "Step: 1200  \tValid loss: 0.4340921938419342\n"
     ]
    }
   ],
   "source": [
    "neurons = 8\n",
    "\n",
    "\n",
    "for num, subj_file_path in enumerate(subj_files_list):\n",
    "    print(num)\n",
    "# for subj_file_path in [subj_files_list[0]]:\n",
    "    \n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "                \n",
    "#     file_path = file_path + \"/OddEvenPlays\"\n",
    "    file_path = file_path + \"/OddEvenPlays/RandomizedPlays10\"\n",
    "\n",
    "    train_data_df= pd.read_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df = pd.read_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df = pd.read_csv(file_path+\"/val_data.csv\")\n",
    "\n",
    " \n",
    "    train_X, train_y, test_X, test_y,val_X,val_y = data_split_odd_even(train_data_df,test_data_df,val_data_df)\n",
    "\n",
    "    pretraining = False;\n",
    "    metric_out_df, prob_train, prob_test, prob_val = train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y)\n",
    "    \n",
    "    print(metric_out_df)\n",
    "    \n",
    "    metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    " \n",
    "    prob_train_df = pd.DataFrame(prob_train.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_test_df = pd.DataFrame(prob_test.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_val_df = pd.DataFrame(prob_val.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "\n",
    "\n",
    "# ################################\n",
    "    prob_train_df.to_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_test_df.to_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_val_df.to_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "# #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
