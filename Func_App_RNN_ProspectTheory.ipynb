{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as sc_stats\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "onehot_encoder=OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "time_steps = 1\n",
    "inputs = 8\n",
    "outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_releveant_features(task_df):\n",
    "\n",
    "\n",
    "    task_df['PrevOutcome']=task_df['Outcome'].shift(1)\n",
    "    task_df.loc[1,'PrevOutcome']= 0\n",
    "\n",
    "    task_df['PrevChoice']=task_df['Choice'].shift(1)\n",
    "    task_df.loc[1,'PrevChoice']= 0\n",
    "\n",
    "    task_df['PrevSafe']=task_df['Safe'].shift(1)\n",
    "    task_df.loc[1,'PrevSafe']= 0\n",
    "\n",
    "    task_df['PrevBigRisky']=task_df['BigRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevBigRisky']= 0\n",
    "\n",
    "    task_df['PrevSmallRisky']=task_df['SmallRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevSmallRisky']= 0\n",
    "    \n",
    "#     task_df['PrevRT']=task_df['RT'].shift(1)\n",
    "#     task_df.loc[1,'PrevRT']= N\n",
    "    \n",
    "    \n",
    "    \n",
    "    return task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_split_data(data,start_chunk,end_chunk):\n",
    "    \n",
    "    a=[k for k in range(start_chunk,end_chunk)]\n",
    "    out=[]\n",
    "\n",
    "    for d in range(0,data.shape[0],20):\n",
    "\n",
    "        c= [c+d for c in a]\n",
    "        out = out+c\n",
    "\n",
    "    while out[-1]>=data.shape[0]-1:\n",
    "        out.pop()\n",
    "#     return out\n",
    "    return data[out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y): \n",
    "    reset_graph()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50000\n",
    "    batch_size = int(train_X.shape[0]/2)\n",
    "    # batch_size = 100\n",
    "    length = train_X.shape[0]\n",
    "    display = 100\n",
    "    neurons = neurons\n",
    "\n",
    "    num_batches = 100\n",
    "    seq_len = 10\n",
    "\n",
    "    percent_above_PT = 1\n",
    "\n",
    "    train_threshold = 1.5#PT_R2 + percent_above_PT\n",
    "\n",
    "\n",
    "    save_step = 100\n",
    "\n",
    "\n",
    "    best_loss_val = np.infty\n",
    "    checks_since_last_progress = 0\n",
    "    max_checks_without_progress = 1000\n",
    "\n",
    "\n",
    "    # clear graph (if any) before running\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, time_steps, inputs])\n",
    "\n",
    "    y = tf.placeholder(tf.float32, [None, outputs])\n",
    "\n",
    "    # LSTM Cell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=neurons, activation=tf.nn.relu)\n",
    "    cell_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "    # pass into Dense layer\n",
    "    stacked_outputs = tf.reshape(cell_outputs, [-1, neurons])\n",
    "    out = tf.layers.dense(inputs=stacked_outputs, units=outputs)\n",
    "\n",
    "    probability = tf.nn.softmax(out)\n",
    "\n",
    "    # squared error loss or cost function for linear regression\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=y, logits=out))\n",
    "\n",
    "    # optimizer to minimize cost\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels =  tf.argmax(y, 1),\n",
    "                          predictions = tf.argmax(out, 1),\n",
    "                          name = \"accuracy\")\n",
    "    precision = tf.metrics.precision(labels=tf.argmax(y, 1),\n",
    "                                 predictions=tf.argmax(out, 1),\n",
    "                                 name=\"precision\")\n",
    "    recall = tf.metrics.recall(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"recall\")\n",
    "    f1 = 2 * accuracy[1] * recall[1] / ( precision[1] + recall[1] )\n",
    "\n",
    "    acc_up,acc_val = accuracy\n",
    "    auc = tf.metrics.auc(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"auc\")\n",
    "    \n",
    "    valid_store = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #######################\n",
    "#         saver.restore(sess, \"./checkpts/Original_RNN_LSTM_8features_v2.ckpt\")\n",
    "#         saver.restore(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "        \n",
    "        if pretraining == True:\n",
    "\n",
    "            saver.restore(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        #######################\n",
    "        \n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "\n",
    "        # Train the model\n",
    "        for steps in range(epochs):\n",
    "            mini_batch = zip(range(0, length, batch_size),\n",
    "                       range(batch_size, length+1, batch_size))\n",
    "\n",
    "            # train data in mini-batches\n",
    "            for (start, end) in mini_batch:\n",
    "    #             print(start,end)\n",
    "                sess.run(training_op, feed_dict = {X: train_X[start:end,:,:],\n",
    "                                                   y: train_y[start:end,:]}) \n",
    "\n",
    "            ## train data in batches of length subsequence\n",
    "\n",
    "    #         for k in range(num_batches):\n",
    "    #             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "\n",
    "    #             sess.run(training_op, feed_dict = {X:X_seq,y:y_seq}) \n",
    "            loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "            loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "            # print training performance \n",
    "            if (steps+1) % display == 0:\n",
    "                # evaluate loss function on training set\n",
    "\n",
    "\n",
    "                loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining loss: {}'.format((steps+1), loss_fn))\n",
    "\n",
    "                acc_train = acc_val.eval(feed_dict={X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining accuracy: {}'.format((steps+1), acc_train))\n",
    "\n",
    "\n",
    "                acc_test = acc_val.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest accuracy: {}'.format((steps+1), acc_test))\n",
    "\n",
    "                loss_test = loss.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest loss: {}'.format((steps+1), loss_test))\n",
    "\n",
    "                accu_val = acc_val.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "                loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "                print('Step: {}  \\tValid loss: {}'.format((steps+1), loss_val))\n",
    "\n",
    "                valid_store.append(loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (1 + loss_fn/np.log(0.5)) > train_threshold:\n",
    "                    print(\"Threshold achieved, quit training\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            if loss_val < best_loss_val:\n",
    "\n",
    "                        best_loss_val = loss_val\n",
    "                        checks_since_last_progress = 0\n",
    "            else:\n",
    "                            checks_since_last_progress += 1\n",
    "\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if checks_since_last_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "\n",
    "            if (steps+1) % save_step ==0:\n",
    "                                save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#                 save_path = saver.save(sess, \"./checkpts/RNN_Internet_LSTM_model_5features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     evaluate model accuracy\n",
    "        acc, prec, recall, f1, AUC = sess.run([accuracy, precision, recall, f1,auc],\n",
    "                                         feed_dict = {X: train_X, y: train_y})\n",
    "        prob_train = probability.eval(feed_dict = {X: train_X, y: train_y})\n",
    "        prob_test = probability.eval(feed_dict = {X: test_X, y: test_y})\n",
    "        prob_valid = probability.eval(feed_dict = {X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nEvaluation  on training set')\n",
    "        print('Accuracy:', acc[1])\n",
    "        print('Precision:', prec[1])\n",
    "        print('Recall:', recall[1])\n",
    "        print('F1 score:', f1)\n",
    "        print('AUC:', AUC[1])\n",
    "        \n",
    "      \n",
    "    \n",
    "    \n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/LaterDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## APP DATA\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_APPDATA_RNN_LSTM_8features.ckpt\")\n",
    "        save_path = saver.save(sess, \"./checkpts/Later_v2_APPDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "    metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,AUC[1],loss_fn,accu_val,best_loss_val,acc_test,loss_test,neurons,learning_rate,epochs,steps]).reshape(-1,14),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\",\"loss\",\"accuracy_val\",\"loss_val\",\"accuracy_test\",\"loss_test\",\"neurons\",\"learning_rate\",\"n_epochs\",\"steps\"])\n",
    "    return metric_out_df, prob_train, prob_test, prob_valid\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def random_subsequence(X,y,seq_len):\n",
    "    rnd  = random.randint(0,len(X)-seq_len)\n",
    "    X_seq, y_seq = X[rnd:rnd+seq_len,:], y[rnd:rnd+seq_len,:]\n",
    "    return X_seq, y_seq\n",
    "\n",
    "    print(y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ways of cutting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odd plays train, even plays test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\")\n",
    "dir_path =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"\n",
    "\n",
    "subj_files_list =[]; ## list of subject_files fullfilling a criteria\n",
    "\n",
    "dir_files = [i for i in os.listdir(dir_path) if i.startswith('sub')]\n",
    "\n",
    "for subj_file_path in dir_files:\n",
    "\n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "    mypath =file_path\n",
    "    \n",
    "    play_names = [i for i in os.listdir(mypath) if os.path.isfile(os.path.join(mypath,i)) and i.startswith('app')]   \n",
    "    \n",
    "    if len(play_names) >= 50: ## criteria\n",
    "        subj_files_list.append(subj_file_path)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subj_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "num_shuffles=5\n",
    "for num, subj_file_path in enumerate(subj_files_list[]):\n",
    "    print(num)\n",
    "# for subj_file_path in [subj_files_list[0]]:\n",
    "    \n",
    "#     train_data,test_data, val_data = np.empty((0,task_df.columns.shape[0])),  np.empty((0,task_df.columns.shape[0])), np.empty((0,task_df.columns.shape[0]))\n",
    "    train_data,test_data, val_data = np.empty((0,15)),  np.empty((0,15)), np.empty((0,15))\n",
    "\n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "    mypath =file_path\n",
    "        \n",
    "    comp_task_train_df = pd.DataFrame()\n",
    "\n",
    "    play_names = [i for i in os.listdir(mypath) if os.path.isfile(os.path.join(mypath,i)) and i.startswith('app')]    \n",
    "\n",
    "    for randomization_counter in range(0,num_shuffles):\n",
    "            randomized_play_names= random.sample(play_names,len(play_names))\n",
    "            \n",
    "            for play_num, play_name in enumerate(randomized_play_names):\n",
    "#         for play_num,play_name in enumerate(play_names):\n",
    "\n",
    "                file_name = file_path + \"/\" + play_name\n",
    "                task_df = pd.read_csv(file_name)\n",
    "                task_df = add_releveant_features(task_df)\n",
    "\n",
    "                if np.mod(play_num,2)==0: ## odd trials\n",
    "                    train_data = np.append(train_data,task_df[task_df.TrialNum>1].values, axis=0)\n",
    "\n",
    "                else:\n",
    "                    test_data =  np.append(test_data, task_df[task_df.TrialNum>1].values[0:16], axis=0)\n",
    "                    val_data =  np.append(val_data, task_df[task_df.TrialNum>1].values[16:], axis=0)\n",
    "\n",
    "\n",
    "    train_data_df= pd.DataFrame(train_data,columns=task_df.columns)\n",
    "    val_data_df = pd.DataFrame(test_data,columns=task_df.columns)\n",
    "    test_data_df= pd.DataFrame(val_data,columns=task_df.columns)\n",
    "\n",
    "#     file_path = file_path + \"/OddEvenPlays/\"\n",
    "    file_path = file_path + \"/OddEvenPlays/RandomizedPlays10\"\n",
    "\n",
    "#     os.mkdir(file_path)\n",
    "    train_data_df.to_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df.to_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df.to_csv(file_path+\"/val_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_odd_even(train_data_df,test_data_df,val_data_df):\n",
    "\n",
    "#     train_len = 29\n",
    "#     test_len = 14\n",
    "#     val_len = 15\n",
    "\n",
    "    ##----------------- UNCOMMENT BELOW\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "    \n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "####### Prev O + C+ R + CurrO--------------------\n",
    " \n",
    "    train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    \n",
    "    ######## sampling \n",
    "    \n",
    "    \n",
    "#### - Prev RT+C+R+O + Curr O----------------------\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### PRE TRAINING\n",
    "#     stop = int(0.7*len(train_X))\n",
    "#     print(stop)\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int((len(train_X)-stop)/2)], train_X[stop+int((len(train_X)-stop)/2):],train_y[:stop], train_y[stop:stop+int((len(train_X)-stop)/2)], train_y[stop+int((len(train_X)-stop)/2):]\n",
    "    \n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = train_X, test_X, test_X, train_y, test_y, test_y\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(val_X.shape)\n",
    "    print(val_y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "    # # center and scale\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.fit_transform(test_X)\n",
    "    val_X = scaler.fit_transform(val_X)\n",
    "\n",
    "\n",
    "    train_X = train_X[:,None,:]\n",
    "    val_X = val_X[:,None,:]\n",
    "    test_X = test_X[:,None,:]\n",
    "\n",
    "\n",
    "    # # one-hot encode the outputs\n",
    "\n",
    "    onehot_encoder = OneHotEncoder()\n",
    "    encode_categorical = train_y.reshape(len(train_y), 1)\n",
    "    encode_categorical_test = test_y.reshape(len(test_y), 1)\n",
    "    encode_categorical_val = val_y.reshape(len(val_y),1)\n",
    "\n",
    "\n",
    "    train_y = onehot_encoder.fit_transform(encode_categorical).toarray()\n",
    "    test_y = onehot_encoder.fit_transform(encode_categorical_test).toarray()\n",
    "    val_y = onehot_encoder.fit_transform(encode_categorical_val).toarray()\n",
    "\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, val_X,val_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(5945, 8)\n",
      "(5945, 1)\n",
      "(3200, 8)\n",
      "(3200, 1)\n",
      "(2600, 8)\n",
      "(2600, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-45e26c98b632>:36: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-45e26c98b632>:37: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/ritwik7/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-5-45e26c98b632>:41: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/ritwik7/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/ritwik7/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:2002: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Step: 100  \tTraining loss: 0.354576051235199\n",
      "Step: 100  \tTraining accuracy: 0.8576955199241638\n",
      "Step: 100  \tValid loss: 0.34452301263809204\n",
      "Step: 200  \tTraining loss: 0.30858197808265686\n",
      "Step: 200  \tTraining accuracy: 0.8671000599861145\n",
      "Step: 200  \tValid loss: 0.3039250373840332\n",
      "Step: 300  \tTraining loss: 0.3038502037525177\n",
      "Step: 300  \tTraining accuracy: 0.8742992877960205\n",
      "Step: 300  \tValid loss: 0.2980923354625702\n",
      "Step: 400  \tTraining loss: 0.3012202978134155\n",
      "Step: 400  \tTraining accuracy: 0.878363311290741\n",
      "Step: 400  \tValid loss: 0.29423993825912476\n",
      "Step: 500  \tTraining loss: 0.29967963695526123\n",
      "Step: 500  \tTraining accuracy: 0.8812092542648315\n",
      "Step: 500  \tValid loss: 0.2916617691516876\n",
      "Step: 600  \tTraining loss: 0.2988569438457489\n",
      "Step: 600  \tTraining accuracy: 0.8832534551620483\n",
      "Step: 600  \tValid loss: 0.2901608943939209\n",
      "Step: 700  \tTraining loss: 0.2983165979385376\n",
      "Step: 700  \tTraining accuracy: 0.8847346901893616\n",
      "Step: 700  \tValid loss: 0.28924426436424255\n",
      "Step: 800  \tTraining loss: 0.2978316843509674\n",
      "Step: 800  \tTraining accuracy: 0.8858325481414795\n",
      "Step: 800  \tValid loss: 0.2884589433670044\n",
      "Step: 900  \tTraining loss: 0.29740196466445923\n",
      "Step: 900  \tTraining accuracy: 0.8867223858833313\n",
      "Step: 900  \tValid loss: 0.28785955905914307\n",
      "Step: 1000  \tTraining loss: 0.29698824882507324\n",
      "Step: 1000  \tTraining accuracy: 0.8875414133071899\n",
      "Step: 1000  \tValid loss: 0.2873373329639435\n",
      "Step: 1100  \tTraining loss: 0.2965342104434967\n",
      "Step: 1100  \tTraining accuracy: 0.8882288336753845\n",
      "Step: 1100  \tValid loss: 0.28680557012557983\n",
      "Step: 1200  \tTraining loss: 0.29592785239219666\n",
      "Step: 1200  \tTraining accuracy: 0.8888115882873535\n",
      "Step: 1200  \tValid loss: 0.2860667407512665\n",
      "Step: 1300  \tTraining loss: 0.2949099540710449\n",
      "Step: 1300  \tTraining accuracy: 0.8894509077072144\n",
      "Step: 1300  \tValid loss: 0.2847048044204712\n",
      "Step: 1400  \tTraining loss: 0.29379698634147644\n",
      "Step: 1400  \tTraining accuracy: 0.8900207877159119\n",
      "Step: 1400  \tValid loss: 0.28305643796920776\n",
      "Step: 1500  \tTraining loss: 0.2926422655582428\n",
      "Step: 1500  \tTraining accuracy: 0.8904710412025452\n",
      "Step: 1500  \tValid loss: 0.2815142869949341\n",
      "Step: 1600  \tTraining loss: 0.2916029393672943\n",
      "Step: 1600  \tTraining accuracy: 0.8908631801605225\n",
      "Step: 1600  \tValid loss: 0.28004124760627747\n",
      "Step: 1700  \tTraining loss: 0.29070615768432617\n",
      "Step: 1700  \tTraining accuracy: 0.8912593722343445\n",
      "Step: 1700  \tValid loss: 0.27892112731933594\n",
      "Step: 1800  \tTraining loss: 0.2898780107498169\n",
      "Step: 1800  \tTraining accuracy: 0.8916686773300171\n",
      "Step: 1800  \tValid loss: 0.2779676020145416\n",
      "Step: 1900  \tTraining loss: 0.2890363335609436\n",
      "Step: 1900  \tTraining accuracy: 0.8921396136283875\n",
      "Step: 1900  \tValid loss: 0.27702251076698303\n",
      "Step: 2000  \tTraining loss: 0.28827664256095886\n",
      "Step: 2000  \tTraining accuracy: 0.8925753235816956\n",
      "Step: 2000  \tValid loss: 0.2761688232421875\n",
      "Step: 2100  \tTraining loss: 0.2874537408351898\n",
      "Step: 2100  \tTraining accuracy: 0.8929768204689026\n",
      "Step: 2100  \tValid loss: 0.2753234803676605\n",
      "Step: 2200  \tTraining loss: 0.28667834401130676\n",
      "Step: 2200  \tTraining accuracy: 0.8933607935905457\n",
      "Step: 2200  \tValid loss: 0.2744874358177185\n",
      "Step: 2300  \tTraining loss: 0.2859298884868622\n",
      "Step: 2300  \tTraining accuracy: 0.8937408924102783\n",
      "Step: 2300  \tValid loss: 0.2736380696296692\n",
      "Step: 2400  \tTraining loss: 0.2851939797401428\n",
      "Step: 2400  \tTraining accuracy: 0.8941394090652466\n",
      "Step: 2400  \tValid loss: 0.2727733254432678\n",
      "Step: 2500  \tTraining loss: 0.28447845578193665\n",
      "Step: 2500  \tTraining accuracy: 0.8945261836051941\n",
      "Step: 2500  \tValid loss: 0.2719362676143646\n",
      "Step: 2600  \tTraining loss: 0.2837883234024048\n",
      "Step: 2600  \tTraining accuracy: 0.8948726654052734\n",
      "Step: 2600  \tValid loss: 0.27113255858421326\n",
      "Step: 2700  \tTraining loss: 0.28313279151916504\n",
      "Step: 2700  \tTraining accuracy: 0.8951961994171143\n",
      "Step: 2700  \tValid loss: 0.2703711986541748\n",
      "Step: 2800  \tTraining loss: 0.28251218795776367\n",
      "Step: 2800  \tTraining accuracy: 0.8955023884773254\n",
      "Step: 2800  \tValid loss: 0.26964429020881653\n",
      "Step: 2900  \tTraining loss: 0.28192752599716187\n",
      "Step: 2900  \tTraining accuracy: 0.8957781195640564\n",
      "Step: 2900  \tValid loss: 0.26894715428352356\n",
      "Step: 3000  \tTraining loss: 0.2813710868358612\n",
      "Step: 3000  \tTraining accuracy: 0.8960784673690796\n",
      "Step: 3000  \tValid loss: 0.26830777525901794\n",
      "Step: 3100  \tTraining loss: 0.28082549571990967\n",
      "Step: 3100  \tTraining accuracy: 0.8963730931282043\n",
      "Step: 3100  \tValid loss: 0.2676564157009125\n",
      "Step: 3200  \tTraining loss: 0.2803216576576233\n",
      "Step: 3200  \tTraining accuracy: 0.8966544270515442\n",
      "Step: 3200  \tValid loss: 0.26706254482269287\n",
      "Step: 3300  \tTraining loss: 0.27985405921936035\n",
      "Step: 3300  \tTraining accuracy: 0.896949827671051\n",
      "Step: 3300  \tValid loss: 0.266520231962204\n",
      "Step: 3400  \tTraining loss: 0.27941983938217163\n",
      "Step: 3400  \tTraining accuracy: 0.8972530961036682\n",
      "Step: 3400  \tValid loss: 0.2660224437713623\n",
      "Step: 3500  \tTraining loss: 0.27902156114578247\n",
      "Step: 3500  \tTraining accuracy: 0.8975387215614319\n",
      "Step: 3500  \tValid loss: 0.2655833959579468\n",
      "Step: 3600  \tTraining loss: 0.278659850358963\n",
      "Step: 3600  \tTraining accuracy: 0.8977986574172974\n",
      "Step: 3600  \tValid loss: 0.26519882678985596\n",
      "Step: 3700  \tTraining loss: 0.2783358693122864\n",
      "Step: 3700  \tTraining accuracy: 0.8980560302734375\n",
      "Step: 3700  \tValid loss: 0.2648576498031616\n",
      "Step: 3800  \tTraining loss: 0.27804529666900635\n",
      "Step: 3800  \tTraining accuracy: 0.8983019590377808\n",
      "Step: 3800  \tValid loss: 0.2645564079284668\n",
      "Step: 3900  \tTraining loss: 0.27775880694389343\n",
      "Step: 3900  \tTraining accuracy: 0.8985439538955688\n",
      "Step: 3900  \tValid loss: 0.2642870247364044\n",
      "Step: 4000  \tTraining loss: 0.2775038182735443\n",
      "Step: 4000  \tTraining accuracy: 0.8987866640090942\n",
      "Step: 4000  \tValid loss: 0.2640244662761688\n",
      "Step: 4100  \tTraining loss: 0.27728071808815\n",
      "Step: 4100  \tTraining accuracy: 0.8990299701690674\n",
      "Step: 4100  \tValid loss: 0.26381903886795044\n",
      "Step: 4200  \tTraining loss: 0.27707841992378235\n",
      "Step: 4200  \tTraining accuracy: 0.8992676734924316\n",
      "Step: 4200  \tValid loss: 0.26363810896873474\n",
      "Step: 4300  \tTraining loss: 0.2768920063972473\n",
      "Step: 4300  \tTraining accuracy: 0.8994942307472229\n",
      "Step: 4300  \tValid loss: 0.26348477602005005\n",
      "Step: 4400  \tTraining loss: 0.27671995759010315\n",
      "Step: 4400  \tTraining accuracy: 0.8997005820274353\n",
      "Step: 4400  \tValid loss: 0.2633552849292755\n",
      "Step: 4500  \tTraining loss: 0.2765604555606842\n",
      "Step: 4500  \tTraining accuracy: 0.8998804092407227\n",
      "Step: 4500  \tValid loss: 0.2632310390472412\n",
      "Step: 4600  \tTraining loss: 0.2764109671115875\n",
      "Step: 4600  \tTraining accuracy: 0.9000411629676819\n",
      "Step: 4600  \tValid loss: 0.2631225287914276\n",
      "Step: 4700  \tTraining loss: 0.2762693166732788\n",
      "Step: 4700  \tTraining accuracy: 0.9001950025558472\n",
      "Step: 4700  \tValid loss: 0.26301905512809753\n",
      "Step: 4800  \tTraining loss: 0.27613502740859985\n",
      "Step: 4800  \tTraining accuracy: 0.9003369212150574\n",
      "Step: 4800  \tValid loss: 0.26291781663894653\n",
      "Step: 4900  \tTraining loss: 0.2760065495967865\n",
      "Step: 4900  \tTraining accuracy: 0.9004660248756409\n",
      "Step: 4900  \tValid loss: 0.2628156840801239\n",
      "Step: 5000  \tTraining loss: 0.2758823037147522\n",
      "Step: 5000  \tTraining accuracy: 0.9005898833274841\n",
      "Step: 5000  \tValid loss: 0.2627217769622803\n",
      "Step: 5100  \tTraining loss: 0.2757621109485626\n",
      "Step: 5100  \tTraining accuracy: 0.900712251663208\n",
      "Step: 5100  \tValid loss: 0.2626330554485321\n",
      "Step: 5200  \tTraining loss: 0.2756458818912506\n",
      "Step: 5200  \tTraining accuracy: 0.9008298516273499\n",
      "Step: 5200  \tValid loss: 0.26255089044570923\n",
      "Step: 5300  \tTraining loss: 0.27553287148475647\n",
      "Step: 5300  \tTraining accuracy: 0.9009429216384888\n",
      "Step: 5300  \tValid loss: 0.2624691128730774\n",
      "Step: 5400  \tTraining loss: 0.2754230797290802\n",
      "Step: 5400  \tTraining accuracy: 0.9010502099990845\n",
      "Step: 5400  \tValid loss: 0.26239538192749023\n",
      "Step: 5500  \tTraining loss: 0.27531710267066956\n",
      "Step: 5500  \tTraining accuracy: 0.9011504650115967\n",
      "Step: 5500  \tValid loss: 0.2623221278190613\n",
      "Step: 5600  \tTraining loss: 0.27521318197250366\n",
      "Step: 5600  \tTraining accuracy: 0.9012670516967773\n",
      "Step: 5600  \tValid loss: 0.2622486650943756\n",
      "Step: 5700  \tTraining loss: 0.27511066198349\n",
      "Step: 5700  \tTraining accuracy: 0.9013885259628296\n",
      "Step: 5700  \tValid loss: 0.2621755301952362\n",
      "Step: 5800  \tTraining loss: 0.27500927448272705\n",
      "Step: 5800  \tTraining accuracy: 0.9015116691589355\n",
      "Step: 5800  \tValid loss: 0.2621041238307953\n",
      "Step: 5900  \tTraining loss: 0.27490901947021484\n",
      "Step: 5900  \tTraining accuracy: 0.9016306400299072\n",
      "Step: 5900  \tValid loss: 0.2620429992675781\n",
      "Step: 6000  \tTraining loss: 0.274809330701828\n",
      "Step: 6000  \tTraining accuracy: 0.901745617389679\n",
      "Step: 6000  \tValid loss: 0.2619852125644684\n",
      "Step: 6100  \tTraining loss: 0.274709016084671\n",
      "Step: 6100  \tTraining accuracy: 0.9018553495407104\n",
      "Step: 6100  \tValid loss: 0.2619255483150482\n",
      "Step: 6200  \tTraining loss: 0.2746078073978424\n",
      "Step: 6200  \tTraining accuracy: 0.9019560217857361\n",
      "Step: 6200  \tValid loss: 0.26186639070510864\n",
      "Step: 6300  \tTraining loss: 0.27450552582740784\n",
      "Step: 6300  \tTraining accuracy: 0.9020534157752991\n",
      "Step: 6300  \tValid loss: 0.2618149220943451\n",
      "Step: 6400  \tTraining loss: 0.2744014859199524\n",
      "Step: 6400  \tTraining accuracy: 0.9021450877189636\n",
      "Step: 6400  \tValid loss: 0.261765718460083\n",
      "Step: 6500  \tTraining loss: 0.27429625391960144\n",
      "Step: 6500  \tTraining accuracy: 0.9022405743598938\n",
      "Step: 6500  \tValid loss: 0.2617201805114746\n",
      "Step: 6600  \tTraining loss: 0.27418091893196106\n",
      "Step: 6600  \tTraining accuracy: 0.9023252725601196\n",
      "Step: 6600  \tValid loss: 0.26169031858444214\n",
      "Step: 6700  \tTraining loss: 0.27406901121139526\n",
      "Step: 6700  \tTraining accuracy: 0.9023984670639038\n",
      "Step: 6700  \tValid loss: 0.26164138317108154\n",
      "Step: 6800  \tTraining loss: 0.2739574909210205\n",
      "Step: 6800  \tTraining accuracy: 0.9024670124053955\n",
      "Step: 6800  \tValid loss: 0.2615821659564972\n",
      "Step: 6900  \tTraining loss: 0.27384626865386963\n",
      "Step: 6900  \tTraining accuracy: 0.9025298357009888\n",
      "Step: 6900  \tValid loss: 0.2615209221839905\n",
      "Step: 7000  \tTraining loss: 0.2737356126308441\n",
      "Step: 7000  \tTraining accuracy: 0.9025883674621582\n",
      "Step: 7000  \tValid loss: 0.2614591121673584\n",
      "Step: 7100  \tTraining loss: 0.273625910282135\n",
      "Step: 7100  \tTraining accuracy: 0.9026500582695007\n",
      "Step: 7100  \tValid loss: 0.26140540838241577\n",
      "Step: 7200  \tTraining loss: 0.27351731061935425\n",
      "Step: 7200  \tTraining accuracy: 0.9027040600776672\n",
      "Step: 7200  \tValid loss: 0.261349618434906\n",
      "Step: 7300  \tTraining loss: 0.27340731024742126\n",
      "Step: 7300  \tTraining accuracy: 0.9027578234672546\n",
      "Step: 7300  \tValid loss: 0.2612614035606384\n",
      "Step: 7400  \tTraining loss: 0.27329373359680176\n",
      "Step: 7400  \tTraining accuracy: 0.9028123617172241\n",
      "Step: 7400  \tValid loss: 0.2611628770828247\n",
      "Step: 7500  \tTraining loss: 0.273181289434433\n",
      "Step: 7500  \tTraining accuracy: 0.902868926525116\n",
      "Step: 7500  \tValid loss: 0.2610836327075958\n",
      "Step: 7600  \tTraining loss: 0.27306973934173584\n",
      "Step: 7600  \tTraining accuracy: 0.9029262065887451\n",
      "Step: 7600  \tValid loss: 0.2610245645046234\n",
      "Step: 7700  \tTraining loss: 0.27295851707458496\n",
      "Step: 7700  \tTraining accuracy: 0.9029819965362549\n",
      "Step: 7700  \tValid loss: 0.2609614133834839\n",
      "Step: 7800  \tTraining loss: 0.27284765243530273\n",
      "Step: 7800  \tTraining accuracy: 0.9030352234840393\n",
      "Step: 7800  \tValid loss: 0.2609040141105652\n",
      "Step: 7900  \tTraining loss: 0.27273669838905334\n",
      "Step: 7900  \tTraining accuracy: 0.9030827879905701\n",
      "Step: 7900  \tValid loss: 0.26083919405937195\n",
      "Step: 8000  \tTraining loss: 0.27262574434280396\n",
      "Step: 8000  \tTraining accuracy: 0.9031313061714172\n",
      "Step: 8000  \tValid loss: 0.2607785165309906\n",
      "Step: 8100  \tTraining loss: 0.272514671087265\n",
      "Step: 8100  \tTraining accuracy: 0.903181791305542\n",
      "Step: 8100  \tValid loss: 0.26071688532829285\n",
      "Step: 8200  \tTraining loss: 0.27240344882011414\n",
      "Step: 8200  \tTraining accuracy: 0.9032310247421265\n",
      "Step: 8200  \tValid loss: 0.26065415143966675\n",
      "Step: 8300  \tTraining loss: 0.27229180932044983\n",
      "Step: 8300  \tTraining accuracy: 0.90327388048172\n",
      "Step: 8300  \tValid loss: 0.26058417558670044\n",
      "Step: 8400  \tTraining loss: 0.2721792757511139\n",
      "Step: 8400  \tTraining accuracy: 0.9033106565475464\n",
      "Step: 8400  \tValid loss: 0.26051196455955505\n",
      "Step: 8500  \tTraining loss: 0.2720666527748108\n",
      "Step: 8500  \tTraining accuracy: 0.9033465385437012\n",
      "Step: 8500  \tValid loss: 0.2604426443576813\n",
      "Step: 8600  \tTraining loss: 0.2719530463218689\n",
      "Step: 8600  \tTraining accuracy: 0.9033815860748291\n",
      "Step: 8600  \tValid loss: 0.260367751121521\n",
      "Step: 8700  \tTraining loss: 0.27183860540390015\n",
      "Step: 8700  \tTraining accuracy: 0.9034138321876526\n",
      "Step: 8700  \tValid loss: 0.26028749346733093\n",
      "Step: 8800  \tTraining loss: 0.2717230021953583\n",
      "Step: 8800  \tTraining accuracy: 0.9034424424171448\n",
      "Step: 8800  \tValid loss: 0.26020318269729614\n",
      "Step: 8900  \tTraining loss: 0.271605908870697\n",
      "Step: 8900  \tTraining accuracy: 0.9034703969955444\n",
      "Step: 8900  \tValid loss: 0.2601149380207062\n",
      "Step: 9000  \tTraining loss: 0.2714873254299164\n",
      "Step: 9000  \tTraining accuracy: 0.9034977555274963\n",
      "Step: 9000  \tValid loss: 0.2600231170654297\n",
      "Step: 9100  \tTraining loss: 0.27136683464050293\n",
      "Step: 9100  \tTraining accuracy: 0.9035226106643677\n",
      "Step: 9100  \tValid loss: 0.2599294185638428\n",
      "Step: 9200  \tTraining loss: 0.27124419808387756\n",
      "Step: 9200  \tTraining accuracy: 0.9035441279411316\n",
      "Step: 9200  \tValid loss: 0.2598329186439514\n",
      "Step: 9300  \tTraining loss: 0.2711188495159149\n",
      "Step: 9300  \tTraining accuracy: 0.9035651683807373\n",
      "Step: 9300  \tValid loss: 0.2597304582595825\n",
      "Step: 9400  \tTraining loss: 0.2709909677505493\n",
      "Step: 9400  \tTraining accuracy: 0.9035857915878296\n",
      "Step: 9400  \tValid loss: 0.25962206721305847\n",
      "Step: 9500  \tTraining loss: 0.2708599269390106\n",
      "Step: 9500  \tTraining accuracy: 0.9036059379577637\n",
      "Step: 9500  \tValid loss: 0.2595066726207733\n",
      "Step: 9600  \tTraining loss: 0.2707253396511078\n",
      "Step: 9600  \tTraining accuracy: 0.9036256670951843\n",
      "Step: 9600  \tValid loss: 0.2593858540058136\n",
      "Step: 9700  \tTraining loss: 0.2705870270729065\n",
      "Step: 9700  \tTraining accuracy: 0.9036450386047363\n",
      "Step: 9700  \tValid loss: 0.2592600882053375\n",
      "Step: 9800  \tTraining loss: 0.2704451084136963\n",
      "Step: 9800  \tTraining accuracy: 0.9036639332771301\n",
      "Step: 9800  \tValid loss: 0.2591385543346405\n",
      "Step: 9900  \tTraining loss: 0.27029934525489807\n",
      "Step: 9900  \tTraining accuracy: 0.9036825299263\n",
      "Step: 9900  \tValid loss: 0.25901028513908386\n",
      "Step: 10000  \tTraining loss: 0.2701491415500641\n",
      "Step: 10000  \tTraining accuracy: 0.9036998152732849\n",
      "Step: 10000  \tValid loss: 0.25887590646743774\n",
      "Step: 10100  \tTraining loss: 0.2699947655200958\n",
      "Step: 10100  \tTraining accuracy: 0.9037176370620728\n",
      "Step: 10100  \tValid loss: 0.2587399482727051\n",
      "Step: 10200  \tTraining loss: 0.26983657479286194\n",
      "Step: 10200  \tTraining accuracy: 0.9037351608276367\n",
      "Step: 10200  \tValid loss: 0.25860244035720825\n",
      "Step: 10300  \tTraining loss: 0.269674688577652\n",
      "Step: 10300  \tTraining accuracy: 0.9037522673606873\n",
      "Step: 10300  \tValid loss: 0.2584642469882965\n",
      "Step: 10400  \tTraining loss: 0.2695094645023346\n",
      "Step: 10400  \tTraining accuracy: 0.9037690758705139\n",
      "Step: 10400  \tValid loss: 0.2583286166191101\n",
      "Step: 10500  \tTraining loss: 0.2693415582180023\n",
      "Step: 10500  \tTraining accuracy: 0.9037855863571167\n",
      "Step: 10500  \tValid loss: 0.2581997811794281\n",
      "Step: 10600  \tTraining loss: 0.26916182041168213\n",
      "Step: 10600  \tTraining accuracy: 0.9038017392158508\n",
      "Step: 10600  \tValid loss: 0.25810107588768005\n",
      "Step: 10700  \tTraining loss: 0.2689836621284485\n",
      "Step: 10700  \tTraining accuracy: 0.9038152098655701\n",
      "Step: 10700  \tValid loss: 0.25799891352653503\n",
      "Step: 10800  \tTraining loss: 0.26880785822868347\n",
      "Step: 10800  \tTraining accuracy: 0.9038244485855103\n",
      "Step: 10800  \tValid loss: 0.25788044929504395\n",
      "Step: 10900  \tTraining loss: 0.2686325013637543\n",
      "Step: 10900  \tTraining accuracy: 0.903831958770752\n",
      "Step: 10900  \tValid loss: 0.2577681243419647\n",
      "Step: 11000  \tTraining loss: 0.26845794916152954\n",
      "Step: 11000  \tTraining accuracy: 0.9038393497467041\n",
      "Step: 11000  \tValid loss: 0.2576594054698944\n",
      "Step: 11100  \tTraining loss: 0.2682848274707794\n",
      "Step: 11100  \tTraining accuracy: 0.9038466215133667\n",
      "Step: 11100  \tValid loss: 0.2575559616088867\n",
      "Step: 11200  \tTraining loss: 0.26811325550079346\n",
      "Step: 11200  \tTraining accuracy: 0.9038552641868591\n",
      "Step: 11200  \tValid loss: 0.25745558738708496\n",
      "Step: 11300  \tTraining loss: 0.26794350147247314\n",
      "Step: 11300  \tTraining accuracy: 0.9038659930229187\n",
      "Step: 11300  \tValid loss: 0.2573641240596771\n",
      "Step: 11400  \tTraining loss: 0.2677604556083679\n",
      "Step: 11400  \tTraining accuracy: 0.9038758277893066\n",
      "Step: 11400  \tValid loss: 0.2572561204433441\n",
      "Step: 11500  \tTraining loss: 0.2675945460796356\n",
      "Step: 11500  \tTraining accuracy: 0.9038810133934021\n",
      "Step: 11500  \tValid loss: 0.25717049837112427\n",
      "Step: 11600  \tTraining loss: 0.26743316650390625\n",
      "Step: 11600  \tTraining accuracy: 0.9038868546485901\n",
      "Step: 11600  \tValid loss: 0.2570948600769043\n",
      "Step: 11700  \tTraining loss: 0.2672744393348694\n",
      "Step: 11700  \tTraining accuracy: 0.9038896560668945\n",
      "Step: 11700  \tValid loss: 0.25702258944511414\n",
      "Step: 11800  \tTraining loss: 0.26711753010749817\n",
      "Step: 11800  \tTraining accuracy: 0.9038909673690796\n",
      "Step: 11800  \tValid loss: 0.2569509446620941\n",
      "Step: 11900  \tTraining loss: 0.2669637203216553\n",
      "Step: 11900  \tTraining accuracy: 0.903892993927002\n",
      "Step: 11900  \tValid loss: 0.2568909823894501\n",
      "Step: 12000  \tTraining loss: 0.26681411266326904\n",
      "Step: 12000  \tTraining accuracy: 0.9038956761360168\n",
      "Step: 12000  \tValid loss: 0.25683751702308655\n",
      "Step: 12100  \tTraining loss: 0.26666703820228577\n",
      "Step: 12100  \tTraining accuracy: 0.9038976430892944\n",
      "Step: 12100  \tValid loss: 0.25678569078445435\n",
      "Step: 12200  \tTraining loss: 0.26652368903160095\n",
      "Step: 12200  \tTraining accuracy: 0.9038988351821899\n",
      "Step: 12200  \tValid loss: 0.25673922896385193\n",
      "Step: 12300  \tTraining loss: 0.2663799822330475\n",
      "Step: 12300  \tTraining accuracy: 0.9039013981819153\n",
      "Step: 12300  \tValid loss: 0.2566725015640259\n",
      "Step: 12400  \tTraining loss: 0.26624032855033875\n",
      "Step: 12400  \tTraining accuracy: 0.9039053320884705\n",
      "Step: 12400  \tValid loss: 0.2566220164299011\n",
      "Step: 12500  \tTraining loss: 0.2661040127277374\n",
      "Step: 12500  \tTraining accuracy: 0.9039112329483032\n",
      "Step: 12500  \tValid loss: 0.25658175349235535\n",
      "Step: 12600  \tTraining loss: 0.26597046852111816\n",
      "Step: 12600  \tTraining accuracy: 0.9039191007614136\n",
      "Step: 12600  \tValid loss: 0.2565479576587677\n",
      "Step: 12700  \tTraining loss: 0.2658262848854065\n",
      "Step: 12700  \tTraining accuracy: 0.9039281606674194\n",
      "Step: 12700  \tValid loss: 0.25648969411849976\n",
      "Step: 12800  \tTraining loss: 0.2656930387020111\n",
      "Step: 12800  \tTraining accuracy: 0.9039350748062134\n",
      "Step: 12800  \tValid loss: 0.25645574927330017\n",
      "Step: 12900  \tTraining loss: 0.26556462049484253\n",
      "Step: 12900  \tTraining accuracy: 0.9039405584335327\n",
      "Step: 12900  \tValid loss: 0.25643911957740784\n",
      "Step: 13000  \tTraining loss: 0.26543954014778137\n",
      "Step: 13000  \tTraining accuracy: 0.9039459824562073\n",
      "Step: 13000  \tValid loss: 0.25642159581184387\n",
      "Step: 13100  \tTraining loss: 0.2653176188468933\n",
      "Step: 13100  \tTraining accuracy: 0.9039512872695923\n",
      "Step: 13100  \tValid loss: 0.2564053237438202\n",
      "Step: 13200  \tTraining loss: 0.2651987075805664\n",
      "Step: 13200  \tTraining accuracy: 0.9039565324783325\n",
      "Step: 13200  \tValid loss: 0.2563896179199219\n",
      "Step: 13300  \tTraining loss: 0.2650834023952484\n",
      "Step: 13300  \tTraining accuracy: 0.9039616584777832\n",
      "Step: 13300  \tValid loss: 0.25637978315353394\n",
      "Step: 13400  \tTraining loss: 0.26497212052345276\n",
      "Step: 13400  \tTraining accuracy: 0.9039667844772339\n",
      "Step: 13400  \tValid loss: 0.25637829303741455\n",
      "Step: 13500  \tTraining loss: 0.2648634910583496\n",
      "Step: 13500  \tTraining accuracy: 0.9039686322212219\n",
      "Step: 13500  \tValid loss: 0.2563757002353668\n",
      "Step: 13600  \tTraining loss: 0.26475751399993896\n",
      "Step: 13600  \tTraining accuracy: 0.9039704203605652\n",
      "Step: 13600  \tValid loss: 0.25637373328208923\n",
      "Step: 13700  \tTraining loss: 0.26465386152267456\n",
      "Step: 13700  \tTraining accuracy: 0.903974711894989\n",
      "Step: 13700  \tValid loss: 0.25637298822402954\n",
      "Step: 13800  \tTraining loss: 0.264552503824234\n",
      "Step: 13800  \tTraining accuracy: 0.9039795398712158\n",
      "Step: 13800  \tValid loss: 0.2563735544681549\n",
      "Step: 13900  \tTraining loss: 0.26445329189300537\n",
      "Step: 13900  \tTraining accuracy: 0.9039843082427979\n",
      "Step: 13900  \tValid loss: 0.2563745379447937\n",
      "Step: 14000  \tTraining loss: 0.26435625553131104\n",
      "Step: 14000  \tTraining accuracy: 0.9039890170097351\n",
      "Step: 14000  \tValid loss: 0.25637662410736084\n",
      "Step: 14100  \tTraining loss: 0.2642611861228943\n",
      "Step: 14100  \tTraining accuracy: 0.9039936661720276\n",
      "Step: 14100  \tValid loss: 0.25637927651405334\n",
      "Step: 14200  \tTraining loss: 0.26416799426078796\n",
      "Step: 14200  \tTraining accuracy: 0.9039981961250305\n",
      "Step: 14200  \tValid loss: 0.25638362765312195\n",
      "Step: 14300  \tTraining loss: 0.26407647132873535\n",
      "Step: 14300  \tTraining accuracy: 0.9040027260780334\n",
      "Step: 14300  \tValid loss: 0.25638943910598755\n",
      "Step: 14400  \tTraining loss: 0.2639867663383484\n",
      "Step: 14400  \tTraining accuracy: 0.9040071964263916\n",
      "Step: 14400  \tValid loss: 0.25639626383781433\n",
      "Step: 14500  \tTraining loss: 0.2638987600803375\n",
      "Step: 14500  \tTraining accuracy: 0.9040133357048035\n",
      "Step: 14500  \tValid loss: 0.25640395283699036\n",
      "Step: 14600  \tTraining loss: 0.2638121545314789\n",
      "Step: 14600  \tTraining accuracy: 0.9040205478668213\n",
      "Step: 14600  \tValid loss: 0.25641247630119324\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.9040277\n",
      "Precision: 0.91903436\n",
      "Recall: 0.9705825\n",
      "F1 score: 0.9286893\n",
      "AUC: 0.727608\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.904028   0.919034  0.970582  0.928689  0.727608  0.263754      0.904037   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.256373        0.90403    0.23591      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  14667.0  \n",
      "1\n",
      "(21315, 8)\n",
      "(21315, 1)\n",
      "(11760, 8)\n",
      "(11760, 1)\n",
      "(9555, 8)\n",
      "(9555, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5308483839035034\n",
      "Step: 100  \tTraining accuracy: 0.7456251382827759\n",
      "Step: 100  \tValid loss: 0.5323190689086914\n",
      "Step: 200  \tTraining loss: 0.5080155730247498\n",
      "Step: 200  \tTraining accuracy: 0.7477988600730896\n",
      "Step: 200  \tValid loss: 0.5122761130332947\n",
      "Step: 300  \tTraining loss: 0.4790612757205963\n",
      "Step: 300  \tTraining accuracy: 0.7522120475769043\n",
      "Step: 300  \tValid loss: 0.48429223895072937\n",
      "Step: 400  \tTraining loss: 0.42741942405700684\n",
      "Step: 400  \tTraining accuracy: 0.7610669732093811\n",
      "Step: 400  \tValid loss: 0.4308953881263733\n",
      "Step: 500  \tTraining loss: 0.4028255343437195\n",
      "Step: 500  \tTraining accuracy: 0.770401656627655\n",
      "Step: 500  \tValid loss: 0.40474647283554077\n",
      "Step: 600  \tTraining loss: 0.3972308039665222\n",
      "Step: 600  \tTraining accuracy: 0.7781886458396912\n",
      "Step: 600  \tValid loss: 0.3983350694179535\n",
      "Step: 700  \tTraining loss: 0.39569175243377686\n",
      "Step: 700  \tTraining accuracy: 0.7838538885116577\n",
      "Step: 700  \tValid loss: 0.3965577781200409\n",
      "Step: 800  \tTraining loss: 0.39351972937583923\n",
      "Step: 800  \tTraining accuracy: 0.7880709767341614\n",
      "Step: 800  \tValid loss: 0.3952887952327728\n",
      "Step: 900  \tTraining loss: 0.391937792301178\n",
      "Step: 900  \tTraining accuracy: 0.791309654712677\n",
      "Step: 900  \tValid loss: 0.3935883939266205\n",
      "Step: 1000  \tTraining loss: 0.3901033401489258\n",
      "Step: 1000  \tTraining accuracy: 0.7939232587814331\n",
      "Step: 1000  \tValid loss: 0.3914259374141693\n",
      "Step: 1100  \tTraining loss: 0.38797447085380554\n",
      "Step: 1100  \tTraining accuracy: 0.7961395382881165\n",
      "Step: 1100  \tValid loss: 0.3888727128505707\n",
      "Step: 1200  \tTraining loss: 0.38579633831977844\n",
      "Step: 1200  \tTraining accuracy: 0.7979989647865295\n",
      "Step: 1200  \tValid loss: 0.38644590973854065\n",
      "Step: 1300  \tTraining loss: 0.3837737739086151\n",
      "Step: 1300  \tTraining accuracy: 0.7996078133583069\n",
      "Step: 1300  \tValid loss: 0.3842507600784302\n",
      "Step: 1400  \tTraining loss: 0.3819548189640045\n",
      "Step: 1400  \tTraining accuracy: 0.8010599613189697\n",
      "Step: 1400  \tValid loss: 0.3823641538619995\n",
      "Step: 1500  \tTraining loss: 0.3805761933326721\n",
      "Step: 1500  \tTraining accuracy: 0.8023441433906555\n",
      "Step: 1500  \tValid loss: 0.38122814893722534\n",
      "Step: 1600  \tTraining loss: 0.3794313073158264\n",
      "Step: 1600  \tTraining accuracy: 0.8035004734992981\n",
      "Step: 1600  \tValid loss: 0.3803040683269501\n",
      "Step: 1700  \tTraining loss: 0.37845054268836975\n",
      "Step: 1700  \tTraining accuracy: 0.8045607209205627\n",
      "Step: 1700  \tValid loss: 0.3794531524181366\n",
      "Step: 1800  \tTraining loss: 0.37757226824760437\n",
      "Step: 1800  \tTraining accuracy: 0.8055011630058289\n",
      "Step: 1800  \tValid loss: 0.37867048382759094\n",
      "Step: 1900  \tTraining loss: 0.3767958879470825\n",
      "Step: 1900  \tTraining accuracy: 0.8063741326332092\n",
      "Step: 1900  \tValid loss: 0.3780250549316406\n",
      "Step: 2000  \tTraining loss: 0.37605762481689453\n",
      "Step: 2000  \tTraining accuracy: 0.8072441816329956\n",
      "Step: 2000  \tValid loss: 0.37750348448753357\n",
      "Step: 2100  \tTraining loss: 0.37540560960769653\n",
      "Step: 2100  \tTraining accuracy: 0.8080706000328064\n",
      "Step: 2100  \tValid loss: 0.3771399259567261\n",
      "Step: 2200  \tTraining loss: 0.37481391429901123\n",
      "Step: 2200  \tTraining accuracy: 0.8088331818580627\n",
      "Step: 2200  \tValid loss: 0.3768315017223358\n",
      "Step: 2300  \tTraining loss: 0.37423574924468994\n",
      "Step: 2300  \tTraining accuracy: 0.8095686435699463\n",
      "Step: 2300  \tValid loss: 0.3765449821949005\n",
      "Step: 2400  \tTraining loss: 0.3736492991447449\n",
      "Step: 2400  \tTraining accuracy: 0.8102824687957764\n",
      "Step: 2400  \tValid loss: 0.3762584924697876\n",
      "Step: 2500  \tTraining loss: 0.37306860089302063\n",
      "Step: 2500  \tTraining accuracy: 0.810957133769989\n",
      "Step: 2500  \tValid loss: 0.37597960233688354\n",
      "Step: 2600  \tTraining loss: 0.37248751521110535\n",
      "Step: 2600  \tTraining accuracy: 0.8115898966789246\n",
      "Step: 2600  \tValid loss: 0.3757288157939911\n",
      "Step: 2700  \tTraining loss: 0.3718990385532379\n",
      "Step: 2700  \tTraining accuracy: 0.8121864795684814\n",
      "Step: 2700  \tValid loss: 0.3754446804523468\n",
      "Step: 2800  \tTraining loss: 0.37131115794181824\n",
      "Step: 2800  \tTraining accuracy: 0.8127464652061462\n",
      "Step: 2800  \tValid loss: 0.3751155436038971\n",
      "Step: 2900  \tTraining loss: 0.3707505762577057\n",
      "Step: 2900  \tTraining accuracy: 0.8132976293563843\n",
      "Step: 2900  \tValid loss: 0.3748745322227478\n",
      "Step: 3000  \tTraining loss: 0.37022894620895386\n",
      "Step: 3000  \tTraining accuracy: 0.8138114213943481\n",
      "Step: 3000  \tValid loss: 0.37462979555130005\n",
      "Step: 3100  \tTraining loss: 0.3696907162666321\n",
      "Step: 3100  \tTraining accuracy: 0.8142845630645752\n",
      "Step: 3100  \tValid loss: 0.37439411878585815\n",
      "Step: 3200  \tTraining loss: 0.3690641522407532\n",
      "Step: 3200  \tTraining accuracy: 0.8147269487380981\n",
      "Step: 3200  \tValid loss: 0.37401247024536133\n",
      "Step: 3300  \tTraining loss: 0.36832839250564575\n",
      "Step: 3300  \tTraining accuracy: 0.8151420950889587\n",
      "Step: 3300  \tValid loss: 0.37352484464645386\n",
      "Step: 3400  \tTraining loss: 0.3675311803817749\n",
      "Step: 3400  \tTraining accuracy: 0.8155401945114136\n",
      "Step: 3400  \tValid loss: 0.3729635775089264\n",
      "Step: 3500  \tTraining loss: 0.3667217493057251\n",
      "Step: 3500  \tTraining accuracy: 0.8159117698669434\n",
      "Step: 3500  \tValid loss: 0.37228670716285706\n",
      "Step: 3600  \tTraining loss: 0.36593687534332275\n",
      "Step: 3600  \tTraining accuracy: 0.8162670731544495\n",
      "Step: 3600  \tValid loss: 0.3716122508049011\n",
      "Step: 3700  \tTraining loss: 0.3651908338069916\n",
      "Step: 3700  \tTraining accuracy: 0.8166054487228394\n",
      "Step: 3700  \tValid loss: 0.3709295094013214\n",
      "Step: 3800  \tTraining loss: 0.36448934674263\n",
      "Step: 3800  \tTraining accuracy: 0.8169145584106445\n",
      "Step: 3800  \tValid loss: 0.37031295895576477\n",
      "Step: 3900  \tTraining loss: 0.3638348877429962\n",
      "Step: 3900  \tTraining accuracy: 0.8172130584716797\n",
      "Step: 3900  \tValid loss: 0.3697390556335449\n",
      "Step: 4000  \tTraining loss: 0.36325782537460327\n",
      "Step: 4000  \tTraining accuracy: 0.8175065517425537\n",
      "Step: 4000  \tValid loss: 0.3692530691623688\n",
      "Step: 4100  \tTraining loss: 0.36273276805877686\n",
      "Step: 4100  \tTraining accuracy: 0.8177774548530579\n",
      "Step: 4100  \tValid loss: 0.3688429594039917\n",
      "Step: 4200  \tTraining loss: 0.36224564909935\n",
      "Step: 4200  \tTraining accuracy: 0.8180460333824158\n",
      "Step: 4200  \tValid loss: 0.368463397026062\n",
      "Step: 4300  \tTraining loss: 0.36178120970726013\n",
      "Step: 4300  \tTraining accuracy: 0.8183157444000244\n",
      "Step: 4300  \tValid loss: 0.36811044812202454\n",
      "Step: 4400  \tTraining loss: 0.36134907603263855\n",
      "Step: 4400  \tTraining accuracy: 0.8185854554176331\n",
      "Step: 4400  \tValid loss: 0.36779478192329407\n",
      "Step: 4500  \tTraining loss: 0.3609490394592285\n",
      "Step: 4500  \tTraining accuracy: 0.8188441395759583\n",
      "Step: 4500  \tValid loss: 0.3675103783607483\n",
      "Step: 4600  \tTraining loss: 0.3605840802192688\n",
      "Step: 4600  \tTraining accuracy: 0.8191141486167908\n",
      "Step: 4600  \tValid loss: 0.3673117756843567\n",
      "Step: 4700  \tTraining loss: 0.3602514863014221\n",
      "Step: 4700  \tTraining accuracy: 0.8193780183792114\n",
      "Step: 4700  \tValid loss: 0.36712899804115295\n",
      "Step: 4800  \tTraining loss: 0.3599494695663452\n",
      "Step: 4800  \tTraining accuracy: 0.8196229338645935\n",
      "Step: 4800  \tValid loss: 0.36690565943717957\n",
      "Step: 4900  \tTraining loss: 0.35967618227005005\n",
      "Step: 4900  \tTraining accuracy: 0.8198519349098206\n",
      "Step: 4900  \tValid loss: 0.366715669631958\n",
      "Step: 5000  \tTraining loss: 0.35943126678466797\n",
      "Step: 5000  \tTraining accuracy: 0.8200740814208984\n",
      "Step: 5000  \tValid loss: 0.36655595898628235\n",
      "Step: 5100  \tTraining loss: 0.35921531915664673\n",
      "Step: 5100  \tTraining accuracy: 0.8203008770942688\n",
      "Step: 5100  \tValid loss: 0.3663782775402069\n",
      "Step: 5200  \tTraining loss: 0.3590204417705536\n",
      "Step: 5200  \tTraining accuracy: 0.8205257058143616\n",
      "Step: 5200  \tValid loss: 0.3662223219871521\n",
      "Step: 5300  \tTraining loss: 0.358835905790329\n",
      "Step: 5300  \tTraining accuracy: 0.8207432627677917\n",
      "Step: 5300  \tValid loss: 0.36608877778053284\n",
      "Step: 5400  \tTraining loss: 0.3586604595184326\n",
      "Step: 5400  \tTraining accuracy: 0.820959746837616\n",
      "Step: 5400  \tValid loss: 0.36595508456230164\n",
      "Step: 5500  \tTraining loss: 0.3584868311882019\n",
      "Step: 5500  \tTraining accuracy: 0.8211734294891357\n",
      "Step: 5500  \tValid loss: 0.36584144830703735\n",
      "Step: 5600  \tTraining loss: 0.3583151698112488\n",
      "Step: 5600  \tTraining accuracy: 0.8213802576065063\n",
      "Step: 5600  \tValid loss: 0.36570826172828674\n",
      "Step: 5700  \tTraining loss: 0.35813677310943604\n",
      "Step: 5700  \tTraining accuracy: 0.8215926885604858\n",
      "Step: 5700  \tValid loss: 0.36559349298477173\n",
      "Step: 5800  \tTraining loss: 0.3579539358615875\n",
      "Step: 5800  \tTraining accuracy: 0.8218038082122803\n",
      "Step: 5800  \tValid loss: 0.36550259590148926\n",
      "Step: 5900  \tTraining loss: 0.35776904225349426\n",
      "Step: 5900  \tTraining accuracy: 0.822023332118988\n",
      "Step: 5900  \tValid loss: 0.3653389513492584\n",
      "Step: 6000  \tTraining loss: 0.35759198665618896\n",
      "Step: 6000  \tTraining accuracy: 0.822245717048645\n",
      "Step: 6000  \tValid loss: 0.36525392532348633\n",
      "Step: 6100  \tTraining loss: 0.3574182689189911\n",
      "Step: 6100  \tTraining accuracy: 0.822466254234314\n",
      "Step: 6100  \tValid loss: 0.3651335537433624\n",
      "Step: 6200  \tTraining loss: 0.35725149512290955\n",
      "Step: 6200  \tTraining accuracy: 0.8226814866065979\n",
      "Step: 6200  \tValid loss: 0.36503124237060547\n",
      "Step: 6300  \tTraining loss: 0.3570888042449951\n",
      "Step: 6300  \tTraining accuracy: 0.8228883147239685\n",
      "Step: 6300  \tValid loss: 0.36494630575180054\n",
      "Step: 6400  \tTraining loss: 0.3569270670413971\n",
      "Step: 6400  \tTraining accuracy: 0.8230831027030945\n",
      "Step: 6400  \tValid loss: 0.36487799882888794\n",
      "Step: 6500  \tTraining loss: 0.35677382349967957\n",
      "Step: 6500  \tTraining accuracy: 0.8232740163803101\n",
      "Step: 6500  \tValid loss: 0.3648112118244171\n",
      "Step: 6600  \tTraining loss: 0.35662007331848145\n",
      "Step: 6600  \tTraining accuracy: 0.8234698176383972\n",
      "Step: 6600  \tValid loss: 0.3647882640361786\n",
      "Step: 6700  \tTraining loss: 0.3564750850200653\n",
      "Step: 6700  \tTraining accuracy: 0.8236636519432068\n",
      "Step: 6700  \tValid loss: 0.36462485790252686\n",
      "Step: 6800  \tTraining loss: 0.35633793473243713\n",
      "Step: 6800  \tTraining accuracy: 0.8238531351089478\n",
      "Step: 6800  \tValid loss: 0.3645443022251129\n",
      "Step: 6900  \tTraining loss: 0.3561772108078003\n",
      "Step: 6900  \tTraining accuracy: 0.8240360617637634\n",
      "Step: 6900  \tValid loss: 0.36445969343185425\n",
      "Step: 7000  \tTraining loss: 0.3559611439704895\n",
      "Step: 7000  \tTraining accuracy: 0.8242079615592957\n",
      "Step: 7000  \tValid loss: 0.3644050359725952\n",
      "Step: 7100  \tTraining loss: 0.3558129072189331\n",
      "Step: 7100  \tTraining accuracy: 0.8243726491928101\n",
      "Step: 7100  \tValid loss: 0.36428549885749817\n",
      "Step: 7200  \tTraining loss: 0.35567331314086914\n",
      "Step: 7200  \tTraining accuracy: 0.8245373964309692\n",
      "Step: 7200  \tValid loss: 0.3641877770423889\n",
      "Step: 7300  \tTraining loss: 0.3554929792881012\n",
      "Step: 7300  \tTraining accuracy: 0.8247014284133911\n",
      "Step: 7300  \tValid loss: 0.36413076519966125\n",
      "Step: 7400  \tTraining loss: 0.3553408682346344\n",
      "Step: 7400  \tTraining accuracy: 0.8248628973960876\n",
      "Step: 7400  \tValid loss: 0.3639802932739258\n",
      "Step: 7500  \tTraining loss: 0.3551892638206482\n",
      "Step: 7500  \tTraining accuracy: 0.8250285387039185\n",
      "Step: 7500  \tValid loss: 0.36384817957878113\n",
      "Step: 7600  \tTraining loss: 0.3550620377063751\n",
      "Step: 7600  \tTraining accuracy: 0.825189471244812\n",
      "Step: 7600  \tValid loss: 0.36371567845344543\n",
      "Step: 7700  \tTraining loss: 0.35495537519454956\n",
      "Step: 7700  \tTraining accuracy: 0.8253465294837952\n",
      "Step: 7700  \tValid loss: 0.36364904046058655\n",
      "Step: 7800  \tTraining loss: 0.3548530340194702\n",
      "Step: 7800  \tTraining accuracy: 0.8254961967468262\n",
      "Step: 7800  \tValid loss: 0.36350661516189575\n",
      "Step: 7900  \tTraining loss: 0.35475432872772217\n",
      "Step: 7900  \tTraining accuracy: 0.8256465196609497\n",
      "Step: 7900  \tValid loss: 0.36340123414993286\n",
      "Step: 8000  \tTraining loss: 0.3546581268310547\n",
      "Step: 8000  \tTraining accuracy: 0.8257966637611389\n",
      "Step: 8000  \tValid loss: 0.3633122146129608\n",
      "Step: 8100  \tTraining loss: 0.3545731008052826\n",
      "Step: 8100  \tTraining accuracy: 0.8259479403495789\n",
      "Step: 8100  \tValid loss: 0.3632108271121979\n",
      "Step: 8200  \tTraining loss: 0.35448455810546875\n",
      "Step: 8200  \tTraining accuracy: 0.8260952830314636\n",
      "Step: 8200  \tValid loss: 0.36317670345306396\n",
      "Step: 8300  \tTraining loss: 0.354402631521225\n",
      "Step: 8300  \tTraining accuracy: 0.8262376189231873\n",
      "Step: 8300  \tValid loss: 0.3630932867527008\n",
      "Step: 8400  \tTraining loss: 0.35432273149490356\n",
      "Step: 8400  \tTraining accuracy: 0.8263751268386841\n",
      "Step: 8400  \tValid loss: 0.36303678154945374\n",
      "Step: 8500  \tTraining loss: 0.3542449176311493\n",
      "Step: 8500  \tTraining accuracy: 0.8265124559402466\n",
      "Step: 8500  \tValid loss: 0.3629980683326721\n",
      "Step: 8600  \tTraining loss: 0.35417047142982483\n",
      "Step: 8600  \tTraining accuracy: 0.8266503810882568\n",
      "Step: 8600  \tValid loss: 0.3628990948200226\n",
      "Step: 8700  \tTraining loss: 0.3540952503681183\n",
      "Step: 8700  \tTraining accuracy: 0.8267862200737\n",
      "Step: 8700  \tValid loss: 0.36285898089408875\n",
      "Step: 8800  \tTraining loss: 0.35402217507362366\n",
      "Step: 8800  \tTraining accuracy: 0.826913058757782\n",
      "Step: 8800  \tValid loss: 0.3628264367580414\n",
      "Step: 8900  \tTraining loss: 0.3539506494998932\n",
      "Step: 8900  \tTraining accuracy: 0.8270401954650879\n",
      "Step: 8900  \tValid loss: 0.3627511262893677\n",
      "Step: 9000  \tTraining loss: 0.35388123989105225\n",
      "Step: 9000  \tTraining accuracy: 0.8271650075912476\n",
      "Step: 9000  \tValid loss: 0.3626960515975952\n",
      "Step: 9100  \tTraining loss: 0.3538139760494232\n",
      "Step: 9100  \tTraining accuracy: 0.8272883892059326\n",
      "Step: 9100  \tValid loss: 0.36264050006866455\n",
      "Step: 9200  \tTraining loss: 0.3537505567073822\n",
      "Step: 9200  \tTraining accuracy: 0.8274105787277222\n",
      "Step: 9200  \tValid loss: 0.3625583052635193\n",
      "Step: 9300  \tTraining loss: 0.35368528962135315\n",
      "Step: 9300  \tTraining accuracy: 0.8275304436683655\n",
      "Step: 9300  \tValid loss: 0.36250972747802734\n",
      "Step: 9400  \tTraining loss: 0.35361891984939575\n",
      "Step: 9400  \tTraining accuracy: 0.8276466727256775\n",
      "Step: 9400  \tValid loss: 0.362501323223114\n",
      "Step: 9500  \tTraining loss: 0.3535563051700592\n",
      "Step: 9500  \tTraining accuracy: 0.8277597427368164\n",
      "Step: 9500  \tValid loss: 0.3624670207500458\n",
      "Step: 9600  \tTraining loss: 0.3534972071647644\n",
      "Step: 9600  \tTraining accuracy: 0.827872097492218\n",
      "Step: 9600  \tValid loss: 0.3623705804347992\n",
      "Step: 9700  \tTraining loss: 0.3534344732761383\n",
      "Step: 9700  \tTraining accuracy: 0.8279809951782227\n",
      "Step: 9700  \tValid loss: 0.36236488819122314\n",
      "Step: 9800  \tTraining loss: 0.3533753752708435\n",
      "Step: 9800  \tTraining accuracy: 0.8280883431434631\n",
      "Step: 9800  \tValid loss: 0.36231207847595215\n",
      "Step: 9900  \tTraining loss: 0.3533165454864502\n",
      "Step: 9900  \tTraining accuracy: 0.8281925320625305\n",
      "Step: 9900  \tValid loss: 0.3622919023036957\n",
      "Step: 10000  \tTraining loss: 0.35325878858566284\n",
      "Step: 10000  \tTraining accuracy: 0.8282913565635681\n",
      "Step: 10000  \tValid loss: 0.36225736141204834\n",
      "Step: 10100  \tTraining loss: 0.35320109128952026\n",
      "Step: 10100  \tTraining accuracy: 0.8283898234367371\n",
      "Step: 10100  \tValid loss: 0.3621754050254822\n",
      "Step: 10200  \tTraining loss: 0.3531435430049896\n",
      "Step: 10200  \tTraining accuracy: 0.8284875154495239\n",
      "Step: 10200  \tValid loss: 0.3621492385864258\n",
      "Step: 10300  \tTraining loss: 0.35308778285980225\n",
      "Step: 10300  \tTraining accuracy: 0.8285819292068481\n",
      "Step: 10300  \tValid loss: 0.3621520698070526\n",
      "Step: 10400  \tTraining loss: 0.3530316948890686\n",
      "Step: 10400  \tTraining accuracy: 0.8286729454994202\n",
      "Step: 10400  \tValid loss: 0.36207112669944763\n",
      "Step: 10500  \tTraining loss: 0.35297656059265137\n",
      "Step: 10500  \tTraining accuracy: 0.8287602066993713\n",
      "Step: 10500  \tValid loss: 0.3620779812335968\n",
      "Step: 10600  \tTraining loss: 0.35292214155197144\n",
      "Step: 10600  \tTraining accuracy: 0.8288422226905823\n",
      "Step: 10600  \tValid loss: 0.362074077129364\n",
      "Step: 10700  \tTraining loss: 0.35286787152290344\n",
      "Step: 10700  \tTraining accuracy: 0.8289214372634888\n",
      "Step: 10700  \tValid loss: 0.3620498478412628\n",
      "Step: 10800  \tTraining loss: 0.35281190276145935\n",
      "Step: 10800  \tTraining accuracy: 0.8289991021156311\n",
      "Step: 10800  \tValid loss: 0.3619626462459564\n",
      "Step: 10900  \tTraining loss: 0.3527592122554779\n",
      "Step: 10900  \tTraining accuracy: 0.8290740847587585\n",
      "Step: 10900  \tValid loss: 0.361920028924942\n",
      "Step: 11000  \tTraining loss: 0.3527056872844696\n",
      "Step: 11000  \tTraining accuracy: 0.8291468620300293\n",
      "Step: 11000  \tValid loss: 0.36190006136894226\n",
      "Step: 11100  \tTraining loss: 0.3526533246040344\n",
      "Step: 11100  \tTraining accuracy: 0.8292174339294434\n",
      "Step: 11100  \tValid loss: 0.36192548274993896\n",
      "Step: 11200  \tTraining loss: 0.3526003658771515\n",
      "Step: 11200  \tTraining accuracy: 0.829287588596344\n",
      "Step: 11200  \tValid loss: 0.36185139417648315\n",
      "Step: 11300  \tTraining loss: 0.3525474965572357\n",
      "Step: 11300  \tTraining accuracy: 0.8293527364730835\n",
      "Step: 11300  \tValid loss: 0.36183875799179077\n",
      "Step: 11400  \tTraining loss: 0.35249561071395874\n",
      "Step: 11400  \tTraining accuracy: 0.82941734790802\n",
      "Step: 11400  \tValid loss: 0.3618125021457672\n",
      "Step: 11500  \tTraining loss: 0.3524402976036072\n",
      "Step: 11500  \tTraining accuracy: 0.829483687877655\n",
      "Step: 11500  \tValid loss: 0.36183032393455505\n",
      "Step: 11600  \tTraining loss: 0.3523857295513153\n",
      "Step: 11600  \tTraining accuracy: 0.8295507431030273\n",
      "Step: 11600  \tValid loss: 0.3617860674858093\n",
      "Step: 11700  \tTraining loss: 0.3523311913013458\n",
      "Step: 11700  \tTraining accuracy: 0.8296146392822266\n",
      "Step: 11700  \tValid loss: 0.36172762513160706\n",
      "Step: 11800  \tTraining loss: 0.3522709310054779\n",
      "Step: 11800  \tTraining accuracy: 0.8296760320663452\n",
      "Step: 11800  \tValid loss: 0.36175620555877686\n",
      "Step: 11900  \tTraining loss: 0.35221531987190247\n",
      "Step: 11900  \tTraining accuracy: 0.8297399282455444\n",
      "Step: 11900  \tValid loss: 0.3617527186870575\n",
      "Step: 12000  \tTraining loss: 0.35215890407562256\n",
      "Step: 12000  \tTraining accuracy: 0.8298022150993347\n",
      "Step: 12000  \tValid loss: 0.3617108166217804\n",
      "Step: 12100  \tTraining loss: 0.35208776593208313\n",
      "Step: 12100  \tTraining accuracy: 0.8298647999763489\n",
      "Step: 12100  \tValid loss: 0.3616524338722229\n",
      "Step: 12200  \tTraining loss: 0.3520191013813019\n",
      "Step: 12200  \tTraining accuracy: 0.829927921295166\n",
      "Step: 12200  \tValid loss: 0.3616301119327545\n",
      "Step: 12300  \tTraining loss: 0.35188981890678406\n",
      "Step: 12300  \tTraining accuracy: 0.8299890160560608\n",
      "Step: 12300  \tValid loss: 0.36150091886520386\n",
      "Step: 12400  \tTraining loss: 0.35179346799850464\n",
      "Step: 12400  \tTraining accuracy: 0.8300472497940063\n",
      "Step: 12400  \tValid loss: 0.36143723130226135\n",
      "Step: 12500  \tTraining loss: 0.3517136573791504\n",
      "Step: 12500  \tTraining accuracy: 0.8301045894622803\n",
      "Step: 12500  \tValid loss: 0.3613542914390564\n",
      "Step: 12600  \tTraining loss: 0.35164403915405273\n",
      "Step: 12600  \tTraining accuracy: 0.8301615118980408\n",
      "Step: 12600  \tValid loss: 0.3613279461860657\n",
      "Step: 12700  \tTraining loss: 0.35157808661460876\n",
      "Step: 12700  \tTraining accuracy: 0.8302210569381714\n",
      "Step: 12700  \tValid loss: 0.36123594641685486\n",
      "Step: 12800  \tTraining loss: 0.3515177369117737\n",
      "Step: 12800  \tTraining accuracy: 0.8302785754203796\n",
      "Step: 12800  \tValid loss: 0.3612169623374939\n",
      "Step: 12900  \tTraining loss: 0.35146066546440125\n",
      "Step: 12900  \tTraining accuracy: 0.8303337693214417\n",
      "Step: 12900  \tValid loss: 0.3612114489078522\n",
      "Step: 13000  \tTraining loss: 0.3514048457145691\n",
      "Step: 13000  \tTraining accuracy: 0.8303897380828857\n",
      "Step: 13000  \tValid loss: 0.3611549735069275\n",
      "Step: 13100  \tTraining loss: 0.35135164856910706\n",
      "Step: 13100  \tTraining accuracy: 0.8304453492164612\n",
      "Step: 13100  \tValid loss: 0.3611307740211487\n",
      "Step: 13200  \tTraining loss: 0.35129785537719727\n",
      "Step: 13200  \tTraining accuracy: 0.8304990530014038\n",
      "Step: 13200  \tValid loss: 0.36110690236091614\n",
      "Step: 13300  \tTraining loss: 0.35124671459198\n",
      "Step: 13300  \tTraining accuracy: 0.8305521607398987\n",
      "Step: 13300  \tValid loss: 0.36111247539520264\n",
      "Step: 13400  \tTraining loss: 0.35119667649269104\n",
      "Step: 13400  \tTraining accuracy: 0.8306030035018921\n",
      "Step: 13400  \tValid loss: 0.3611052334308624\n",
      "Step: 13500  \tTraining loss: 0.35115063190460205\n",
      "Step: 13500  \tTraining accuracy: 0.8306536674499512\n",
      "Step: 13500  \tValid loss: 0.36114227771759033\n",
      "Step: 13600  \tTraining loss: 0.3511020839214325\n",
      "Step: 13600  \tTraining accuracy: 0.830702543258667\n",
      "Step: 13600  \tValid loss: 0.3610590398311615\n",
      "Step: 13700  \tTraining loss: 0.351054310798645\n",
      "Step: 13700  \tTraining accuracy: 0.830751895904541\n",
      "Step: 13700  \tValid loss: 0.3611025810241699\n",
      "Step: 13800  \tTraining loss: 0.3510103225708008\n",
      "Step: 13800  \tTraining accuracy: 0.8307991623878479\n",
      "Step: 13800  \tValid loss: 0.3610496520996094\n",
      "Step: 13900  \tTraining loss: 0.3509652316570282\n",
      "Step: 13900  \tTraining accuracy: 0.830846905708313\n",
      "Step: 13900  \tValid loss: 0.36108705401420593\n",
      "Step: 14000  \tTraining loss: 0.35091447830200195\n",
      "Step: 14000  \tTraining accuracy: 0.8308950066566467\n",
      "Step: 14000  \tValid loss: 0.36103010177612305\n",
      "Step: 14100  \tTraining loss: 0.3508675992488861\n",
      "Step: 14100  \tTraining accuracy: 0.8309400677680969\n",
      "Step: 14100  \tValid loss: 0.3609894812107086\n",
      "Step: 14200  \tTraining loss: 0.35082265734672546\n",
      "Step: 14200  \tTraining accuracy: 0.8309850096702576\n",
      "Step: 14200  \tValid loss: 0.36094290018081665\n",
      "Step: 14300  \tTraining loss: 0.3507785201072693\n",
      "Step: 14300  \tTraining accuracy: 0.8310297727584839\n",
      "Step: 14300  \tValid loss: 0.360973984003067\n",
      "Step: 14400  \tTraining loss: 0.3507362902164459\n",
      "Step: 14400  \tTraining accuracy: 0.8310762643814087\n",
      "Step: 14400  \tValid loss: 0.36091700196266174\n",
      "Step: 14500  \tTraining loss: 0.3506929278373718\n",
      "Step: 14500  \tTraining accuracy: 0.8311219215393066\n",
      "Step: 14500  \tValid loss: 0.3609391748905182\n",
      "Step: 14600  \tTraining loss: 0.35064926743507385\n",
      "Step: 14600  \tTraining accuracy: 0.8311672210693359\n",
      "Step: 14600  \tValid loss: 0.3608701527118683\n",
      "Step: 14700  \tTraining loss: 0.3506074845790863\n",
      "Step: 14700  \tTraining accuracy: 0.8312126398086548\n",
      "Step: 14700  \tValid loss: 0.3608672618865967\n",
      "Step: 14800  \tTraining loss: 0.3505675494670868\n",
      "Step: 14800  \tTraining accuracy: 0.8312572240829468\n",
      "Step: 14800  \tValid loss: 0.3608458936214447\n",
      "Step: 14900  \tTraining loss: 0.35052984952926636\n",
      "Step: 14900  \tTraining accuracy: 0.8313019871711731\n",
      "Step: 14900  \tValid loss: 0.36087778210639954\n",
      "Step: 15000  \tTraining loss: 0.3504912257194519\n",
      "Step: 15000  \tTraining accuracy: 0.8313461542129517\n",
      "Step: 15000  \tValid loss: 0.36085009574890137\n",
      "Step: 15100  \tTraining loss: 0.3504534065723419\n",
      "Step: 15100  \tTraining accuracy: 0.8313900828361511\n",
      "Step: 15100  \tValid loss: 0.36083394289016724\n",
      "Step: 15200  \tTraining loss: 0.3504156768321991\n",
      "Step: 15200  \tTraining accuracy: 0.8314343690872192\n",
      "Step: 15200  \tValid loss: 0.3608088791370392\n",
      "Step: 15300  \tTraining loss: 0.35038089752197266\n",
      "Step: 15300  \tTraining accuracy: 0.831479549407959\n",
      "Step: 15300  \tValid loss: 0.36077383160591125\n",
      "Step: 15400  \tTraining loss: 0.3503512144088745\n",
      "Step: 15400  \tTraining accuracy: 0.8315226435661316\n",
      "Step: 15400  \tValid loss: 0.3608454763889313\n",
      "Step: 15500  \tTraining loss: 0.35031187534332275\n",
      "Step: 15500  \tTraining accuracy: 0.8315659761428833\n",
      "Step: 15500  \tValid loss: 0.3607656955718994\n",
      "Step: 15600  \tTraining loss: 0.3502788543701172\n",
      "Step: 15600  \tTraining accuracy: 0.8316085338592529\n",
      "Step: 15600  \tValid loss: 0.36072707176208496\n",
      "Step: 15700  \tTraining loss: 0.3502381443977356\n",
      "Step: 15700  \tTraining accuracy: 0.8316510319709778\n",
      "Step: 15700  \tValid loss: 0.3607572317123413\n",
      "Step: 15800  \tTraining loss: 0.3501967787742615\n",
      "Step: 15800  \tTraining accuracy: 0.8316937685012817\n",
      "Step: 15800  \tValid loss: 0.3607126772403717\n",
      "Step: 15900  \tTraining loss: 0.3501499891281128\n",
      "Step: 15900  \tTraining accuracy: 0.8317351937294006\n",
      "Step: 15900  \tValid loss: 0.36070412397384644\n",
      "Step: 16000  \tTraining loss: 0.35011065006256104\n",
      "Step: 16000  \tTraining accuracy: 0.8317760825157166\n",
      "Step: 16000  \tValid loss: 0.3607565462589264\n",
      "Step: 16100  \tTraining loss: 0.35006988048553467\n",
      "Step: 16100  \tTraining accuracy: 0.8318164944648743\n",
      "Step: 16100  \tValid loss: 0.36071059107780457\n",
      "Step: 16200  \tTraining loss: 0.3500213921070099\n",
      "Step: 16200  \tTraining accuracy: 0.8318559527397156\n",
      "Step: 16200  \tValid loss: 0.3606937825679779\n",
      "Step: 16300  \tTraining loss: 0.3499731421470642\n",
      "Step: 16300  \tTraining accuracy: 0.8318954706192017\n",
      "Step: 16300  \tValid loss: 0.36068639159202576\n",
      "Step: 16400  \tTraining loss: 0.34987926483154297\n",
      "Step: 16400  \tTraining accuracy: 0.8319331407546997\n",
      "Step: 16400  \tValid loss: 0.3607608675956726\n",
      "Step: 16500  \tTraining loss: 0.34982576966285706\n",
      "Step: 16500  \tTraining accuracy: 0.8319693207740784\n",
      "Step: 16500  \tValid loss: 0.3608076274394989\n",
      "Step: 16600  \tTraining loss: 0.3497844338417053\n",
      "Step: 16600  \tTraining accuracy: 0.8320046067237854\n",
      "Step: 16600  \tValid loss: 0.36080002784729004\n",
      "Step: 16700  \tTraining loss: 0.3497471511363983\n",
      "Step: 16700  \tTraining accuracy: 0.8320392370223999\n",
      "Step: 16700  \tValid loss: 0.3608294725418091\n",
      "Step: 16800  \tTraining loss: 0.3497087359428406\n",
      "Step: 16800  \tTraining accuracy: 0.8320731520652771\n",
      "Step: 16800  \tValid loss: 0.3608522117137909\n",
      "Step: 16900  \tTraining loss: 0.3496745228767395\n",
      "Step: 16900  \tTraining accuracy: 0.8321074843406677\n",
      "Step: 16900  \tValid loss: 0.36078736186027527\n",
      "Step: 17000  \tTraining loss: 0.34963929653167725\n",
      "Step: 17000  \tTraining accuracy: 0.8321425318717957\n",
      "Step: 17000  \tValid loss: 0.3608117401599884\n",
      "Step: 17100  \tTraining loss: 0.3496076464653015\n",
      "Step: 17100  \tTraining accuracy: 0.8321767449378967\n",
      "Step: 17100  \tValid loss: 0.3607757091522217\n",
      "Step: 17200  \tTraining loss: 0.3495774269104004\n",
      "Step: 17200  \tTraining accuracy: 0.8322111368179321\n",
      "Step: 17200  \tValid loss: 0.36075231432914734\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.83224535\n",
      "Precision: 0.8708775\n",
      "Recall: 0.92046815\n",
      "F1 score: 0.8552848\n",
      "AUC: 0.7602155\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.832245   0.870878  0.920468  0.855285  0.760216  0.349557      0.832226   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.360654       0.832218   0.340769      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  17248.0  \n",
      "2\n",
      "(3770, 8)\n",
      "(3770, 1)\n",
      "(2080, 8)\n",
      "(2080, 1)\n",
      "(1690, 8)\n",
      "(1690, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5979021787643433\n",
      "Step: 100  \tTraining accuracy: 0.66366046667099\n",
      "Step: 100  \tValid loss: 0.5855364203453064\n",
      "Step: 200  \tTraining loss: 0.578339695930481\n",
      "Step: 200  \tTraining accuracy: 0.6680813431739807\n",
      "Step: 200  \tValid loss: 0.5642643570899963\n",
      "Step: 300  \tTraining loss: 0.5619208216667175\n",
      "Step: 300  \tTraining accuracy: 0.6778249144554138\n",
      "Step: 300  \tValid loss: 0.5463973879814148\n",
      "Step: 400  \tTraining loss: 0.5540898442268372\n",
      "Step: 400  \tTraining accuracy: 0.6863206028938293\n",
      "Step: 400  \tValid loss: 0.5390844345092773\n",
      "Step: 500  \tTraining loss: 0.5499165058135986\n",
      "Step: 500  \tTraining accuracy: 0.691895067691803\n",
      "Step: 500  \tValid loss: 0.5347586870193481\n",
      "Step: 600  \tTraining loss: 0.5469805002212524\n",
      "Step: 600  \tTraining accuracy: 0.6959971189498901\n",
      "Step: 600  \tValid loss: 0.5325190424919128\n",
      "Step: 700  \tTraining loss: 0.5441792607307434\n",
      "Step: 700  \tTraining accuracy: 0.6991634368896484\n",
      "Step: 700  \tValid loss: 0.530225396156311\n",
      "Step: 800  \tTraining loss: 0.5418705940246582\n",
      "Step: 800  \tTraining accuracy: 0.7015738487243652\n",
      "Step: 800  \tValid loss: 0.5277726650238037\n",
      "Step: 900  \tTraining loss: 0.539789080619812\n",
      "Step: 900  \tTraining accuracy: 0.7033390402793884\n",
      "Step: 900  \tValid loss: 0.5253341197967529\n",
      "Step: 1000  \tTraining loss: 0.5378400683403015\n",
      "Step: 1000  \tTraining accuracy: 0.7046489119529724\n",
      "Step: 1000  \tValid loss: 0.5230484008789062\n",
      "Step: 1100  \tTraining loss: 0.5358409285545349\n",
      "Step: 1100  \tTraining accuracy: 0.706403911113739\n",
      "Step: 1100  \tValid loss: 0.5212449431419373\n",
      "Step: 1200  \tTraining loss: 0.5336657762527466\n",
      "Step: 1200  \tTraining accuracy: 0.7080959677696228\n",
      "Step: 1200  \tValid loss: 0.518798828125\n",
      "Step: 1300  \tTraining loss: 0.5314403772354126\n",
      "Step: 1300  \tTraining accuracy: 0.7094111442565918\n",
      "Step: 1300  \tValid loss: 0.5160771012306213\n",
      "Step: 1400  \tTraining loss: 0.5292994379997253\n",
      "Step: 1400  \tTraining accuracy: 0.7108065485954285\n",
      "Step: 1400  \tValid loss: 0.5134057998657227\n",
      "Step: 1500  \tTraining loss: 0.5273820161819458\n",
      "Step: 1500  \tTraining accuracy: 0.7120186686515808\n",
      "Step: 1500  \tValid loss: 0.5113304257392883\n",
      "Step: 1600  \tTraining loss: 0.5257860422134399\n",
      "Step: 1600  \tTraining accuracy: 0.7130658030509949\n",
      "Step: 1600  \tValid loss: 0.5099310874938965\n",
      "Step: 1700  \tTraining loss: 0.5245062708854675\n",
      "Step: 1700  \tTraining accuracy: 0.7142753601074219\n",
      "Step: 1700  \tValid loss: 0.5088092684745789\n",
      "Step: 1800  \tTraining loss: 0.5234630703926086\n",
      "Step: 1800  \tTraining accuracy: 0.7153618931770325\n",
      "Step: 1800  \tValid loss: 0.5079157948493958\n",
      "Step: 1900  \tTraining loss: 0.5225757956504822\n",
      "Step: 1900  \tTraining accuracy: 0.7162879109382629\n",
      "Step: 1900  \tValid loss: 0.5071729421615601\n",
      "Step: 2000  \tTraining loss: 0.521756112575531\n",
      "Step: 2000  \tTraining accuracy: 0.7172345519065857\n",
      "Step: 2000  \tValid loss: 0.5066143274307251\n",
      "Step: 2100  \tTraining loss: 0.520916759967804\n",
      "Step: 2100  \tTraining accuracy: 0.7181147933006287\n",
      "Step: 2100  \tValid loss: 0.5065430998802185\n",
      "Step: 2200  \tTraining loss: 0.5201300978660583\n",
      "Step: 2200  \tTraining accuracy: 0.718925416469574\n",
      "Step: 2200  \tValid loss: 0.5061373114585876\n",
      "Step: 2300  \tTraining loss: 0.519412100315094\n",
      "Step: 2300  \tTraining accuracy: 0.7197701334953308\n",
      "Step: 2300  \tValid loss: 0.5056890249252319\n",
      "Step: 2400  \tTraining loss: 0.518663227558136\n",
      "Step: 2400  \tTraining accuracy: 0.7206388711929321\n",
      "Step: 2400  \tValid loss: 0.5053887367248535\n",
      "Step: 2500  \tTraining loss: 0.5179175138473511\n",
      "Step: 2500  \tTraining accuracy: 0.7215179204940796\n",
      "Step: 2500  \tValid loss: 0.5050491094589233\n",
      "Step: 2600  \tTraining loss: 0.5172064900398254\n",
      "Step: 2600  \tTraining accuracy: 0.7223331928253174\n",
      "Step: 2600  \tValid loss: 0.5045215487480164\n",
      "Step: 2700  \tTraining loss: 0.5165261626243591\n",
      "Step: 2700  \tTraining accuracy: 0.7231770157814026\n",
      "Step: 2700  \tValid loss: 0.5040462017059326\n",
      "Step: 2800  \tTraining loss: 0.5156873464584351\n",
      "Step: 2800  \tTraining accuracy: 0.7239691615104675\n",
      "Step: 2800  \tValid loss: 0.5030788779258728\n",
      "Step: 2900  \tTraining loss: 0.5150041580200195\n",
      "Step: 2900  \tTraining accuracy: 0.7247196435928345\n",
      "Step: 2900  \tValid loss: 0.5026780366897583\n",
      "Step: 3000  \tTraining loss: 0.5143737196922302\n",
      "Step: 3000  \tTraining accuracy: 0.7254192233085632\n",
      "Step: 3000  \tValid loss: 0.502161979675293\n",
      "Step: 3100  \tTraining loss: 0.5137778520584106\n",
      "Step: 3100  \tTraining accuracy: 0.7260294556617737\n",
      "Step: 3100  \tValid loss: 0.5018623471260071\n",
      "Step: 3200  \tTraining loss: 0.513219952583313\n",
      "Step: 3200  \tTraining accuracy: 0.7265715003013611\n",
      "Step: 3200  \tValid loss: 0.5015225410461426\n",
      "Step: 3300  \tTraining loss: 0.5126876831054688\n",
      "Step: 3300  \tTraining accuracy: 0.7270638942718506\n",
      "Step: 3300  \tValid loss: 0.5011430978775024\n",
      "Step: 3400  \tTraining loss: 0.512167751789093\n",
      "Step: 3400  \tTraining accuracy: 0.7275109887123108\n",
      "Step: 3400  \tValid loss: 0.5006887316703796\n",
      "Step: 3500  \tTraining loss: 0.5116663575172424\n",
      "Step: 3500  \tTraining accuracy: 0.7279629707336426\n",
      "Step: 3500  \tValid loss: 0.5004181265830994\n",
      "Step: 3600  \tTraining loss: 0.5112051963806152\n",
      "Step: 3600  \tTraining accuracy: 0.7283782362937927\n",
      "Step: 3600  \tValid loss: 0.500140368938446\n",
      "Step: 3700  \tTraining loss: 0.5107346773147583\n",
      "Step: 3700  \tTraining accuracy: 0.7287889122962952\n",
      "Step: 3700  \tValid loss: 0.4999825060367584\n",
      "Step: 3800  \tTraining loss: 0.5102996230125427\n",
      "Step: 3800  \tTraining accuracy: 0.7292060256004333\n",
      "Step: 3800  \tValid loss: 0.49980592727661133\n",
      "Step: 3900  \tTraining loss: 0.5098954439163208\n",
      "Step: 3900  \tTraining accuracy: 0.7296393513679504\n",
      "Step: 3900  \tValid loss: 0.4996037483215332\n",
      "Step: 4000  \tTraining loss: 0.5094994902610779\n",
      "Step: 4000  \tTraining accuracy: 0.7300540804862976\n",
      "Step: 4000  \tValid loss: 0.4994450509548187\n",
      "Step: 4100  \tTraining loss: 0.5091280937194824\n",
      "Step: 4100  \tTraining accuracy: 0.7304417490959167\n",
      "Step: 4100  \tValid loss: 0.49924221634864807\n",
      "Step: 4200  \tTraining loss: 0.5087729096412659\n",
      "Step: 4200  \tTraining accuracy: 0.7307692170143127\n",
      "Step: 4200  \tValid loss: 0.49900418519973755\n",
      "Step: 4300  \tTraining loss: 0.5083585381507874\n",
      "Step: 4300  \tTraining accuracy: 0.7310625910758972\n",
      "Step: 4300  \tValid loss: 0.4988064169883728\n",
      "Step: 4400  \tTraining loss: 0.5080059170722961\n",
      "Step: 4400  \tTraining accuracy: 0.7313515543937683\n",
      "Step: 4400  \tValid loss: 0.49852657318115234\n",
      "Step: 4500  \tTraining loss: 0.5076804161071777\n",
      "Step: 4500  \tTraining accuracy: 0.7316156625747681\n",
      "Step: 4500  \tValid loss: 0.4983634948730469\n",
      "Step: 4600  \tTraining loss: 0.5072625875473022\n",
      "Step: 4600  \tTraining accuracy: 0.7318768501281738\n",
      "Step: 4600  \tValid loss: 0.4977697432041168\n",
      "Step: 4700  \tTraining loss: 0.5068775415420532\n",
      "Step: 4700  \tTraining accuracy: 0.7321325540542603\n",
      "Step: 4700  \tValid loss: 0.4972406327724457\n",
      "Step: 4800  \tTraining loss: 0.5065575242042542\n",
      "Step: 4800  \tTraining accuracy: 0.7323942184448242\n",
      "Step: 4800  \tValid loss: 0.49693596363067627\n",
      "Step: 4900  \tTraining loss: 0.5062569975852966\n",
      "Step: 4900  \tTraining accuracy: 0.7326341867446899\n",
      "Step: 4900  \tValid loss: 0.4967063367366791\n",
      "Step: 5000  \tTraining loss: 0.5059401392936707\n",
      "Step: 5000  \tTraining accuracy: 0.7328617572784424\n",
      "Step: 5000  \tValid loss: 0.49644845724105835\n",
      "Step: 5100  \tTraining loss: 0.5056097507476807\n",
      "Step: 5100  \tTraining accuracy: 0.7330803275108337\n",
      "Step: 5100  \tValid loss: 0.49617379903793335\n",
      "Step: 5200  \tTraining loss: 0.5052747130393982\n",
      "Step: 5200  \tTraining accuracy: 0.7332775592803955\n",
      "Step: 5200  \tValid loss: 0.49593958258628845\n",
      "Step: 5300  \tTraining loss: 0.504950225353241\n",
      "Step: 5300  \tTraining accuracy: 0.7334445118904114\n",
      "Step: 5300  \tValid loss: 0.49570193886756897\n",
      "Step: 5400  \tTraining loss: 0.504626989364624\n",
      "Step: 5400  \tTraining accuracy: 0.7336225509643555\n",
      "Step: 5400  \tValid loss: 0.4954768121242523\n",
      "Step: 5500  \tTraining loss: 0.5042446851730347\n",
      "Step: 5500  \tTraining accuracy: 0.7337892055511475\n",
      "Step: 5500  \tValid loss: 0.49536561965942383\n",
      "Step: 5600  \tTraining loss: 0.503883421421051\n",
      "Step: 5600  \tTraining accuracy: 0.733971357345581\n",
      "Step: 5600  \tValid loss: 0.4952297508716583\n",
      "Step: 5700  \tTraining loss: 0.503511369228363\n",
      "Step: 5700  \tTraining accuracy: 0.7341400384902954\n",
      "Step: 5700  \tValid loss: 0.49500027298927307\n",
      "Step: 5800  \tTraining loss: 0.5031452178955078\n",
      "Step: 5800  \tTraining accuracy: 0.7342636585235596\n",
      "Step: 5800  \tValid loss: 0.49481648206710815\n",
      "Step: 5900  \tTraining loss: 0.5027977824211121\n",
      "Step: 5900  \tTraining accuracy: 0.734378457069397\n",
      "Step: 5900  \tValid loss: 0.4945922791957855\n",
      "Step: 6000  \tTraining loss: 0.5024685263633728\n",
      "Step: 6000  \tTraining accuracy: 0.7344872355461121\n",
      "Step: 6000  \tValid loss: 0.4944034218788147\n",
      "Step: 6100  \tTraining loss: 0.5021618008613586\n",
      "Step: 6100  \tTraining accuracy: 0.7345857620239258\n",
      "Step: 6100  \tValid loss: 0.494255006313324\n",
      "Step: 6200  \tTraining loss: 0.501876175403595\n",
      "Step: 6200  \tTraining accuracy: 0.7346660494804382\n",
      "Step: 6200  \tValid loss: 0.49408668279647827\n",
      "Step: 6300  \tTraining loss: 0.5016045570373535\n",
      "Step: 6300  \tTraining accuracy: 0.7347501516342163\n",
      "Step: 6300  \tValid loss: 0.49391210079193115\n",
      "Step: 6400  \tTraining loss: 0.5013394355773926\n",
      "Step: 6400  \tTraining accuracy: 0.7348482608795166\n",
      "Step: 6400  \tValid loss: 0.49375611543655396\n",
      "Step: 6500  \tTraining loss: 0.5010820627212524\n",
      "Step: 6500  \tTraining accuracy: 0.734941303730011\n",
      "Step: 6500  \tValid loss: 0.4935868978500366\n",
      "Step: 6600  \tTraining loss: 0.5008191466331482\n",
      "Step: 6600  \tTraining accuracy: 0.7350355386734009\n",
      "Step: 6600  \tValid loss: 0.4934271574020386\n",
      "Step: 6700  \tTraining loss: 0.5005677342414856\n",
      "Step: 6700  \tTraining accuracy: 0.7351369261741638\n",
      "Step: 6700  \tValid loss: 0.4932838976383209\n",
      "Step: 6800  \tTraining loss: 0.5003247261047363\n",
      "Step: 6800  \tTraining accuracy: 0.7352490425109863\n",
      "Step: 6800  \tValid loss: 0.4931340217590332\n",
      "Step: 6900  \tTraining loss: 0.5000928640365601\n",
      "Step: 6900  \tTraining accuracy: 0.7353637218475342\n",
      "Step: 6900  \tValid loss: 0.49297332763671875\n",
      "Step: 7000  \tTraining loss: 0.499866783618927\n",
      "Step: 7000  \tTraining accuracy: 0.7354846000671387\n",
      "Step: 7000  \tValid loss: 0.4928779602050781\n",
      "Step: 7100  \tTraining loss: 0.49964696168899536\n",
      "Step: 7100  \tTraining accuracy: 0.7356133460998535\n",
      "Step: 7100  \tValid loss: 0.4927810728549957\n",
      "Step: 7200  \tTraining loss: 0.49943307042121887\n",
      "Step: 7200  \tTraining accuracy: 0.7357366681098938\n",
      "Step: 7200  \tValid loss: 0.49267590045928955\n",
      "Step: 7300  \tTraining loss: 0.4991902709007263\n",
      "Step: 7300  \tTraining accuracy: 0.7358474135398865\n",
      "Step: 7300  \tValid loss: 0.4925122857093811\n",
      "Step: 7400  \tTraining loss: 0.49896249175071716\n",
      "Step: 7400  \tTraining accuracy: 0.7359353303909302\n",
      "Step: 7400  \tValid loss: 0.4923373758792877\n",
      "Step: 7500  \tTraining loss: 0.49874866008758545\n",
      "Step: 7500  \tTraining accuracy: 0.736040472984314\n",
      "Step: 7500  \tValid loss: 0.4921712577342987\n",
      "Step: 7600  \tTraining loss: 0.498544305562973\n",
      "Step: 7600  \tTraining accuracy: 0.7361375093460083\n",
      "Step: 7600  \tValid loss: 0.4920007884502411\n",
      "Step: 7700  \tTraining loss: 0.4983459413051605\n",
      "Step: 7700  \tTraining accuracy: 0.7362441420555115\n",
      "Step: 7700  \tValid loss: 0.49186617136001587\n",
      "Step: 7800  \tTraining loss: 0.498151570558548\n",
      "Step: 7800  \tTraining accuracy: 0.7363549470901489\n",
      "Step: 7800  \tValid loss: 0.49173253774642944\n",
      "Step: 7900  \tTraining loss: 0.49794191122055054\n",
      "Step: 7900  \tTraining accuracy: 0.7364526987075806\n",
      "Step: 7900  \tValid loss: 0.49151429533958435\n",
      "Step: 8000  \tTraining loss: 0.4977469742298126\n",
      "Step: 8000  \tTraining accuracy: 0.7365313768386841\n",
      "Step: 8000  \tValid loss: 0.4913262724876404\n",
      "Step: 8100  \tTraining loss: 0.4975472390651703\n",
      "Step: 8100  \tTraining accuracy: 0.7366047501564026\n",
      "Step: 8100  \tValid loss: 0.49113523960113525\n",
      "Step: 8200  \tTraining loss: 0.4972984492778778\n",
      "Step: 8200  \tTraining accuracy: 0.7366698384284973\n",
      "Step: 8200  \tValid loss: 0.4908561110496521\n",
      "Step: 8300  \tTraining loss: 0.49710461497306824\n",
      "Step: 8300  \tTraining accuracy: 0.7367349863052368\n",
      "Step: 8300  \tValid loss: 0.4906197488307953\n",
      "Step: 8400  \tTraining loss: 0.49691441655158997\n",
      "Step: 8400  \tTraining accuracy: 0.7368017435073853\n",
      "Step: 8400  \tValid loss: 0.4904029965400696\n",
      "Step: 8500  \tTraining loss: 0.49672573804855347\n",
      "Step: 8500  \tTraining accuracy: 0.7368731498718262\n",
      "Step: 8500  \tValid loss: 0.4902391731739044\n",
      "Step: 8600  \tTraining loss: 0.4965420365333557\n",
      "Step: 8600  \tTraining accuracy: 0.7369460463523865\n",
      "Step: 8600  \tValid loss: 0.4900631010532379\n",
      "Step: 8700  \tTraining loss: 0.49636462330818176\n",
      "Step: 8700  \tTraining accuracy: 0.7370095252990723\n",
      "Step: 8700  \tValid loss: 0.4898611605167389\n",
      "Step: 8800  \tTraining loss: 0.4961841106414795\n",
      "Step: 8800  \tTraining accuracy: 0.7370700836181641\n",
      "Step: 8800  \tValid loss: 0.4897156357765198\n",
      "Step: 8900  \tTraining loss: 0.4960016906261444\n",
      "Step: 8900  \tTraining accuracy: 0.73714280128479\n",
      "Step: 8900  \tValid loss: 0.4895564913749695\n",
      "Step: 9000  \tTraining loss: 0.495816707611084\n",
      "Step: 9000  \tTraining accuracy: 0.7372123599052429\n",
      "Step: 9000  \tValid loss: 0.48941826820373535\n",
      "Step: 9100  \tTraining loss: 0.49563854932785034\n",
      "Step: 9100  \tTraining accuracy: 0.7372774481773376\n",
      "Step: 9100  \tValid loss: 0.4892171323299408\n",
      "Step: 9200  \tTraining loss: 0.495443195104599\n",
      "Step: 9200  \tTraining accuracy: 0.7373440265655518\n",
      "Step: 9200  \tValid loss: 0.4890788793563843\n",
      "Step: 9300  \tTraining loss: 0.49522677063941956\n",
      "Step: 9300  \tTraining accuracy: 0.7374091148376465\n",
      "Step: 9300  \tValid loss: 0.4889085292816162\n",
      "Step: 9400  \tTraining loss: 0.49504750967025757\n",
      "Step: 9400  \tTraining accuracy: 0.7374728918075562\n",
      "Step: 9400  \tValid loss: 0.4887234568595886\n",
      "Step: 9500  \tTraining loss: 0.4948712885379791\n",
      "Step: 9500  \tTraining accuracy: 0.7375324368476868\n",
      "Step: 9500  \tValid loss: 0.48859578371047974\n",
      "Step: 9600  \tTraining loss: 0.4946984052658081\n",
      "Step: 9600  \tTraining accuracy: 0.7375810742378235\n",
      "Step: 9600  \tValid loss: 0.48846468329429626\n",
      "Step: 9700  \tTraining loss: 0.4945248067378998\n",
      "Step: 9700  \tTraining accuracy: 0.7376382946968079\n",
      "Step: 9700  \tValid loss: 0.4882916808128357\n",
      "Step: 9800  \tTraining loss: 0.4943508505821228\n",
      "Step: 9800  \tTraining accuracy: 0.7376970648765564\n",
      "Step: 9800  \tValid loss: 0.48813846707344055\n",
      "Step: 9900  \tTraining loss: 0.49417853355407715\n",
      "Step: 9900  \tTraining accuracy: 0.7377519607543945\n",
      "Step: 9900  \tValid loss: 0.4879889488220215\n",
      "Step: 10000  \tTraining loss: 0.4939981997013092\n",
      "Step: 10000  \tTraining accuracy: 0.7378084063529968\n",
      "Step: 10000  \tValid loss: 0.4878242313861847\n",
      "Step: 10100  \tTraining loss: 0.49381911754608154\n",
      "Step: 10100  \tTraining accuracy: 0.7378729581832886\n",
      "Step: 10100  \tValid loss: 0.4876795709133148\n",
      "Step: 10200  \tTraining loss: 0.4936375319957733\n",
      "Step: 10200  \tTraining accuracy: 0.7379466891288757\n",
      "Step: 10200  \tValid loss: 0.4875069856643677\n",
      "Step: 10300  \tTraining loss: 0.4934542775154114\n",
      "Step: 10300  \tTraining accuracy: 0.7380229234695435\n",
      "Step: 10300  \tValid loss: 0.48735523223876953\n",
      "Step: 10400  \tTraining loss: 0.49326783418655396\n",
      "Step: 10400  \tTraining accuracy: 0.7381066083908081\n",
      "Step: 10400  \tValid loss: 0.487190306186676\n",
      "Step: 10500  \tTraining loss: 0.493080198764801\n",
      "Step: 10500  \tTraining accuracy: 0.7381886839866638\n",
      "Step: 10500  \tValid loss: 0.48704206943511963\n",
      "Step: 10600  \tTraining loss: 0.49288836121559143\n",
      "Step: 10600  \tTraining accuracy: 0.7382628917694092\n",
      "Step: 10600  \tValid loss: 0.4868775010108948\n",
      "Step: 10700  \tTraining loss: 0.4926741421222687\n",
      "Step: 10700  \tTraining accuracy: 0.7383432388305664\n",
      "Step: 10700  \tValid loss: 0.486571729183197\n",
      "Step: 10800  \tTraining loss: 0.49247241020202637\n",
      "Step: 10800  \tTraining accuracy: 0.7384257316589355\n",
      "Step: 10800  \tValid loss: 0.48637863993644714\n",
      "Step: 10900  \tTraining loss: 0.4922676682472229\n",
      "Step: 10900  \tTraining accuracy: 0.7385079860687256\n",
      "Step: 10900  \tValid loss: 0.48620250821113586\n",
      "Step: 11000  \tTraining loss: 0.4920498728752136\n",
      "Step: 11000  \tTraining accuracy: 0.7385959625244141\n",
      "Step: 11000  \tValid loss: 0.4859231114387512\n",
      "Step: 11100  \tTraining loss: 0.49182912707328796\n",
      "Step: 11100  \tTraining accuracy: 0.7386896014213562\n",
      "Step: 11100  \tValid loss: 0.48566392064094543\n",
      "Step: 11200  \tTraining loss: 0.4916055202484131\n",
      "Step: 11200  \tTraining accuracy: 0.7387815117835999\n",
      "Step: 11200  \tValid loss: 0.48543208837509155\n",
      "Step: 11300  \tTraining loss: 0.4913787841796875\n",
      "Step: 11300  \tTraining accuracy: 0.7388647198677063\n",
      "Step: 11300  \tValid loss: 0.4852108359336853\n",
      "Step: 11400  \tTraining loss: 0.49114909768104553\n",
      "Step: 11400  \tTraining accuracy: 0.7389464974403381\n",
      "Step: 11400  \tValid loss: 0.4850017726421356\n",
      "Step: 11500  \tTraining loss: 0.49091577529907227\n",
      "Step: 11500  \tTraining accuracy: 0.7390279769897461\n",
      "Step: 11500  \tValid loss: 0.48480549454689026\n",
      "Step: 11600  \tTraining loss: 0.4906691610813141\n",
      "Step: 11600  \tTraining accuracy: 0.7391091585159302\n",
      "Step: 11600  \tValid loss: 0.48451581597328186\n",
      "Step: 11700  \tTraining loss: 0.4904089868068695\n",
      "Step: 11700  \tTraining accuracy: 0.7391958236694336\n",
      "Step: 11700  \tValid loss: 0.48425909876823425\n",
      "Step: 11800  \tTraining loss: 0.4901474416255951\n",
      "Step: 11800  \tTraining accuracy: 0.7392866611480713\n",
      "Step: 11800  \tValid loss: 0.4839927554130554\n",
      "Step: 11900  \tTraining loss: 0.4898817837238312\n",
      "Step: 11900  \tTraining accuracy: 0.7393893599510193\n",
      "Step: 11900  \tValid loss: 0.4837317168712616\n",
      "Step: 12000  \tTraining loss: 0.48961037397384644\n",
      "Step: 12000  \tTraining accuracy: 0.7395025491714478\n",
      "Step: 12000  \tValid loss: 0.48341891169548035\n",
      "Step: 12100  \tTraining loss: 0.48933011293411255\n",
      "Step: 12100  \tTraining accuracy: 0.7396094799041748\n",
      "Step: 12100  \tValid loss: 0.4831295311450958\n",
      "Step: 12200  \tTraining loss: 0.4890088438987732\n",
      "Step: 12200  \tTraining accuracy: 0.7397201061248779\n",
      "Step: 12200  \tValid loss: 0.4826776385307312\n",
      "Step: 12300  \tTraining loss: 0.48870646953582764\n",
      "Step: 12300  \tTraining accuracy: 0.7398256659507751\n",
      "Step: 12300  \tValid loss: 0.48232772946357727\n",
      "Step: 12400  \tTraining loss: 0.4884050786495209\n",
      "Step: 12400  \tTraining accuracy: 0.7399381399154663\n",
      "Step: 12400  \tValid loss: 0.48194196820259094\n",
      "Step: 12500  \tTraining loss: 0.4881037473678589\n",
      "Step: 12500  \tTraining accuracy: 0.7400519847869873\n",
      "Step: 12500  \tValid loss: 0.48158639669418335\n",
      "Step: 12600  \tTraining loss: 0.4878060221672058\n",
      "Step: 12600  \tTraining accuracy: 0.740164041519165\n",
      "Step: 12600  \tValid loss: 0.4812573194503784\n",
      "Step: 12700  \tTraining loss: 0.4875130355358124\n",
      "Step: 12700  \tTraining accuracy: 0.7402763366699219\n",
      "Step: 12700  \tValid loss: 0.4809240698814392\n",
      "Step: 12800  \tTraining loss: 0.4872238337993622\n",
      "Step: 12800  \tTraining accuracy: 0.7403827905654907\n",
      "Step: 12800  \tValid loss: 0.48058202862739563\n",
      "Step: 12900  \tTraining loss: 0.48693713545799255\n",
      "Step: 12900  \tTraining accuracy: 0.7404792904853821\n",
      "Step: 12900  \tValid loss: 0.480238139629364\n",
      "Step: 13000  \tTraining loss: 0.4866569936275482\n",
      "Step: 13000  \tTraining accuracy: 0.7405773997306824\n",
      "Step: 13000  \tValid loss: 0.4799104630947113\n",
      "Step: 13100  \tTraining loss: 0.4863796532154083\n",
      "Step: 13100  \tTraining accuracy: 0.7406790852546692\n",
      "Step: 13100  \tValid loss: 0.4795595705509186\n",
      "Step: 13200  \tTraining loss: 0.48611003160476685\n",
      "Step: 13200  \tTraining accuracy: 0.7407852411270142\n",
      "Step: 13200  \tValid loss: 0.4792126715183258\n",
      "Step: 13300  \tTraining loss: 0.4858432710170746\n",
      "Step: 13300  \tTraining accuracy: 0.740889847278595\n",
      "Step: 13300  \tValid loss: 0.4788980782032013\n",
      "Step: 13400  \tTraining loss: 0.4855799376964569\n",
      "Step: 13400  \tTraining accuracy: 0.7409898638725281\n",
      "Step: 13400  \tValid loss: 0.4785543382167816\n",
      "Step: 13500  \tTraining loss: 0.4853231906890869\n",
      "Step: 13500  \tTraining accuracy: 0.7410864233970642\n",
      "Step: 13500  \tValid loss: 0.47819432616233826\n",
      "Step: 13600  \tTraining loss: 0.485073983669281\n",
      "Step: 13600  \tTraining accuracy: 0.7411816120147705\n",
      "Step: 13600  \tValid loss: 0.47788190841674805\n",
      "Step: 13700  \tTraining loss: 0.4848277270793915\n",
      "Step: 13700  \tTraining accuracy: 0.741283118724823\n",
      "Step: 13700  \tValid loss: 0.477573037147522\n",
      "Step: 13800  \tTraining loss: 0.48458775877952576\n",
      "Step: 13800  \tTraining accuracy: 0.7413812279701233\n",
      "Step: 13800  \tValid loss: 0.4772551357746124\n",
      "Step: 13900  \tTraining loss: 0.48435354232788086\n",
      "Step: 13900  \tTraining accuracy: 0.7414759993553162\n",
      "Step: 13900  \tValid loss: 0.47695523500442505\n",
      "Step: 14000  \tTraining loss: 0.48411834239959717\n",
      "Step: 14000  \tTraining accuracy: 0.7415665984153748\n",
      "Step: 14000  \tValid loss: 0.4766604006290436\n",
      "Step: 14100  \tTraining loss: 0.48388397693634033\n",
      "Step: 14100  \tTraining accuracy: 0.7416539788246155\n",
      "Step: 14100  \tValid loss: 0.4763341248035431\n",
      "Step: 14200  \tTraining loss: 0.483656644821167\n",
      "Step: 14200  \tTraining accuracy: 0.7417411208152771\n",
      "Step: 14200  \tValid loss: 0.47604700922966003\n",
      "Step: 14300  \tTraining loss: 0.4834355115890503\n",
      "Step: 14300  \tTraining accuracy: 0.7418288588523865\n",
      "Step: 14300  \tValid loss: 0.47577327489852905\n",
      "Step: 14400  \tTraining loss: 0.4832197427749634\n",
      "Step: 14400  \tTraining accuracy: 0.7419134974479675\n",
      "Step: 14400  \tValid loss: 0.4755175709724426\n",
      "Step: 14500  \tTraining loss: 0.48301011323928833\n",
      "Step: 14500  \tTraining accuracy: 0.7419924139976501\n",
      "Step: 14500  \tValid loss: 0.4752708375453949\n",
      "Step: 14600  \tTraining loss: 0.48280516266822815\n",
      "Step: 14600  \tTraining accuracy: 0.7420720458030701\n",
      "Step: 14600  \tValid loss: 0.47504860162734985\n",
      "Step: 14700  \tTraining loss: 0.4826032221317291\n",
      "Step: 14700  \tTraining accuracy: 0.742150604724884\n",
      "Step: 14700  \tValid loss: 0.4748246371746063\n",
      "Step: 14800  \tTraining loss: 0.48240506649017334\n",
      "Step: 14800  \tTraining accuracy: 0.742228090763092\n",
      "Step: 14800  \tValid loss: 0.4746086001396179\n",
      "Step: 14900  \tTraining loss: 0.4822111427783966\n",
      "Step: 14900  \tTraining accuracy: 0.7423036694526672\n",
      "Step: 14900  \tValid loss: 0.4744095802307129\n",
      "Step: 15000  \tTraining loss: 0.4820239543914795\n",
      "Step: 15000  \tTraining accuracy: 0.7423746585845947\n",
      "Step: 15000  \tValid loss: 0.4741818606853485\n",
      "Step: 15100  \tTraining loss: 0.48183733224868774\n",
      "Step: 15100  \tTraining accuracy: 0.7424482703208923\n",
      "Step: 15100  \tValid loss: 0.47401365637779236\n",
      "Step: 15200  \tTraining loss: 0.48165163397789\n",
      "Step: 15200  \tTraining accuracy: 0.7425234913825989\n",
      "Step: 15200  \tValid loss: 0.47382330894470215\n",
      "Step: 15300  \tTraining loss: 0.48147067427635193\n",
      "Step: 15300  \tTraining accuracy: 0.7425916194915771\n",
      "Step: 15300  \tValid loss: 0.4736661911010742\n",
      "Step: 15400  \tTraining loss: 0.48129069805145264\n",
      "Step: 15400  \tTraining accuracy: 0.7426571846008301\n",
      "Step: 15400  \tValid loss: 0.47348982095718384\n",
      "Step: 15500  \tTraining loss: 0.48111364245414734\n",
      "Step: 15500  \tTraining accuracy: 0.7427150011062622\n",
      "Step: 15500  \tValid loss: 0.4733292758464813\n",
      "Step: 15600  \tTraining loss: 0.48093926906585693\n",
      "Step: 15600  \tTraining accuracy: 0.7427695393562317\n",
      "Step: 15600  \tValid loss: 0.4731625020503998\n",
      "Step: 15700  \tTraining loss: 0.48076605796813965\n",
      "Step: 15700  \tTraining accuracy: 0.7428216934204102\n",
      "Step: 15700  \tValid loss: 0.47303107380867004\n",
      "Step: 15800  \tTraining loss: 0.480593740940094\n",
      "Step: 15800  \tTraining accuracy: 0.7428680658340454\n",
      "Step: 15800  \tValid loss: 0.4728751480579376\n",
      "Step: 15900  \tTraining loss: 0.48042500019073486\n",
      "Step: 15900  \tTraining accuracy: 0.7429139018058777\n",
      "Step: 15900  \tValid loss: 0.4726806879043579\n",
      "Step: 16000  \tTraining loss: 0.4802548289299011\n",
      "Step: 16000  \tTraining accuracy: 0.7429550290107727\n",
      "Step: 16000  \tValid loss: 0.4725550711154938\n",
      "Step: 16100  \tTraining loss: 0.4800867736339569\n",
      "Step: 16100  \tTraining accuracy: 0.7429890036582947\n",
      "Step: 16100  \tValid loss: 0.4723949134349823\n",
      "Step: 16200  \tTraining loss: 0.4799206554889679\n",
      "Step: 16200  \tTraining accuracy: 0.743025004863739\n",
      "Step: 16200  \tValid loss: 0.47223353385925293\n",
      "Step: 16300  \tTraining loss: 0.4797538220882416\n",
      "Step: 16300  \tTraining accuracy: 0.7430622577667236\n",
      "Step: 16300  \tValid loss: 0.47209322452545166\n",
      "Step: 16400  \tTraining loss: 0.4795868694782257\n",
      "Step: 16400  \tTraining accuracy: 0.7431014180183411\n",
      "Step: 16400  \tValid loss: 0.47192591428756714\n",
      "Step: 16500  \tTraining loss: 0.4794204533100128\n",
      "Step: 16500  \tTraining accuracy: 0.743141770362854\n",
      "Step: 16500  \tValid loss: 0.4717758297920227\n",
      "Step: 16600  \tTraining loss: 0.47925570607185364\n",
      "Step: 16600  \tTraining accuracy: 0.743183970451355\n",
      "Step: 16600  \tValid loss: 0.47162967920303345\n",
      "Step: 16700  \tTraining loss: 0.4790888726711273\n",
      "Step: 16700  \tTraining accuracy: 0.7432296872138977\n",
      "Step: 16700  \tValid loss: 0.47148168087005615\n",
      "Step: 16800  \tTraining loss: 0.4789239764213562\n",
      "Step: 16800  \tTraining accuracy: 0.7432764768600464\n",
      "Step: 16800  \tValid loss: 0.4713597893714905\n",
      "Step: 16900  \tTraining loss: 0.47875526547431946\n",
      "Step: 16900  \tTraining accuracy: 0.7433249950408936\n",
      "Step: 16900  \tValid loss: 0.4712570905685425\n",
      "Step: 17000  \tTraining loss: 0.47858476638793945\n",
      "Step: 17000  \tTraining accuracy: 0.7433745861053467\n",
      "Step: 17000  \tValid loss: 0.4711456000804901\n",
      "Step: 17100  \tTraining loss: 0.4784143567085266\n",
      "Step: 17100  \tTraining accuracy: 0.7434235215187073\n",
      "Step: 17100  \tValid loss: 0.4710347652435303\n",
      "Step: 17200  \tTraining loss: 0.4782407581806183\n",
      "Step: 17200  \tTraining accuracy: 0.743472695350647\n",
      "Step: 17200  \tValid loss: 0.47091758251190186\n",
      "Step: 17300  \tTraining loss: 0.47806280851364136\n",
      "Step: 17300  \tTraining accuracy: 0.7435174584388733\n",
      "Step: 17300  \tValid loss: 0.4707847535610199\n",
      "Step: 17400  \tTraining loss: 0.47787120938301086\n",
      "Step: 17400  \tTraining accuracy: 0.7435609698295593\n",
      "Step: 17400  \tValid loss: 0.47067955136299133\n",
      "Step: 17500  \tTraining loss: 0.4776773452758789\n",
      "Step: 17500  \tTraining accuracy: 0.7436062097549438\n",
      "Step: 17500  \tValid loss: 0.47055935859680176\n",
      "Step: 17600  \tTraining loss: 0.4774887263774872\n",
      "Step: 17600  \tTraining accuracy: 0.7436516880989075\n",
      "Step: 17600  \tValid loss: 0.4703994691371918\n",
      "Step: 17700  \tTraining loss: 0.4773015081882477\n",
      "Step: 17700  \tTraining accuracy: 0.7436959743499756\n",
      "Step: 17700  \tValid loss: 0.4702790379524231\n",
      "Step: 17800  \tTraining loss: 0.47711533308029175\n",
      "Step: 17800  \tTraining accuracy: 0.7437344193458557\n",
      "Step: 17800  \tValid loss: 0.4701157808303833\n",
      "Step: 17900  \tTraining loss: 0.4769160747528076\n",
      "Step: 17900  \tTraining accuracy: 0.7437710165977478\n",
      "Step: 17900  \tValid loss: 0.46987369656562805\n",
      "Step: 18000  \tTraining loss: 0.476726770401001\n",
      "Step: 18000  \tTraining accuracy: 0.7438071966171265\n",
      "Step: 18000  \tValid loss: 0.4697078466415405\n",
      "Step: 18100  \tTraining loss: 0.476539671421051\n",
      "Step: 18100  \tTraining accuracy: 0.7438386082649231\n",
      "Step: 18100  \tValid loss: 0.46956712007522583\n",
      "Step: 18200  \tTraining loss: 0.4763520359992981\n",
      "Step: 18200  \tTraining accuracy: 0.7438673973083496\n",
      "Step: 18200  \tValid loss: 0.4694342613220215\n",
      "Step: 18300  \tTraining loss: 0.47616469860076904\n",
      "Step: 18300  \tTraining accuracy: 0.743895947933197\n",
      "Step: 18300  \tValid loss: 0.4693068265914917\n",
      "Step: 18400  \tTraining loss: 0.4759786128997803\n",
      "Step: 18400  \tTraining accuracy: 0.7439248561859131\n",
      "Step: 18400  \tValid loss: 0.46917638182640076\n",
      "Step: 18500  \tTraining loss: 0.4757553040981293\n",
      "Step: 18500  \tTraining accuracy: 0.7439498901367188\n",
      "Step: 18500  \tValid loss: 0.46904370188713074\n",
      "Step: 18600  \tTraining loss: 0.4755235016345978\n",
      "Step: 18600  \tTraining accuracy: 0.7439767718315125\n",
      "Step: 18600  \tValid loss: 0.4688609838485718\n",
      "Step: 18700  \tTraining loss: 0.4753226935863495\n",
      "Step: 18700  \tTraining accuracy: 0.7440140247344971\n",
      "Step: 18700  \tValid loss: 0.4688081741333008\n",
      "Step: 18800  \tTraining loss: 0.47514569759368896\n",
      "Step: 18800  \tTraining accuracy: 0.744053065776825\n",
      "Step: 18800  \tValid loss: 0.4687548875808716\n",
      "Step: 18900  \tTraining loss: 0.4749850034713745\n",
      "Step: 18900  \tTraining accuracy: 0.7440916299819946\n",
      "Step: 18900  \tValid loss: 0.46865829825401306\n",
      "Step: 19000  \tTraining loss: 0.4748299717903137\n",
      "Step: 19000  \tTraining accuracy: 0.7441298365592957\n",
      "Step: 19000  \tValid loss: 0.46856459975242615\n",
      "Step: 19100  \tTraining loss: 0.47467944025993347\n",
      "Step: 19100  \tTraining accuracy: 0.744166910648346\n",
      "Step: 19100  \tValid loss: 0.4684655964374542\n",
      "Step: 19200  \tTraining loss: 0.4745326340198517\n",
      "Step: 19200  \tTraining accuracy: 0.7442091107368469\n",
      "Step: 19200  \tValid loss: 0.4683772623538971\n",
      "Step: 19300  \tTraining loss: 0.47439128160476685\n",
      "Step: 19300  \tTraining accuracy: 0.7442516088485718\n",
      "Step: 19300  \tValid loss: 0.4682594835758209\n",
      "Step: 19400  \tTraining loss: 0.4742482602596283\n",
      "Step: 19400  \tTraining accuracy: 0.7442902326583862\n",
      "Step: 19400  \tValid loss: 0.46820148825645447\n",
      "Step: 19500  \tTraining loss: 0.47410038113594055\n",
      "Step: 19500  \tTraining accuracy: 0.7443284392356873\n",
      "Step: 19500  \tValid loss: 0.46805381774902344\n",
      "Step: 19600  \tTraining loss: 0.4739525020122528\n",
      "Step: 19600  \tTraining accuracy: 0.7443696856498718\n",
      "Step: 19600  \tValid loss: 0.46799159049987793\n",
      "Step: 19700  \tTraining loss: 0.4738113284111023\n",
      "Step: 19700  \tTraining accuracy: 0.7444138526916504\n",
      "Step: 19700  \tValid loss: 0.4679360091686249\n",
      "Step: 19800  \tTraining loss: 0.4736740291118622\n",
      "Step: 19800  \tTraining accuracy: 0.744462251663208\n",
      "Step: 19800  \tValid loss: 0.46787571907043457\n",
      "Step: 19900  \tTraining loss: 0.4735388159751892\n",
      "Step: 19900  \tTraining accuracy: 0.7445115447044373\n",
      "Step: 19900  \tValid loss: 0.4678233563899994\n",
      "Step: 20000  \tTraining loss: 0.4734056293964386\n",
      "Step: 20000  \tTraining accuracy: 0.7445563673973083\n",
      "Step: 20000  \tValid loss: 0.46777021884918213\n",
      "Step: 20100  \tTraining loss: 0.47327694296836853\n",
      "Step: 20100  \tTraining accuracy: 0.7445954084396362\n",
      "Step: 20100  \tValid loss: 0.4677213430404663\n",
      "Step: 20200  \tTraining loss: 0.473145991563797\n",
      "Step: 20200  \tTraining accuracy: 0.7446354031562805\n",
      "Step: 20200  \tValid loss: 0.46768349409103394\n",
      "Step: 20300  \tTraining loss: 0.47301626205444336\n",
      "Step: 20300  \tTraining accuracy: 0.7446749806404114\n",
      "Step: 20300  \tValid loss: 0.4676499366760254\n",
      "Step: 20400  \tTraining loss: 0.47289130091667175\n",
      "Step: 20400  \tTraining accuracy: 0.7447167634963989\n",
      "Step: 20400  \tValid loss: 0.46759331226348877\n",
      "Step: 20500  \tTraining loss: 0.47276434302330017\n",
      "Step: 20500  \tTraining accuracy: 0.7447595000267029\n",
      "Step: 20500  \tValid loss: 0.4675588011741638\n",
      "Step: 20600  \tTraining loss: 0.4726286232471466\n",
      "Step: 20600  \tTraining accuracy: 0.7448024153709412\n",
      "Step: 20600  \tValid loss: 0.467587411403656\n",
      "Step: 20700  \tTraining loss: 0.4724830090999603\n",
      "Step: 20700  \tTraining accuracy: 0.7448423504829407\n",
      "Step: 20700  \tValid loss: 0.4676019847393036\n",
      "Step: 20800  \tTraining loss: 0.47234347462654114\n",
      "Step: 20800  \tTraining accuracy: 0.7448800206184387\n",
      "Step: 20800  \tValid loss: 0.4675827622413635\n",
      "Step: 20900  \tTraining loss: 0.472209095954895\n",
      "Step: 20900  \tTraining accuracy: 0.7449191808700562\n",
      "Step: 20900  \tValid loss: 0.46758079528808594\n",
      "Step: 21000  \tTraining loss: 0.47207796573638916\n",
      "Step: 21000  \tTraining accuracy: 0.744961142539978\n",
      "Step: 21000  \tValid loss: 0.4675714373588562\n",
      "Step: 21100  \tTraining loss: 0.4719502925872803\n",
      "Step: 21100  \tTraining accuracy: 0.7450071573257446\n",
      "Step: 21100  \tValid loss: 0.4675566554069519\n",
      "Step: 21200  \tTraining loss: 0.47182655334472656\n",
      "Step: 21200  \tTraining accuracy: 0.7450539469718933\n",
      "Step: 21200  \tValid loss: 0.46752792596817017\n",
      "Step: 21300  \tTraining loss: 0.47171148657798767\n",
      "Step: 21300  \tTraining accuracy: 0.7450990676879883\n",
      "Step: 21300  \tValid loss: 0.46747472882270813\n",
      "Step: 21400  \tTraining loss: 0.4715961217880249\n",
      "Step: 21400  \tTraining accuracy: 0.7451450228691101\n",
      "Step: 21400  \tValid loss: 0.4674489498138428\n",
      "Step: 21500  \tTraining loss: 0.4714817702770233\n",
      "Step: 21500  \tTraining accuracy: 0.7451886534690857\n",
      "Step: 21500  \tValid loss: 0.46743112802505493\n",
      "Step: 21600  \tTraining loss: 0.47136974334716797\n",
      "Step: 21600  \tTraining accuracy: 0.7452306747436523\n",
      "Step: 21600  \tValid loss: 0.46741577982902527\n",
      "Step: 21700  \tTraining loss: 0.47125884890556335\n",
      "Step: 21700  \tTraining accuracy: 0.7452772259712219\n",
      "Step: 21700  \tValid loss: 0.4673973619937897\n",
      "Step: 21800  \tTraining loss: 0.4711506962776184\n",
      "Step: 21800  \tTraining accuracy: 0.7453209161758423\n",
      "Step: 21800  \tValid loss: 0.46736037731170654\n",
      "Step: 21900  \tTraining loss: 0.47103700041770935\n",
      "Step: 21900  \tTraining accuracy: 0.7453641891479492\n",
      "Step: 21900  \tValid loss: 0.4673801064491272\n",
      "Step: 22000  \tTraining loss: 0.4709298312664032\n",
      "Step: 22000  \tTraining accuracy: 0.745406448841095\n",
      "Step: 22000  \tValid loss: 0.4673633277416229\n",
      "Step: 22100  \tTraining loss: 0.47082072496414185\n",
      "Step: 22100  \tTraining accuracy: 0.7454537153244019\n",
      "Step: 22100  \tValid loss: 0.46734580397605896\n",
      "Step: 22200  \tTraining loss: 0.4707147181034088\n",
      "Step: 22200  \tTraining accuracy: 0.7455047965049744\n",
      "Step: 22200  \tValid loss: 0.46730858087539673\n",
      "Step: 22300  \tTraining loss: 0.47060832381248474\n",
      "Step: 22300  \tTraining accuracy: 0.7455577850341797\n",
      "Step: 22300  \tValid loss: 0.4673101305961609\n",
      "Step: 22400  \tTraining loss: 0.4704989790916443\n",
      "Step: 22400  \tTraining accuracy: 0.7456102967262268\n",
      "Step: 22400  \tValid loss: 0.4672819972038269\n",
      "Step: 22500  \tTraining loss: 0.47038882970809937\n",
      "Step: 22500  \tTraining accuracy: 0.7456623315811157\n",
      "Step: 22500  \tValid loss: 0.467227578163147\n",
      "Step: 22600  \tTraining loss: 0.47028061747550964\n",
      "Step: 22600  \tTraining accuracy: 0.7457138895988464\n",
      "Step: 22600  \tValid loss: 0.46720176935195923\n",
      "Step: 22700  \tTraining loss: 0.4701756536960602\n",
      "Step: 22700  \tTraining accuracy: 0.7457650303840637\n",
      "Step: 22700  \tValid loss: 0.46719154715538025\n",
      "Step: 22800  \tTraining loss: 0.47007158398628235\n",
      "Step: 22800  \tTraining accuracy: 0.7458156943321228\n",
      "Step: 22800  \tValid loss: 0.4671816825866699\n",
      "Step: 22900  \tTraining loss: 0.4699650704860687\n",
      "Step: 22900  \tTraining accuracy: 0.7458659410476685\n",
      "Step: 22900  \tValid loss: 0.46718883514404297\n",
      "Step: 23000  \tTraining loss: 0.4698629379272461\n",
      "Step: 23000  \tTraining accuracy: 0.7459140419960022\n",
      "Step: 23000  \tValid loss: 0.4671977460384369\n",
      "Step: 23100  \tTraining loss: 0.46975836157798767\n",
      "Step: 23100  \tTraining accuracy: 0.7459599375724792\n",
      "Step: 23100  \tValid loss: 0.46721044182777405\n",
      "Step: 23200  \tTraining loss: 0.46965718269348145\n",
      "Step: 23200  \tTraining accuracy: 0.7460089325904846\n",
      "Step: 23200  \tValid loss: 0.4672050178050995\n",
      "Step: 23300  \tTraining loss: 0.46955612301826477\n",
      "Step: 23300  \tTraining accuracy: 0.746060311794281\n",
      "Step: 23300  \tValid loss: 0.46723228693008423\n",
      "Step: 23400  \tTraining loss: 0.4694550633430481\n",
      "Step: 23400  \tTraining accuracy: 0.7461118102073669\n",
      "Step: 23400  \tValid loss: 0.4672333896160126\n",
      "Step: 23500  \tTraining loss: 0.4693553149700165\n",
      "Step: 23500  \tTraining accuracy: 0.7461651563644409\n",
      "Step: 23500  \tValid loss: 0.467250257730484\n",
      "Step: 23600  \tTraining loss: 0.4692549705505371\n",
      "Step: 23600  \tTraining accuracy: 0.7462180256843567\n",
      "Step: 23600  \tValid loss: 0.4672504663467407\n",
      "Step: 23700  \tTraining loss: 0.46915528178215027\n",
      "Step: 23700  \tTraining accuracy: 0.746270477771759\n",
      "Step: 23700  \tValid loss: 0.4672473967075348\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7463225\n",
      "Precision: 0.78507674\n",
      "Recall: 0.8856222\n",
      "F1 score: 0.7912375\n",
      "AUC: 0.70893383\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.746323   0.785077  0.885622  0.791237  0.708934  0.469117      0.746283   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.467167        0.74625   0.532734      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  23739.0  \n",
      "3\n",
      "(3915, 8)\n",
      "(3915, 1)\n",
      "(2080, 8)\n",
      "(2080, 1)\n",
      "(1690, 8)\n",
      "(1690, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5443854331970215\n",
      "Step: 100  \tTraining accuracy: 0.7402299046516418\n",
      "Step: 100  \tValid loss: 0.5622454285621643\n",
      "Step: 200  \tTraining loss: 0.516231894493103\n",
      "Step: 200  \tTraining accuracy: 0.7402586340904236\n",
      "Step: 200  \tValid loss: 0.5391093492507935\n",
      "Step: 300  \tTraining loss: 0.4887220561504364\n",
      "Step: 300  \tTraining accuracy: 0.7442053556442261\n",
      "Step: 300  \tValid loss: 0.5161009430885315\n",
      "Step: 400  \tTraining loss: 0.4505140781402588\n",
      "Step: 400  \tTraining accuracy: 0.7519466280937195\n",
      "Step: 400  \tValid loss: 0.4840161204338074\n",
      "Step: 500  \tTraining loss: 0.4101460874080658\n",
      "Step: 500  \tTraining accuracy: 0.7598326206207275\n",
      "Step: 500  \tValid loss: 0.4504571855068207\n",
      "Step: 600  \tTraining loss: 0.3893895745277405\n",
      "Step: 600  \tTraining accuracy: 0.7677373886108398\n",
      "Step: 600  \tValid loss: 0.4347098469734192\n",
      "Step: 700  \tTraining loss: 0.3811158835887909\n",
      "Step: 700  \tTraining accuracy: 0.7746726870536804\n",
      "Step: 700  \tValid loss: 0.42928314208984375\n",
      "Step: 800  \tTraining loss: 0.3775855004787445\n",
      "Step: 800  \tTraining accuracy: 0.7803500294685364\n",
      "Step: 800  \tValid loss: 0.4270687401294708\n",
      "Step: 900  \tTraining loss: 0.3756044805049896\n",
      "Step: 900  \tTraining accuracy: 0.7848153710365295\n",
      "Step: 900  \tValid loss: 0.4255731403827667\n",
      "Step: 1000  \tTraining loss: 0.37417545914649963\n",
      "Step: 1000  \tTraining accuracy: 0.7883825898170471\n",
      "Step: 1000  \tValid loss: 0.4241933226585388\n",
      "Step: 1100  \tTraining loss: 0.3729790449142456\n",
      "Step: 1100  \tTraining accuracy: 0.7913947701454163\n",
      "Step: 1100  \tValid loss: 0.4228370785713196\n",
      "Step: 1200  \tTraining loss: 0.37189629673957825\n",
      "Step: 1200  \tTraining accuracy: 0.7938269972801208\n",
      "Step: 1200  \tValid loss: 0.4214938282966614\n",
      "Step: 1300  \tTraining loss: 0.37088045477867126\n",
      "Step: 1300  \tTraining accuracy: 0.7959120273590088\n",
      "Step: 1300  \tValid loss: 0.42018115520477295\n",
      "Step: 1400  \tTraining loss: 0.3699110448360443\n",
      "Step: 1400  \tTraining accuracy: 0.7977461218833923\n",
      "Step: 1400  \tValid loss: 0.4189116358757019\n",
      "Step: 1500  \tTraining loss: 0.36898159980773926\n",
      "Step: 1500  \tTraining accuracy: 0.7993273735046387\n",
      "Step: 1500  \tValid loss: 0.41770458221435547\n",
      "Step: 1600  \tTraining loss: 0.3680887520313263\n",
      "Step: 1600  \tTraining accuracy: 0.8006879687309265\n",
      "Step: 1600  \tValid loss: 0.41657161712646484\n",
      "Step: 1700  \tTraining loss: 0.36723271012306213\n",
      "Step: 1700  \tTraining accuracy: 0.8018600940704346\n",
      "Step: 1700  \tValid loss: 0.41551846265792847\n",
      "Step: 1800  \tTraining loss: 0.366416871547699\n",
      "Step: 1800  \tTraining accuracy: 0.8028760552406311\n",
      "Step: 1800  \tValid loss: 0.4145525395870209\n",
      "Step: 1900  \tTraining loss: 0.36564216017723083\n",
      "Step: 1900  \tTraining accuracy: 0.8038033246994019\n",
      "Step: 1900  \tValid loss: 0.413677841424942\n",
      "Step: 2000  \tTraining loss: 0.36490726470947266\n",
      "Step: 2000  \tTraining accuracy: 0.8047622442245483\n",
      "Step: 2000  \tValid loss: 0.4128935933113098\n",
      "Step: 2100  \tTraining loss: 0.3642096519470215\n",
      "Step: 2100  \tTraining accuracy: 0.8056974411010742\n",
      "Step: 2100  \tValid loss: 0.41219523549079895\n",
      "Step: 2200  \tTraining loss: 0.3635381758213043\n",
      "Step: 2200  \tTraining accuracy: 0.8066121935844421\n",
      "Step: 2200  \tValid loss: 0.41156890988349915\n",
      "Step: 2300  \tTraining loss: 0.3628826141357422\n",
      "Step: 2300  \tTraining accuracy: 0.8074514865875244\n",
      "Step: 2300  \tValid loss: 0.4110001027584076\n",
      "Step: 2400  \tTraining loss: 0.3622472882270813\n",
      "Step: 2400  \tTraining accuracy: 0.8082138895988464\n",
      "Step: 2400  \tValid loss: 0.4104786813259125\n",
      "Step: 2500  \tTraining loss: 0.3616432547569275\n",
      "Step: 2500  \tTraining accuracy: 0.8089246153831482\n",
      "Step: 2500  \tValid loss: 0.41001424193382263\n",
      "Step: 2600  \tTraining loss: 0.3610771894454956\n",
      "Step: 2600  \tTraining accuracy: 0.8095694780349731\n",
      "Step: 2600  \tValid loss: 0.40961092710494995\n",
      "Step: 2700  \tTraining loss: 0.36054959893226624\n",
      "Step: 2700  \tTraining accuracy: 0.810200035572052\n",
      "Step: 2700  \tValid loss: 0.40927553176879883\n",
      "Step: 2800  \tTraining loss: 0.36005455255508423\n",
      "Step: 2800  \tTraining accuracy: 0.8107941746711731\n",
      "Step: 2800  \tValid loss: 0.408988356590271\n",
      "Step: 2900  \tTraining loss: 0.3595810830593109\n",
      "Step: 2900  \tTraining accuracy: 0.8113694787025452\n",
      "Step: 2900  \tValid loss: 0.4087374806404114\n",
      "Step: 3000  \tTraining loss: 0.35911765694618225\n",
      "Step: 3000  \tTraining accuracy: 0.8118969798088074\n",
      "Step: 3000  \tValid loss: 0.4085049629211426\n",
      "Step: 3100  \tTraining loss: 0.3586505055427551\n",
      "Step: 3100  \tTraining accuracy: 0.8123728632926941\n",
      "Step: 3100  \tValid loss: 0.4082660973072052\n",
      "Step: 3200  \tTraining loss: 0.3581658899784088\n",
      "Step: 3200  \tTraining accuracy: 0.8128143548965454\n",
      "Step: 3200  \tValid loss: 0.4080020785331726\n",
      "Step: 3300  \tTraining loss: 0.3576432466506958\n",
      "Step: 3300  \tTraining accuracy: 0.8132287263870239\n",
      "Step: 3300  \tValid loss: 0.407690167427063\n",
      "Step: 3400  \tTraining loss: 0.3570774793624878\n",
      "Step: 3400  \tTraining accuracy: 0.8136261105537415\n",
      "Step: 3400  \tValid loss: 0.40728941559791565\n",
      "Step: 3500  \tTraining loss: 0.3564802408218384\n",
      "Step: 3500  \tTraining accuracy: 0.8139740824699402\n",
      "Step: 3500  \tValid loss: 0.40681958198547363\n",
      "Step: 3600  \tTraining loss: 0.3558635115623474\n",
      "Step: 3600  \tTraining accuracy: 0.8142988085746765\n",
      "Step: 3600  \tValid loss: 0.40632444620132446\n",
      "Step: 3700  \tTraining loss: 0.3552459478378296\n",
      "Step: 3700  \tTraining accuracy: 0.8146235346794128\n",
      "Step: 3700  \tValid loss: 0.4058292806148529\n",
      "Step: 3800  \tTraining loss: 0.3546305298805237\n",
      "Step: 3800  \tTraining accuracy: 0.8149552345275879\n",
      "Step: 3800  \tValid loss: 0.40533778071403503\n",
      "Step: 3900  \tTraining loss: 0.3540198802947998\n",
      "Step: 3900  \tTraining accuracy: 0.8152562379837036\n",
      "Step: 3900  \tValid loss: 0.4048207402229309\n",
      "Step: 4000  \tTraining loss: 0.35341760516166687\n",
      "Step: 4000  \tTraining accuracy: 0.815525472164154\n",
      "Step: 4000  \tValid loss: 0.40436437726020813\n",
      "Step: 4100  \tTraining loss: 0.3528296947479248\n",
      "Step: 4100  \tTraining accuracy: 0.8157878518104553\n",
      "Step: 4100  \tValid loss: 0.4039435386657715\n",
      "Step: 4200  \tTraining loss: 0.35225218534469604\n",
      "Step: 4200  \tTraining accuracy: 0.8160407543182373\n",
      "Step: 4200  \tValid loss: 0.4035545885562897\n",
      "Step: 4300  \tTraining loss: 0.35168927907943726\n",
      "Step: 4300  \tTraining accuracy: 0.8162817358970642\n",
      "Step: 4300  \tValid loss: 0.4031633734703064\n",
      "Step: 4400  \tTraining loss: 0.35111451148986816\n",
      "Step: 4400  \tTraining accuracy: 0.816520631313324\n",
      "Step: 4400  \tValid loss: 0.4029185175895691\n",
      "Step: 4500  \tTraining loss: 0.35052451491355896\n",
      "Step: 4500  \tTraining accuracy: 0.8167546391487122\n",
      "Step: 4500  \tValid loss: 0.40272441506385803\n",
      "Step: 4600  \tTraining loss: 0.34996718168258667\n",
      "Step: 4600  \tTraining accuracy: 0.8169783353805542\n",
      "Step: 4600  \tValid loss: 0.4023961126804352\n",
      "Step: 4700  \tTraining loss: 0.3494281768798828\n",
      "Step: 4700  \tTraining accuracy: 0.8171980381011963\n",
      "Step: 4700  \tValid loss: 0.4020065367221832\n",
      "Step: 4800  \tTraining loss: 0.34890463948249817\n",
      "Step: 4800  \tTraining accuracy: 0.8174029588699341\n",
      "Step: 4800  \tValid loss: 0.40158432722091675\n",
      "Step: 4900  \tTraining loss: 0.3483940362930298\n",
      "Step: 4900  \tTraining accuracy: 0.8175994753837585\n",
      "Step: 4900  \tValid loss: 0.40116068720817566\n",
      "Step: 5000  \tTraining loss: 0.34789708256721497\n",
      "Step: 5000  \tTraining accuracy: 0.8177985548973083\n",
      "Step: 5000  \tValid loss: 0.40071427822113037\n",
      "Step: 5100  \tTraining loss: 0.34741079807281494\n",
      "Step: 5100  \tTraining accuracy: 0.8179923295974731\n",
      "Step: 5100  \tValid loss: 0.4003041088581085\n",
      "Step: 5200  \tTraining loss: 0.3468473255634308\n",
      "Step: 5200  \tTraining accuracy: 0.8181735277175903\n",
      "Step: 5200  \tValid loss: 0.3994458019733429\n",
      "Step: 5300  \tTraining loss: 0.3463311791419983\n",
      "Step: 5300  \tTraining accuracy: 0.8183577656745911\n",
      "Step: 5300  \tValid loss: 0.3991895914077759\n",
      "Step: 5400  \tTraining loss: 0.34580448269844055\n",
      "Step: 5400  \tTraining accuracy: 0.8185375332832336\n",
      "Step: 5400  \tValid loss: 0.39853718876838684\n",
      "Step: 5500  \tTraining loss: 0.34527191519737244\n",
      "Step: 5500  \tTraining accuracy: 0.8187154531478882\n",
      "Step: 5500  \tValid loss: 0.397899866104126\n",
      "Step: 5600  \tTraining loss: 0.3447655439376831\n",
      "Step: 5600  \tTraining accuracy: 0.8188682198524475\n",
      "Step: 5600  \tValid loss: 0.3973901867866516\n",
      "Step: 5700  \tTraining loss: 0.3442719578742981\n",
      "Step: 5700  \tTraining accuracy: 0.8190132975578308\n",
      "Step: 5700  \tValid loss: 0.3969210088253021\n",
      "Step: 5800  \tTraining loss: 0.34378981590270996\n",
      "Step: 5800  \tTraining accuracy: 0.8191623687744141\n",
      "Step: 5800  \tValid loss: 0.3964685797691345\n",
      "Step: 5900  \tTraining loss: 0.34331706166267395\n",
      "Step: 5900  \tTraining accuracy: 0.8193063139915466\n",
      "Step: 5900  \tValid loss: 0.39601781964302063\n",
      "Step: 6000  \tTraining loss: 0.34284523129463196\n",
      "Step: 6000  \tTraining accuracy: 0.8194454908370972\n",
      "Step: 6000  \tValid loss: 0.39554083347320557\n",
      "Step: 6100  \tTraining loss: 0.34237971901893616\n",
      "Step: 6100  \tTraining accuracy: 0.819586455821991\n",
      "Step: 6100  \tValid loss: 0.395079106092453\n",
      "Step: 6200  \tTraining loss: 0.34191790223121643\n",
      "Step: 6200  \tTraining accuracy: 0.8197080492973328\n",
      "Step: 6200  \tValid loss: 0.39462825655937195\n",
      "Step: 6300  \tTraining loss: 0.34145891666412354\n",
      "Step: 6300  \tTraining accuracy: 0.8198341131210327\n",
      "Step: 6300  \tValid loss: 0.3942022919654846\n",
      "Step: 6400  \tTraining loss: 0.34100309014320374\n",
      "Step: 6400  \tTraining accuracy: 0.8199623227119446\n",
      "Step: 6400  \tValid loss: 0.3937855064868927\n",
      "Step: 6500  \tTraining loss: 0.34055066108703613\n",
      "Step: 6500  \tTraining accuracy: 0.8200945854187012\n",
      "Step: 6500  \tValid loss: 0.39337411522865295\n",
      "Step: 6600  \tTraining loss: 0.34010136127471924\n",
      "Step: 6600  \tTraining accuracy: 0.8202268481254578\n",
      "Step: 6600  \tValid loss: 0.392995148897171\n",
      "Step: 6700  \tTraining loss: 0.33965879678726196\n",
      "Step: 6700  \tTraining accuracy: 0.8203609585762024\n",
      "Step: 6700  \tValid loss: 0.39262911677360535\n",
      "Step: 6800  \tTraining loss: 0.3392154276371002\n",
      "Step: 6800  \tTraining accuracy: 0.820483386516571\n",
      "Step: 6800  \tValid loss: 0.39227405190467834\n",
      "Step: 6900  \tTraining loss: 0.3387793004512787\n",
      "Step: 6900  \tTraining accuracy: 0.820602297782898\n",
      "Step: 6900  \tValid loss: 0.39192289113998413\n",
      "Step: 7000  \tTraining loss: 0.3383471369743347\n",
      "Step: 7000  \tTraining accuracy: 0.8207083940505981\n",
      "Step: 7000  \tValid loss: 0.3915860652923584\n",
      "Step: 7100  \tTraining loss: 0.33792781829833984\n",
      "Step: 7100  \tTraining accuracy: 0.8208059072494507\n",
      "Step: 7100  \tValid loss: 0.39126086235046387\n",
      "Step: 7200  \tTraining loss: 0.3375014364719391\n",
      "Step: 7200  \tTraining accuracy: 0.8209043741226196\n",
      "Step: 7200  \tValid loss: 0.3909205496311188\n",
      "Step: 7300  \tTraining loss: 0.3370867371559143\n",
      "Step: 7300  \tTraining accuracy: 0.8210055232048035\n",
      "Step: 7300  \tValid loss: 0.39061906933784485\n",
      "Step: 7400  \tTraining loss: 0.3366723954677582\n",
      "Step: 7400  \tTraining accuracy: 0.8211003541946411\n",
      "Step: 7400  \tValid loss: 0.39028307795524597\n",
      "Step: 7500  \tTraining loss: 0.3358460068702698\n",
      "Step: 7500  \tTraining accuracy: 0.8212048411369324\n",
      "Step: 7500  \tValid loss: 0.3895648419857025\n",
      "Step: 7600  \tTraining loss: 0.3353131413459778\n",
      "Step: 7600  \tTraining accuracy: 0.821330726146698\n",
      "Step: 7600  \tValid loss: 0.38898327946662903\n",
      "Step: 7700  \tTraining loss: 0.3347899913787842\n",
      "Step: 7700  \tTraining accuracy: 0.8214566707611084\n",
      "Step: 7700  \tValid loss: 0.3885607421398163\n",
      "Step: 7800  \tTraining loss: 0.3342800736427307\n",
      "Step: 7800  \tTraining accuracy: 0.8215659856796265\n",
      "Step: 7800  \tValid loss: 0.3881892263889313\n",
      "Step: 7900  \tTraining loss: 0.3338056802749634\n",
      "Step: 7900  \tTraining accuracy: 0.8216542601585388\n",
      "Step: 7900  \tValid loss: 0.38784894347190857\n",
      "Step: 8000  \tTraining loss: 0.33336302638053894\n",
      "Step: 8000  \tTraining accuracy: 0.8217534422874451\n",
      "Step: 8000  \tValid loss: 0.38758620619773865\n",
      "Step: 8100  \tTraining loss: 0.3329433798789978\n",
      "Step: 8100  \tTraining accuracy: 0.8218469023704529\n",
      "Step: 8100  \tValid loss: 0.38731008768081665\n",
      "Step: 8200  \tTraining loss: 0.3325384855270386\n",
      "Step: 8200  \tTraining accuracy: 0.8219316601753235\n",
      "Step: 8200  \tValid loss: 0.3870738744735718\n",
      "Step: 8300  \tTraining loss: 0.3321459889411926\n",
      "Step: 8300  \tTraining accuracy: 0.8220160007476807\n",
      "Step: 8300  \tValid loss: 0.38686853647232056\n",
      "Step: 8400  \tTraining loss: 0.3317701518535614\n",
      "Step: 8400  \tTraining accuracy: 0.8221029043197632\n",
      "Step: 8400  \tValid loss: 0.3866428732872009\n",
      "Step: 8500  \tTraining loss: 0.3314090669155121\n",
      "Step: 8500  \tTraining accuracy: 0.8221847414970398\n",
      "Step: 8500  \tValid loss: 0.38644173741340637\n",
      "Step: 8600  \tTraining loss: 0.33105483651161194\n",
      "Step: 8600  \tTraining accuracy: 0.8222707509994507\n",
      "Step: 8600  \tValid loss: 0.38622352480888367\n",
      "Step: 8700  \tTraining loss: 0.33071449398994446\n",
      "Step: 8700  \tTraining accuracy: 0.8223577737808228\n",
      "Step: 8700  \tValid loss: 0.386075884103775\n",
      "Step: 8800  \tTraining loss: 0.3303762674331665\n",
      "Step: 8800  \tTraining accuracy: 0.8224472403526306\n",
      "Step: 8800  \tValid loss: 0.3859129846096039\n",
      "Step: 8900  \tTraining loss: 0.3300477862358093\n",
      "Step: 8900  \tTraining accuracy: 0.8225435614585876\n",
      "Step: 8900  \tValid loss: 0.38574719429016113\n",
      "Step: 9000  \tTraining loss: 0.3297252357006073\n",
      "Step: 9000  \tTraining accuracy: 0.822640597820282\n",
      "Step: 9000  \tValid loss: 0.385648638010025\n",
      "Step: 9100  \tTraining loss: 0.3294094204902649\n",
      "Step: 9100  \tTraining accuracy: 0.8227340579032898\n",
      "Step: 9100  \tValid loss: 0.3855492174625397\n",
      "Step: 9200  \tTraining loss: 0.3290995657444\n",
      "Step: 9200  \tTraining accuracy: 0.8228311538696289\n",
      "Step: 9200  \tValid loss: 0.38543859124183655\n",
      "Step: 9300  \tTraining loss: 0.3287948966026306\n",
      "Step: 9300  \tTraining accuracy: 0.8229387998580933\n",
      "Step: 9300  \tValid loss: 0.3853498697280884\n",
      "Step: 9400  \tTraining loss: 0.3284958600997925\n",
      "Step: 9400  \tTraining accuracy: 0.8230441808700562\n",
      "Step: 9400  \tValid loss: 0.385263592004776\n",
      "Step: 9500  \tTraining loss: 0.3282020092010498\n",
      "Step: 9500  \tTraining accuracy: 0.8231472969055176\n",
      "Step: 9500  \tValid loss: 0.3852120041847229\n",
      "Step: 9600  \tTraining loss: 0.3279126286506653\n",
      "Step: 9600  \tTraining accuracy: 0.8232578039169312\n",
      "Step: 9600  \tValid loss: 0.3851250112056732\n",
      "Step: 9700  \tTraining loss: 0.32762810587882996\n",
      "Step: 9700  \tTraining accuracy: 0.823371410369873\n",
      "Step: 9700  \tValid loss: 0.38508498668670654\n",
      "Step: 9800  \tTraining loss: 0.32734864950180054\n",
      "Step: 9800  \tTraining accuracy: 0.8234880566596985\n",
      "Step: 9800  \tValid loss: 0.3850329518318176\n",
      "Step: 9900  \tTraining loss: 0.32707399129867554\n",
      "Step: 9900  \tTraining accuracy: 0.8236023187637329\n",
      "Step: 9900  \tValid loss: 0.3849901258945465\n",
      "Step: 10000  \tTraining loss: 0.32680296897888184\n",
      "Step: 10000  \tTraining accuracy: 0.8237181901931763\n",
      "Step: 10000  \tValid loss: 0.3849537670612335\n",
      "Step: 10100  \tTraining loss: 0.32653677463531494\n",
      "Step: 10100  \tTraining accuracy: 0.8238343596458435\n",
      "Step: 10100  \tValid loss: 0.38492149114608765\n",
      "Step: 10200  \tTraining loss: 0.3262752294540405\n",
      "Step: 10200  \tTraining accuracy: 0.8239443898200989\n",
      "Step: 10200  \tValid loss: 0.38489818572998047\n",
      "Step: 10300  \tTraining loss: 0.3260173201560974\n",
      "Step: 10300  \tTraining accuracy: 0.824049711227417\n",
      "Step: 10300  \tValid loss: 0.38488808274269104\n",
      "Step: 10400  \tTraining loss: 0.32576271891593933\n",
      "Step: 10400  \tTraining accuracy: 0.8241530060768127\n",
      "Step: 10400  \tValid loss: 0.38491082191467285\n",
      "Step: 10500  \tTraining loss: 0.32551389932632446\n",
      "Step: 10500  \tTraining accuracy: 0.8242518305778503\n",
      "Step: 10500  \tValid loss: 0.38487064838409424\n",
      "Step: 10600  \tTraining loss: 0.3252660036087036\n",
      "Step: 10600  \tTraining accuracy: 0.824342668056488\n",
      "Step: 10600  \tValid loss: 0.3848561942577362\n",
      "Step: 10700  \tTraining loss: 0.32502448558807373\n",
      "Step: 10700  \tTraining accuracy: 0.8244280815124512\n",
      "Step: 10700  \tValid loss: 0.3848455250263214\n",
      "Step: 10800  \tTraining loss: 0.32478442788124084\n",
      "Step: 10800  \tTraining accuracy: 0.8245179653167725\n",
      "Step: 10800  \tValid loss: 0.38482755422592163\n",
      "Step: 10900  \tTraining loss: 0.32454901933670044\n",
      "Step: 10900  \tTraining accuracy: 0.8246074318885803\n",
      "Step: 10900  \tValid loss: 0.38478803634643555\n",
      "Step: 11000  \tTraining loss: 0.32432112097740173\n",
      "Step: 11000  \tTraining accuracy: 0.82469642162323\n",
      "Step: 11000  \tValid loss: 0.38480326533317566\n",
      "Step: 11100  \tTraining loss: 0.3240867257118225\n",
      "Step: 11100  \tTraining accuracy: 0.8247908353805542\n",
      "Step: 11100  \tValid loss: 0.38478416204452515\n",
      "Step: 11200  \tTraining loss: 0.3238604664802551\n",
      "Step: 11200  \tTraining accuracy: 0.8248777389526367\n",
      "Step: 11200  \tValid loss: 0.3847880959510803\n",
      "Step: 11300  \tTraining loss: 0.323635458946228\n",
      "Step: 11300  \tTraining accuracy: 0.8249492645263672\n",
      "Step: 11300  \tValid loss: 0.3848150372505188\n",
      "Step: 11400  \tTraining loss: 0.3234129250049591\n",
      "Step: 11400  \tTraining accuracy: 0.8250194787979126\n",
      "Step: 11400  \tValid loss: 0.38477107882499695\n",
      "Step: 11500  \tTraining loss: 0.323192298412323\n",
      "Step: 11500  \tTraining accuracy: 0.8250873684883118\n",
      "Step: 11500  \tValid loss: 0.3847697675228119\n",
      "Step: 11600  \tTraining loss: 0.3229762017726898\n",
      "Step: 11600  \tTraining accuracy: 0.8251540660858154\n",
      "Step: 11600  \tValid loss: 0.3847775459289551\n",
      "Step: 11700  \tTraining loss: 0.3227577209472656\n",
      "Step: 11700  \tTraining accuracy: 0.8252195715904236\n",
      "Step: 11700  \tValid loss: 0.3847368359565735\n",
      "Step: 11800  \tTraining loss: 0.32254353165626526\n",
      "Step: 11800  \tTraining accuracy: 0.8252840638160706\n",
      "Step: 11800  \tValid loss: 0.3847495913505554\n",
      "Step: 11900  \tTraining loss: 0.3223291039466858\n",
      "Step: 11900  \tTraining accuracy: 0.8253440856933594\n",
      "Step: 11900  \tValid loss: 0.3847416043281555\n",
      "Step: 12000  \tTraining loss: 0.3221159279346466\n",
      "Step: 12000  \tTraining accuracy: 0.8254075050354004\n",
      "Step: 12000  \tValid loss: 0.38474419713020325\n",
      "Step: 12100  \tTraining loss: 0.32190462946891785\n",
      "Step: 12100  \tTraining accuracy: 0.8254687786102295\n",
      "Step: 12100  \tValid loss: 0.38470736145973206\n",
      "Step: 12200  \tTraining loss: 0.32170024514198303\n",
      "Step: 12200  \tTraining accuracy: 0.8255193829536438\n",
      "Step: 12200  \tValid loss: 0.38476091623306274\n",
      "Step: 12300  \tTraining loss: 0.321489542722702\n",
      "Step: 12300  \tTraining accuracy: 0.8255628347396851\n",
      "Step: 12300  \tValid loss: 0.38471606373786926\n",
      "Step: 12400  \tTraining loss: 0.3212828040122986\n",
      "Step: 12400  \tTraining accuracy: 0.8256108164787292\n",
      "Step: 12400  \tValid loss: 0.38467535376548767\n",
      "Step: 12500  \tTraining loss: 0.3210776150226593\n",
      "Step: 12500  \tTraining accuracy: 0.8256590366363525\n",
      "Step: 12500  \tValid loss: 0.3846632242202759\n",
      "Step: 12600  \tTraining loss: 0.3208731710910797\n",
      "Step: 12600  \tTraining accuracy: 0.8257055282592773\n",
      "Step: 12600  \tValid loss: 0.38459959626197815\n",
      "Step: 12700  \tTraining loss: 0.32067182660102844\n",
      "Step: 12700  \tTraining accuracy: 0.8257502317428589\n",
      "Step: 12700  \tValid loss: 0.3845897316932678\n",
      "Step: 12800  \tTraining loss: 0.32047027349472046\n",
      "Step: 12800  \tTraining accuracy: 0.8257962465286255\n",
      "Step: 12800  \tValid loss: 0.3845628798007965\n",
      "Step: 12900  \tTraining loss: 0.3202727735042572\n",
      "Step: 12900  \tTraining accuracy: 0.8258435726165771\n",
      "Step: 12900  \tValid loss: 0.38450610637664795\n",
      "Step: 13000  \tTraining loss: 0.3200761675834656\n",
      "Step: 13000  \tTraining accuracy: 0.8258882164955139\n",
      "Step: 13000  \tValid loss: 0.3844428062438965\n",
      "Step: 13100  \tTraining loss: 0.31988340616226196\n",
      "Step: 13100  \tTraining accuracy: 0.8259351253509521\n",
      "Step: 13100  \tValid loss: 0.38442733883857727\n",
      "Step: 13200  \tTraining loss: 0.3196929097175598\n",
      "Step: 13200  \tTraining accuracy: 0.8259872198104858\n",
      "Step: 13200  \tValid loss: 0.38438573479652405\n",
      "Step: 13300  \tTraining loss: 0.31950631737709045\n",
      "Step: 13300  \tTraining accuracy: 0.8260385990142822\n",
      "Step: 13300  \tValid loss: 0.38441208004951477\n",
      "Step: 13400  \tTraining loss: 0.3193223774433136\n",
      "Step: 13400  \tTraining accuracy: 0.8260920643806458\n",
      "Step: 13400  \tValid loss: 0.38436204195022583\n",
      "Step: 13500  \tTraining loss: 0.3191417157649994\n",
      "Step: 13500  \tTraining accuracy: 0.8261438012123108\n",
      "Step: 13500  \tValid loss: 0.3843514025211334\n",
      "Step: 13600  \tTraining loss: 0.31896382570266724\n",
      "Step: 13600  \tTraining accuracy: 0.8261928558349609\n",
      "Step: 13600  \tValid loss: 0.3843218982219696\n",
      "Step: 13700  \tTraining loss: 0.3187890946865082\n",
      "Step: 13700  \tTraining accuracy: 0.8262345194816589\n",
      "Step: 13700  \tValid loss: 0.38428255915641785\n",
      "Step: 13800  \tTraining loss: 0.31861644983291626\n",
      "Step: 13800  \tTraining accuracy: 0.826277494430542\n",
      "Step: 13800  \tValid loss: 0.3842792212963104\n",
      "Step: 13900  \tTraining loss: 0.31844663619995117\n",
      "Step: 13900  \tTraining accuracy: 0.8263235688209534\n",
      "Step: 13900  \tValid loss: 0.38427478075027466\n",
      "Step: 14000  \tTraining loss: 0.3182791769504547\n",
      "Step: 14000  \tTraining accuracy: 0.8263745903968811\n",
      "Step: 14000  \tValid loss: 0.38426142930984497\n",
      "Step: 14100  \tTraining loss: 0.3181140720844269\n",
      "Step: 14100  \tTraining accuracy: 0.8264248967170715\n",
      "Step: 14100  \tValid loss: 0.38424426317214966\n",
      "Step: 14200  \tTraining loss: 0.3179514706134796\n",
      "Step: 14200  \tTraining accuracy: 0.8264781832695007\n",
      "Step: 14200  \tValid loss: 0.384238600730896\n",
      "Step: 14300  \tTraining loss: 0.31779128313064575\n",
      "Step: 14300  \tTraining accuracy: 0.8265324831008911\n",
      "Step: 14300  \tValid loss: 0.384238600730896\n",
      "Step: 14400  \tTraining loss: 0.31763240694999695\n",
      "Step: 14400  \tTraining accuracy: 0.8265969753265381\n",
      "Step: 14400  \tValid loss: 0.3842187821865082\n",
      "Step: 14500  \tTraining loss: 0.3174760341644287\n",
      "Step: 14500  \tTraining accuracy: 0.8266578316688538\n",
      "Step: 14500  \tValid loss: 0.38421767950057983\n",
      "Step: 14600  \tTraining loss: 0.3173218071460724\n",
      "Step: 14600  \tTraining accuracy: 0.8267205357551575\n",
      "Step: 14600  \tValid loss: 0.384210467338562\n",
      "Step: 14700  \tTraining loss: 0.3171694278717041\n",
      "Step: 14700  \tTraining accuracy: 0.8267797827720642\n",
      "Step: 14700  \tValid loss: 0.38420772552490234\n",
      "Step: 14800  \tTraining loss: 0.3170188367366791\n",
      "Step: 14800  \tTraining accuracy: 0.8268355131149292\n",
      "Step: 14800  \tValid loss: 0.3841991424560547\n",
      "Step: 14900  \tTraining loss: 0.31687068939208984\n",
      "Step: 14900  \tTraining accuracy: 0.8268870115280151\n",
      "Step: 14900  \tValid loss: 0.38419201970100403\n",
      "Step: 15000  \tTraining loss: 0.3167246878147125\n",
      "Step: 15000  \tTraining accuracy: 0.8269351720809937\n",
      "Step: 15000  \tValid loss: 0.3842066526412964\n",
      "Step: 15100  \tTraining loss: 0.31658005714416504\n",
      "Step: 15100  \tTraining accuracy: 0.8269827365875244\n",
      "Step: 15100  \tValid loss: 0.3842235207557678\n",
      "Step: 15200  \tTraining loss: 0.31643736362457275\n",
      "Step: 15200  \tTraining accuracy: 0.8270296454429626\n",
      "Step: 15200  \tValid loss: 0.3842374384403229\n",
      "Step: 15300  \tTraining loss: 0.3162955343723297\n",
      "Step: 15300  \tTraining accuracy: 0.8270759582519531\n",
      "Step: 15300  \tValid loss: 0.38422295451164246\n",
      "Step: 15400  \tTraining loss: 0.31615644693374634\n",
      "Step: 15400  \tTraining accuracy: 0.8271191716194153\n",
      "Step: 15400  \tValid loss: 0.3842449188232422\n",
      "Step: 15500  \tTraining loss: 0.31601688265800476\n",
      "Step: 15500  \tTraining accuracy: 0.8271600604057312\n",
      "Step: 15500  \tValid loss: 0.38421785831451416\n",
      "Step: 15600  \tTraining loss: 0.3158790171146393\n",
      "Step: 15600  \tTraining accuracy: 0.8271996378898621\n",
      "Step: 15600  \tValid loss: 0.3842134475708008\n",
      "Step: 15700  \tTraining loss: 0.31574299931526184\n",
      "Step: 15700  \tTraining accuracy: 0.8272353410720825\n",
      "Step: 15700  \tValid loss: 0.3842141926288605\n",
      "Step: 15800  \tTraining loss: 0.3156079053878784\n",
      "Step: 15800  \tTraining accuracy: 0.8272706270217896\n",
      "Step: 15800  \tValid loss: 0.3842165470123291\n",
      "Step: 15900  \tTraining loss: 0.3154737055301666\n",
      "Step: 15900  \tTraining accuracy: 0.8273054361343384\n",
      "Step: 15900  \tValid loss: 0.3841668963432312\n",
      "Step: 16000  \tTraining loss: 0.31534022092819214\n",
      "Step: 16000  \tTraining accuracy: 0.8273398280143738\n",
      "Step: 16000  \tValid loss: 0.38417595624923706\n",
      "Step: 16100  \tTraining loss: 0.3152095377445221\n",
      "Step: 16100  \tTraining accuracy: 0.8273802995681763\n",
      "Step: 16100  \tValid loss: 0.38417938351631165\n",
      "Step: 16200  \tTraining loss: 0.3150777220726013\n",
      "Step: 16200  \tTraining accuracy: 0.8274202346801758\n",
      "Step: 16200  \tValid loss: 0.38415244221687317\n",
      "Step: 16300  \tTraining loss: 0.3149467408657074\n",
      "Step: 16300  \tTraining accuracy: 0.8274669051170349\n",
      "Step: 16300  \tValid loss: 0.38412144780158997\n",
      "Step: 16400  \tTraining loss: 0.31481799483299255\n",
      "Step: 16400  \tTraining accuracy: 0.8275193572044373\n",
      "Step: 16400  \tValid loss: 0.3841139078140259\n",
      "Step: 16500  \tTraining loss: 0.31469014286994934\n",
      "Step: 16500  \tTraining accuracy: 0.8275711536407471\n",
      "Step: 16500  \tValid loss: 0.3840875029563904\n",
      "Step: 16600  \tTraining loss: 0.31456276774406433\n",
      "Step: 16600  \tTraining accuracy: 0.8276223540306091\n",
      "Step: 16600  \tValid loss: 0.3840516209602356\n",
      "Step: 16700  \tTraining loss: 0.31443652510643005\n",
      "Step: 16700  \tTraining accuracy: 0.8276729583740234\n",
      "Step: 16700  \tValid loss: 0.3840350806713104\n",
      "Step: 16800  \tTraining loss: 0.3143097460269928\n",
      "Step: 16800  \tTraining accuracy: 0.8277252316474915\n",
      "Step: 16800  \tValid loss: 0.38399866223335266\n",
      "Step: 16900  \tTraining loss: 0.3141838312149048\n",
      "Step: 16900  \tTraining accuracy: 0.8277815580368042\n",
      "Step: 16900  \tValid loss: 0.3839796781539917\n",
      "Step: 17000  \tTraining loss: 0.3140594959259033\n",
      "Step: 17000  \tTraining accuracy: 0.8278380036354065\n",
      "Step: 17000  \tValid loss: 0.3839488625526428\n",
      "Step: 17100  \tTraining loss: 0.31393635272979736\n",
      "Step: 17100  \tTraining accuracy: 0.8278937339782715\n",
      "Step: 17100  \tValid loss: 0.3838872015476227\n",
      "Step: 17200  \tTraining loss: 0.3138129413127899\n",
      "Step: 17200  \tTraining accuracy: 0.8279488682746887\n",
      "Step: 17200  \tValid loss: 0.3838455080986023\n",
      "Step: 17300  \tTraining loss: 0.3136901557445526\n",
      "Step: 17300  \tTraining accuracy: 0.8280063271522522\n",
      "Step: 17300  \tValid loss: 0.38382843136787415\n",
      "Step: 17400  \tTraining loss: 0.31356820464134216\n",
      "Step: 17400  \tTraining accuracy: 0.8280661702156067\n",
      "Step: 17400  \tValid loss: 0.38378220796585083\n",
      "Step: 17500  \tTraining loss: 0.31344664096832275\n",
      "Step: 17500  \tTraining accuracy: 0.8281267881393433\n",
      "Step: 17500  \tValid loss: 0.38370317220687866\n",
      "Step: 17600  \tTraining loss: 0.3133258819580078\n",
      "Step: 17600  \tTraining accuracy: 0.8281904458999634\n",
      "Step: 17600  \tValid loss: 0.3836614489555359\n",
      "Step: 17700  \tTraining loss: 0.3132052719593048\n",
      "Step: 17700  \tTraining accuracy: 0.8282533884048462\n",
      "Step: 17700  \tValid loss: 0.38360339403152466\n",
      "Step: 17800  \tTraining loss: 0.31308531761169434\n",
      "Step: 17800  \tTraining accuracy: 0.8283156156539917\n",
      "Step: 17800  \tValid loss: 0.38355734944343567\n",
      "Step: 17900  \tTraining loss: 0.3129662573337555\n",
      "Step: 17900  \tTraining accuracy: 0.8283742070198059\n",
      "Step: 17900  \tValid loss: 0.3834924101829529\n",
      "Step: 18000  \tTraining loss: 0.3128467798233032\n",
      "Step: 18000  \tTraining accuracy: 0.8284270763397217\n",
      "Step: 18000  \tValid loss: 0.3834354877471924\n",
      "Step: 18100  \tTraining loss: 0.3127293586730957\n",
      "Step: 18100  \tTraining accuracy: 0.8284743428230286\n",
      "Step: 18100  \tValid loss: 0.383360892534256\n",
      "Step: 18200  \tTraining loss: 0.31261077523231506\n",
      "Step: 18200  \tTraining accuracy: 0.8285195827484131\n",
      "Step: 18200  \tValid loss: 0.38331764936447144\n",
      "Step: 18300  \tTraining loss: 0.3124934434890747\n",
      "Step: 18300  \tTraining accuracy: 0.8285644054412842\n",
      "Step: 18300  \tValid loss: 0.3832709789276123\n",
      "Step: 18400  \tTraining loss: 0.31237661838531494\n",
      "Step: 18400  \tTraining accuracy: 0.8286086916923523\n",
      "Step: 18400  \tValid loss: 0.383199542760849\n",
      "Step: 18500  \tTraining loss: 0.31226032972335815\n",
      "Step: 18500  \tTraining accuracy: 0.8286504149436951\n",
      "Step: 18500  \tValid loss: 0.38315537571907043\n",
      "Step: 18600  \tTraining loss: 0.31214359402656555\n",
      "Step: 18600  \tTraining accuracy: 0.8286916613578796\n",
      "Step: 18600  \tValid loss: 0.3830626606941223\n",
      "Step: 18700  \tTraining loss: 0.3120284676551819\n",
      "Step: 18700  \tTraining accuracy: 0.8287345767021179\n",
      "Step: 18700  \tValid loss: 0.3829856812953949\n",
      "Step: 18800  \tTraining loss: 0.31191372871398926\n",
      "Step: 18800  \tTraining accuracy: 0.828777015209198\n",
      "Step: 18800  \tValid loss: 0.38293352723121643\n",
      "Step: 18900  \tTraining loss: 0.311799019575119\n",
      "Step: 18900  \tTraining accuracy: 0.8288190364837646\n",
      "Step: 18900  \tValid loss: 0.3828642666339874\n",
      "Step: 19000  \tTraining loss: 0.3116563856601715\n",
      "Step: 19000  \tTraining accuracy: 0.8288633227348328\n",
      "Step: 19000  \tValid loss: 0.38267239928245544\n",
      "Step: 19100  \tTraining loss: 0.3115307688713074\n",
      "Step: 19100  \tTraining accuracy: 0.8289058208465576\n",
      "Step: 19100  \tValid loss: 0.382488489151001\n",
      "Step: 19200  \tTraining loss: 0.31141045689582825\n",
      "Step: 19200  \tTraining accuracy: 0.8289478421211243\n",
      "Step: 19200  \tValid loss: 0.3823416531085968\n",
      "Step: 19300  \tTraining loss: 0.3112891912460327\n",
      "Step: 19300  \tTraining accuracy: 0.828988790512085\n",
      "Step: 19300  \tValid loss: 0.3821786642074585\n",
      "Step: 19400  \tTraining loss: 0.31116557121276855\n",
      "Step: 19400  \tTraining accuracy: 0.8290286064147949\n",
      "Step: 19400  \tValid loss: 0.38208943605422974\n",
      "Step: 19500  \tTraining loss: 0.31104031205177307\n",
      "Step: 19500  \tTraining accuracy: 0.8290680050849915\n",
      "Step: 19500  \tValid loss: 0.38193416595458984\n",
      "Step: 19600  \tTraining loss: 0.31091856956481934\n",
      "Step: 19600  \tTraining accuracy: 0.829105019569397\n",
      "Step: 19600  \tValid loss: 0.3817775547504425\n",
      "Step: 19700  \tTraining loss: 0.3107972741127014\n",
      "Step: 19700  \tTraining accuracy: 0.829140305519104\n",
      "Step: 19700  \tValid loss: 0.3816491365432739\n",
      "Step: 19800  \tTraining loss: 0.3106769323348999\n",
      "Step: 19800  \tTraining accuracy: 0.8291752934455872\n",
      "Step: 19800  \tValid loss: 0.38152411580085754\n",
      "Step: 19900  \tTraining loss: 0.31055688858032227\n",
      "Step: 19900  \tTraining accuracy: 0.8292098641395569\n",
      "Step: 19900  \tValid loss: 0.3814103901386261\n",
      "Step: 20000  \tTraining loss: 0.3104383647441864\n",
      "Step: 20000  \tTraining accuracy: 0.8292441368103027\n",
      "Step: 20000  \tValid loss: 0.3812812268733978\n",
      "Step: 20100  \tTraining loss: 0.31031736731529236\n",
      "Step: 20100  \tTraining accuracy: 0.8292780518531799\n",
      "Step: 20100  \tValid loss: 0.3811437487602234\n",
      "Step: 20200  \tTraining loss: 0.3102012574672699\n",
      "Step: 20200  \tTraining accuracy: 0.8293135762214661\n",
      "Step: 20200  \tValid loss: 0.38099536299705505\n",
      "Step: 20300  \tTraining loss: 0.3100847005844116\n",
      "Step: 20300  \tTraining accuracy: 0.8293532133102417\n",
      "Step: 20300  \tValid loss: 0.38093140721321106\n",
      "Step: 20400  \tTraining loss: 0.30996638536453247\n",
      "Step: 20400  \tTraining accuracy: 0.8293924927711487\n",
      "Step: 20400  \tValid loss: 0.3808087408542633\n",
      "Step: 20500  \tTraining loss: 0.30985015630722046\n",
      "Step: 20500  \tTraining accuracy: 0.829431414604187\n",
      "Step: 20500  \tValid loss: 0.38068804144859314\n",
      "Step: 20600  \tTraining loss: 0.3097369372844696\n",
      "Step: 20600  \tTraining accuracy: 0.8294699192047119\n",
      "Step: 20600  \tValid loss: 0.38061442971229553\n",
      "Step: 20700  \tTraining loss: 0.30962181091308594\n",
      "Step: 20700  \tTraining accuracy: 0.8295080661773682\n",
      "Step: 20700  \tValid loss: 0.3804817199707031\n",
      "Step: 20800  \tTraining loss: 0.3095073103904724\n",
      "Step: 20800  \tTraining accuracy: 0.829545795917511\n",
      "Step: 20800  \tValid loss: 0.38039684295654297\n",
      "Step: 20900  \tTraining loss: 0.3093911409378052\n",
      "Step: 20900  \tTraining accuracy: 0.8295807242393494\n",
      "Step: 20900  \tValid loss: 0.38029512763023376\n",
      "Step: 21000  \tTraining loss: 0.30927881598472595\n",
      "Step: 21000  \tTraining accuracy: 0.8296146988868713\n",
      "Step: 21000  \tValid loss: 0.3801783621311188\n",
      "Step: 21100  \tTraining loss: 0.3091646134853363\n",
      "Step: 21100  \tTraining accuracy: 0.8296501636505127\n",
      "Step: 21100  \tValid loss: 0.3801191449165344\n",
      "Step: 21200  \tTraining loss: 0.3090527355670929\n",
      "Step: 21200  \tTraining accuracy: 0.8296890258789062\n",
      "Step: 21200  \tValid loss: 0.38003331422805786\n",
      "Step: 21300  \tTraining loss: 0.3089420199394226\n",
      "Step: 21300  \tTraining accuracy: 0.8297299146652222\n",
      "Step: 21300  \tValid loss: 0.3799712657928467\n",
      "Step: 21400  \tTraining loss: 0.30883046984672546\n",
      "Step: 21400  \tTraining accuracy: 0.8297705054283142\n",
      "Step: 21400  \tValid loss: 0.37986791133880615\n",
      "Step: 21500  \tTraining loss: 0.30871865153312683\n",
      "Step: 21500  \tTraining accuracy: 0.8298088312149048\n",
      "Step: 21500  \tValid loss: 0.3797850012779236\n",
      "Step: 21600  \tTraining loss: 0.30861032009124756\n",
      "Step: 21600  \tTraining accuracy: 0.8298467993736267\n",
      "Step: 21600  \tValid loss: 0.37973299622535706\n",
      "Step: 21700  \tTraining loss: 0.3084988296031952\n",
      "Step: 21700  \tTraining accuracy: 0.8298844695091248\n",
      "Step: 21700  \tValid loss: 0.37962767481803894\n",
      "Step: 21800  \tTraining loss: 0.3083893954753876\n",
      "Step: 21800  \tTraining accuracy: 0.8299241662025452\n",
      "Step: 21800  \tValid loss: 0.37953218817710876\n",
      "Step: 21900  \tTraining loss: 0.3082781732082367\n",
      "Step: 21900  \tTraining accuracy: 0.8299658298492432\n",
      "Step: 21900  \tValid loss: 0.37947094440460205\n",
      "Step: 22000  \tTraining loss: 0.30816781520843506\n",
      "Step: 22000  \tTraining accuracy: 0.8300083875656128\n",
      "Step: 22000  \tValid loss: 0.3793739378452301\n",
      "Step: 22100  \tTraining loss: 0.3080584704875946\n",
      "Step: 22100  \tTraining accuracy: 0.8300504684448242\n",
      "Step: 22100  \tValid loss: 0.3792853057384491\n",
      "Step: 22200  \tTraining loss: 0.30795037746429443\n",
      "Step: 22200  \tTraining accuracy: 0.8300922513008118\n",
      "Step: 22200  \tValid loss: 0.3792110085487366\n",
      "Step: 22300  \tTraining loss: 0.30784109234809875\n",
      "Step: 22300  \tTraining accuracy: 0.8301336169242859\n",
      "Step: 22300  \tValid loss: 0.37916818261146545\n",
      "Step: 22400  \tTraining loss: 0.3077335059642792\n",
      "Step: 22400  \tTraining accuracy: 0.8301734328269958\n",
      "Step: 22400  \tValid loss: 0.379083514213562\n",
      "Step: 22500  \tTraining loss: 0.30762946605682373\n",
      "Step: 22500  \tTraining accuracy: 0.8302111625671387\n",
      "Step: 22500  \tValid loss: 0.3789995610713959\n",
      "Step: 22600  \tTraining loss: 0.3075222671031952\n",
      "Step: 22600  \tTraining accuracy: 0.8302485942840576\n",
      "Step: 22600  \tValid loss: 0.3789447844028473\n",
      "Step: 22700  \tTraining loss: 0.3074157238006592\n",
      "Step: 22700  \tTraining accuracy: 0.8302839398384094\n",
      "Step: 22700  \tValid loss: 0.37889719009399414\n",
      "Step: 22800  \tTraining loss: 0.3073098957538605\n",
      "Step: 22800  \tTraining accuracy: 0.8303178548812866\n",
      "Step: 22800  \tValid loss: 0.3788453936576843\n",
      "Step: 22900  \tTraining loss: 0.3072008788585663\n",
      "Step: 22900  \tTraining accuracy: 0.830348014831543\n",
      "Step: 22900  \tValid loss: 0.37879058718681335\n",
      "Step: 23000  \tTraining loss: 0.3070976138114929\n",
      "Step: 23000  \tTraining accuracy: 0.8303756713867188\n",
      "Step: 23000  \tValid loss: 0.37870705127716064\n",
      "Step: 23100  \tTraining loss: 0.30699044466018677\n",
      "Step: 23100  \tTraining accuracy: 0.8304014205932617\n",
      "Step: 23100  \tValid loss: 0.3786400854587555\n",
      "Step: 23200  \tTraining loss: 0.30688372254371643\n",
      "Step: 23200  \tTraining accuracy: 0.8304257392883301\n",
      "Step: 23200  \tValid loss: 0.37856388092041016\n",
      "Step: 23300  \tTraining loss: 0.3067843019962311\n",
      "Step: 23300  \tTraining accuracy: 0.8304516077041626\n",
      "Step: 23300  \tValid loss: 0.378531277179718\n",
      "Step: 23400  \tTraining loss: 0.30667608976364136\n",
      "Step: 23400  \tTraining accuracy: 0.8304800391197205\n",
      "Step: 23400  \tValid loss: 0.3784063458442688\n",
      "Step: 23500  \tTraining loss: 0.3065708875656128\n",
      "Step: 23500  \tTraining accuracy: 0.8305093050003052\n",
      "Step: 23500  \tValid loss: 0.37835168838500977\n",
      "Step: 23600  \tTraining loss: 0.3064633309841156\n",
      "Step: 23600  \tTraining accuracy: 0.8305383324623108\n",
      "Step: 23600  \tValid loss: 0.37828925251960754\n",
      "Step: 23700  \tTraining loss: 0.30636289715766907\n",
      "Step: 23700  \tTraining accuracy: 0.8305671215057373\n",
      "Step: 23700  \tValid loss: 0.3781908452510834\n",
      "Step: 23800  \tTraining loss: 0.3062556982040405\n",
      "Step: 23800  \tTraining accuracy: 0.830593466758728\n",
      "Step: 23800  \tValid loss: 0.3781663477420807\n",
      "Step: 23900  \tTraining loss: 0.30615076422691345\n",
      "Step: 23900  \tTraining accuracy: 0.8306190371513367\n",
      "Step: 23900  \tValid loss: 0.37809091806411743\n",
      "Step: 24000  \tTraining loss: 0.3060474097728729\n",
      "Step: 24000  \tTraining accuracy: 0.830644428730011\n",
      "Step: 24000  \tValid loss: 0.3780059814453125\n",
      "Step: 24100  \tTraining loss: 0.3059437572956085\n",
      "Step: 24100  \tTraining accuracy: 0.8306711912155151\n",
      "Step: 24100  \tValid loss: 0.37796157598495483\n",
      "Step: 24200  \tTraining loss: 0.30583661794662476\n",
      "Step: 24200  \tTraining accuracy: 0.8306987881660461\n",
      "Step: 24200  \tValid loss: 0.37789130210876465\n",
      "Step: 24300  \tTraining loss: 0.30573299527168274\n",
      "Step: 24300  \tTraining accuracy: 0.8307262063026428\n",
      "Step: 24300  \tValid loss: 0.377817302942276\n",
      "Step: 24400  \tTraining loss: 0.3056296706199646\n",
      "Step: 24400  \tTraining accuracy: 0.8307517766952515\n",
      "Step: 24400  \tValid loss: 0.37775611877441406\n",
      "Step: 24500  \tTraining loss: 0.30552610754966736\n",
      "Step: 24500  \tTraining accuracy: 0.8307760953903198\n",
      "Step: 24500  \tValid loss: 0.3776688873767853\n",
      "Step: 24600  \tTraining loss: 0.30542394518852234\n",
      "Step: 24600  \tTraining accuracy: 0.8308002352714539\n",
      "Step: 24600  \tValid loss: 0.37760859727859497\n",
      "Step: 24700  \tTraining loss: 0.30532053112983704\n",
      "Step: 24700  \tTraining accuracy: 0.8308267593383789\n",
      "Step: 24700  \tValid loss: 0.37751635909080505\n",
      "Step: 24800  \tTraining loss: 0.3052213788032532\n",
      "Step: 24800  \tTraining accuracy: 0.8308557271957397\n",
      "Step: 24800  \tValid loss: 0.37750086188316345\n",
      "Step: 24900  \tTraining loss: 0.30511438846588135\n",
      "Step: 24900  \tTraining accuracy: 0.8308844566345215\n",
      "Step: 24900  \tValid loss: 0.37736940383911133\n",
      "Step: 25000  \tTraining loss: 0.30501309037208557\n",
      "Step: 25000  \tTraining accuracy: 0.8309129476547241\n",
      "Step: 25000  \tValid loss: 0.3772881031036377\n",
      "Step: 25100  \tTraining loss: 0.3049089014530182\n",
      "Step: 25100  \tTraining accuracy: 0.8309412598609924\n",
      "Step: 25100  \tValid loss: 0.377236008644104\n",
      "Step: 25200  \tTraining loss: 0.3048076331615448\n",
      "Step: 25200  \tTraining accuracy: 0.8309692740440369\n",
      "Step: 25200  \tValid loss: 0.3771587610244751\n",
      "Step: 25300  \tTraining loss: 0.3047054409980774\n",
      "Step: 25300  \tTraining accuracy: 0.830997109413147\n",
      "Step: 25300  \tValid loss: 0.3770703673362732\n",
      "Step: 25400  \tTraining loss: 0.30460456013679504\n",
      "Step: 25400  \tTraining accuracy: 0.831024706363678\n",
      "Step: 25400  \tValid loss: 0.37699878215789795\n",
      "Step: 25500  \tTraining loss: 0.30450350046157837\n",
      "Step: 25500  \tTraining accuracy: 0.8310521245002747\n",
      "Step: 25500  \tValid loss: 0.376898854970932\n",
      "Step: 25600  \tTraining loss: 0.3044019341468811\n",
      "Step: 25600  \tTraining accuracy: 0.8310793042182922\n",
      "Step: 25600  \tValid loss: 0.37681785225868225\n",
      "Step: 25700  \tTraining loss: 0.3043017089366913\n",
      "Step: 25700  \tTraining accuracy: 0.8311092853546143\n",
      "Step: 25700  \tValid loss: 0.37673264741897583\n",
      "Step: 25800  \tTraining loss: 0.3042023181915283\n",
      "Step: 25800  \tTraining accuracy: 0.8311411142349243\n",
      "Step: 25800  \tValid loss: 0.3766607940196991\n",
      "Step: 25900  \tTraining loss: 0.3041013777256012\n",
      "Step: 25900  \tTraining accuracy: 0.8311726450920105\n",
      "Step: 25900  \tValid loss: 0.3765772879123688\n",
      "Step: 26000  \tTraining loss: 0.30400219559669495\n",
      "Step: 26000  \tTraining accuracy: 0.8312039375305176\n",
      "Step: 26000  \tValid loss: 0.3764839172363281\n",
      "Step: 26100  \tTraining loss: 0.3039035499095917\n",
      "Step: 26100  \tTraining accuracy: 0.8312350511550903\n",
      "Step: 26100  \tValid loss: 0.37640005350112915\n",
      "Step: 26200  \tTraining loss: 0.3038051128387451\n",
      "Step: 26200  \tTraining accuracy: 0.8312658667564392\n",
      "Step: 26200  \tValid loss: 0.3762855529785156\n",
      "Step: 26300  \tTraining loss: 0.30370768904685974\n",
      "Step: 26300  \tTraining accuracy: 0.831296443939209\n",
      "Step: 26300  \tValid loss: 0.3762156665325165\n",
      "Step: 26400  \tTraining loss: 0.3036099672317505\n",
      "Step: 26400  \tTraining accuracy: 0.8313268423080444\n",
      "Step: 26400  \tValid loss: 0.3761250078678131\n",
      "Step: 26500  \tTraining loss: 0.3035133481025696\n",
      "Step: 26500  \tTraining accuracy: 0.8313579559326172\n",
      "Step: 26500  \tValid loss: 0.37604326009750366\n",
      "Step: 26600  \tTraining loss: 0.3034142255783081\n",
      "Step: 26600  \tTraining accuracy: 0.8313902616500854\n",
      "Step: 26600  \tValid loss: 0.3759390115737915\n",
      "Step: 26700  \tTraining loss: 0.3033178448677063\n",
      "Step: 26700  \tTraining accuracy: 0.8314223885536194\n",
      "Step: 26700  \tValid loss: 0.3758411705493927\n",
      "Step: 26800  \tTraining loss: 0.30322161316871643\n",
      "Step: 26800  \tTraining accuracy: 0.8314542770385742\n",
      "Step: 26800  \tValid loss: 0.3757565915584564\n",
      "Step: 26900  \tTraining loss: 0.3031262159347534\n",
      "Step: 26900  \tTraining accuracy: 0.8314839601516724\n",
      "Step: 26900  \tValid loss: 0.37565141916275024\n",
      "Step: 27000  \tTraining loss: 0.3030308187007904\n",
      "Step: 27000  \tTraining accuracy: 0.8315129280090332\n",
      "Step: 27000  \tValid loss: 0.3755568265914917\n",
      "Step: 27100  \tTraining loss: 0.30293506383895874\n",
      "Step: 27100  \tTraining accuracy: 0.8315417170524597\n",
      "Step: 27100  \tValid loss: 0.3754580616950989\n",
      "Step: 27200  \tTraining loss: 0.3028407394886017\n",
      "Step: 27200  \tTraining accuracy: 0.8315708041191101\n",
      "Step: 27200  \tValid loss: 0.3753482401371002\n",
      "Step: 27300  \tTraining loss: 0.3027458190917969\n",
      "Step: 27300  \tTraining accuracy: 0.8316024541854858\n",
      "Step: 27300  \tValid loss: 0.3752731680870056\n",
      "Step: 27400  \tTraining loss: 0.3026546835899353\n",
      "Step: 27400  \tTraining accuracy: 0.8316386938095093\n",
      "Step: 27400  \tValid loss: 0.3751829266548157\n",
      "Step: 27500  \tTraining loss: 0.30255982279777527\n",
      "Step: 27500  \tTraining accuracy: 0.8316760659217834\n",
      "Step: 27500  \tValid loss: 0.3750765919685364\n",
      "Step: 27600  \tTraining loss: 0.3024698495864868\n",
      "Step: 27600  \tTraining accuracy: 0.8317131996154785\n",
      "Step: 27600  \tValid loss: 0.3749947249889374\n",
      "Step: 27700  \tTraining loss: 0.3023742735385895\n",
      "Step: 27700  \tTraining accuracy: 0.8317500352859497\n",
      "Step: 27700  \tValid loss: 0.3748782277107239\n",
      "Step: 27800  \tTraining loss: 0.30228373408317566\n",
      "Step: 27800  \tTraining accuracy: 0.8317894339561462\n",
      "Step: 27800  \tValid loss: 0.3747822642326355\n",
      "Step: 27900  \tTraining loss: 0.3021913766860962\n",
      "Step: 27900  \tTraining accuracy: 0.8318294286727905\n",
      "Step: 27900  \tValid loss: 0.374648779630661\n",
      "Step: 28000  \tTraining loss: 0.30209922790527344\n",
      "Step: 28000  \tTraining accuracy: 0.8318668603897095\n",
      "Step: 28000  \tValid loss: 0.3745828866958618\n",
      "Step: 28100  \tTraining loss: 0.3020084798336029\n",
      "Step: 28100  \tTraining accuracy: 0.8319044709205627\n",
      "Step: 28100  \tValid loss: 0.3744669556617737\n",
      "Step: 28200  \tTraining loss: 0.3019184470176697\n",
      "Step: 28200  \tTraining accuracy: 0.8319423198699951\n",
      "Step: 28200  \tValid loss: 0.3743635416030884\n",
      "Step: 28300  \tTraining loss: 0.30182820558547974\n",
      "Step: 28300  \tTraining accuracy: 0.8319798707962036\n",
      "Step: 28300  \tValid loss: 0.3742745518684387\n",
      "Step: 28400  \tTraining loss: 0.3017388880252838\n",
      "Step: 28400  \tTraining accuracy: 0.8320184946060181\n",
      "Step: 28400  \tValid loss: 0.3741642236709595\n",
      "Step: 28500  \tTraining loss: 0.3016510605812073\n",
      "Step: 28500  \tTraining accuracy: 0.8320578336715698\n",
      "Step: 28500  \tValid loss: 0.37406206130981445\n",
      "Step: 28600  \tTraining loss: 0.3015621602535248\n",
      "Step: 28600  \tTraining accuracy: 0.8320977687835693\n",
      "Step: 28600  \tValid loss: 0.3739517331123352\n",
      "Step: 28700  \tTraining loss: 0.3014737069606781\n",
      "Step: 28700  \tTraining accuracy: 0.8321342468261719\n",
      "Step: 28700  \tValid loss: 0.3738445043563843\n",
      "Step: 28800  \tTraining loss: 0.3013858497142792\n",
      "Step: 28800  \tTraining accuracy: 0.8321704864501953\n",
      "Step: 28800  \tValid loss: 0.37374454736709595\n",
      "Step: 28900  \tTraining loss: 0.30129778385162354\n",
      "Step: 28900  \tTraining accuracy: 0.8322064876556396\n",
      "Step: 28900  \tValid loss: 0.37364742159843445\n",
      "Step: 29000  \tTraining loss: 0.3012107312679291\n",
      "Step: 29000  \tTraining accuracy: 0.8322439789772034\n",
      "Step: 29000  \tValid loss: 0.37353116273880005\n",
      "Step: 29100  \tTraining loss: 0.301123708486557\n",
      "Step: 29100  \tTraining accuracy: 0.8322834968566895\n",
      "Step: 29100  \tValid loss: 0.3734463155269623\n",
      "Step: 29200  \tTraining loss: 0.3010382354259491\n",
      "Step: 29200  \tTraining accuracy: 0.8323231935501099\n",
      "Step: 29200  \tValid loss: 0.3733704388141632\n",
      "Step: 29300  \tTraining loss: 0.3009502589702606\n",
      "Step: 29300  \tTraining accuracy: 0.8323647975921631\n",
      "Step: 29300  \tValid loss: 0.3732529580593109\n",
      "Step: 29400  \tTraining loss: 0.3008643090724945\n",
      "Step: 29400  \tTraining accuracy: 0.8324061632156372\n",
      "Step: 29400  \tValid loss: 0.3731549084186554\n",
      "Step: 29500  \tTraining loss: 0.3007785975933075\n",
      "Step: 29500  \tTraining accuracy: 0.8324472308158875\n",
      "Step: 29500  \tValid loss: 0.37304845452308655\n",
      "Step: 29600  \tTraining loss: 0.3006943464279175\n",
      "Step: 29600  \tTraining accuracy: 0.8324880599975586\n",
      "Step: 29600  \tValid loss: 0.3729613125324249\n",
      "Step: 29700  \tTraining loss: 0.30060797929763794\n",
      "Step: 29700  \tTraining accuracy: 0.8325285315513611\n",
      "Step: 29700  \tValid loss: 0.3728654384613037\n",
      "Step: 29800  \tTraining loss: 0.3005220592021942\n",
      "Step: 29800  \tTraining accuracy: 0.8325696587562561\n",
      "Step: 29800  \tValid loss: 0.37276434898376465\n",
      "Step: 29900  \tTraining loss: 0.30043932795524597\n",
      "Step: 29900  \tTraining accuracy: 0.8326117992401123\n",
      "Step: 29900  \tValid loss: 0.37267979979515076\n",
      "Step: 30000  \tTraining loss: 0.3003532290458679\n",
      "Step: 30000  \tTraining accuracy: 0.8326537013053894\n",
      "Step: 30000  \tValid loss: 0.37254956364631653\n",
      "Step: 30100  \tTraining loss: 0.3002655804157257\n",
      "Step: 30100  \tTraining accuracy: 0.8326953053474426\n",
      "Step: 30100  \tValid loss: 0.3724512457847595\n",
      "Step: 30200  \tTraining loss: 0.3001776933670044\n",
      "Step: 30200  \tTraining accuracy: 0.832736611366272\n",
      "Step: 30200  \tValid loss: 0.3723645508289337\n",
      "Step: 30300  \tTraining loss: 0.2996690273284912\n",
      "Step: 30300  \tTraining accuracy: 0.832780659198761\n",
      "Step: 30300  \tValid loss: 0.3720107972621918\n",
      "Step: 30400  \tTraining loss: 0.2994508147239685\n",
      "Step: 30400  \tTraining accuracy: 0.8328312635421753\n",
      "Step: 30400  \tValid loss: 0.3721911311149597\n",
      "Step: 30500  \tTraining loss: 0.29929572343826294\n",
      "Step: 30500  \tTraining accuracy: 0.8328807353973389\n",
      "Step: 30500  \tValid loss: 0.3722643256187439\n",
      "Step: 30600  \tTraining loss: 0.29915884137153625\n",
      "Step: 30600  \tTraining accuracy: 0.8329285383224487\n",
      "Step: 30600  \tValid loss: 0.3722105026245117\n",
      "Step: 30700  \tTraining loss: 0.29903504252433777\n",
      "Step: 30700  \tTraining accuracy: 0.8329769372940063\n",
      "Step: 30700  \tValid loss: 0.37214043736457825\n",
      "Step: 30800  \tTraining loss: 0.29891836643218994\n",
      "Step: 30800  \tTraining accuracy: 0.8330245614051819\n",
      "Step: 30800  \tValid loss: 0.37208542227745056\n",
      "Step: 30900  \tTraining loss: 0.2988058030605316\n",
      "Step: 30900  \tTraining accuracy: 0.8330702185630798\n",
      "Step: 30900  \tValid loss: 0.37198787927627563\n",
      "Step: 31000  \tTraining loss: 0.2986984848976135\n",
      "Step: 31000  \tTraining accuracy: 0.8331155776977539\n",
      "Step: 31000  \tValid loss: 0.37191107869148254\n",
      "Step: 31100  \tTraining loss: 0.2985924482345581\n",
      "Step: 31100  \tTraining accuracy: 0.8331605792045593\n",
      "Step: 31100  \tValid loss: 0.371808260679245\n",
      "Step: 31200  \tTraining loss: 0.298488587141037\n",
      "Step: 31200  \tTraining accuracy: 0.8332053422927856\n",
      "Step: 31200  \tValid loss: 0.3717104196548462\n",
      "Step: 31300  \tTraining loss: 0.2983892261981964\n",
      "Step: 31300  \tTraining accuracy: 0.8332498669624329\n",
      "Step: 31300  \tValid loss: 0.37162482738494873\n",
      "Step: 31400  \tTraining loss: 0.29828834533691406\n",
      "Step: 31400  \tTraining accuracy: 0.8332940340042114\n",
      "Step: 31400  \tValid loss: 0.3715238571166992\n",
      "Step: 31500  \tTraining loss: 0.2981853485107422\n",
      "Step: 31500  \tTraining accuracy: 0.833341658115387\n",
      "Step: 31500  \tValid loss: 0.3714047968387604\n",
      "Step: 31600  \tTraining loss: 0.29808729887008667\n",
      "Step: 31600  \tTraining accuracy: 0.833392322063446\n",
      "Step: 31600  \tValid loss: 0.3713191747665405\n",
      "Step: 31700  \tTraining loss: 0.29798758029937744\n",
      "Step: 31700  \tTraining accuracy: 0.8334434628486633\n",
      "Step: 31700  \tValid loss: 0.3712072968482971\n",
      "Step: 31800  \tTraining loss: 0.2978880703449249\n",
      "Step: 31800  \tTraining accuracy: 0.8334954977035522\n",
      "Step: 31800  \tValid loss: 0.37109559774398804\n",
      "Step: 31900  \tTraining loss: 0.2977895438671112\n",
      "Step: 31900  \tTraining accuracy: 0.8335443735122681\n",
      "Step: 31900  \tValid loss: 0.3709966838359833\n",
      "Step: 32000  \tTraining loss: 0.2976905107498169\n",
      "Step: 32000  \tTraining accuracy: 0.8335928916931152\n",
      "Step: 32000  \tValid loss: 0.3709038197994232\n",
      "Step: 32100  \tTraining loss: 0.2975940406322479\n",
      "Step: 32100  \tTraining accuracy: 0.8336423635482788\n",
      "Step: 32100  \tValid loss: 0.3707876205444336\n",
      "Step: 32200  \tTraining loss: 0.29749542474746704\n",
      "Step: 32200  \tTraining accuracy: 0.8336923122406006\n",
      "Step: 32200  \tValid loss: 0.3706967830657959\n",
      "Step: 32300  \tTraining loss: 0.2973979413509369\n",
      "Step: 32300  \tTraining accuracy: 0.8337427973747253\n",
      "Step: 32300  \tValid loss: 0.37060311436653137\n",
      "Step: 32400  \tTraining loss: 0.29730308055877686\n",
      "Step: 32400  \tTraining accuracy: 0.8337913155555725\n",
      "Step: 32400  \tValid loss: 0.3704856336116791\n",
      "Step: 32500  \tTraining loss: 0.2972055673599243\n",
      "Step: 32500  \tTraining accuracy: 0.833842396736145\n",
      "Step: 32500  \tValid loss: 0.370414137840271\n",
      "Step: 32600  \tTraining loss: 0.2970775365829468\n",
      "Step: 32600  \tTraining accuracy: 0.8338910937309265\n",
      "Step: 32600  \tValid loss: 0.37031489610671997\n",
      "Step: 32700  \tTraining loss: 0.29697349667549133\n",
      "Step: 32700  \tTraining accuracy: 0.8339415788650513\n",
      "Step: 32700  \tValid loss: 0.3703994154930115\n",
      "Step: 32800  \tTraining loss: 0.296869695186615\n",
      "Step: 32800  \tTraining accuracy: 0.8339881300926208\n",
      "Step: 32800  \tValid loss: 0.3703210949897766\n",
      "Step: 32900  \tTraining loss: 0.2967732548713684\n",
      "Step: 32900  \tTraining accuracy: 0.8340351581573486\n",
      "Step: 32900  \tValid loss: 0.37023741006851196\n",
      "Step: 33000  \tTraining loss: 0.2966752350330353\n",
      "Step: 33000  \tTraining accuracy: 0.8340827226638794\n",
      "Step: 33000  \tValid loss: 0.3701359033584595\n",
      "Step: 33100  \tTraining loss: 0.2965748906135559\n",
      "Step: 33100  \tTraining accuracy: 0.8341299891471863\n",
      "Step: 33100  \tValid loss: 0.37003251910209656\n",
      "Step: 33200  \tTraining loss: 0.296478807926178\n",
      "Step: 33200  \tTraining accuracy: 0.8341769576072693\n",
      "Step: 33200  \tValid loss: 0.3699343800544739\n",
      "Step: 33300  \tTraining loss: 0.296381413936615\n",
      "Step: 33300  \tTraining accuracy: 0.8342236876487732\n",
      "Step: 33300  \tValid loss: 0.36983269453048706\n",
      "Step: 33400  \tTraining loss: 0.29628312587738037\n",
      "Step: 33400  \tTraining accuracy: 0.8342701196670532\n",
      "Step: 33400  \tValid loss: 0.3697640001773834\n",
      "Step: 33500  \tTraining loss: 0.296190470457077\n",
      "Step: 33500  \tTraining accuracy: 0.8343162536621094\n",
      "Step: 33500  \tValid loss: 0.36968159675598145\n",
      "Step: 33600  \tTraining loss: 0.29609304666519165\n",
      "Step: 33600  \tTraining accuracy: 0.8343621492385864\n",
      "Step: 33600  \tValid loss: 0.3696068227291107\n",
      "Step: 33700  \tTraining loss: 0.29599595069885254\n",
      "Step: 33700  \tTraining accuracy: 0.8344077467918396\n",
      "Step: 33700  \tValid loss: 0.36951982975006104\n",
      "Step: 33800  \tTraining loss: 0.295899361371994\n",
      "Step: 33800  \tTraining accuracy: 0.8344530463218689\n",
      "Step: 33800  \tValid loss: 0.3694160580635071\n",
      "Step: 33900  \tTraining loss: 0.29580405354499817\n",
      "Step: 33900  \tTraining accuracy: 0.8344981074333191\n",
      "Step: 33900  \tValid loss: 0.36935630440711975\n",
      "Step: 34000  \tTraining loss: 0.2957097589969635\n",
      "Step: 34000  \tTraining accuracy: 0.8345429301261902\n",
      "Step: 34000  \tValid loss: 0.3692774176597595\n",
      "Step: 34100  \tTraining loss: 0.29561087489128113\n",
      "Step: 34100  \tTraining accuracy: 0.8345893621444702\n",
      "Step: 34100  \tValid loss: 0.3691720962524414\n",
      "Step: 34200  \tTraining loss: 0.29551807045936584\n",
      "Step: 34200  \tTraining accuracy: 0.8346355557441711\n",
      "Step: 34200  \tValid loss: 0.36908605694770813\n",
      "Step: 34300  \tTraining loss: 0.29542428255081177\n",
      "Step: 34300  \tTraining accuracy: 0.8346803188323975\n",
      "Step: 34300  \tValid loss: 0.36903926730155945\n",
      "Step: 34400  \tTraining loss: 0.29532450437545776\n",
      "Step: 34400  \tTraining accuracy: 0.834725558757782\n",
      "Step: 34400  \tValid loss: 0.36892837285995483\n",
      "Step: 34500  \tTraining loss: 0.2952304780483246\n",
      "Step: 34500  \tTraining accuracy: 0.8347709774971008\n",
      "Step: 34500  \tValid loss: 0.36886122822761536\n",
      "Step: 34600  \tTraining loss: 0.2951342463493347\n",
      "Step: 34600  \tTraining accuracy: 0.8348168134689331\n",
      "Step: 34600  \tValid loss: 0.3687728941440582\n",
      "Step: 34700  \tTraining loss: 0.29504162073135376\n",
      "Step: 34700  \tTraining accuracy: 0.834863543510437\n",
      "Step: 34700  \tValid loss: 0.36870890855789185\n",
      "Step: 34800  \tTraining loss: 0.29494166374206543\n",
      "Step: 34800  \tTraining accuracy: 0.834909975528717\n",
      "Step: 34800  \tValid loss: 0.36861008405685425\n",
      "Step: 34900  \tTraining loss: 0.2948494851589203\n",
      "Step: 34900  \tTraining accuracy: 0.8349546790122986\n",
      "Step: 34900  \tValid loss: 0.3685212731361389\n",
      "Step: 35000  \tTraining loss: 0.29475051164627075\n",
      "Step: 35000  \tTraining accuracy: 0.8349998593330383\n",
      "Step: 35000  \tValid loss: 0.3684641420841217\n",
      "Step: 35100  \tTraining loss: 0.29465430974960327\n",
      "Step: 35100  \tTraining accuracy: 0.835043728351593\n",
      "Step: 35100  \tValid loss: 0.3683721423149109\n",
      "Step: 35200  \tTraining loss: 0.29455775022506714\n",
      "Step: 35200  \tTraining accuracy: 0.8350872993469238\n",
      "Step: 35200  \tValid loss: 0.36830273270606995\n",
      "Step: 35300  \tTraining loss: 0.29446420073509216\n",
      "Step: 35300  \tTraining accuracy: 0.8351294994354248\n",
      "Step: 35300  \tValid loss: 0.3682522773742676\n",
      "Step: 35400  \tTraining loss: 0.29436665773391724\n",
      "Step: 35400  \tTraining accuracy: 0.8351725339889526\n",
      "Step: 35400  \tValid loss: 0.3681437373161316\n",
      "Step: 35500  \tTraining loss: 0.29427433013916016\n",
      "Step: 35500  \tTraining accuracy: 0.8352153897285461\n",
      "Step: 35500  \tValid loss: 0.36806148290634155\n",
      "Step: 35600  \tTraining loss: 0.2941710948944092\n",
      "Step: 35600  \tTraining accuracy: 0.8352580070495605\n",
      "Step: 35600  \tValid loss: 0.36800238490104675\n",
      "Step: 35700  \tTraining loss: 0.29407498240470886\n",
      "Step: 35700  \tTraining accuracy: 0.8352996110916138\n",
      "Step: 35700  \tValid loss: 0.3679238557815552\n",
      "Step: 35800  \tTraining loss: 0.2939755618572235\n",
      "Step: 35800  \tTraining accuracy: 0.83534175157547\n",
      "Step: 35800  \tValid loss: 0.3678745627403259\n",
      "Step: 35900  \tTraining loss: 0.293876975774765\n",
      "Step: 35900  \tTraining accuracy: 0.8353835940361023\n",
      "Step: 35900  \tValid loss: 0.36778420209884644\n",
      "Step: 36000  \tTraining loss: 0.29378440976142883\n",
      "Step: 36000  \tTraining accuracy: 0.8354252576828003\n",
      "Step: 36000  \tValid loss: 0.36776530742645264\n",
      "Step: 36100  \tTraining loss: 0.29368099570274353\n",
      "Step: 36100  \tTraining accuracy: 0.8354670405387878\n",
      "Step: 36100  \tValid loss: 0.3676505982875824\n",
      "Step: 36200  \tTraining loss: 0.29358232021331787\n",
      "Step: 36200  \tTraining accuracy: 0.8355100154876709\n",
      "Step: 36200  \tValid loss: 0.3675861954689026\n",
      "Step: 36300  \tTraining loss: 0.2934824526309967\n",
      "Step: 36300  \tTraining accuracy: 0.8355510234832764\n",
      "Step: 36300  \tValid loss: 0.36750665307044983\n",
      "Step: 36400  \tTraining loss: 0.2933894097805023\n",
      "Step: 36400  \tTraining accuracy: 0.835591733455658\n",
      "Step: 36400  \tValid loss: 0.3674241602420807\n",
      "Step: 36500  \tTraining loss: 0.2932853102684021\n",
      "Step: 36500  \tTraining accuracy: 0.8356322646141052\n",
      "Step: 36500  \tValid loss: 0.3673931956291199\n",
      "Step: 36600  \tTraining loss: 0.29318466782569885\n",
      "Step: 36600  \tTraining accuracy: 0.8356725573539734\n",
      "Step: 36600  \tValid loss: 0.3673230707645416\n",
      "Step: 36700  \tTraining loss: 0.29308420419692993\n",
      "Step: 36700  \tTraining accuracy: 0.8357126116752625\n",
      "Step: 36700  \tValid loss: 0.36725538969039917\n",
      "Step: 36800  \tTraining loss: 0.2929854691028595\n",
      "Step: 36800  \tTraining accuracy: 0.8357524275779724\n",
      "Step: 36800  \tValid loss: 0.36720573902130127\n",
      "Step: 36900  \tTraining loss: 0.2928829491138458\n",
      "Step: 36900  \tTraining accuracy: 0.835792064666748\n",
      "Step: 36900  \tValid loss: 0.36713719367980957\n",
      "Step: 37000  \tTraining loss: 0.2927842438220978\n",
      "Step: 37000  \tTraining accuracy: 0.8358315229415894\n",
      "Step: 37000  \tValid loss: 0.3670770823955536\n",
      "Step: 37100  \tTraining loss: 0.2926824390888214\n",
      "Step: 37100  \tTraining accuracy: 0.8358706831932068\n",
      "Step: 37100  \tValid loss: 0.3670238256454468\n",
      "Step: 37200  \tTraining loss: 0.29258206486701965\n",
      "Step: 37200  \tTraining accuracy: 0.8359097242355347\n",
      "Step: 37200  \tValid loss: 0.3669731914997101\n",
      "Step: 37300  \tTraining loss: 0.2924775779247284\n",
      "Step: 37300  \tTraining accuracy: 0.8359484672546387\n",
      "Step: 37300  \tValid loss: 0.36690521240234375\n",
      "Step: 37400  \tTraining loss: 0.29237625002861023\n",
      "Step: 37400  \tTraining accuracy: 0.8359870910644531\n",
      "Step: 37400  \tValid loss: 0.3668559193611145\n",
      "Step: 37500  \tTraining loss: 0.29227209091186523\n",
      "Step: 37500  \tTraining accuracy: 0.8360254168510437\n",
      "Step: 37500  \tValid loss: 0.36679211258888245\n",
      "Step: 37600  \tTraining loss: 0.2921724021434784\n",
      "Step: 37600  \tTraining accuracy: 0.8360618948936462\n",
      "Step: 37600  \tValid loss: 0.3667570948600769\n",
      "Step: 37700  \tTraining loss: 0.29206666350364685\n",
      "Step: 37700  \tTraining accuracy: 0.8360981345176697\n",
      "Step: 37700  \tValid loss: 0.3666536808013916\n",
      "Step: 37800  \tTraining loss: 0.2919639050960541\n",
      "Step: 37800  \tTraining accuracy: 0.8361341953277588\n",
      "Step: 37800  \tValid loss: 0.36660075187683105\n",
      "Step: 37900  \tTraining loss: 0.2918560802936554\n",
      "Step: 37900  \tTraining accuracy: 0.8361703753471375\n",
      "Step: 37900  \tValid loss: 0.36657941341400146\n",
      "Step: 38000  \tTraining loss: 0.29175272583961487\n",
      "Step: 38000  \tTraining accuracy: 0.836207389831543\n",
      "Step: 38000  \tValid loss: 0.3665069043636322\n",
      "Step: 38100  \tTraining loss: 0.2916499972343445\n",
      "Step: 38100  \tTraining accuracy: 0.8362445831298828\n",
      "Step: 38100  \tValid loss: 0.3665040135383606\n",
      "Step: 38200  \tTraining loss: 0.2915410101413727\n",
      "Step: 38200  \tTraining accuracy: 0.8362815976142883\n",
      "Step: 38200  \tValid loss: 0.3664201498031616\n",
      "Step: 38300  \tTraining loss: 0.2914348840713501\n",
      "Step: 38300  \tTraining accuracy: 0.8363183736801147\n",
      "Step: 38300  \tValid loss: 0.3663707971572876\n",
      "Step: 38400  \tTraining loss: 0.29133182764053345\n",
      "Step: 38400  \tTraining accuracy: 0.8363543152809143\n",
      "Step: 38400  \tValid loss: 0.3663613498210907\n",
      "Step: 38500  \tTraining loss: 0.29122355580329895\n",
      "Step: 38500  \tTraining accuracy: 0.8363890647888184\n",
      "Step: 38500  \tValid loss: 0.3662773370742798\n",
      "Step: 38600  \tTraining loss: 0.2911187708377838\n",
      "Step: 38600  \tTraining accuracy: 0.836426317691803\n",
      "Step: 38600  \tValid loss: 0.36624860763549805\n",
      "Step: 38700  \tTraining loss: 0.2910133898258209\n",
      "Step: 38700  \tTraining accuracy: 0.8364640474319458\n",
      "Step: 38700  \tValid loss: 0.3662160336971283\n",
      "Step: 38800  \tTraining loss: 0.2909092307090759\n",
      "Step: 38800  \tTraining accuracy: 0.8365015387535095\n",
      "Step: 38800  \tValid loss: 0.3661426901817322\n",
      "Step: 38900  \tTraining loss: 0.29080498218536377\n",
      "Step: 38900  \tTraining accuracy: 0.8365378975868225\n",
      "Step: 38900  \tValid loss: 0.3661075532436371\n",
      "Step: 39000  \tTraining loss: 0.29070261120796204\n",
      "Step: 39000  \tTraining accuracy: 0.8365733623504639\n",
      "Step: 39000  \tValid loss: 0.3660622537136078\n",
      "Step: 39100  \tTraining loss: 0.2905999422073364\n",
      "Step: 39100  \tTraining accuracy: 0.8366086483001709\n",
      "Step: 39100  \tValid loss: 0.36605265736579895\n",
      "Step: 39200  \tTraining loss: 0.2904967963695526\n",
      "Step: 39200  \tTraining accuracy: 0.8366438150405884\n",
      "Step: 39200  \tValid loss: 0.3659975528717041\n",
      "Step: 39300  \tTraining loss: 0.29039666056632996\n",
      "Step: 39300  \tTraining accuracy: 0.8366787433624268\n",
      "Step: 39300  \tValid loss: 0.36592745780944824\n",
      "Step: 39400  \tTraining loss: 0.2902969717979431\n",
      "Step: 39400  \tTraining accuracy: 0.8367134928703308\n",
      "Step: 39400  \tValid loss: 0.3659207224845886\n",
      "Step: 39500  \tTraining loss: 0.29019737243652344\n",
      "Step: 39500  \tTraining accuracy: 0.8367484211921692\n",
      "Step: 39500  \tValid loss: 0.36586299538612366\n",
      "Step: 39600  \tTraining loss: 0.29010120034217834\n",
      "Step: 39600  \tTraining accuracy: 0.8367828130722046\n",
      "Step: 39600  \tValid loss: 0.3658364415168762\n",
      "Step: 39700  \tTraining loss: 0.2900031507015228\n",
      "Step: 39700  \tTraining accuracy: 0.8368170261383057\n",
      "Step: 39700  \tValid loss: 0.3657756447792053\n",
      "Step: 39800  \tTraining loss: 0.28990638256073\n",
      "Step: 39800  \tTraining accuracy: 0.8368494510650635\n",
      "Step: 39800  \tValid loss: 0.36573782563209534\n",
      "Step: 39900  \tTraining loss: 0.2898111939430237\n",
      "Step: 39900  \tTraining accuracy: 0.8368827104568481\n",
      "Step: 39900  \tValid loss: 0.3656958043575287\n",
      "Step: 40000  \tTraining loss: 0.2897160053253174\n",
      "Step: 40000  \tTraining accuracy: 0.836916446685791\n",
      "Step: 40000  \tValid loss: 0.365658700466156\n",
      "Step: 40100  \tTraining loss: 0.2896219491958618\n",
      "Step: 40100  \tTraining accuracy: 0.8369500041007996\n",
      "Step: 40100  \tValid loss: 0.36562278866767883\n",
      "Step: 40200  \tTraining loss: 0.2895292341709137\n",
      "Step: 40200  \tTraining accuracy: 0.8369833827018738\n",
      "Step: 40200  \tValid loss: 0.36555254459381104\n",
      "Step: 40300  \tTraining loss: 0.28943708539009094\n",
      "Step: 40300  \tTraining accuracy: 0.837017297744751\n",
      "Step: 40300  \tValid loss: 0.3655185401439667\n",
      "Step: 40400  \tTraining loss: 0.2893466055393219\n",
      "Step: 40400  \tTraining accuracy: 0.8370519280433655\n",
      "Step: 40400  \tValid loss: 0.36548087000846863\n",
      "Step: 40500  \tTraining loss: 0.2892555892467499\n",
      "Step: 40500  \tTraining accuracy: 0.8370851874351501\n",
      "Step: 40500  \tValid loss: 0.36548322439193726\n",
      "Step: 40600  \tTraining loss: 0.28916582465171814\n",
      "Step: 40600  \tTraining accuracy: 0.8371179103851318\n",
      "Step: 40600  \tValid loss: 0.36542022228240967\n",
      "Step: 40700  \tTraining loss: 0.2890743315219879\n",
      "Step: 40700  \tTraining accuracy: 0.8371504545211792\n",
      "Step: 40700  \tValid loss: 0.36538517475128174\n",
      "Step: 40800  \tTraining loss: 0.2889840602874756\n",
      "Step: 40800  \tTraining accuracy: 0.837182879447937\n",
      "Step: 40800  \tValid loss: 0.3653552830219269\n",
      "Step: 40900  \tTraining loss: 0.28889521956443787\n",
      "Step: 40900  \tTraining accuracy: 0.8372151255607605\n",
      "Step: 40900  \tValid loss: 0.3653290271759033\n",
      "Step: 41000  \tTraining loss: 0.28880876302719116\n",
      "Step: 41000  \tTraining accuracy: 0.8372472524642944\n",
      "Step: 41000  \tValid loss: 0.36530011892318726\n",
      "Step: 41100  \tTraining loss: 0.28872010111808777\n",
      "Step: 41100  \tTraining accuracy: 0.8372775912284851\n",
      "Step: 41100  \tValid loss: 0.36535000801086426\n",
      "Step: 41200  \tTraining loss: 0.2886374592781067\n",
      "Step: 41200  \tTraining accuracy: 0.8373078107833862\n",
      "Step: 41200  \tValid loss: 0.36537134647369385\n",
      "Step: 41300  \tTraining loss: 0.2885484993457794\n",
      "Step: 41300  \tTraining accuracy: 0.837337851524353\n",
      "Step: 41300  \tValid loss: 0.3653288185596466\n",
      "Step: 41400  \tTraining loss: 0.28846389055252075\n",
      "Step: 41400  \tTraining accuracy: 0.8373677730560303\n",
      "Step: 41400  \tValid loss: 0.3653298020362854\n",
      "Step: 41500  \tTraining loss: 0.2883775532245636\n",
      "Step: 41500  \tTraining accuracy: 0.8373978137969971\n",
      "Step: 41500  \tValid loss: 0.36531582474708557\n",
      "Step: 41600  \tTraining loss: 0.2882895767688751\n",
      "Step: 41600  \tTraining accuracy: 0.8374274373054504\n",
      "Step: 41600  \tValid loss: 0.36527779698371887\n",
      "Step: 41700  \tTraining loss: 0.2882041931152344\n",
      "Step: 41700  \tTraining accuracy: 0.8374569416046143\n",
      "Step: 41700  \tValid loss: 0.3652675747871399\n",
      "Step: 41800  \tTraining loss: 0.2881165146827698\n",
      "Step: 41800  \tTraining accuracy: 0.8374862670898438\n",
      "Step: 41800  \tValid loss: 0.3652312755584717\n",
      "Step: 41900  \tTraining loss: 0.2880323529243469\n",
      "Step: 41900  \tTraining accuracy: 0.8375154733657837\n",
      "Step: 41900  \tValid loss: 0.36520707607269287\n",
      "Step: 42000  \tTraining loss: 0.28795063495635986\n",
      "Step: 42000  \tTraining accuracy: 0.8375445604324341\n",
      "Step: 42000  \tValid loss: 0.36522024869918823\n",
      "Step: 42100  \tTraining loss: 0.287865549325943\n",
      "Step: 42100  \tTraining accuracy: 0.8375734686851501\n",
      "Step: 42100  \tValid loss: 0.3651699423789978\n",
      "Step: 42200  \tTraining loss: 0.2877838611602783\n",
      "Step: 42200  \tTraining accuracy: 0.8376022577285767\n",
      "Step: 42200  \tValid loss: 0.36521217226982117\n",
      "Step: 42300  \tTraining loss: 0.28770002722740173\n",
      "Step: 42300  \tTraining accuracy: 0.8376309275627136\n",
      "Step: 42300  \tValid loss: 0.36520159244537354\n",
      "Step: 42400  \tTraining loss: 0.2876225709915161\n",
      "Step: 42400  \tTraining accuracy: 0.8376594185829163\n",
      "Step: 42400  \tValid loss: 0.36523905396461487\n",
      "Step: 42500  \tTraining loss: 0.28753653168678284\n",
      "Step: 42500  \tTraining accuracy: 0.8376877903938293\n",
      "Step: 42500  \tValid loss: 0.3652055263519287\n",
      "Step: 42600  \tTraining loss: 0.28745731711387634\n",
      "Step: 42600  \tTraining accuracy: 0.8377148509025574\n",
      "Step: 42600  \tValid loss: 0.36520734429359436\n",
      "Step: 42700  \tTraining loss: 0.28737491369247437\n",
      "Step: 42700  \tTraining accuracy: 0.8377414345741272\n",
      "Step: 42700  \tValid loss: 0.3651989698410034\n",
      "Step: 42800  \tTraining loss: 0.28729504346847534\n",
      "Step: 42800  \tTraining accuracy: 0.8377678990364075\n",
      "Step: 42800  \tValid loss: 0.3652019202709198\n",
      "Step: 42900  \tTraining loss: 0.2872152626514435\n",
      "Step: 42900  \tTraining accuracy: 0.8377942442893982\n",
      "Step: 42900  \tValid loss: 0.3652161657810211\n",
      "Step: 43000  \tTraining loss: 0.28713923692703247\n",
      "Step: 43000  \tTraining accuracy: 0.8378204703330994\n",
      "Step: 43000  \tValid loss: 0.365174800157547\n",
      "Step: 43100  \tTraining loss: 0.28705811500549316\n",
      "Step: 43100  \tTraining accuracy: 0.837846577167511\n",
      "Step: 43100  \tValid loss: 0.36525973677635193\n",
      "Step: 43200  \tTraining loss: 0.28697848320007324\n",
      "Step: 43200  \tTraining accuracy: 0.8378725051879883\n",
      "Step: 43200  \tValid loss: 0.36526691913604736\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8378987\n",
      "Precision: 0.90399724\n",
      "Recall: 0.9130435\n",
      "F1 score: 0.8420703\n",
      "AUC: 0.81837034\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0  0.837899   0.903997  0.913043   0.84207  0.81837  0.286935      0.837868   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.365128       0.837877   0.322862      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  43251.0  \n",
      "4\n",
      "(3625, 8)\n",
      "(3625, 1)\n",
      "(2000, 8)\n",
      "(2000, 1)\n",
      "(1625, 8)\n",
      "(1625, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5059772729873657\n",
      "Step: 100  \tTraining accuracy: 0.789793074131012\n",
      "Step: 100  \tValid loss: 0.5125677585601807\n",
      "Step: 200  \tTraining loss: 0.45009520649909973\n",
      "Step: 200  \tTraining accuracy: 0.7866666913032532\n",
      "Step: 200  \tValid loss: 0.4692537784576416\n",
      "Step: 300  \tTraining loss: 0.4317542016506195\n",
      "Step: 300  \tTraining accuracy: 0.7885241508483887\n",
      "Step: 300  \tValid loss: 0.4508286118507385\n",
      "Step: 400  \tTraining loss: 0.40983930230140686\n",
      "Step: 400  \tTraining accuracy: 0.7903448343276978\n",
      "Step: 400  \tValid loss: 0.4279695451259613\n",
      "Step: 500  \tTraining loss: 0.38105177879333496\n",
      "Step: 500  \tTraining accuracy: 0.7923065423965454\n",
      "Step: 500  \tValid loss: 0.3980466425418854\n",
      "Step: 600  \tTraining loss: 0.34569019079208374\n",
      "Step: 600  \tTraining accuracy: 0.7974169254302979\n",
      "Step: 600  \tValid loss: 0.3620009124279022\n",
      "Step: 700  \tTraining loss: 0.31178945302963257\n",
      "Step: 700  \tTraining accuracy: 0.8040955066680908\n",
      "Step: 700  \tValid loss: 0.3287279009819031\n",
      "Step: 800  \tTraining loss: 0.28863441944122314\n",
      "Step: 800  \tTraining accuracy: 0.8112735748291016\n",
      "Step: 800  \tValid loss: 0.30693623423576355\n",
      "Step: 900  \tTraining loss: 0.27574941515922546\n",
      "Step: 900  \tTraining accuracy: 0.8180608749389648\n",
      "Step: 900  \tValid loss: 0.2951629161834717\n",
      "Step: 1000  \tTraining loss: 0.2684690058231354\n",
      "Step: 1000  \tTraining accuracy: 0.8236951231956482\n",
      "Step: 1000  \tValid loss: 0.28848814964294434\n",
      "Step: 1100  \tTraining loss: 0.2639514207839966\n",
      "Step: 1100  \tTraining accuracy: 0.8287422060966492\n",
      "Step: 1100  \tValid loss: 0.2842697203159332\n",
      "Step: 1200  \tTraining loss: 0.2609493136405945\n",
      "Step: 1200  \tTraining accuracy: 0.8331274390220642\n",
      "Step: 1200  \tValid loss: 0.2814677059650421\n",
      "Step: 1300  \tTraining loss: 0.2588671147823334\n",
      "Step: 1300  \tTraining accuracy: 0.8368441462516785\n",
      "Step: 1300  \tValid loss: 0.2795930504798889\n",
      "Step: 1400  \tTraining loss: 0.25737106800079346\n",
      "Step: 1400  \tTraining accuracy: 0.8400306701660156\n",
      "Step: 1400  \tValid loss: 0.27834972739219666\n",
      "Step: 1500  \tTraining loss: 0.2562582790851593\n",
      "Step: 1500  \tTraining accuracy: 0.8429869413375854\n",
      "Step: 1500  \tValid loss: 0.2775385081768036\n",
      "Step: 1600  \tTraining loss: 0.2553977966308594\n",
      "Step: 1600  \tTraining accuracy: 0.8456863164901733\n",
      "Step: 1600  \tValid loss: 0.2770192325115204\n",
      "Step: 1700  \tTraining loss: 0.2547048032283783\n",
      "Step: 1700  \tTraining accuracy: 0.8481839299201965\n",
      "Step: 1700  \tValid loss: 0.27669650316238403\n",
      "Step: 1800  \tTraining loss: 0.2541258931159973\n",
      "Step: 1800  \tTraining accuracy: 0.8503566384315491\n",
      "Step: 1800  \tValid loss: 0.2765059769153595\n",
      "Step: 1900  \tTraining loss: 0.2536259591579437\n",
      "Step: 1900  \tTraining accuracy: 0.8523094058036804\n",
      "Step: 1900  \tValid loss: 0.2764032781124115\n",
      "Step: 2000  \tTraining loss: 0.25318193435668945\n",
      "Step: 2000  \tTraining accuracy: 0.8540335893630981\n",
      "Step: 2000  \tValid loss: 0.2763572335243225\n",
      "Step: 2100  \tTraining loss: 0.2527786195278168\n",
      "Step: 2100  \tTraining accuracy: 0.8556097745895386\n",
      "Step: 2100  \tValid loss: 0.27634522318840027\n",
      "Step: 2200  \tTraining loss: 0.2524053156375885\n",
      "Step: 2200  \tTraining accuracy: 0.857064962387085\n",
      "Step: 2200  \tValid loss: 0.2763497233390808\n",
      "Step: 2300  \tTraining loss: 0.2520544230937958\n",
      "Step: 2300  \tTraining accuracy: 0.8583908081054688\n",
      "Step: 2300  \tValid loss: 0.2763569951057434\n",
      "Step: 2400  \tTraining loss: 0.25172045826911926\n",
      "Step: 2400  \tTraining accuracy: 0.8596155643463135\n",
      "Step: 2400  \tValid loss: 0.2763553559780121\n",
      "Step: 2500  \tTraining loss: 0.25139856338500977\n",
      "Step: 2500  \tTraining accuracy: 0.8607515692710876\n",
      "Step: 2500  \tValid loss: 0.2763350307941437\n",
      "Step: 2600  \tTraining loss: 0.25108495354652405\n",
      "Step: 2600  \tTraining accuracy: 0.8617985248565674\n",
      "Step: 2600  \tValid loss: 0.2762877941131592\n",
      "Step: 2700  \tTraining loss: 0.250776082277298\n",
      "Step: 2700  \tTraining accuracy: 0.8627768158912659\n",
      "Step: 2700  \tValid loss: 0.2762068510055542\n",
      "Step: 2800  \tTraining loss: 0.2504686713218689\n",
      "Step: 2800  \tTraining accuracy: 0.863673985004425\n",
      "Step: 2800  \tValid loss: 0.2760868966579437\n",
      "Step: 2900  \tTraining loss: 0.2501591742038727\n",
      "Step: 2900  \tTraining accuracy: 0.8645033240318298\n",
      "Step: 2900  \tValid loss: 0.2759244441986084\n",
      "Step: 3000  \tTraining loss: 0.24984487891197205\n",
      "Step: 3000  \tTraining accuracy: 0.8652858138084412\n",
      "Step: 3000  \tValid loss: 0.2757166624069214\n",
      "Step: 3100  \tTraining loss: 0.24952219426631927\n",
      "Step: 3100  \tTraining accuracy: 0.8660395741462708\n",
      "Step: 3100  \tValid loss: 0.27546262741088867\n",
      "Step: 3200  \tTraining loss: 0.24918776750564575\n",
      "Step: 3200  \tTraining accuracy: 0.8667279481887817\n",
      "Step: 3200  \tValid loss: 0.2751632034778595\n",
      "Step: 3300  \tTraining loss: 0.2488381415605545\n",
      "Step: 3300  \tTraining accuracy: 0.8673740029335022\n",
      "Step: 3300  \tValid loss: 0.27482062578201294\n",
      "Step: 3400  \tTraining loss: 0.2484700232744217\n",
      "Step: 3400  \tTraining accuracy: 0.868014395236969\n",
      "Step: 3400  \tValid loss: 0.27443909645080566\n",
      "Step: 3500  \tTraining loss: 0.2480808049440384\n",
      "Step: 3500  \tTraining accuracy: 0.8686016798019409\n",
      "Step: 3500  \tValid loss: 0.2740249037742615\n",
      "Step: 3600  \tTraining loss: 0.24766966700553894\n",
      "Step: 3600  \tTraining accuracy: 0.8691520094871521\n",
      "Step: 3600  \tValid loss: 0.2735861837863922\n",
      "Step: 3700  \tTraining loss: 0.24723932147026062\n",
      "Step: 3700  \tTraining accuracy: 0.8696797490119934\n",
      "Step: 3700  \tValid loss: 0.27313363552093506\n",
      "Step: 3800  \tTraining loss: 0.2467934638261795\n",
      "Step: 3800  \tTraining accuracy: 0.8701572418212891\n",
      "Step: 3800  \tValid loss: 0.2726789116859436\n",
      "Step: 3900  \tTraining loss: 0.24633628129959106\n",
      "Step: 3900  \tTraining accuracy: 0.8706350326538086\n",
      "Step: 3900  \tValid loss: 0.2722329795360565\n",
      "Step: 4000  \tTraining loss: 0.24587062001228333\n",
      "Step: 4000  \tTraining accuracy: 0.8710781335830688\n",
      "Step: 4000  \tValid loss: 0.27180394530296326\n",
      "Step: 4100  \tTraining loss: 0.24539914727210999\n",
      "Step: 4100  \tTraining accuracy: 0.8715027570724487\n",
      "Step: 4100  \tValid loss: 0.2713969051837921\n",
      "Step: 4200  \tTraining loss: 0.24492447078227997\n",
      "Step: 4200  \tTraining accuracy: 0.8719069361686707\n",
      "Step: 4200  \tValid loss: 0.2710147798061371\n",
      "Step: 4300  \tTraining loss: 0.24444955587387085\n",
      "Step: 4300  \tTraining accuracy: 0.8722823262214661\n",
      "Step: 4300  \tValid loss: 0.27065908908843994\n",
      "Step: 4400  \tTraining loss: 0.24397599697113037\n",
      "Step: 4400  \tTraining accuracy: 0.8726563453674316\n",
      "Step: 4400  \tValid loss: 0.27032923698425293\n",
      "Step: 4500  \tTraining loss: 0.24350489675998688\n",
      "Step: 4500  \tTraining accuracy: 0.8730507493019104\n",
      "Step: 4500  \tValid loss: 0.270021915435791\n",
      "Step: 4600  \tTraining loss: 0.24303683638572693\n",
      "Step: 4600  \tTraining accuracy: 0.8734005093574524\n",
      "Step: 4600  \tValid loss: 0.26973292231559753\n",
      "Step: 4700  \tTraining loss: 0.24257135391235352\n",
      "Step: 4700  \tTraining accuracy: 0.8737145066261292\n",
      "Step: 4700  \tValid loss: 0.26945760846138\n",
      "Step: 4800  \tTraining loss: 0.24210892617702484\n",
      "Step: 4800  \tTraining accuracy: 0.8740094304084778\n",
      "Step: 4800  \tValid loss: 0.26919233798980713\n",
      "Step: 4900  \tTraining loss: 0.24165017902851105\n",
      "Step: 4900  \tTraining accuracy: 0.8743149638175964\n",
      "Step: 4900  \tValid loss: 0.2689345180988312\n",
      "Step: 5000  \tTraining loss: 0.2411956489086151\n",
      "Step: 5000  \tTraining accuracy: 0.8746053576469421\n",
      "Step: 5000  \tValid loss: 0.26868170499801636\n",
      "Step: 5100  \tTraining loss: 0.24074649810791016\n",
      "Step: 5100  \tTraining accuracy: 0.8748897314071655\n",
      "Step: 5100  \tValid loss: 0.2684318721294403\n",
      "Step: 5200  \tTraining loss: 0.24030336737632751\n",
      "Step: 5200  \tTraining accuracy: 0.8751898407936096\n",
      "Step: 5200  \tValid loss: 0.2681831121444702\n",
      "Step: 5300  \tTraining loss: 0.2398671805858612\n",
      "Step: 5300  \tTraining accuracy: 0.8754863739013672\n",
      "Step: 5300  \tValid loss: 0.26793327927589417\n",
      "Step: 5400  \tTraining loss: 0.23943793773651123\n",
      "Step: 5400  \tTraining accuracy: 0.8757821321487427\n",
      "Step: 5400  \tValid loss: 0.2676808536052704\n",
      "Step: 5500  \tTraining loss: 0.23901602625846863\n",
      "Step: 5500  \tTraining accuracy: 0.876056969165802\n",
      "Step: 5500  \tValid loss: 0.26742449402809143\n",
      "Step: 5600  \tTraining loss: 0.23860108852386475\n",
      "Step: 5600  \tTraining accuracy: 0.8763342499732971\n",
      "Step: 5600  \tValid loss: 0.26716363430023193\n",
      "Step: 5700  \tTraining loss: 0.23819313943386078\n",
      "Step: 5700  \tTraining accuracy: 0.8766188621520996\n",
      "Step: 5700  \tValid loss: 0.2668980062007904\n",
      "Step: 5800  \tTraining loss: 0.23779180645942688\n",
      "Step: 5800  \tTraining accuracy: 0.8768935799598694\n",
      "Step: 5800  \tValid loss: 0.26662784814834595\n",
      "Step: 5900  \tTraining loss: 0.23739708960056305\n",
      "Step: 5900  \tTraining accuracy: 0.8771517872810364\n",
      "Step: 5900  \tValid loss: 0.2663537561893463\n",
      "Step: 6000  \tTraining loss: 0.237008735537529\n",
      "Step: 6000  \tTraining accuracy: 0.8774105906486511\n",
      "Step: 6000  \tValid loss: 0.26607659459114075\n",
      "Step: 6100  \tTraining loss: 0.23662666976451874\n",
      "Step: 6100  \tTraining accuracy: 0.8776677250862122\n",
      "Step: 6100  \tValid loss: 0.2657971680164337\n",
      "Step: 6200  \tTraining loss: 0.23625098168849945\n",
      "Step: 6200  \tTraining accuracy: 0.8779299259185791\n",
      "Step: 6200  \tValid loss: 0.2655170261859894\n",
      "Step: 6300  \tTraining loss: 0.23588144779205322\n",
      "Step: 6300  \tTraining accuracy: 0.8781771063804626\n",
      "Step: 6300  \tValid loss: 0.26523715257644653\n",
      "Step: 6400  \tTraining loss: 0.2355184406042099\n",
      "Step: 6400  \tTraining accuracy: 0.8784295320510864\n",
      "Step: 6400  \tValid loss: 0.26495927572250366\n",
      "Step: 6500  \tTraining loss: 0.235161691904068\n",
      "Step: 6500  \tTraining accuracy: 0.8786869645118713\n",
      "Step: 6500  \tValid loss: 0.2646847069263458\n",
      "Step: 6600  \tTraining loss: 0.23481179773807526\n",
      "Step: 6600  \tTraining accuracy: 0.8789365887641907\n",
      "Step: 6600  \tValid loss: 0.26441508531570435\n",
      "Step: 6700  \tTraining loss: 0.23446856439113617\n",
      "Step: 6700  \tTraining accuracy: 0.879182755947113\n",
      "Step: 6700  \tValid loss: 0.26415184140205383\n",
      "Step: 6800  \tTraining loss: 0.23413224518299103\n",
      "Step: 6800  \tTraining accuracy: 0.879438042640686\n",
      "Step: 6800  \tValid loss: 0.2638964056968689\n",
      "Step: 6900  \tTraining loss: 0.23380248248577118\n",
      "Step: 6900  \tTraining accuracy: 0.8796798586845398\n",
      "Step: 6900  \tValid loss: 0.26364970207214355\n",
      "Step: 7000  \tTraining loss: 0.2334795743227005\n",
      "Step: 7000  \tTraining accuracy: 0.8799146413803101\n",
      "Step: 7000  \tValid loss: 0.2634129822254181\n",
      "Step: 7100  \tTraining loss: 0.23316331207752228\n",
      "Step: 7100  \tTraining accuracy: 0.8801428079605103\n",
      "Step: 7100  \tValid loss: 0.2631870210170746\n",
      "Step: 7200  \tTraining loss: 0.2328534722328186\n",
      "Step: 7200  \tTraining accuracy: 0.8803742527961731\n",
      "Step: 7200  \tValid loss: 0.26297253370285034\n",
      "Step: 7300  \tTraining loss: 0.23254987597465515\n",
      "Step: 7300  \tTraining accuracy: 0.8805992603302002\n",
      "Step: 7300  \tValid loss: 0.262769877910614\n",
      "Step: 7400  \tTraining loss: 0.23225240409374237\n",
      "Step: 7400  \tTraining accuracy: 0.8808144330978394\n",
      "Step: 7400  \tValid loss: 0.26257938146591187\n",
      "Step: 7500  \tTraining loss: 0.23196075856685638\n",
      "Step: 7500  \tTraining accuracy: 0.8810331225395203\n",
      "Step: 7500  \tValid loss: 0.2624010741710663\n",
      "Step: 7600  \tTraining loss: 0.231674462556839\n",
      "Step: 7600  \tTraining accuracy: 0.8812587261199951\n",
      "Step: 7600  \tValid loss: 0.2622346878051758\n",
      "Step: 7700  \tTraining loss: 0.23139353096485138\n",
      "Step: 7700  \tTraining accuracy: 0.881478488445282\n",
      "Step: 7700  \tValid loss: 0.2620798945426941\n",
      "Step: 7800  \tTraining loss: 0.23111753165721893\n",
      "Step: 7800  \tTraining accuracy: 0.8816925287246704\n",
      "Step: 7800  \tValid loss: 0.26193615794181824\n",
      "Step: 7900  \tTraining loss: 0.23084613680839539\n",
      "Step: 7900  \tTraining accuracy: 0.8819064497947693\n",
      "Step: 7900  \tValid loss: 0.2618025541305542\n",
      "Step: 8000  \tTraining loss: 0.23057904839515686\n",
      "Step: 8000  \tTraining accuracy: 0.8821149468421936\n",
      "Step: 8000  \tValid loss: 0.2616782784461975\n",
      "Step: 8100  \tTraining loss: 0.23031572997570038\n",
      "Step: 8100  \tTraining accuracy: 0.8823182582855225\n",
      "Step: 8100  \tValid loss: 0.26156264543533325\n",
      "Step: 8200  \tTraining loss: 0.2300558090209961\n",
      "Step: 8200  \tTraining accuracy: 0.8825216889381409\n",
      "Step: 8200  \tValid loss: 0.26145440340042114\n",
      "Step: 8300  \tTraining loss: 0.22979925572872162\n",
      "Step: 8300  \tTraining accuracy: 0.8827118277549744\n",
      "Step: 8300  \tValid loss: 0.26135289669036865\n",
      "Step: 8400  \tTraining loss: 0.22954504191875458\n",
      "Step: 8400  \tTraining accuracy: 0.8828907608985901\n",
      "Step: 8400  \tValid loss: 0.2612571716308594\n",
      "Step: 8500  \tTraining loss: 0.22929349541664124\n",
      "Step: 8500  \tTraining accuracy: 0.8830638527870178\n",
      "Step: 8500  \tValid loss: 0.26116618514060974\n",
      "Step: 8600  \tTraining loss: 0.22904342412948608\n",
      "Step: 8600  \tTraining accuracy: 0.8832361102104187\n",
      "Step: 8600  \tValid loss: 0.26107898354530334\n",
      "Step: 8700  \tTraining loss: 0.2287951409816742\n",
      "Step: 8700  \tTraining accuracy: 0.8834108114242554\n",
      "Step: 8700  \tValid loss: 0.26099497079849243\n",
      "Step: 8800  \tTraining loss: 0.22854766249656677\n",
      "Step: 8800  \tTraining accuracy: 0.8835814595222473\n",
      "Step: 8800  \tValid loss: 0.2609134018421173\n",
      "Step: 8900  \tTraining loss: 0.2283010631799698\n",
      "Step: 8900  \tTraining accuracy: 0.8837451934814453\n",
      "Step: 8900  \tValid loss: 0.2608336806297302\n",
      "Step: 9000  \tTraining loss: 0.22805489599704742\n",
      "Step: 9000  \tTraining accuracy: 0.8839036822319031\n",
      "Step: 9000  \tValid loss: 0.26075494289398193\n",
      "Step: 9100  \tTraining loss: 0.22780869901180267\n",
      "Step: 9100  \tTraining accuracy: 0.8840693235397339\n",
      "Step: 9100  \tValid loss: 0.260676771402359\n",
      "Step: 9200  \tTraining loss: 0.22756235301494598\n",
      "Step: 9200  \tTraining accuracy: 0.8842359185218811\n",
      "Step: 9200  \tValid loss: 0.260598748922348\n",
      "Step: 9300  \tTraining loss: 0.22731539607048035\n",
      "Step: 9300  \tTraining accuracy: 0.884398877620697\n",
      "Step: 9300  \tValid loss: 0.2605205774307251\n",
      "Step: 9400  \tTraining loss: 0.2270677238702774\n",
      "Step: 9400  \tTraining accuracy: 0.8845524787902832\n",
      "Step: 9400  \tValid loss: 0.2604416012763977\n",
      "Step: 9500  \tTraining loss: 0.22681939601898193\n",
      "Step: 9500  \tTraining accuracy: 0.8846983909606934\n",
      "Step: 9500  \tValid loss: 0.2603616416454315\n",
      "Step: 9600  \tTraining loss: 0.22656984627246857\n",
      "Step: 9600  \tTraining accuracy: 0.8848369717597961\n",
      "Step: 9600  \tValid loss: 0.26028069853782654\n",
      "Step: 9700  \tTraining loss: 0.22631926834583282\n",
      "Step: 9700  \tTraining accuracy: 0.8849712610244751\n",
      "Step: 9700  \tValid loss: 0.26019829511642456\n",
      "Step: 9800  \tTraining loss: 0.2260671854019165\n",
      "Step: 9800  \tTraining accuracy: 0.8851027488708496\n",
      "Step: 9800  \tValid loss: 0.2601144015789032\n",
      "Step: 9900  \tTraining loss: 0.22581380605697632\n",
      "Step: 9900  \tTraining accuracy: 0.8852315545082092\n",
      "Step: 9900  \tValid loss: 0.26002898812294006\n",
      "Step: 10000  \tTraining loss: 0.22555899620056152\n",
      "Step: 10000  \tTraining accuracy: 0.8853661417961121\n",
      "Step: 10000  \tValid loss: 0.2599419057369232\n",
      "Step: 10100  \tTraining loss: 0.22530262172222137\n",
      "Step: 10100  \tTraining accuracy: 0.8855035305023193\n",
      "Step: 10100  \tValid loss: 0.25985321402549744\n",
      "Step: 10200  \tTraining loss: 0.2250448763370514\n",
      "Step: 10200  \tTraining accuracy: 0.8856409192085266\n",
      "Step: 10200  \tValid loss: 0.2597629427909851\n",
      "Step: 10300  \tTraining loss: 0.22478562593460083\n",
      "Step: 10300  \tTraining accuracy: 0.885775625705719\n",
      "Step: 10300  \tValid loss: 0.25967106223106384\n",
      "Step: 10400  \tTraining loss: 0.22452504932880402\n",
      "Step: 10400  \tTraining accuracy: 0.8859077095985413\n",
      "Step: 10400  \tValid loss: 0.25957781076431274\n",
      "Step: 10500  \tTraining loss: 0.2242630124092102\n",
      "Step: 10500  \tTraining accuracy: 0.886037290096283\n",
      "Step: 10500  \tValid loss: 0.2594832181930542\n",
      "Step: 10600  \tTraining loss: 0.22399963438510895\n",
      "Step: 10600  \tTraining accuracy: 0.8861631155014038\n",
      "Step: 10600  \tValid loss: 0.2593875825405121\n",
      "Step: 10700  \tTraining loss: 0.2237352579832077\n",
      "Step: 10700  \tTraining accuracy: 0.8862865567207336\n",
      "Step: 10700  \tValid loss: 0.2592909038066864\n",
      "Step: 10800  \tTraining loss: 0.2234697788953781\n",
      "Step: 10800  \tTraining accuracy: 0.8864076733589172\n",
      "Step: 10800  \tValid loss: 0.2591932415962219\n",
      "Step: 10900  \tTraining loss: 0.2232033610343933\n",
      "Step: 10900  \tTraining accuracy: 0.8865177035331726\n",
      "Step: 10900  \tValid loss: 0.25909510254859924\n",
      "Step: 11000  \tTraining loss: 0.22293604910373688\n",
      "Step: 11000  \tTraining accuracy: 0.8866257071495056\n",
      "Step: 11000  \tValid loss: 0.2589959502220154\n",
      "Step: 11100  \tTraining loss: 0.2226680964231491\n",
      "Step: 11100  \tTraining accuracy: 0.8867367506027222\n",
      "Step: 11100  \tValid loss: 0.2588961720466614\n",
      "Step: 11200  \tTraining loss: 0.22239944338798523\n",
      "Step: 11200  \tTraining accuracy: 0.8868495225906372\n",
      "Step: 11200  \tValid loss: 0.25879546999931335\n",
      "Step: 11300  \tTraining loss: 0.2221301645040512\n",
      "Step: 11300  \tTraining accuracy: 0.886967658996582\n",
      "Step: 11300  \tValid loss: 0.25869396328926086\n",
      "Step: 11400  \tTraining loss: 0.2218606323003769\n",
      "Step: 11400  \tTraining accuracy: 0.8870849013328552\n",
      "Step: 11400  \tValid loss: 0.2585916817188263\n",
      "Step: 11500  \tTraining loss: 0.2215907722711563\n",
      "Step: 11500  \tTraining accuracy: 0.887200117111206\n",
      "Step: 11500  \tValid loss: 0.25848880410194397\n",
      "Step: 11600  \tTraining loss: 0.22131749987602234\n",
      "Step: 11600  \tTraining accuracy: 0.8873133063316345\n",
      "Step: 11600  \tValid loss: 0.25835102796554565\n",
      "Step: 11700  \tTraining loss: 0.22103963792324066\n",
      "Step: 11700  \tTraining accuracy: 0.8874233961105347\n",
      "Step: 11700  \tValid loss: 0.258187472820282\n",
      "Step: 11800  \tTraining loss: 0.22076237201690674\n",
      "Step: 11800  \tTraining accuracy: 0.8875269293785095\n",
      "Step: 11800  \tValid loss: 0.258034884929657\n",
      "Step: 11900  \tTraining loss: 0.22048573195934296\n",
      "Step: 11900  \tTraining accuracy: 0.8876286745071411\n",
      "Step: 11900  \tValid loss: 0.25789082050323486\n",
      "Step: 12000  \tTraining loss: 0.22019648551940918\n",
      "Step: 12000  \tTraining accuracy: 0.887732207775116\n",
      "Step: 12000  \tValid loss: 0.2577746510505676\n",
      "Step: 12100  \tTraining loss: 0.219930961728096\n",
      "Step: 12100  \tTraining accuracy: 0.8878363370895386\n",
      "Step: 12100  \tValid loss: 0.25771620869636536\n",
      "Step: 12200  \tTraining loss: 0.2196694314479828\n",
      "Step: 12200  \tTraining accuracy: 0.8879352807998657\n",
      "Step: 12200  \tValid loss: 0.25764569640159607\n",
      "Step: 12300  \tTraining loss: 0.21941208839416504\n",
      "Step: 12300  \tTraining accuracy: 0.8880214095115662\n",
      "Step: 12300  \tValid loss: 0.2575646936893463\n",
      "Step: 12400  \tTraining loss: 0.21915939450263977\n",
      "Step: 12400  \tTraining accuracy: 0.8881027698516846\n",
      "Step: 12400  \tValid loss: 0.2574756145477295\n",
      "Step: 12500  \tTraining loss: 0.21891169250011444\n",
      "Step: 12500  \tTraining accuracy: 0.8881850242614746\n",
      "Step: 12500  \tValid loss: 0.25738099217414856\n",
      "Step: 12600  \tTraining loss: 0.21866931021213531\n",
      "Step: 12600  \tTraining accuracy: 0.8882681727409363\n",
      "Step: 12600  \tValid loss: 0.25728240609169006\n",
      "Step: 12700  \tTraining loss: 0.21843238174915314\n",
      "Step: 12700  \tTraining accuracy: 0.8883587121963501\n",
      "Step: 12700  \tValid loss: 0.257180392742157\n",
      "Step: 12800  \tTraining loss: 0.21820053458213806\n",
      "Step: 12800  \tTraining accuracy: 0.8884533047676086\n",
      "Step: 12800  \tValid loss: 0.2570754289627075\n",
      "Step: 12900  \tTraining loss: 0.2179735153913498\n",
      "Step: 12900  \tTraining accuracy: 0.8885517120361328\n",
      "Step: 12900  \tValid loss: 0.25696685910224915\n",
      "Step: 13000  \tTraining loss: 0.21775084733963013\n",
      "Step: 13000  \tTraining accuracy: 0.8886486291885376\n",
      "Step: 13000  \tValid loss: 0.256853848695755\n",
      "Step: 13100  \tTraining loss: 0.2175317406654358\n",
      "Step: 13100  \tTraining accuracy: 0.8887441158294678\n",
      "Step: 13100  \tValid loss: 0.25673580169677734\n",
      "Step: 13200  \tTraining loss: 0.21731597185134888\n",
      "Step: 13200  \tTraining accuracy: 0.8888380527496338\n",
      "Step: 13200  \tValid loss: 0.25661206245422363\n",
      "Step: 13300  \tTraining loss: 0.217102512717247\n",
      "Step: 13300  \tTraining accuracy: 0.8889296054840088\n",
      "Step: 13300  \tValid loss: 0.2564815580844879\n",
      "Step: 13400  \tTraining loss: 0.21689103543758392\n",
      "Step: 13400  \tTraining accuracy: 0.889019787311554\n",
      "Step: 13400  \tValid loss: 0.25634413957595825\n",
      "Step: 13500  \tTraining loss: 0.2166803777217865\n",
      "Step: 13500  \tTraining accuracy: 0.8891096115112305\n",
      "Step: 13500  \tValid loss: 0.25619837641716003\n",
      "Step: 13600  \tTraining loss: 0.21646994352340698\n",
      "Step: 13600  \tTraining accuracy: 0.8891981244087219\n",
      "Step: 13600  \tValid loss: 0.25604313611984253\n",
      "Step: 13700  \tTraining loss: 0.21625865995883942\n",
      "Step: 13700  \tTraining accuracy: 0.8892853260040283\n",
      "Step: 13700  \tValid loss: 0.2558802366256714\n",
      "Step: 13800  \tTraining loss: 0.21604564785957336\n",
      "Step: 13800  \tTraining accuracy: 0.8893762826919556\n",
      "Step: 13800  \tValid loss: 0.25569984316825867\n",
      "Step: 13900  \tTraining loss: 0.21583005785942078\n",
      "Step: 13900  \tTraining accuracy: 0.8894669413566589\n",
      "Step: 13900  \tValid loss: 0.25550296902656555\n",
      "Step: 14000  \tTraining loss: 0.21561068296432495\n",
      "Step: 14000  \tTraining accuracy: 0.8895533084869385\n",
      "Step: 14000  \tValid loss: 0.2552887201309204\n",
      "Step: 14100  \tTraining loss: 0.21538633108139038\n",
      "Step: 14100  \tTraining accuracy: 0.8896365165710449\n",
      "Step: 14100  \tValid loss: 0.255056232213974\n",
      "Step: 14200  \tTraining loss: 0.2151561677455902\n",
      "Step: 14200  \tTraining accuracy: 0.8897214531898499\n",
      "Step: 14200  \tValid loss: 0.25480568408966064\n",
      "Step: 14300  \tTraining loss: 0.21491996943950653\n",
      "Step: 14300  \tTraining accuracy: 0.8898051977157593\n",
      "Step: 14300  \tValid loss: 0.25453224778175354\n",
      "Step: 14400  \tTraining loss: 0.21467776596546173\n",
      "Step: 14400  \tTraining accuracy: 0.8898839354515076\n",
      "Step: 14400  \tValid loss: 0.2542244493961334\n",
      "Step: 14500  \tTraining loss: 0.21443036198616028\n",
      "Step: 14500  \tTraining accuracy: 0.8899625539779663\n",
      "Step: 14500  \tValid loss: 0.2539537250995636\n",
      "Step: 14600  \tTraining loss: 0.21417880058288574\n",
      "Step: 14600  \tTraining accuracy: 0.8900400400161743\n",
      "Step: 14600  \tValid loss: 0.25367066264152527\n",
      "Step: 14700  \tTraining loss: 0.21392391622066498\n",
      "Step: 14700  \tTraining accuracy: 0.8901165127754211\n",
      "Step: 14700  \tValid loss: 0.25335603952407837\n",
      "Step: 14800  \tTraining loss: 0.21366718411445618\n",
      "Step: 14800  \tTraining accuracy: 0.8901947140693665\n",
      "Step: 14800  \tValid loss: 0.25305965542793274\n",
      "Step: 14900  \tTraining loss: 0.21340858936309814\n",
      "Step: 14900  \tTraining accuracy: 0.8902719020843506\n",
      "Step: 14900  \tValid loss: 0.2527228593826294\n",
      "Step: 15000  \tTraining loss: 0.21314826607704163\n",
      "Step: 15000  \tTraining accuracy: 0.8903508186340332\n",
      "Step: 15000  \tValid loss: 0.25245752930641174\n",
      "Step: 15100  \tTraining loss: 0.21288427710533142\n",
      "Step: 15100  \tTraining accuracy: 0.8904305100440979\n",
      "Step: 15100  \tValid loss: 0.25209301710128784\n",
      "Step: 15200  \tTraining loss: 0.21261683106422424\n",
      "Step: 15200  \tTraining accuracy: 0.8905054926872253\n",
      "Step: 15200  \tValid loss: 0.2517867684364319\n",
      "Step: 15300  \tTraining loss: 0.21234463155269623\n",
      "Step: 15300  \tTraining accuracy: 0.8905795216560364\n",
      "Step: 15300  \tValid loss: 0.2514551281929016\n",
      "Step: 15400  \tTraining loss: 0.2120669186115265\n",
      "Step: 15400  \tTraining accuracy: 0.890652596950531\n",
      "Step: 15400  \tValid loss: 0.25112423300743103\n",
      "Step: 15500  \tTraining loss: 0.21178361773490906\n",
      "Step: 15500  \tTraining accuracy: 0.8907211422920227\n",
      "Step: 15500  \tValid loss: 0.25078752636909485\n",
      "Step: 15600  \tTraining loss: 0.21149347722530365\n",
      "Step: 15600  \tTraining accuracy: 0.8907896876335144\n",
      "Step: 15600  \tValid loss: 0.25044721364974976\n",
      "Step: 15700  \tTraining loss: 0.2111961841583252\n",
      "Step: 15700  \tTraining accuracy: 0.8908547163009644\n",
      "Step: 15700  \tValid loss: 0.2500724196434021\n",
      "Step: 15800  \tTraining loss: 0.21089138090610504\n",
      "Step: 15800  \tTraining accuracy: 0.8909215331077576\n",
      "Step: 15800  \tValid loss: 0.24976348876953125\n",
      "Step: 15900  \tTraining loss: 0.21057787537574768\n",
      "Step: 15900  \tTraining accuracy: 0.8909883499145508\n",
      "Step: 15900  \tValid loss: 0.24936041235923767\n",
      "Step: 16000  \tTraining loss: 0.2102569341659546\n",
      "Step: 16000  \tTraining accuracy: 0.8910621404647827\n",
      "Step: 16000  \tValid loss: 0.2490178346633911\n",
      "Step: 16100  \tTraining loss: 0.20992933213710785\n",
      "Step: 16100  \tTraining accuracy: 0.89113849401474\n",
      "Step: 16100  \tValid loss: 0.24860188364982605\n",
      "Step: 16200  \tTraining loss: 0.20959581434726715\n",
      "Step: 16200  \tTraining accuracy: 0.8912104368209839\n",
      "Step: 16200  \tValid loss: 0.24815021455287933\n",
      "Step: 16300  \tTraining loss: 0.20925989747047424\n",
      "Step: 16300  \tTraining accuracy: 0.8912848830223083\n",
      "Step: 16300  \tValid loss: 0.24772515892982483\n",
      "Step: 16400  \tTraining loss: 0.20892195403575897\n",
      "Step: 16400  \tTraining accuracy: 0.8913601040840149\n",
      "Step: 16400  \tValid loss: 0.24725347757339478\n",
      "Step: 16500  \tTraining loss: 0.20858465135097504\n",
      "Step: 16500  \tTraining accuracy: 0.8914344310760498\n",
      "Step: 16500  \tValid loss: 0.24673880636692047\n",
      "Step: 16600  \tTraining loss: 0.20825035870075226\n",
      "Step: 16600  \tTraining accuracy: 0.8915103673934937\n",
      "Step: 16600  \tValid loss: 0.24626001715660095\n",
      "Step: 16700  \tTraining loss: 0.20791979134082794\n",
      "Step: 16700  \tTraining accuracy: 0.8915829062461853\n",
      "Step: 16700  \tValid loss: 0.24567779898643494\n",
      "Step: 16800  \tTraining loss: 0.2075883001089096\n",
      "Step: 16800  \tTraining accuracy: 0.8916537165641785\n",
      "Step: 16800  \tValid loss: 0.24513676762580872\n",
      "Step: 16900  \tTraining loss: 0.20726266503334045\n",
      "Step: 16900  \tTraining accuracy: 0.8917237520217896\n",
      "Step: 16900  \tValid loss: 0.24457043409347534\n",
      "Step: 17000  \tTraining loss: 0.20694872736930847\n",
      "Step: 17000  \tTraining accuracy: 0.891792893409729\n",
      "Step: 17000  \tValid loss: 0.2441045194864273\n",
      "Step: 17100  \tTraining loss: 0.20664054155349731\n",
      "Step: 17100  \tTraining accuracy: 0.8918564319610596\n",
      "Step: 17100  \tValid loss: 0.24349632859230042\n",
      "Step: 17200  \tTraining loss: 0.2063368409872055\n",
      "Step: 17200  \tTraining accuracy: 0.8919151425361633\n",
      "Step: 17200  \tValid loss: 0.24293942749500275\n",
      "Step: 17300  \tTraining loss: 0.20603948831558228\n",
      "Step: 17300  \tTraining accuracy: 0.8919731974601746\n",
      "Step: 17300  \tValid loss: 0.24242563545703888\n",
      "Step: 17400  \tTraining loss: 0.2057477980852127\n",
      "Step: 17400  \tTraining accuracy: 0.8920305967330933\n",
      "Step: 17400  \tValid loss: 0.2418704330921173\n",
      "Step: 17500  \tTraining loss: 0.20546172559261322\n",
      "Step: 17500  \tTraining accuracy: 0.8920865654945374\n",
      "Step: 17500  \tValid loss: 0.24136313796043396\n",
      "Step: 17600  \tTraining loss: 0.20518247783184052\n",
      "Step: 17600  \tTraining accuracy: 0.892142653465271\n",
      "Step: 17600  \tValid loss: 0.24090465903282166\n",
      "Step: 17700  \tTraining loss: 0.20491020381450653\n",
      "Step: 17700  \tTraining accuracy: 0.8922043442726135\n",
      "Step: 17700  \tValid loss: 0.2403569519519806\n",
      "Step: 17800  \tTraining loss: 0.2046438455581665\n",
      "Step: 17800  \tTraining accuracy: 0.892270028591156\n",
      "Step: 17800  \tValid loss: 0.23993168771266937\n",
      "Step: 17900  \tTraining loss: 0.20438314974308014\n",
      "Step: 17900  \tTraining accuracy: 0.8923357725143433\n",
      "Step: 17900  \tValid loss: 0.23955604434013367\n",
      "Step: 18000  \tTraining loss: 0.204129159450531\n",
      "Step: 18000  \tTraining accuracy: 0.8923946022987366\n",
      "Step: 18000  \tValid loss: 0.23913533985614777\n",
      "Step: 18100  \tTraining loss: 0.2038825899362564\n",
      "Step: 18100  \tTraining accuracy: 0.8924527764320374\n",
      "Step: 18100  \tValid loss: 0.23883840441703796\n",
      "Step: 18200  \tTraining loss: 0.20364101231098175\n",
      "Step: 18200  \tTraining accuracy: 0.8925141096115112\n",
      "Step: 18200  \tValid loss: 0.23840801417827606\n",
      "Step: 18300  \tTraining loss: 0.20340505242347717\n",
      "Step: 18300  \tTraining accuracy: 0.8925785422325134\n",
      "Step: 18300  \tValid loss: 0.23805662989616394\n",
      "Step: 18400  \tTraining loss: 0.20317426323890686\n",
      "Step: 18400  \tTraining accuracy: 0.8926438093185425\n",
      "Step: 18400  \tValid loss: 0.23775321245193481\n",
      "Step: 18500  \tTraining loss: 0.20294705033302307\n",
      "Step: 18500  \tTraining accuracy: 0.8927090764045715\n",
      "Step: 18500  \tValid loss: 0.23738336563110352\n",
      "Step: 18600  \tTraining loss: 0.20272397994995117\n",
      "Step: 18600  \tTraining accuracy: 0.8927751779556274\n",
      "Step: 18600  \tValid loss: 0.23703601956367493\n",
      "Step: 18700  \tTraining loss: 0.20250669121742249\n",
      "Step: 18700  \tTraining accuracy: 0.8928412795066833\n",
      "Step: 18700  \tValid loss: 0.23657019436359406\n",
      "Step: 18800  \tTraining loss: 0.20229138433933258\n",
      "Step: 18800  \tTraining accuracy: 0.8929081559181213\n",
      "Step: 18800  \tValid loss: 0.23638099431991577\n",
      "Step: 18900  \tTraining loss: 0.2020801454782486\n",
      "Step: 18900  \tTraining accuracy: 0.8929750323295593\n",
      "Step: 18900  \tValid loss: 0.23594874143600464\n",
      "Step: 19000  \tTraining loss: 0.20187242329120636\n",
      "Step: 19000  \tTraining accuracy: 0.89304119348526\n",
      "Step: 19000  \tValid loss: 0.23560982942581177\n",
      "Step: 19100  \tTraining loss: 0.20166517794132233\n",
      "Step: 19100  \tTraining accuracy: 0.8931081295013428\n",
      "Step: 19100  \tValid loss: 0.23526769876480103\n",
      "Step: 19200  \tTraining loss: 0.20146146416664124\n",
      "Step: 19200  \tTraining accuracy: 0.893171489238739\n",
      "Step: 19200  \tValid loss: 0.23489998281002045\n",
      "Step: 19300  \tTraining loss: 0.20126137137413025\n",
      "Step: 19300  \tTraining accuracy: 0.8932291865348816\n",
      "Step: 19300  \tValid loss: 0.23454642295837402\n",
      "Step: 19400  \tTraining loss: 0.20106323063373566\n",
      "Step: 19400  \tTraining accuracy: 0.8932834267616272\n",
      "Step: 19400  \tValid loss: 0.23431850969791412\n",
      "Step: 19500  \tTraining loss: 0.20086920261383057\n",
      "Step: 19500  \tTraining accuracy: 0.8933364152908325\n",
      "Step: 19500  \tValid loss: 0.23387011885643005\n",
      "Step: 19600  \tTraining loss: 0.20067699253559113\n",
      "Step: 19600  \tTraining accuracy: 0.8933874368667603\n",
      "Step: 19600  \tValid loss: 0.2335001528263092\n",
      "Step: 19700  \tTraining loss: 0.20048756897449493\n",
      "Step: 19700  \tTraining accuracy: 0.893437922000885\n",
      "Step: 19700  \tValid loss: 0.2332226186990738\n",
      "Step: 19800  \tTraining loss: 0.20030352473258972\n",
      "Step: 19800  \tTraining accuracy: 0.8934913873672485\n",
      "Step: 19800  \tValid loss: 0.2327653169631958\n",
      "Step: 19900  \tTraining loss: 0.2001194953918457\n",
      "Step: 19900  \tTraining accuracy: 0.893549919128418\n",
      "Step: 19900  \tValid loss: 0.2324838936328888\n",
      "Step: 20000  \tTraining loss: 0.19993950426578522\n",
      "Step: 20000  \tTraining accuracy: 0.8936071395874023\n",
      "Step: 20000  \tValid loss: 0.23213745653629303\n",
      "Step: 20100  \tTraining loss: 0.1997612863779068\n",
      "Step: 20100  \tTraining accuracy: 0.893660306930542\n",
      "Step: 20100  \tValid loss: 0.23184503614902496\n",
      "Step: 20200  \tTraining loss: 0.19958548247814178\n",
      "Step: 20200  \tTraining accuracy: 0.8937102556228638\n",
      "Step: 20200  \tValid loss: 0.2314557284116745\n",
      "Step: 20300  \tTraining loss: 0.1994113028049469\n",
      "Step: 20300  \tTraining accuracy: 0.8937597274780273\n",
      "Step: 20300  \tValid loss: 0.23112963140010834\n",
      "Step: 20400  \tTraining loss: 0.19923895597457886\n",
      "Step: 20400  \tTraining accuracy: 0.8938046097755432\n",
      "Step: 20400  \tValid loss: 0.23080793023109436\n",
      "Step: 20500  \tTraining loss: 0.1990710347890854\n",
      "Step: 20500  \tTraining accuracy: 0.8938484191894531\n",
      "Step: 20500  \tValid loss: 0.2303534895181656\n",
      "Step: 20600  \tTraining loss: 0.1989022046327591\n",
      "Step: 20600  \tTraining accuracy: 0.8938931226730347\n",
      "Step: 20600  \tValid loss: 0.23013025522232056\n",
      "Step: 20700  \tTraining loss: 0.1987365484237671\n",
      "Step: 20700  \tTraining accuracy: 0.893932044506073\n",
      "Step: 20700  \tValid loss: 0.22982817888259888\n",
      "Step: 20800  \tTraining loss: 0.19857439398765564\n",
      "Step: 20800  \tTraining accuracy: 0.8939685821533203\n",
      "Step: 20800  \tValid loss: 0.22942736744880676\n",
      "Step: 20900  \tTraining loss: 0.19841368496418\n",
      "Step: 20900  \tTraining accuracy: 0.8940034508705139\n",
      "Step: 20900  \tValid loss: 0.22911621630191803\n",
      "Step: 21000  \tTraining loss: 0.19825367629528046\n",
      "Step: 21000  \tTraining accuracy: 0.8940380215644836\n",
      "Step: 21000  \tValid loss: 0.22880807518959045\n",
      "Step: 21100  \tTraining loss: 0.19809648394584656\n",
      "Step: 21100  \tTraining accuracy: 0.8940722346305847\n",
      "Step: 21100  \tValid loss: 0.228499174118042\n",
      "Step: 21200  \tTraining loss: 0.19794365763664246\n",
      "Step: 21200  \tTraining accuracy: 0.8941068053245544\n",
      "Step: 21200  \tValid loss: 0.22812509536743164\n",
      "Step: 21300  \tTraining loss: 0.19779060781002045\n",
      "Step: 21300  \tTraining accuracy: 0.8941410183906555\n",
      "Step: 21300  \tValid loss: 0.22786591947078705\n",
      "Step: 21400  \tTraining loss: 0.1976427584886551\n",
      "Step: 21400  \tTraining accuracy: 0.8941774964332581\n",
      "Step: 21400  \tValid loss: 0.22749246656894684\n",
      "Step: 21500  \tTraining loss: 0.19749405980110168\n",
      "Step: 21500  \tTraining accuracy: 0.8942136764526367\n",
      "Step: 21500  \tValid loss: 0.2273406982421875\n",
      "Step: 21600  \tTraining loss: 0.1973491907119751\n",
      "Step: 21600  \tTraining accuracy: 0.8942475318908691\n",
      "Step: 21600  \tValid loss: 0.22695709764957428\n",
      "Step: 21700  \tTraining loss: 0.1972055733203888\n",
      "Step: 21700  \tTraining accuracy: 0.8942810893058777\n",
      "Step: 21700  \tValid loss: 0.22669385373592377\n",
      "Step: 21800  \tTraining loss: 0.19706369936466217\n",
      "Step: 21800  \tTraining accuracy: 0.8943144083023071\n",
      "Step: 21800  \tValid loss: 0.22636550664901733\n",
      "Step: 21900  \tTraining loss: 0.1969234198331833\n",
      "Step: 21900  \tTraining accuracy: 0.8943473696708679\n",
      "Step: 21900  \tValid loss: 0.2260473370552063\n",
      "Step: 22000  \tTraining loss: 0.1967855989933014\n",
      "Step: 22000  \tTraining accuracy: 0.8943806290626526\n",
      "Step: 22000  \tValid loss: 0.2258368283510208\n",
      "Step: 22100  \tTraining loss: 0.19665181636810303\n",
      "Step: 22100  \tTraining accuracy: 0.8944136500358582\n",
      "Step: 22100  \tValid loss: 0.2255880981683731\n",
      "Step: 22200  \tTraining loss: 0.19651950895786285\n",
      "Step: 22200  \tTraining accuracy: 0.8944463133811951\n",
      "Step: 22200  \tValid loss: 0.22538378834724426\n",
      "Step: 22300  \tTraining loss: 0.1963881403207779\n",
      "Step: 22300  \tTraining accuracy: 0.8944787383079529\n",
      "Step: 22300  \tValid loss: 0.22520877420902252\n",
      "Step: 22400  \tTraining loss: 0.1962600201368332\n",
      "Step: 22400  \tTraining accuracy: 0.8945108652114868\n",
      "Step: 22400  \tValid loss: 0.22487975656986237\n",
      "Step: 22500  \tTraining loss: 0.19613240659236908\n",
      "Step: 22500  \tTraining accuracy: 0.8945451378822327\n",
      "Step: 22500  \tValid loss: 0.22472751140594482\n",
      "Step: 22600  \tTraining loss: 0.19600877165794373\n",
      "Step: 22600  \tTraining accuracy: 0.8945772647857666\n",
      "Step: 22600  \tValid loss: 0.22441956400871277\n",
      "Step: 22700  \tTraining loss: 0.19588495790958405\n",
      "Step: 22700  \tTraining accuracy: 0.8946090936660767\n",
      "Step: 22700  \tValid loss: 0.22420446574687958\n",
      "Step: 22800  \tTraining loss: 0.1957622766494751\n",
      "Step: 22800  \tTraining accuracy: 0.8946437239646912\n",
      "Step: 22800  \tValid loss: 0.22401709854602814\n",
      "Step: 22900  \tTraining loss: 0.19564208388328552\n",
      "Step: 22900  \tTraining accuracy: 0.894679844379425\n",
      "Step: 22900  \tValid loss: 0.22380571067333221\n",
      "Step: 23000  \tTraining loss: 0.19552317261695862\n",
      "Step: 23000  \tTraining accuracy: 0.8947174549102783\n",
      "Step: 23000  \tValid loss: 0.22358685731887817\n",
      "Step: 23100  \tTraining loss: 0.1954050362110138\n",
      "Step: 23100  \tTraining accuracy: 0.8947571516036987\n",
      "Step: 23100  \tValid loss: 0.22348512709140778\n",
      "Step: 23200  \tTraining loss: 0.19528909027576447\n",
      "Step: 23200  \tTraining accuracy: 0.8948000073432922\n",
      "Step: 23200  \tValid loss: 0.22317489981651306\n",
      "Step: 23300  \tTraining loss: 0.19517460465431213\n",
      "Step: 23300  \tTraining accuracy: 0.8948425650596619\n",
      "Step: 23300  \tValid loss: 0.2229350507259369\n",
      "Step: 23400  \tTraining loss: 0.1950598508119583\n",
      "Step: 23400  \tTraining accuracy: 0.8948858976364136\n",
      "Step: 23400  \tValid loss: 0.22277559340000153\n",
      "Step: 23500  \tTraining loss: 0.19494779407978058\n",
      "Step: 23500  \tTraining accuracy: 0.8949283361434937\n",
      "Step: 23500  \tValid loss: 0.22258417308330536\n",
      "Step: 23600  \tTraining loss: 0.19483628869056702\n",
      "Step: 23600  \tTraining accuracy: 0.8949703574180603\n",
      "Step: 23600  \tValid loss: 0.22242847084999084\n",
      "Step: 23700  \tTraining loss: 0.1947265863418579\n",
      "Step: 23700  \tTraining accuracy: 0.8950120210647583\n",
      "Step: 23700  \tValid loss: 0.2221764326095581\n",
      "Step: 23800  \tTraining loss: 0.19461709260940552\n",
      "Step: 23800  \tTraining accuracy: 0.8950533866882324\n",
      "Step: 23800  \tValid loss: 0.22204893827438354\n",
      "Step: 23900  \tTraining loss: 0.19450995326042175\n",
      "Step: 23900  \tTraining accuracy: 0.8950943350791931\n",
      "Step: 23900  \tValid loss: 0.2218492031097412\n",
      "Step: 24000  \tTraining loss: 0.19440245628356934\n",
      "Step: 24000  \tTraining accuracy: 0.8951338529586792\n",
      "Step: 24000  \tValid loss: 0.22166936099529266\n",
      "Step: 24100  \tTraining loss: 0.19429640471935272\n",
      "Step: 24100  \tTraining accuracy: 0.8951730132102966\n",
      "Step: 24100  \tValid loss: 0.22163310647010803\n",
      "Step: 24200  \tTraining loss: 0.19419188797473907\n",
      "Step: 24200  \tTraining accuracy: 0.8952118158340454\n",
      "Step: 24200  \tValid loss: 0.2213899791240692\n",
      "Step: 24300  \tTraining loss: 0.19408808648586273\n",
      "Step: 24300  \tTraining accuracy: 0.8952503204345703\n",
      "Step: 24300  \tValid loss: 0.22125695645809174\n",
      "Step: 24400  \tTraining loss: 0.19398610293865204\n",
      "Step: 24400  \tTraining accuracy: 0.8952879905700684\n",
      "Step: 24400  \tValid loss: 0.22107870876789093\n",
      "Step: 24500  \tTraining loss: 0.19388291239738464\n",
      "Step: 24500  \tTraining accuracy: 0.8953253030776978\n",
      "Step: 24500  \tValid loss: 0.22098489105701447\n",
      "Step: 24600  \tTraining loss: 0.19378264248371124\n",
      "Step: 24600  \tTraining accuracy: 0.8953617811203003\n",
      "Step: 24600  \tValid loss: 0.22077316045761108\n",
      "Step: 24700  \tTraining loss: 0.19368219375610352\n",
      "Step: 24700  \tTraining accuracy: 0.8953979015350342\n",
      "Step: 24700  \tValid loss: 0.22066333889961243\n",
      "Step: 24800  \tTraining loss: 0.19358275830745697\n",
      "Step: 24800  \tTraining accuracy: 0.895433783531189\n",
      "Step: 24800  \tValid loss: 0.22052165865898132\n",
      "Step: 24900  \tTraining loss: 0.193482905626297\n",
      "Step: 24900  \tTraining accuracy: 0.8954693675041199\n",
      "Step: 24900  \tValid loss: 0.220378577709198\n",
      "Step: 25000  \tTraining loss: 0.19338569045066833\n",
      "Step: 25000  \tTraining accuracy: 0.8955057859420776\n",
      "Step: 25000  \tValid loss: 0.2202538698911667\n",
      "Step: 25100  \tTraining loss: 0.1932879388332367\n",
      "Step: 25100  \tTraining accuracy: 0.8955424427986145\n",
      "Step: 25100  \tValid loss: 0.22010210156440735\n",
      "Step: 25200  \tTraining loss: 0.19319121539592743\n",
      "Step: 25200  \tTraining accuracy: 0.8955788016319275\n",
      "Step: 25200  \tValid loss: 0.21997833251953125\n",
      "Step: 25300  \tTraining loss: 0.19309549033641815\n",
      "Step: 25300  \tTraining accuracy: 0.8956137895584106\n",
      "Step: 25300  \tValid loss: 0.21982905268669128\n",
      "Step: 25400  \tTraining loss: 0.19299982488155365\n",
      "Step: 25400  \tTraining accuracy: 0.8956479430198669\n",
      "Step: 25400  \tValid loss: 0.21973425149917603\n",
      "Step: 25500  \tTraining loss: 0.19290581345558167\n",
      "Step: 25500  \tTraining accuracy: 0.8956834673881531\n",
      "Step: 25500  \tValid loss: 0.21957582235336304\n",
      "Step: 25600  \tTraining loss: 0.192811518907547\n",
      "Step: 25600  \tTraining accuracy: 0.8957187533378601\n",
      "Step: 25600  \tValid loss: 0.21947796642780304\n",
      "Step: 25700  \tTraining loss: 0.19271832704544067\n",
      "Step: 25700  \tTraining accuracy: 0.8957553505897522\n",
      "Step: 25700  \tValid loss: 0.21935266256332397\n",
      "Step: 25800  \tTraining loss: 0.19262497127056122\n",
      "Step: 25800  \tTraining accuracy: 0.8957916498184204\n",
      "Step: 25800  \tValid loss: 0.21924230456352234\n",
      "Step: 25900  \tTraining loss: 0.19253318011760712\n",
      "Step: 25900  \tTraining accuracy: 0.8958276510238647\n",
      "Step: 25900  \tValid loss: 0.21911486983299255\n",
      "Step: 26000  \tTraining loss: 0.19244316220283508\n",
      "Step: 26000  \tTraining accuracy: 0.89586341381073\n",
      "Step: 26000  \tValid loss: 0.21899756789207458\n",
      "Step: 26100  \tTraining loss: 0.1923510730266571\n",
      "Step: 26100  \tTraining accuracy: 0.8958988785743713\n",
      "Step: 26100  \tValid loss: 0.2189389318227768\n",
      "Step: 26200  \tTraining loss: 0.1922619491815567\n",
      "Step: 26200  \tTraining accuracy: 0.8959356546401978\n",
      "Step: 26200  \tValid loss: 0.21877850592136383\n",
      "Step: 26300  \tTraining loss: 0.1921718418598175\n",
      "Step: 26300  \tTraining accuracy: 0.8959737420082092\n",
      "Step: 26300  \tValid loss: 0.21872228384017944\n",
      "Step: 26400  \tTraining loss: 0.1920832246541977\n",
      "Step: 26400  \tTraining accuracy: 0.896012544631958\n",
      "Step: 26400  \tValid loss: 0.21855410933494568\n",
      "Step: 26500  \tTraining loss: 0.1919945627450943\n",
      "Step: 26500  \tTraining accuracy: 0.8960521221160889\n",
      "Step: 26500  \tValid loss: 0.2184603214263916\n",
      "Step: 26600  \tTraining loss: 0.1919068992137909\n",
      "Step: 26600  \tTraining accuracy: 0.8960914611816406\n",
      "Step: 26600  \tValid loss: 0.21831448376178741\n",
      "Step: 26700  \tTraining loss: 0.19181858003139496\n",
      "Step: 26700  \tTraining accuracy: 0.8961314558982849\n",
      "Step: 26700  \tValid loss: 0.21827340126037598\n",
      "Step: 26800  \tTraining loss: 0.19173245131969452\n",
      "Step: 26800  \tTraining accuracy: 0.8961727619171143\n",
      "Step: 26800  \tValid loss: 0.2181583195924759\n",
      "Step: 26900  \tTraining loss: 0.1916460394859314\n",
      "Step: 26900  \tTraining accuracy: 0.8962121605873108\n",
      "Step: 26900  \tValid loss: 0.21802853047847748\n",
      "Step: 27000  \tTraining loss: 0.19156016409397125\n",
      "Step: 27000  \tTraining accuracy: 0.8962502479553223\n",
      "Step: 27000  \tValid loss: 0.21796593070030212\n",
      "Step: 27100  \tTraining loss: 0.19147488474845886\n",
      "Step: 27100  \tTraining accuracy: 0.8962880969047546\n",
      "Step: 27100  \tValid loss: 0.21782684326171875\n",
      "Step: 27200  \tTraining loss: 0.19139008224010468\n",
      "Step: 27200  \tTraining accuracy: 0.8963236212730408\n",
      "Step: 27200  \tValid loss: 0.21773794293403625\n",
      "Step: 27300  \tTraining loss: 0.1913067102432251\n",
      "Step: 27300  \tTraining accuracy: 0.8963583707809448\n",
      "Step: 27300  \tValid loss: 0.21762807667255402\n",
      "Step: 27400  \tTraining loss: 0.191221684217453\n",
      "Step: 27400  \tTraining accuracy: 0.8963949084281921\n",
      "Step: 27400  \tValid loss: 0.21754461526870728\n",
      "Step: 27500  \tTraining loss: 0.19113782048225403\n",
      "Step: 27500  \tTraining accuracy: 0.896432638168335\n",
      "Step: 27500  \tValid loss: 0.21748271584510803\n",
      "Step: 27600  \tTraining loss: 0.19105467200279236\n",
      "Step: 27600  \tTraining accuracy: 0.8964701294898987\n",
      "Step: 27600  \tValid loss: 0.21734362840652466\n",
      "Step: 27700  \tTraining loss: 0.1909707635641098\n",
      "Step: 27700  \tTraining accuracy: 0.8965068459510803\n",
      "Step: 27700  \tValid loss: 0.21732787787914276\n",
      "Step: 27800  \tTraining loss: 0.19088943302631378\n",
      "Step: 27800  \tTraining accuracy: 0.8965432643890381\n",
      "Step: 27800  \tValid loss: 0.21717366576194763\n",
      "Step: 27900  \tTraining loss: 0.1908063441514969\n",
      "Step: 27900  \tTraining accuracy: 0.8965794444084167\n",
      "Step: 27900  \tValid loss: 0.21711784601211548\n",
      "Step: 28000  \tTraining loss: 0.19072407484054565\n",
      "Step: 28000  \tTraining accuracy: 0.8966153860092163\n",
      "Step: 28000  \tValid loss: 0.21700286865234375\n",
      "Step: 28100  \tTraining loss: 0.19064220786094666\n",
      "Step: 28100  \tTraining accuracy: 0.896651029586792\n",
      "Step: 28100  \tValid loss: 0.21692925691604614\n",
      "Step: 28200  \tTraining loss: 0.19056035578250885\n",
      "Step: 28200  \tTraining accuracy: 0.8966864943504333\n",
      "Step: 28200  \tValid loss: 0.21684008836746216\n",
      "Step: 28300  \tTraining loss: 0.1904795914888382\n",
      "Step: 28300  \tTraining accuracy: 0.8967216610908508\n",
      "Step: 28300  \tValid loss: 0.21674653887748718\n",
      "Step: 28400  \tTraining loss: 0.19039830565452576\n",
      "Step: 28400  \tTraining accuracy: 0.8967565298080444\n",
      "Step: 28400  \tValid loss: 0.21665383875370026\n",
      "Step: 28500  \tTraining loss: 0.19031724333763123\n",
      "Step: 28500  \tTraining accuracy: 0.8967912197113037\n",
      "Step: 28500  \tValid loss: 0.2166167050600052\n",
      "Step: 28600  \tTraining loss: 0.19023676216602325\n",
      "Step: 28600  \tTraining accuracy: 0.8968237042427063\n",
      "Step: 28600  \tValid loss: 0.21649135649204254\n",
      "Step: 28700  \tTraining loss: 0.1901562362909317\n",
      "Step: 28700  \tTraining accuracy: 0.8968564867973328\n",
      "Step: 28700  \tValid loss: 0.21636691689491272\n",
      "Step: 28800  \tTraining loss: 0.1900751292705536\n",
      "Step: 28800  \tTraining accuracy: 0.8968923687934875\n",
      "Step: 28800  \tValid loss: 0.21634487807750702\n",
      "Step: 28900  \tTraining loss: 0.189994677901268\n",
      "Step: 28900  \tTraining accuracy: 0.8969270586967468\n",
      "Step: 28900  \tValid loss: 0.21622183918952942\n",
      "Step: 29000  \tTraining loss: 0.1899135410785675\n",
      "Step: 29000  \tTraining accuracy: 0.8969614505767822\n",
      "Step: 29000  \tValid loss: 0.21615241467952728\n",
      "Step: 29100  \tTraining loss: 0.18983320891857147\n",
      "Step: 29100  \tTraining accuracy: 0.8969956636428833\n",
      "Step: 29100  \tValid loss: 0.21603180468082428\n",
      "Step: 29200  \tTraining loss: 0.1897507607936859\n",
      "Step: 29200  \tTraining accuracy: 0.8970305919647217\n",
      "Step: 29200  \tValid loss: 0.2159654200077057\n",
      "Step: 29300  \tTraining loss: 0.18966814875602722\n",
      "Step: 29300  \tTraining accuracy: 0.8970652222633362\n",
      "Step: 29300  \tValid loss: 0.21584205329418182\n",
      "Step: 29400  \tTraining loss: 0.18958143889904022\n",
      "Step: 29400  \tTraining accuracy: 0.8970996737480164\n",
      "Step: 29400  \tValid loss: 0.2157025933265686\n",
      "Step: 29500  \tTraining loss: 0.18949277698993683\n",
      "Step: 29500  \tTraining accuracy: 0.8971338868141174\n",
      "Step: 29500  \tValid loss: 0.21558380126953125\n",
      "Step: 29600  \tTraining loss: 0.18941152095794678\n",
      "Step: 29600  \tTraining accuracy: 0.8971678614616394\n",
      "Step: 29600  \tValid loss: 0.21553939580917358\n",
      "Step: 29700  \tTraining loss: 0.18933162093162537\n",
      "Step: 29700  \tTraining accuracy: 0.8972011208534241\n",
      "Step: 29700  \tValid loss: 0.2154155820608139\n",
      "Step: 29800  \tTraining loss: 0.1892508566379547\n",
      "Step: 29800  \tTraining accuracy: 0.8972342014312744\n",
      "Step: 29800  \tValid loss: 0.21538254618644714\n",
      "Step: 29900  \tTraining loss: 0.18917116522789001\n",
      "Step: 29900  \tTraining accuracy: 0.897265613079071\n",
      "Step: 29900  \tValid loss: 0.21529100835323334\n",
      "Step: 30000  \tTraining loss: 0.18909147381782532\n",
      "Step: 30000  \tTraining accuracy: 0.8972986936569214\n",
      "Step: 30000  \tValid loss: 0.21520240604877472\n",
      "Step: 30100  \tTraining loss: 0.1890113204717636\n",
      "Step: 30100  \tTraining accuracy: 0.8973320126533508\n",
      "Step: 30100  \tValid loss: 0.2151356190443039\n",
      "Step: 30200  \tTraining loss: 0.18893083930015564\n",
      "Step: 30200  \tTraining accuracy: 0.897365152835846\n",
      "Step: 30200  \tValid loss: 0.21506333351135254\n",
      "Step: 30300  \tTraining loss: 0.1888495236635208\n",
      "Step: 30300  \tTraining accuracy: 0.8973979949951172\n",
      "Step: 30300  \tValid loss: 0.21496352553367615\n",
      "Step: 30400  \tTraining loss: 0.18876875936985016\n",
      "Step: 30400  \tTraining accuracy: 0.8974306583404541\n",
      "Step: 30400  \tValid loss: 0.21485234797000885\n",
      "Step: 30500  \tTraining loss: 0.18868644535541534\n",
      "Step: 30500  \tTraining accuracy: 0.8974612951278687\n",
      "Step: 30500  \tValid loss: 0.2147885113954544\n",
      "Step: 30600  \tTraining loss: 0.18860436975955963\n",
      "Step: 30600  \tTraining accuracy: 0.8974917531013489\n",
      "Step: 30600  \tValid loss: 0.214725062251091\n",
      "Step: 30700  \tTraining loss: 0.18852147459983826\n",
      "Step: 30700  \tTraining accuracy: 0.89752197265625\n",
      "Step: 30700  \tValid loss: 0.21466857194900513\n",
      "Step: 30800  \tTraining loss: 0.18843922019004822\n",
      "Step: 30800  \tTraining accuracy: 0.8975515365600586\n",
      "Step: 30800  \tValid loss: 0.214572474360466\n",
      "Step: 30900  \tTraining loss: 0.1883581578731537\n",
      "Step: 30900  \tTraining accuracy: 0.8975809812545776\n",
      "Step: 30900  \tValid loss: 0.21440747380256653\n",
      "Step: 31000  \tTraining loss: 0.188273623585701\n",
      "Step: 31000  \tTraining accuracy: 0.8976101875305176\n",
      "Step: 31000  \tValid loss: 0.2144036591053009\n",
      "Step: 31100  \tTraining loss: 0.18819116055965424\n",
      "Step: 31100  \tTraining accuracy: 0.8976391553878784\n",
      "Step: 31100  \tValid loss: 0.21426157653331757\n",
      "Step: 31200  \tTraining loss: 0.18810786306858063\n",
      "Step: 31200  \tTraining accuracy: 0.8976680040359497\n",
      "Step: 31200  \tValid loss: 0.21417468786239624\n",
      "Step: 31300  \tTraining loss: 0.18802335858345032\n",
      "Step: 31300  \tTraining accuracy: 0.8976975679397583\n",
      "Step: 31300  \tValid loss: 0.21409879624843597\n",
      "Step: 31400  \tTraining loss: 0.18793880939483643\n",
      "Step: 31400  \tTraining accuracy: 0.8977268934249878\n",
      "Step: 31400  \tValid loss: 0.21400973200798035\n",
      "Step: 31500  \tTraining loss: 0.1878536194562912\n",
      "Step: 31500  \tTraining accuracy: 0.897756040096283\n",
      "Step: 31500  \tValid loss: 0.2139035016298294\n",
      "Step: 31600  \tTraining loss: 0.1877683848142624\n",
      "Step: 31600  \tTraining accuracy: 0.8977845907211304\n",
      "Step: 31600  \tValid loss: 0.2137930691242218\n",
      "Step: 31700  \tTraining loss: 0.18768242001533508\n",
      "Step: 31700  \tTraining accuracy: 0.8978151082992554\n",
      "Step: 31700  \tValid loss: 0.21370813250541687\n",
      "Step: 31800  \tTraining loss: 0.18759551644325256\n",
      "Step: 31800  \tTraining accuracy: 0.897845447063446\n",
      "Step: 31800  \tValid loss: 0.21360592544078827\n",
      "Step: 31900  \tTraining loss: 0.18750803172588348\n",
      "Step: 31900  \tTraining accuracy: 0.8978756070137024\n",
      "Step: 31900  \tValid loss: 0.21351207792758942\n",
      "Step: 32000  \tTraining loss: 0.18741977214813232\n",
      "Step: 32000  \tTraining accuracy: 0.8979060053825378\n",
      "Step: 32000  \tValid loss: 0.21343021094799042\n",
      "Step: 32100  \tTraining loss: 0.18733179569244385\n",
      "Step: 32100  \tTraining accuracy: 0.8979349136352539\n",
      "Step: 32100  \tValid loss: 0.2132948338985443\n",
      "Step: 32200  \tTraining loss: 0.18724246323108673\n",
      "Step: 32200  \tTraining accuracy: 0.8979636430740356\n",
      "Step: 32200  \tValid loss: 0.2132079005241394\n",
      "Step: 32300  \tTraining loss: 0.18715155124664307\n",
      "Step: 32300  \tTraining accuracy: 0.8979935050010681\n",
      "Step: 32300  \tValid loss: 0.21314309537410736\n",
      "Step: 32400  \tTraining loss: 0.18706205487251282\n",
      "Step: 32400  \tTraining accuracy: 0.8980227112770081\n",
      "Step: 32400  \tValid loss: 0.21297693252563477\n",
      "Step: 32500  \tTraining loss: 0.18697039783000946\n",
      "Step: 32500  \tTraining accuracy: 0.8980517387390137\n",
      "Step: 32500  \tValid loss: 0.2128971815109253\n",
      "Step: 32600  \tTraining loss: 0.1868785321712494\n",
      "Step: 32600  \tTraining accuracy: 0.8980789184570312\n",
      "Step: 32600  \tValid loss: 0.2127799540758133\n",
      "Step: 32700  \tTraining loss: 0.18678505718708038\n",
      "Step: 32700  \tTraining accuracy: 0.898107647895813\n",
      "Step: 32700  \tValid loss: 0.2127167135477066\n",
      "Step: 32800  \tTraining loss: 0.1866919845342636\n",
      "Step: 32800  \tTraining accuracy: 0.8981344699859619\n",
      "Step: 32800  \tValid loss: 0.2125817835330963\n",
      "Step: 32900  \tTraining loss: 0.18659858405590057\n",
      "Step: 32900  \tTraining accuracy: 0.8981611132621765\n",
      "Step: 32900  \tValid loss: 0.2124655544757843\n",
      "Step: 33000  \tTraining loss: 0.18650344014167786\n",
      "Step: 33000  \tTraining accuracy: 0.8981893062591553\n",
      "Step: 33000  \tValid loss: 0.2123807668685913\n",
      "Step: 33100  \tTraining loss: 0.18640922009944916\n",
      "Step: 33100  \tTraining accuracy: 0.898215651512146\n",
      "Step: 33100  \tValid loss: 0.21225307881832123\n",
      "Step: 33200  \tTraining loss: 0.1863153725862503\n",
      "Step: 33200  \tTraining accuracy: 0.8982402086257935\n",
      "Step: 33200  \tValid loss: 0.21207492053508759\n",
      "Step: 33300  \tTraining loss: 0.18621955811977386\n",
      "Step: 33300  \tTraining accuracy: 0.8982633352279663\n",
      "Step: 33300  \tValid loss: 0.2120041698217392\n",
      "Step: 33400  \tTraining loss: 0.18612416088581085\n",
      "Step: 33400  \tTraining accuracy: 0.8982879519462585\n",
      "Step: 33400  \tValid loss: 0.2118874043226242\n",
      "Step: 33500  \tTraining loss: 0.1860291212797165\n",
      "Step: 33500  \tTraining accuracy: 0.8983107805252075\n",
      "Step: 33500  \tValid loss: 0.21175995469093323\n",
      "Step: 33600  \tTraining loss: 0.18593476712703705\n",
      "Step: 33600  \tTraining accuracy: 0.8983335494995117\n",
      "Step: 33600  \tValid loss: 0.21159610152244568\n",
      "Step: 33700  \tTraining loss: 0.1858409196138382\n",
      "Step: 33700  \tTraining accuracy: 0.8983548879623413\n",
      "Step: 33700  \tValid loss: 0.21142321825027466\n",
      "Step: 33800  \tTraining loss: 0.18574412167072296\n",
      "Step: 33800  \tTraining accuracy: 0.8983772993087769\n",
      "Step: 33800  \tValid loss: 0.21138949692249298\n",
      "Step: 33900  \tTraining loss: 0.1856486052274704\n",
      "Step: 33900  \tTraining accuracy: 0.8984020948410034\n",
      "Step: 33900  \tValid loss: 0.21128204464912415\n",
      "Step: 34000  \tTraining loss: 0.18555429577827454\n",
      "Step: 34000  \tTraining accuracy: 0.8984258770942688\n",
      "Step: 34000  \tValid loss: 0.21112942695617676\n",
      "Step: 34100  \tTraining loss: 0.18546031415462494\n",
      "Step: 34100  \tTraining accuracy: 0.8984499573707581\n",
      "Step: 34100  \tValid loss: 0.21095675230026245\n",
      "Step: 34200  \tTraining loss: 0.18536430597305298\n",
      "Step: 34200  \tTraining accuracy: 0.8984726667404175\n",
      "Step: 34200  \tValid loss: 0.21090884506702423\n",
      "Step: 34300  \tTraining loss: 0.1852683573961258\n",
      "Step: 34300  \tTraining accuracy: 0.8984972834587097\n",
      "Step: 34300  \tValid loss: 0.21089273691177368\n",
      "Step: 34400  \tTraining loss: 0.18517659604549408\n",
      "Step: 34400  \tTraining accuracy: 0.8985196948051453\n",
      "Step: 34400  \tValid loss: 0.2105795294046402\n",
      "Step: 34500  \tTraining loss: 0.1850820928812027\n",
      "Step: 34500  \tTraining accuracy: 0.8985416293144226\n",
      "Step: 34500  \tValid loss: 0.21045038104057312\n",
      "Step: 34600  \tTraining loss: 0.18498636782169342\n",
      "Step: 34600  \tTraining accuracy: 0.8985649943351746\n",
      "Step: 34600  \tValid loss: 0.21036380529403687\n",
      "Step: 34700  \tTraining loss: 0.18489070236682892\n",
      "Step: 34700  \tTraining accuracy: 0.898588240146637\n",
      "Step: 34700  \tValid loss: 0.21027350425720215\n",
      "Step: 34800  \tTraining loss: 0.18479762971401215\n",
      "Step: 34800  \tTraining accuracy: 0.8986121416091919\n",
      "Step: 34800  \tValid loss: 0.21008111536502838\n",
      "Step: 34900  \tTraining loss: 0.18470399081707\n",
      "Step: 34900  \tTraining accuracy: 0.8986351490020752\n",
      "Step: 34900  \tValid loss: 0.20992863178253174\n",
      "Step: 35000  \tTraining loss: 0.1846095323562622\n",
      "Step: 35000  \tTraining accuracy: 0.89865642786026\n",
      "Step: 35000  \tValid loss: 0.20977996289730072\n",
      "Step: 35100  \tTraining loss: 0.1845136433839798\n",
      "Step: 35100  \tTraining accuracy: 0.8986791372299194\n",
      "Step: 35100  \tValid loss: 0.20964619517326355\n",
      "Step: 35200  \tTraining loss: 0.18441779911518097\n",
      "Step: 35200  \tTraining accuracy: 0.8987001776695251\n",
      "Step: 35200  \tValid loss: 0.20956622064113617\n",
      "Step: 35300  \tTraining loss: 0.1843239665031433\n",
      "Step: 35300  \tTraining accuracy: 0.8987218141555786\n",
      "Step: 35300  \tValid loss: 0.20941248536109924\n",
      "Step: 35400  \tTraining loss: 0.1842258870601654\n",
      "Step: 35400  \tTraining accuracy: 0.8987430334091187\n",
      "Step: 35400  \tValid loss: 0.20935556292533875\n",
      "Step: 35500  \tTraining loss: 0.1841297447681427\n",
      "Step: 35500  \tTraining accuracy: 0.8987652063369751\n",
      "Step: 35500  \tValid loss: 0.2092358022928238\n",
      "Step: 35600  \tTraining loss: 0.18403343856334686\n",
      "Step: 35600  \tTraining accuracy: 0.8987857699394226\n",
      "Step: 35600  \tValid loss: 0.2091338187456131\n",
      "Step: 35700  \tTraining loss: 0.18393485248088837\n",
      "Step: 35700  \tTraining accuracy: 0.8988077640533447\n",
      "Step: 35700  \tValid loss: 0.20906592905521393\n",
      "Step: 35800  \tTraining loss: 0.18383920192718506\n",
      "Step: 35800  \tTraining accuracy: 0.8988296389579773\n",
      "Step: 35800  \tValid loss: 0.20885980129241943\n",
      "Step: 35900  \tTraining loss: 0.18374136090278625\n",
      "Step: 35900  \tTraining accuracy: 0.8988528847694397\n",
      "Step: 35900  \tValid loss: 0.20874890685081482\n",
      "Step: 36000  \tTraining loss: 0.18364328145980835\n",
      "Step: 36000  \tTraining accuracy: 0.8988737463951111\n",
      "Step: 36000  \tValid loss: 0.2086193710565567\n",
      "Step: 36100  \tTraining loss: 0.1835453361272812\n",
      "Step: 36100  \tTraining accuracy: 0.8988944292068481\n",
      "Step: 36100  \tValid loss: 0.2084624320268631\n",
      "Step: 36200  \tTraining loss: 0.18344423174858093\n",
      "Step: 36200  \tTraining accuracy: 0.8989150524139404\n",
      "Step: 36200  \tValid loss: 0.20841753482818604\n",
      "Step: 36300  \tTraining loss: 0.1833454966545105\n",
      "Step: 36300  \tTraining accuracy: 0.8989363312721252\n",
      "Step: 36300  \tValid loss: 0.20824630558490753\n",
      "Step: 36400  \tTraining loss: 0.18324525654315948\n",
      "Step: 36400  \tTraining accuracy: 0.8989585638046265\n",
      "Step: 36400  \tValid loss: 0.20810705423355103\n",
      "Step: 36500  \tTraining loss: 0.18314316868782043\n",
      "Step: 36500  \tTraining accuracy: 0.8989811539649963\n",
      "Step: 36500  \tValid loss: 0.20802660286426544\n",
      "Step: 36600  \tTraining loss: 0.18304210901260376\n",
      "Step: 36600  \tTraining accuracy: 0.8990035653114319\n",
      "Step: 36600  \tValid loss: 0.20786522328853607\n",
      "Step: 36700  \tTraining loss: 0.182942733168602\n",
      "Step: 36700  \tTraining accuracy: 0.8990254402160645\n",
      "Step: 36700  \tValid loss: 0.20766139030456543\n",
      "Step: 36800  \tTraining loss: 0.18283721804618835\n",
      "Step: 36800  \tTraining accuracy: 0.8990472555160522\n",
      "Step: 36800  \tValid loss: 0.20760391652584076\n",
      "Step: 36900  \tTraining loss: 0.18273332715034485\n",
      "Step: 36900  \tTraining accuracy: 0.8990678191184998\n",
      "Step: 36900  \tValid loss: 0.20746232569217682\n",
      "Step: 37000  \tTraining loss: 0.18262805044651031\n",
      "Step: 37000  \tTraining accuracy: 0.8990874886512756\n",
      "Step: 37000  \tValid loss: 0.20735305547714233\n",
      "Step: 37100  \tTraining loss: 0.18252114951610565\n",
      "Step: 37100  \tTraining accuracy: 0.8991070985794067\n",
      "Step: 37100  \tValid loss: 0.20729495584964752\n",
      "Step: 37200  \tTraining loss: 0.18241560459136963\n",
      "Step: 37200  \tTraining accuracy: 0.8991265892982483\n",
      "Step: 37200  \tValid loss: 0.20712575316429138\n",
      "Step: 37300  \tTraining loss: 0.18230678141117096\n",
      "Step: 37300  \tTraining accuracy: 0.8991433382034302\n",
      "Step: 37300  \tValid loss: 0.20711420476436615\n",
      "Step: 37400  \tTraining loss: 0.1822015941143036\n",
      "Step: 37400  \tTraining accuracy: 0.8991600275039673\n",
      "Step: 37400  \tValid loss: 0.2068495750427246\n",
      "Step: 37500  \tTraining loss: 0.18209053575992584\n",
      "Step: 37500  \tTraining accuracy: 0.8991755247116089\n",
      "Step: 37500  \tValid loss: 0.20682744681835175\n",
      "Step: 37600  \tTraining loss: 0.18198338150978088\n",
      "Step: 37600  \tTraining accuracy: 0.8991906046867371\n",
      "Step: 37600  \tValid loss: 0.20659960806369781\n",
      "Step: 37700  \tTraining loss: 0.1818719059228897\n",
      "Step: 37700  \tTraining accuracy: 0.8992070555686951\n",
      "Step: 37700  \tValid loss: 0.20650692284107208\n",
      "Step: 37800  \tTraining loss: 0.18176130950450897\n",
      "Step: 37800  \tTraining accuracy: 0.8992233872413635\n",
      "Step: 37800  \tValid loss: 0.20634125173091888\n",
      "Step: 37900  \tTraining loss: 0.18164806067943573\n",
      "Step: 37900  \tTraining accuracy: 0.8992396593093872\n",
      "Step: 37900  \tValid loss: 0.2062721997499466\n",
      "Step: 38000  \tTraining loss: 0.18153506517410278\n",
      "Step: 38000  \tTraining accuracy: 0.8992558121681213\n",
      "Step: 38000  \tValid loss: 0.20614799857139587\n",
      "Step: 38100  \tTraining loss: 0.18142499029636383\n",
      "Step: 38100  \tTraining accuracy: 0.8992719054222107\n",
      "Step: 38100  \tValid loss: 0.20590245723724365\n",
      "Step: 38200  \tTraining loss: 0.1813066452741623\n",
      "Step: 38200  \tTraining accuracy: 0.8992879390716553\n",
      "Step: 38200  \tValid loss: 0.20587770640850067\n",
      "Step: 38300  \tTraining loss: 0.18119333684444427\n",
      "Step: 38300  \tTraining accuracy: 0.8993038535118103\n",
      "Step: 38300  \tValid loss: 0.2056836485862732\n",
      "Step: 38400  \tTraining loss: 0.18107356131076813\n",
      "Step: 38400  \tTraining accuracy: 0.8993197083473206\n",
      "Step: 38400  \tValid loss: 0.2056986689567566\n",
      "Step: 38500  \tTraining loss: 0.18095873296260834\n",
      "Step: 38500  \tTraining accuracy: 0.899334728717804\n",
      "Step: 38500  \tValid loss: 0.20546932518482208\n",
      "Step: 38600  \tTraining loss: 0.18084070086479187\n",
      "Step: 38600  \tTraining accuracy: 0.8993496894836426\n",
      "Step: 38600  \tValid loss: 0.20535458624362946\n",
      "Step: 38700  \tTraining loss: 0.1807229071855545\n",
      "Step: 38700  \tTraining accuracy: 0.899366021156311\n",
      "Step: 38700  \tValid loss: 0.20519888401031494\n",
      "Step: 38800  \tTraining loss: 0.18060418963432312\n",
      "Step: 38800  \tTraining accuracy: 0.8993808031082153\n",
      "Step: 38800  \tValid loss: 0.20506714284420013\n",
      "Step: 38900  \tTraining loss: 0.1804841011762619\n",
      "Step: 38900  \tTraining accuracy: 0.8993948698043823\n",
      "Step: 38900  \tValid loss: 0.20494665205478668\n",
      "Step: 39000  \tTraining loss: 0.18036438524723053\n",
      "Step: 39000  \tTraining accuracy: 0.8994081020355225\n",
      "Step: 39000  \tValid loss: 0.20480090379714966\n",
      "Step: 39100  \tTraining loss: 0.18024340271949768\n",
      "Step: 39100  \tTraining accuracy: 0.8994198441505432\n",
      "Step: 39100  \tValid loss: 0.20464779436588287\n",
      "Step: 39200  \tTraining loss: 0.18012259900569916\n",
      "Step: 39200  \tTraining accuracy: 0.8994329571723938\n",
      "Step: 39200  \tValid loss: 0.20450927317142487\n",
      "Step: 39300  \tTraining loss: 0.18000034987926483\n",
      "Step: 39300  \tTraining accuracy: 0.8994442224502563\n",
      "Step: 39300  \tValid loss: 0.20438805222511292\n",
      "Step: 39400  \tTraining loss: 0.17987877130508423\n",
      "Step: 39400  \tTraining accuracy: 0.8994554877281189\n",
      "Step: 39400  \tValid loss: 0.20424288511276245\n",
      "Step: 39500  \tTraining loss: 0.17975717782974243\n",
      "Step: 39500  \tTraining accuracy: 0.8994677066802979\n",
      "Step: 39500  \tValid loss: 0.20405890047550201\n",
      "Step: 39600  \tTraining loss: 0.17963311076164246\n",
      "Step: 39600  \tTraining accuracy: 0.8994780778884888\n",
      "Step: 39600  \tValid loss: 0.20395229756832123\n",
      "Step: 39700  \tTraining loss: 0.17951077222824097\n",
      "Step: 39700  \tTraining accuracy: 0.8994884490966797\n",
      "Step: 39700  \tValid loss: 0.20378336310386658\n",
      "Step: 39800  \tTraining loss: 0.179387629032135\n",
      "Step: 39800  \tTraining accuracy: 0.8994977474212646\n",
      "Step: 39800  \tValid loss: 0.20363092422485352\n",
      "Step: 39900  \tTraining loss: 0.1792641133069992\n",
      "Step: 39900  \tTraining accuracy: 0.8995069265365601\n",
      "Step: 39900  \tValid loss: 0.20348158478736877\n",
      "Step: 40000  \tTraining loss: 0.17913879454135895\n",
      "Step: 40000  \tTraining accuracy: 0.8995157480239868\n",
      "Step: 40000  \tValid loss: 0.20347069203853607\n",
      "Step: 40100  \tTraining loss: 0.17901842296123505\n",
      "Step: 40100  \tTraining accuracy: 0.8995245695114136\n",
      "Step: 40100  \tValid loss: 0.20316505432128906\n",
      "Step: 40200  \tTraining loss: 0.1788955181837082\n",
      "Step: 40200  \tTraining accuracy: 0.8995333313941956\n",
      "Step: 40200  \tValid loss: 0.20301967859268188\n",
      "Step: 40300  \tTraining loss: 0.1787758469581604\n",
      "Step: 40300  \tTraining accuracy: 0.8995433449745178\n",
      "Step: 40300  \tValid loss: 0.20279279351234436\n",
      "Step: 40400  \tTraining loss: 0.1786540150642395\n",
      "Step: 40400  \tTraining accuracy: 0.8995537161827087\n",
      "Step: 40400  \tValid loss: 0.20269721746444702\n",
      "Step: 40500  \tTraining loss: 0.1785300374031067\n",
      "Step: 40500  \tTraining accuracy: 0.899566113948822\n",
      "Step: 40500  \tValid loss: 0.2025541067123413\n",
      "Step: 40600  \tTraining loss: 0.17840930819511414\n",
      "Step: 40600  \tTraining accuracy: 0.8995797634124756\n",
      "Step: 40600  \tValid loss: 0.2023516297340393\n",
      "Step: 40700  \tTraining loss: 0.17828771471977234\n",
      "Step: 40700  \tTraining accuracy: 0.8995933532714844\n",
      "Step: 40700  \tValid loss: 0.20218798518180847\n",
      "Step: 40800  \tTraining loss: 0.17816680669784546\n",
      "Step: 40800  \tTraining accuracy: 0.8996075391769409\n",
      "Step: 40800  \tValid loss: 0.20202305912971497\n",
      "Step: 40900  \tTraining loss: 0.17804618179798126\n",
      "Step: 40900  \tTraining accuracy: 0.8996216654777527\n",
      "Step: 40900  \tValid loss: 0.20186756551265717\n",
      "Step: 41000  \tTraining loss: 0.17792478203773499\n",
      "Step: 41000  \tTraining accuracy: 0.8996350765228271\n",
      "Step: 41000  \tValid loss: 0.2017526924610138\n",
      "Step: 41100  \tTraining loss: 0.17780578136444092\n",
      "Step: 41100  \tTraining accuracy: 0.8996483683586121\n",
      "Step: 41100  \tValid loss: 0.20156025886535645\n",
      "Step: 41200  \tTraining loss: 0.17768578231334686\n",
      "Step: 41200  \tTraining accuracy: 0.8996613025665283\n",
      "Step: 41200  \tValid loss: 0.20140287280082703\n",
      "Step: 41300  \tTraining loss: 0.17757080495357513\n",
      "Step: 41300  \tTraining accuracy: 0.8996741771697998\n",
      "Step: 41300  \tValid loss: 0.20123088359832764\n",
      "Step: 41400  \tTraining loss: 0.17745241522789001\n",
      "Step: 41400  \tTraining accuracy: 0.8996869325637817\n",
      "Step: 41400  \tValid loss: 0.20114563405513763\n",
      "Step: 41500  \tTraining loss: 0.1773376166820526\n",
      "Step: 41500  \tTraining accuracy: 0.8996996879577637\n",
      "Step: 41500  \tValid loss: 0.20089185237884521\n",
      "Step: 41600  \tTraining loss: 0.1772220879793167\n",
      "Step: 41600  \tTraining accuracy: 0.899712324142456\n",
      "Step: 41600  \tValid loss: 0.20072592794895172\n",
      "Step: 41700  \tTraining loss: 0.17710493505001068\n",
      "Step: 41700  \tTraining accuracy: 0.8997232913970947\n",
      "Step: 41700  \tValid loss: 0.20073793828487396\n",
      "Step: 41800  \tTraining loss: 0.17699354887008667\n",
      "Step: 41800  \tTraining accuracy: 0.8997341990470886\n",
      "Step: 41800  \tValid loss: 0.20039483904838562\n",
      "Step: 41900  \tTraining loss: 0.17687979340553284\n",
      "Step: 41900  \tTraining accuracy: 0.8997450470924377\n",
      "Step: 41900  \tValid loss: 0.2002905011177063\n",
      "Step: 42000  \tTraining loss: 0.17676608264446259\n",
      "Step: 42000  \tTraining accuracy: 0.8997558951377869\n",
      "Step: 42000  \tValid loss: 0.20019547641277313\n",
      "Step: 42100  \tTraining loss: 0.17665834724903107\n",
      "Step: 42100  \tTraining accuracy: 0.8997675776481628\n",
      "Step: 42100  \tValid loss: 0.1999378353357315\n",
      "Step: 42200  \tTraining loss: 0.17654648423194885\n",
      "Step: 42200  \tTraining accuracy: 0.8997799158096313\n",
      "Step: 42200  \tValid loss: 0.19983656704425812\n",
      "Step: 42300  \tTraining loss: 0.17643535137176514\n",
      "Step: 42300  \tTraining accuracy: 0.89979088306427\n",
      "Step: 42300  \tValid loss: 0.19977743923664093\n",
      "Step: 42400  \tTraining loss: 0.17632734775543213\n",
      "Step: 42400  \tTraining accuracy: 0.8998050689697266\n",
      "Step: 42400  \tValid loss: 0.19959786534309387\n",
      "Step: 42500  \tTraining loss: 0.1762191206216812\n",
      "Step: 42500  \tTraining accuracy: 0.8998178839683533\n",
      "Step: 42500  \tValid loss: 0.199553444981575\n",
      "Step: 42600  \tTraining loss: 0.17611481249332428\n",
      "Step: 42600  \tTraining accuracy: 0.8998319506645203\n",
      "Step: 42600  \tValid loss: 0.19923356175422668\n",
      "Step: 42700  \tTraining loss: 0.17600828409194946\n",
      "Step: 42700  \tTraining accuracy: 0.8998456001281738\n",
      "Step: 42700  \tValid loss: 0.19924114644527435\n",
      "Step: 42800  \tTraining loss: 0.17590449750423431\n",
      "Step: 42800  \tTraining accuracy: 0.8998578786849976\n",
      "Step: 42800  \tValid loss: 0.19900229573249817\n",
      "Step: 42900  \tTraining loss: 0.1758040338754654\n",
      "Step: 42900  \tTraining accuracy: 0.8998727202415466\n",
      "Step: 42900  \tValid loss: 0.19877712428569794\n",
      "Step: 43000  \tTraining loss: 0.1757003664970398\n",
      "Step: 43000  \tTraining accuracy: 0.8998861312866211\n",
      "Step: 43000  \tValid loss: 0.1988411694765091\n",
      "Step: 43100  \tTraining loss: 0.17560593783855438\n",
      "Step: 43100  \tTraining accuracy: 0.8999008536338806\n",
      "Step: 43100  \tValid loss: 0.19843557476997375\n",
      "Step: 43200  \tTraining loss: 0.17550118267536163\n",
      "Step: 43200  \tTraining accuracy: 0.8999138474464417\n",
      "Step: 43200  \tValid loss: 0.19849444925785065\n",
      "Step: 43300  \tTraining loss: 0.17540428042411804\n",
      "Step: 43300  \tTraining accuracy: 0.8999287486076355\n",
      "Step: 43300  \tValid loss: 0.19831417500972748\n",
      "Step: 43400  \tTraining loss: 0.17530621588230133\n",
      "Step: 43400  \tTraining accuracy: 0.8999406695365906\n",
      "Step: 43400  \tValid loss: 0.19817803800106049\n",
      "Step: 43500  \tTraining loss: 0.17520837485790253\n",
      "Step: 43500  \tTraining accuracy: 0.8999515771865845\n",
      "Step: 43500  \tValid loss: 0.1980733573436737\n",
      "Step: 43600  \tTraining loss: 0.1751098334789276\n",
      "Step: 43600  \tTraining accuracy: 0.8999624848365784\n",
      "Step: 43600  \tValid loss: 0.19808098673820496\n",
      "Step: 43700  \tTraining loss: 0.17501665651798248\n",
      "Step: 43700  \tTraining accuracy: 0.8999732732772827\n",
      "Step: 43700  \tValid loss: 0.19783523678779602\n",
      "Step: 43800  \tTraining loss: 0.17492251098155975\n",
      "Step: 43800  \tTraining accuracy: 0.8999840617179871\n",
      "Step: 43800  \tValid loss: 0.1977066695690155\n",
      "Step: 43900  \tTraining loss: 0.17482762038707733\n",
      "Step: 43900  \tTraining accuracy: 0.8999938368797302\n",
      "Step: 43900  \tValid loss: 0.19762203097343445\n",
      "Step: 44000  \tTraining loss: 0.17473328113555908\n",
      "Step: 44000  \tTraining accuracy: 0.9000029563903809\n",
      "Step: 44000  \tValid loss: 0.19757746160030365\n",
      "Step: 44100  \tTraining loss: 0.17464078962802887\n",
      "Step: 44100  \tTraining accuracy: 0.9000108242034912\n",
      "Step: 44100  \tValid loss: 0.19749398529529572\n",
      "Step: 44200  \tTraining loss: 0.17455150187015533\n",
      "Step: 44200  \tTraining accuracy: 0.9000179767608643\n",
      "Step: 44200  \tValid loss: 0.197279691696167\n",
      "Step: 44300  \tTraining loss: 0.17445948719978333\n",
      "Step: 44300  \tTraining accuracy: 0.9000250697135925\n",
      "Step: 44300  \tValid loss: 0.19723384082317352\n",
      "Step: 44400  \tTraining loss: 0.17436742782592773\n",
      "Step: 44400  \tTraining accuracy: 0.9000328183174133\n",
      "Step: 44400  \tValid loss: 0.1972375512123108\n",
      "Step: 44500  \tTraining loss: 0.17427833378314972\n",
      "Step: 44500  \tTraining accuracy: 0.9000405073165894\n",
      "Step: 44500  \tValid loss: 0.19711105525493622\n",
      "Step: 44600  \tTraining loss: 0.17418956756591797\n",
      "Step: 44600  \tTraining accuracy: 0.9000475406646729\n",
      "Step: 44600  \tValid loss: 0.1969704031944275\n",
      "Step: 44700  \tTraining loss: 0.174103781580925\n",
      "Step: 44700  \tTraining accuracy: 0.9000548124313354\n",
      "Step: 44700  \tValid loss: 0.19678331911563873\n",
      "Step: 44800  \tTraining loss: 0.1740160435438156\n",
      "Step: 44800  \tTraining accuracy: 0.900062084197998\n",
      "Step: 44800  \tValid loss: 0.1966925859451294\n",
      "Step: 44900  \tTraining loss: 0.17392787337303162\n",
      "Step: 44900  \tTraining accuracy: 0.9000693559646606\n",
      "Step: 44900  \tValid loss: 0.1965930163860321\n",
      "Step: 45000  \tTraining loss: 0.17384086549282074\n",
      "Step: 45000  \tTraining accuracy: 0.9000765681266785\n",
      "Step: 45000  \tValid loss: 0.19649408757686615\n",
      "Step: 45100  \tTraining loss: 0.17375163733959198\n",
      "Step: 45100  \tTraining accuracy: 0.9000828266143799\n",
      "Step: 45100  \tValid loss: 0.1964685022830963\n",
      "Step: 45200  \tTraining loss: 0.17366842925548553\n",
      "Step: 45200  \tTraining accuracy: 0.9000902771949768\n",
      "Step: 45200  \tValid loss: 0.19629712402820587\n",
      "Step: 45300  \tTraining loss: 0.1735796332359314\n",
      "Step: 45300  \tTraining accuracy: 0.900097668170929\n",
      "Step: 45300  \tValid loss: 0.1962769627571106\n",
      "Step: 45400  \tTraining loss: 0.173495352268219\n",
      "Step: 45400  \tTraining accuracy: 0.9001050591468811\n",
      "Step: 45400  \tValid loss: 0.1961938440799713\n",
      "Step: 45500  \tTraining loss: 0.1734085977077484\n",
      "Step: 45500  \tTraining accuracy: 0.9001124501228333\n",
      "Step: 45500  \tValid loss: 0.19611628353595734\n",
      "Step: 45600  \tTraining loss: 0.17332278192043304\n",
      "Step: 45600  \tTraining accuracy: 0.9001197814941406\n",
      "Step: 45600  \tValid loss: 0.1960422694683075\n",
      "Step: 45700  \tTraining loss: 0.17323942482471466\n",
      "Step: 45700  \tTraining accuracy: 0.9001270532608032\n",
      "Step: 45700  \tValid loss: 0.1959010362625122\n",
      "Step: 45800  \tTraining loss: 0.17315442860126495\n",
      "Step: 45800  \tTraining accuracy: 0.9001343250274658\n",
      "Step: 45800  \tValid loss: 0.1958186775445938\n",
      "Step: 45900  \tTraining loss: 0.17306849360466003\n",
      "Step: 45900  \tTraining accuracy: 0.9001415371894836\n",
      "Step: 45900  \tValid loss: 0.19583545625209808\n",
      "Step: 46000  \tTraining loss: 0.17298611998558044\n",
      "Step: 46000  \tTraining accuracy: 0.9001472592353821\n",
      "Step: 46000  \tValid loss: 0.19564418494701385\n",
      "Step: 46100  \tTraining loss: 0.1729038804769516\n",
      "Step: 46100  \tTraining accuracy: 0.9001526236534119\n",
      "Step: 46100  \tValid loss: 0.19547782838344574\n",
      "Step: 46200  \tTraining loss: 0.17281799018383026\n",
      "Step: 46200  \tTraining accuracy: 0.9001579284667969\n",
      "Step: 46200  \tValid loss: 0.19547930359840393\n",
      "Step: 46300  \tTraining loss: 0.17274537682533264\n",
      "Step: 46300  \tTraining accuracy: 0.9001632928848267\n",
      "Step: 46300  \tValid loss: 0.19517001509666443\n",
      "Step: 46400  \tTraining loss: 0.17265062034130096\n",
      "Step: 46400  \tTraining accuracy: 0.9001685976982117\n",
      "Step: 46400  \tValid loss: 0.19528022408485413\n",
      "Step: 46500  \tTraining loss: 0.17256392538547516\n",
      "Step: 46500  \tTraining accuracy: 0.9001744389533997\n",
      "Step: 46500  \tValid loss: 0.19529405236244202\n",
      "Step: 46600  \tTraining loss: 0.17248184978961945\n",
      "Step: 46600  \tTraining accuracy: 0.9001796841621399\n",
      "Step: 46600  \tValid loss: 0.19515971839427948\n",
      "Step: 46700  \tTraining loss: 0.17239953577518463\n",
      "Step: 46700  \tTraining accuracy: 0.9001837372779846\n",
      "Step: 46700  \tValid loss: 0.19503337144851685\n",
      "Step: 46800  \tTraining loss: 0.1723148673772812\n",
      "Step: 46800  \tTraining accuracy: 0.90018630027771\n",
      "Step: 46800  \tValid loss: 0.19499947130680084\n",
      "Step: 46900  \tTraining loss: 0.1722331941127777\n",
      "Step: 46900  \tTraining accuracy: 0.9001903533935547\n",
      "Step: 46900  \tValid loss: 0.1948515921831131\n",
      "Step: 47000  \tTraining loss: 0.17214767634868622\n",
      "Step: 47000  \tTraining accuracy: 0.9001940488815308\n",
      "Step: 47000  \tValid loss: 0.1949702352285385\n",
      "Step: 47100  \tTraining loss: 0.17206765711307526\n",
      "Step: 47100  \tTraining accuracy: 0.9001980423927307\n",
      "Step: 47100  \tValid loss: 0.1946975141763687\n",
      "Step: 47200  \tTraining loss: 0.17198802530765533\n",
      "Step: 47200  \tTraining accuracy: 0.9002019762992859\n",
      "Step: 47200  \tValid loss: 0.19452141225337982\n",
      "Step: 47300  \tTraining loss: 0.1719045341014862\n",
      "Step: 47300  \tTraining accuracy: 0.9002059698104858\n",
      "Step: 47300  \tValid loss: 0.19447559118270874\n",
      "Step: 47400  \tTraining loss: 0.1718222200870514\n",
      "Step: 47400  \tTraining accuracy: 0.9002093076705933\n",
      "Step: 47400  \tValid loss: 0.19440704584121704\n",
      "Step: 47500  \tTraining loss: 0.17173996567726135\n",
      "Step: 47500  \tTraining accuracy: 0.9002129435539246\n",
      "Step: 47500  \tValid loss: 0.19434276223182678\n",
      "Step: 47600  \tTraining loss: 0.1716562956571579\n",
      "Step: 47600  \tTraining accuracy: 0.9002165198326111\n",
      "Step: 47600  \tValid loss: 0.19434228539466858\n",
      "Step: 47700  \tTraining loss: 0.17157725989818573\n",
      "Step: 47700  \tTraining accuracy: 0.9002201557159424\n",
      "Step: 47700  \tValid loss: 0.19416747987270355\n",
      "Step: 47800  \tTraining loss: 0.17149360477924347\n",
      "Step: 47800  \tTraining accuracy: 0.9002245664596558\n",
      "Step: 47800  \tValid loss: 0.1942891627550125\n",
      "Step: 47900  \tTraining loss: 0.17141512036323547\n",
      "Step: 47900  \tTraining accuracy: 0.9002281427383423\n",
      "Step: 47900  \tValid loss: 0.19402632117271423\n",
      "Step: 48000  \tTraining loss: 0.17133179306983948\n",
      "Step: 48000  \tTraining accuracy: 0.9002325534820557\n",
      "Step: 48000  \tValid loss: 0.19400574266910553\n",
      "Step: 48100  \tTraining loss: 0.17125222086906433\n",
      "Step: 48100  \tTraining accuracy: 0.9002358317375183\n",
      "Step: 48100  \tValid loss: 0.19383344054222107\n",
      "Step: 48200  \tTraining loss: 0.17117053270339966\n",
      "Step: 48200  \tTraining accuracy: 0.9002390503883362\n",
      "Step: 48200  \tValid loss: 0.19371652603149414\n",
      "Step: 48300  \tTraining loss: 0.1710820496082306\n",
      "Step: 48300  \tTraining accuracy: 0.9002431035041809\n",
      "Step: 48300  \tValid loss: 0.19377762079238892\n",
      "Step: 48400  \tTraining loss: 0.17100591957569122\n",
      "Step: 48400  \tTraining accuracy: 0.9002457857131958\n",
      "Step: 48400  \tValid loss: 0.19354385137557983\n",
      "Step: 48500  \tTraining loss: 0.17090485990047455\n",
      "Step: 48500  \tTraining accuracy: 0.9002498388290405\n",
      "Step: 48500  \tValid loss: 0.19365140795707703\n",
      "Step: 48600  \tTraining loss: 0.1708143651485443\n",
      "Step: 48600  \tTraining accuracy: 0.9002538323402405\n",
      "Step: 48600  \tValid loss: 0.19354842603206635\n",
      "Step: 48700  \tTraining loss: 0.17071470618247986\n",
      "Step: 48700  \tTraining accuracy: 0.9002578854560852\n",
      "Step: 48700  \tValid loss: 0.19355209171772003\n",
      "Step: 48800  \tTraining loss: 0.1706230789422989\n",
      "Step: 48800  \tTraining accuracy: 0.9002618789672852\n",
      "Step: 48800  \tValid loss: 0.1934150755405426\n",
      "Step: 48900  \tTraining loss: 0.17053456604480743\n",
      "Step: 48900  \tTraining accuracy: 0.9002658128738403\n",
      "Step: 48900  \tValid loss: 0.19339047372341156\n",
      "Step: 49000  \tTraining loss: 0.1704452484846115\n",
      "Step: 49000  \tTraining accuracy: 0.9002698063850403\n",
      "Step: 49000  \tValid loss: 0.19328506290912628\n",
      "Step: 49100  \tTraining loss: 0.1703597605228424\n",
      "Step: 49100  \tTraining accuracy: 0.9002732038497925\n",
      "Step: 49100  \tValid loss: 0.19304978847503662\n",
      "Step: 49200  \tTraining loss: 0.17024385929107666\n",
      "Step: 49200  \tTraining accuracy: 0.9002782702445984\n",
      "Step: 49200  \tValid loss: 0.19319839775562286\n",
      "Step: 49300  \tTraining loss: 0.17013221979141235\n",
      "Step: 49300  \tTraining accuracy: 0.9002835750579834\n",
      "Step: 49300  \tValid loss: 0.19309654831886292\n",
      "Step: 49400  \tTraining loss: 0.17001914978027344\n",
      "Step: 49400  \tTraining accuracy: 0.9002894163131714\n",
      "Step: 49400  \tValid loss: 0.19304779171943665\n",
      "Step: 49500  \tTraining loss: 0.16992463171482086\n",
      "Step: 49500  \tTraining accuracy: 0.9002966284751892\n",
      "Step: 49500  \tValid loss: 0.19294482469558716\n",
      "Step: 49600  \tTraining loss: 0.16983121633529663\n",
      "Step: 49600  \tTraining accuracy: 0.9003030061721802\n",
      "Step: 49600  \tValid loss: 0.19293463230133057\n",
      "Step: 49700  \tTraining loss: 0.16974098980426788\n",
      "Step: 49700  \tTraining accuracy: 0.9003104567527771\n",
      "Step: 49700  \tValid loss: 0.19289323687553406\n",
      "Step: 49800  \tTraining loss: 0.1696530282497406\n",
      "Step: 49800  \tTraining accuracy: 0.9003178477287292\n",
      "Step: 49800  \tValid loss: 0.192832350730896\n",
      "Step: 49900  \tTraining loss: 0.16955886781215668\n",
      "Step: 49900  \tTraining accuracy: 0.9003252387046814\n",
      "Step: 49900  \tValid loss: 0.19293271005153656\n",
      "Step: 50000  \tTraining loss: 0.16947053372859955\n",
      "Step: 50000  \tTraining accuracy: 0.9003326296806335\n",
      "Step: 50000  \tValid loss: 0.19287024438381195\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.90033996\n",
      "Precision: 0.89642286\n",
      "Recall: 0.94485086\n",
      "F1 score: 0.92402005\n",
      "AUC: 0.9199363\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0   0.90034   0.896423  0.944851   0.92402  0.919936  0.169471      0.900321   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.192667       0.900315   0.327444      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  49999.0  \n",
      "5\n",
      "(4205, 8)\n",
      "(4205, 1)\n",
      "(2320, 8)\n",
      "(2320, 1)\n",
      "(1885, 8)\n",
      "(1885, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6601394414901733\n",
      "Step: 100  \tTraining accuracy: 0.5778834819793701\n",
      "Step: 100  \tValid loss: 0.6497406959533691\n",
      "Step: 200  \tTraining loss: 0.6462661623954773\n",
      "Step: 200  \tTraining accuracy: 0.5975425839424133\n",
      "Step: 200  \tValid loss: 0.6389305591583252\n",
      "Step: 300  \tTraining loss: 0.639360785484314\n",
      "Step: 300  \tTraining accuracy: 0.6109869480133057\n",
      "Step: 300  \tValid loss: 0.6321077346801758\n",
      "Step: 400  \tTraining loss: 0.6308640241622925\n",
      "Step: 400  \tTraining accuracy: 0.6195345520973206\n",
      "Step: 400  \tValid loss: 0.6217405796051025\n",
      "Step: 500  \tTraining loss: 0.6190350651741028\n",
      "Step: 500  \tTraining accuracy: 0.626767098903656\n",
      "Step: 500  \tValid loss: 0.6078482270240784\n",
      "Step: 600  \tTraining loss: 0.6063826084136963\n",
      "Step: 600  \tTraining accuracy: 0.6343098282814026\n",
      "Step: 600  \tValid loss: 0.5940039157867432\n",
      "Step: 700  \tTraining loss: 0.596228837966919\n",
      "Step: 700  \tTraining accuracy: 0.641232967376709\n",
      "Step: 700  \tValid loss: 0.5831160545349121\n",
      "Step: 800  \tTraining loss: 0.588765025138855\n",
      "Step: 800  \tTraining accuracy: 0.6471343636512756\n",
      "Step: 800  \tValid loss: 0.5759193897247314\n",
      "Step: 900  \tTraining loss: 0.5830683708190918\n",
      "Step: 900  \tTraining accuracy: 0.6520388722419739\n",
      "Step: 900  \tValid loss: 0.5702369213104248\n",
      "Step: 1000  \tTraining loss: 0.5787449479103088\n",
      "Step: 1000  \tTraining accuracy: 0.6561486721038818\n",
      "Step: 1000  \tValid loss: 0.5664424300193787\n",
      "Step: 1100  \tTraining loss: 0.5754089951515198\n",
      "Step: 1100  \tTraining accuracy: 0.6595323085784912\n",
      "Step: 1100  \tValid loss: 0.5637377500534058\n",
      "Step: 1200  \tTraining loss: 0.5725688338279724\n",
      "Step: 1200  \tTraining accuracy: 0.6623067855834961\n",
      "Step: 1200  \tValid loss: 0.5617989301681519\n",
      "Step: 1300  \tTraining loss: 0.5701363682746887\n",
      "Step: 1300  \tTraining accuracy: 0.6646848917007446\n",
      "Step: 1300  \tValid loss: 0.5601677894592285\n",
      "Step: 1400  \tTraining loss: 0.5679869651794434\n",
      "Step: 1400  \tTraining accuracy: 0.6668339967727661\n",
      "Step: 1400  \tValid loss: 0.5589589476585388\n",
      "Step: 1500  \tTraining loss: 0.5660427808761597\n",
      "Step: 1500  \tTraining accuracy: 0.6687440872192383\n",
      "Step: 1500  \tValid loss: 0.5575113892555237\n",
      "Step: 1600  \tTraining loss: 0.5643194913864136\n",
      "Step: 1600  \tTraining accuracy: 0.670637845993042\n",
      "Step: 1600  \tValid loss: 0.5564021468162537\n",
      "Step: 1700  \tTraining loss: 0.5625817775726318\n",
      "Step: 1700  \tTraining accuracy: 0.6722444295883179\n",
      "Step: 1700  \tValid loss: 0.5547836422920227\n",
      "Step: 1800  \tTraining loss: 0.5609636306762695\n",
      "Step: 1800  \tTraining accuracy: 0.6736538410186768\n",
      "Step: 1800  \tValid loss: 0.5537583231925964\n",
      "Step: 1900  \tTraining loss: 0.5593522191047668\n",
      "Step: 1900  \tTraining accuracy: 0.674885094165802\n",
      "Step: 1900  \tValid loss: 0.5525100231170654\n",
      "Step: 2000  \tTraining loss: 0.5577090382575989\n",
      "Step: 2000  \tTraining accuracy: 0.676020622253418\n",
      "Step: 2000  \tValid loss: 0.5514443516731262\n",
      "Step: 2100  \tTraining loss: 0.556079089641571\n",
      "Step: 2100  \tTraining accuracy: 0.6771497130393982\n",
      "Step: 2100  \tValid loss: 0.5504372715950012\n",
      "Step: 2200  \tTraining loss: 0.5545734763145447\n",
      "Step: 2200  \tTraining accuracy: 0.6782568097114563\n",
      "Step: 2200  \tValid loss: 0.5493715405464172\n",
      "Step: 2300  \tTraining loss: 0.5532450675964355\n",
      "Step: 2300  \tTraining accuracy: 0.6794292330741882\n",
      "Step: 2300  \tValid loss: 0.54852694272995\n",
      "Step: 2400  \tTraining loss: 0.5521021485328674\n",
      "Step: 2400  \tTraining accuracy: 0.6805576086044312\n",
      "Step: 2400  \tValid loss: 0.5477597713470459\n",
      "Step: 2500  \tTraining loss: 0.5511151552200317\n",
      "Step: 2500  \tTraining accuracy: 0.6816326379776001\n",
      "Step: 2500  \tValid loss: 0.5471614003181458\n",
      "Step: 2600  \tTraining loss: 0.5502619743347168\n",
      "Step: 2600  \tTraining accuracy: 0.6826886534690857\n",
      "Step: 2600  \tValid loss: 0.5466524362564087\n",
      "Step: 2700  \tTraining loss: 0.5495020151138306\n",
      "Step: 2700  \tTraining accuracy: 0.6837098598480225\n",
      "Step: 2700  \tValid loss: 0.5461653470993042\n",
      "Step: 2800  \tTraining loss: 0.5488206744194031\n",
      "Step: 2800  \tTraining accuracy: 0.6846481561660767\n",
      "Step: 2800  \tValid loss: 0.5456734895706177\n",
      "Step: 2900  \tTraining loss: 0.5481706261634827\n",
      "Step: 2900  \tTraining accuracy: 0.6855456233024597\n",
      "Step: 2900  \tValid loss: 0.5451886653900146\n",
      "Step: 3000  \tTraining loss: 0.5474487543106079\n",
      "Step: 3000  \tTraining accuracy: 0.686366081237793\n",
      "Step: 3000  \tValid loss: 0.5446982383728027\n",
      "Step: 3100  \tTraining loss: 0.546852707862854\n",
      "Step: 3100  \tTraining accuracy: 0.6870937943458557\n",
      "Step: 3100  \tValid loss: 0.5444706082344055\n",
      "Step: 3200  \tTraining loss: 0.5460105538368225\n",
      "Step: 3200  \tTraining accuracy: 0.6877678036689758\n",
      "Step: 3200  \tValid loss: 0.5442430377006531\n",
      "Step: 3300  \tTraining loss: 0.5454115271568298\n",
      "Step: 3300  \tTraining accuracy: 0.688477098941803\n",
      "Step: 3300  \tValid loss: 0.5435958504676819\n",
      "Step: 3400  \tTraining loss: 0.5448490977287292\n",
      "Step: 3400  \tTraining accuracy: 0.6891334056854248\n",
      "Step: 3400  \tValid loss: 0.5429356098175049\n",
      "Step: 3500  \tTraining loss: 0.5443037748336792\n",
      "Step: 3500  \tTraining accuracy: 0.689793050289154\n",
      "Step: 3500  \tValid loss: 0.5424165725708008\n",
      "Step: 3600  \tTraining loss: 0.5435781478881836\n",
      "Step: 3600  \tTraining accuracy: 0.6904489994049072\n",
      "Step: 3600  \tValid loss: 0.5417600870132446\n",
      "Step: 3700  \tTraining loss: 0.5428483486175537\n",
      "Step: 3700  \tTraining accuracy: 0.6911439299583435\n",
      "Step: 3700  \tValid loss: 0.5410654544830322\n",
      "Step: 3800  \tTraining loss: 0.5422311425209045\n",
      "Step: 3800  \tTraining accuracy: 0.6918081641197205\n",
      "Step: 3800  \tValid loss: 0.5405585169792175\n",
      "Step: 3900  \tTraining loss: 0.5416788458824158\n",
      "Step: 3900  \tTraining accuracy: 0.692425549030304\n",
      "Step: 3900  \tValid loss: 0.5401366949081421\n",
      "Step: 4000  \tTraining loss: 0.5411777496337891\n",
      "Step: 4000  \tTraining accuracy: 0.6929875612258911\n",
      "Step: 4000  \tValid loss: 0.5398240685462952\n",
      "Step: 4100  \tTraining loss: 0.5406681895256042\n",
      "Step: 4100  \tTraining accuracy: 0.693513035774231\n",
      "Step: 4100  \tValid loss: 0.5394942760467529\n",
      "Step: 4200  \tTraining loss: 0.5401408672332764\n",
      "Step: 4200  \tTraining accuracy: 0.6940016746520996\n",
      "Step: 4200  \tValid loss: 0.5393046140670776\n",
      "Step: 4300  \tTraining loss: 0.5395013093948364\n",
      "Step: 4300  \tTraining accuracy: 0.6944869756698608\n",
      "Step: 4300  \tValid loss: 0.5390757322311401\n",
      "Step: 4400  \tTraining loss: 0.5389208197593689\n",
      "Step: 4400  \tTraining accuracy: 0.694999098777771\n",
      "Step: 4400  \tValid loss: 0.5386111736297607\n",
      "Step: 4500  \tTraining loss: 0.5383661389350891\n",
      "Step: 4500  \tTraining accuracy: 0.695480227470398\n",
      "Step: 4500  \tValid loss: 0.5380021929740906\n",
      "Step: 4600  \tTraining loss: 0.5378514528274536\n",
      "Step: 4600  \tTraining accuracy: 0.695966362953186\n",
      "Step: 4600  \tValid loss: 0.5376401543617249\n",
      "Step: 4700  \tTraining loss: 0.537283182144165\n",
      "Step: 4700  \tTraining accuracy: 0.6964520215988159\n",
      "Step: 4700  \tValid loss: 0.5368531346321106\n",
      "Step: 4800  \tTraining loss: 0.5366350412368774\n",
      "Step: 4800  \tTraining accuracy: 0.6969372034072876\n",
      "Step: 4800  \tValid loss: 0.536307156085968\n",
      "Step: 4900  \tTraining loss: 0.5359744429588318\n",
      "Step: 4900  \tTraining accuracy: 0.6974220871925354\n",
      "Step: 4900  \tValid loss: 0.536108672618866\n",
      "Step: 5000  \tTraining loss: 0.5354183316230774\n",
      "Step: 5000  \tTraining accuracy: 0.6978728771209717\n",
      "Step: 5000  \tValid loss: 0.5357359647750854\n",
      "Step: 5100  \tTraining loss: 0.5349135398864746\n",
      "Step: 5100  \tTraining accuracy: 0.6982846856117249\n",
      "Step: 5100  \tValid loss: 0.5353577733039856\n",
      "Step: 5200  \tTraining loss: 0.5343679189682007\n",
      "Step: 5200  \tTraining accuracy: 0.6986689567565918\n",
      "Step: 5200  \tValid loss: 0.534896731376648\n",
      "Step: 5300  \tTraining loss: 0.5336750745773315\n",
      "Step: 5300  \tTraining accuracy: 0.6990589499473572\n",
      "Step: 5300  \tValid loss: 0.5344414710998535\n",
      "Step: 5400  \tTraining loss: 0.5331002473831177\n",
      "Step: 5400  \tTraining accuracy: 0.6994721293449402\n",
      "Step: 5400  \tValid loss: 0.5339326858520508\n",
      "Step: 5500  \tTraining loss: 0.5325199961662292\n",
      "Step: 5500  \tTraining accuracy: 0.6998592615127563\n",
      "Step: 5500  \tValid loss: 0.5337944626808167\n",
      "Step: 5600  \tTraining loss: 0.5316272974014282\n",
      "Step: 5600  \tTraining accuracy: 0.7002303004264832\n",
      "Step: 5600  \tValid loss: 0.5332253575325012\n",
      "Step: 5700  \tTraining loss: 0.530865490436554\n",
      "Step: 5700  \tTraining accuracy: 0.7005524635314941\n",
      "Step: 5700  \tValid loss: 0.5327733755111694\n",
      "Step: 5800  \tTraining loss: 0.5301467776298523\n",
      "Step: 5800  \tTraining accuracy: 0.7008509635925293\n",
      "Step: 5800  \tValid loss: 0.5320272445678711\n",
      "Step: 5900  \tTraining loss: 0.5294894576072693\n",
      "Step: 5900  \tTraining accuracy: 0.7011535167694092\n",
      "Step: 5900  \tValid loss: 0.5311591029167175\n",
      "Step: 6000  \tTraining loss: 0.5289534330368042\n",
      "Step: 6000  \tTraining accuracy: 0.7014538645744324\n",
      "Step: 6000  \tValid loss: 0.5305351614952087\n",
      "Step: 6100  \tTraining loss: 0.5284894108772278\n",
      "Step: 6100  \tTraining accuracy: 0.7017364501953125\n",
      "Step: 6100  \tValid loss: 0.5300551056861877\n",
      "Step: 6200  \tTraining loss: 0.528075635433197\n",
      "Step: 6200  \tTraining accuracy: 0.702019453048706\n",
      "Step: 6200  \tValid loss: 0.5296306610107422\n",
      "Step: 6300  \tTraining loss: 0.5276932716369629\n",
      "Step: 6300  \tTraining accuracy: 0.7022801637649536\n",
      "Step: 6300  \tValid loss: 0.529336154460907\n",
      "Step: 6400  \tTraining loss: 0.5273528099060059\n",
      "Step: 6400  \tTraining accuracy: 0.7025232315063477\n",
      "Step: 6400  \tValid loss: 0.5290998220443726\n",
      "Step: 6500  \tTraining loss: 0.5270249843597412\n",
      "Step: 6500  \tTraining accuracy: 0.702756941318512\n",
      "Step: 6500  \tValid loss: 0.5288774967193604\n",
      "Step: 6600  \tTraining loss: 0.5267292857170105\n",
      "Step: 6600  \tTraining accuracy: 0.7029726505279541\n",
      "Step: 6600  \tValid loss: 0.5287190675735474\n",
      "Step: 6700  \tTraining loss: 0.5264276266098022\n",
      "Step: 6700  \tTraining accuracy: 0.7031657695770264\n",
      "Step: 6700  \tValid loss: 0.5284741520881653\n",
      "Step: 6800  \tTraining loss: 0.5261427164077759\n",
      "Step: 6800  \tTraining accuracy: 0.7033584117889404\n",
      "Step: 6800  \tValid loss: 0.5281639099121094\n",
      "Step: 6900  \tTraining loss: 0.5258294343948364\n",
      "Step: 6900  \tTraining accuracy: 0.703559398651123\n",
      "Step: 6900  \tValid loss: 0.528005838394165\n",
      "Step: 7000  \tTraining loss: 0.5255299806594849\n",
      "Step: 7000  \tTraining accuracy: 0.7037819027900696\n",
      "Step: 7000  \tValid loss: 0.5278542637825012\n",
      "Step: 7100  \tTraining loss: 0.5252493023872375\n",
      "Step: 7100  \tTraining accuracy: 0.7039896845817566\n",
      "Step: 7100  \tValid loss: 0.5277670621871948\n",
      "Step: 7200  \tTraining loss: 0.5249928832054138\n",
      "Step: 7200  \tTraining accuracy: 0.704188346862793\n",
      "Step: 7200  \tValid loss: 0.5276833176612854\n",
      "Step: 7300  \tTraining loss: 0.5247333645820618\n",
      "Step: 7300  \tTraining accuracy: 0.7043814659118652\n",
      "Step: 7300  \tValid loss: 0.5275232195854187\n",
      "Step: 7400  \tTraining loss: 0.5244787335395813\n",
      "Step: 7400  \tTraining accuracy: 0.7045855522155762\n",
      "Step: 7400  \tValid loss: 0.5273773670196533\n",
      "Step: 7500  \tTraining loss: 0.5241764783859253\n",
      "Step: 7500  \tTraining accuracy: 0.7047761678695679\n",
      "Step: 7500  \tValid loss: 0.52711021900177\n",
      "Step: 7600  \tTraining loss: 0.523889422416687\n",
      "Step: 7600  \tTraining accuracy: 0.704950749874115\n",
      "Step: 7600  \tValid loss: 0.5269471406936646\n",
      "Step: 7700  \tTraining loss: 0.5235740542411804\n",
      "Step: 7700  \tTraining accuracy: 0.7051036357879639\n",
      "Step: 7700  \tValid loss: 0.5266764163970947\n",
      "Step: 7800  \tTraining loss: 0.5233227610588074\n",
      "Step: 7800  \tTraining accuracy: 0.7052448987960815\n",
      "Step: 7800  \tValid loss: 0.5264190435409546\n",
      "Step: 7900  \tTraining loss: 0.5230793952941895\n",
      "Step: 7900  \tTraining accuracy: 0.7053871154785156\n",
      "Step: 7900  \tValid loss: 0.5263298153877258\n",
      "Step: 8000  \tTraining loss: 0.5228471755981445\n",
      "Step: 8000  \tTraining accuracy: 0.7055497169494629\n",
      "Step: 8000  \tValid loss: 0.5261724591255188\n",
      "Step: 8100  \tTraining loss: 0.522605836391449\n",
      "Step: 8100  \tTraining accuracy: 0.7057155966758728\n",
      "Step: 8100  \tValid loss: 0.5260776877403259\n",
      "Step: 8200  \tTraining loss: 0.5223536491394043\n",
      "Step: 8200  \tTraining accuracy: 0.7058672308921814\n",
      "Step: 8200  \tValid loss: 0.525945246219635\n",
      "Step: 8300  \tTraining loss: 0.5220900774002075\n",
      "Step: 8300  \tTraining accuracy: 0.7060108780860901\n",
      "Step: 8300  \tValid loss: 0.525770902633667\n",
      "Step: 8400  \tTraining loss: 0.5217992663383484\n",
      "Step: 8400  \tTraining accuracy: 0.7061454057693481\n",
      "Step: 8400  \tValid loss: 0.52549809217453\n",
      "Step: 8500  \tTraining loss: 0.521545946598053\n",
      "Step: 8500  \tTraining accuracy: 0.7062767148017883\n",
      "Step: 8500  \tValid loss: 0.5252916812896729\n",
      "Step: 8600  \tTraining loss: 0.5212866067886353\n",
      "Step: 8600  \tTraining accuracy: 0.7063882350921631\n",
      "Step: 8600  \tValid loss: 0.5251352190971375\n",
      "Step: 8700  \tTraining loss: 0.5210463404655457\n",
      "Step: 8700  \tTraining accuracy: 0.7065137028694153\n",
      "Step: 8700  \tValid loss: 0.5250891447067261\n",
      "Step: 8800  \tTraining loss: 0.5207924246788025\n",
      "Step: 8800  \tTraining accuracy: 0.7066417336463928\n",
      "Step: 8800  \tValid loss: 0.5249544978141785\n",
      "Step: 8900  \tTraining loss: 0.5205471515655518\n",
      "Step: 8900  \tTraining accuracy: 0.7067709565162659\n",
      "Step: 8900  \tValid loss: 0.5248134732246399\n",
      "Step: 9000  \tTraining loss: 0.520319938659668\n",
      "Step: 9000  \tTraining accuracy: 0.706903874874115\n",
      "Step: 9000  \tValid loss: 0.5247490406036377\n",
      "Step: 9100  \tTraining loss: 0.5200833082199097\n",
      "Step: 9100  \tTraining accuracy: 0.7070430517196655\n",
      "Step: 9100  \tValid loss: 0.5246627330780029\n",
      "Step: 9200  \tTraining loss: 0.51986163854599\n",
      "Step: 9200  \tTraining accuracy: 0.7071921825408936\n",
      "Step: 9200  \tValid loss: 0.5246187448501587\n",
      "Step: 9300  \tTraining loss: 0.5196527242660522\n",
      "Step: 9300  \tTraining accuracy: 0.7073394060134888\n",
      "Step: 9300  \tValid loss: 0.5247328281402588\n",
      "Step: 9400  \tTraining loss: 0.519415020942688\n",
      "Step: 9400  \tTraining accuracy: 0.7074796557426453\n",
      "Step: 9400  \tValid loss: 0.5247888565063477\n",
      "Step: 9500  \tTraining loss: 0.5191962122917175\n",
      "Step: 9500  \tTraining accuracy: 0.70760178565979\n",
      "Step: 9500  \tValid loss: 0.5247863531112671\n",
      "Step: 9600  \tTraining loss: 0.5189717411994934\n",
      "Step: 9600  \tTraining accuracy: 0.7077276706695557\n",
      "Step: 9600  \tValid loss: 0.5246875882148743\n",
      "Step: 9700  \tTraining loss: 0.5187694430351257\n",
      "Step: 9700  \tTraining accuracy: 0.7078533172607422\n",
      "Step: 9700  \tValid loss: 0.5246569514274597\n",
      "Step: 9800  \tTraining loss: 0.5185746550559998\n",
      "Step: 9800  \tTraining accuracy: 0.7079715728759766\n",
      "Step: 9800  \tValid loss: 0.5246403813362122\n",
      "Step: 9900  \tTraining loss: 0.518382728099823\n",
      "Step: 9900  \tTraining accuracy: 0.7080898284912109\n",
      "Step: 9900  \tValid loss: 0.5246652364730835\n",
      "Step: 10000  \tTraining loss: 0.5181897282600403\n",
      "Step: 10000  \tTraining accuracy: 0.7082033157348633\n",
      "Step: 10000  \tValid loss: 0.5246716141700745\n",
      "Step: 10100  \tTraining loss: 0.518010139465332\n",
      "Step: 10100  \tTraining accuracy: 0.7083062529563904\n",
      "Step: 10100  \tValid loss: 0.5246947407722473\n",
      "Step: 10200  \tTraining loss: 0.5178351998329163\n",
      "Step: 10200  \tTraining accuracy: 0.7084013223648071\n",
      "Step: 10200  \tValid loss: 0.5247384309768677\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7084841\n",
      "Precision: 0.74671304\n",
      "Recall: 0.7946502\n",
      "F1 score: 0.73051834\n",
      "AUC: 0.712818\n",
      "   accuracy  precision   recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.708484   0.746713  0.79465  0.730518  0.712818  0.517711        0.7084   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.524576       0.708355   0.574472      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  10275.0  \n",
      "6\n",
      "(5655, 8)\n",
      "(5655, 1)\n",
      "(3120, 8)\n",
      "(3120, 1)\n",
      "(2535, 8)\n",
      "(2535, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4654049873352051\n",
      "Step: 100  \tTraining accuracy: 0.8279398679733276\n",
      "Step: 100  \tValid loss: 0.4642217457294464\n",
      "Step: 200  \tTraining loss: 0.41400906443595886\n",
      "Step: 200  \tTraining accuracy: 0.8357205986976624\n",
      "Step: 200  \tValid loss: 0.41245344281196594\n",
      "Step: 300  \tTraining loss: 0.40119385719299316\n",
      "Step: 300  \tTraining accuracy: 0.8396463394165039\n",
      "Step: 300  \tValid loss: 0.40010756254196167\n",
      "Step: 400  \tTraining loss: 0.3916477560997009\n",
      "Step: 400  \tTraining accuracy: 0.841581404209137\n",
      "Step: 400  \tValid loss: 0.39090415835380554\n",
      "Step: 500  \tTraining loss: 0.38311195373535156\n",
      "Step: 500  \tTraining accuracy: 0.8430690765380859\n",
      "Step: 500  \tValid loss: 0.38328516483306885\n",
      "Step: 600  \tTraining loss: 0.3740507960319519\n",
      "Step: 600  \tTraining accuracy: 0.844224750995636\n",
      "Step: 600  \tValid loss: 0.37561681866645813\n",
      "Step: 700  \tTraining loss: 0.36532798409461975\n",
      "Step: 700  \tTraining accuracy: 0.8453105092048645\n",
      "Step: 700  \tValid loss: 0.36850398778915405\n",
      "Step: 800  \tTraining loss: 0.35688915848731995\n",
      "Step: 800  \tTraining accuracy: 0.8465664386749268\n",
      "Step: 800  \tValid loss: 0.361910343170166\n",
      "Step: 900  \tTraining loss: 0.3492797017097473\n",
      "Step: 900  \tTraining accuracy: 0.8480470180511475\n",
      "Step: 900  \tValid loss: 0.3565385043621063\n",
      "Step: 1000  \tTraining loss: 0.3430017828941345\n",
      "Step: 1000  \tTraining accuracy: 0.8496161103248596\n",
      "Step: 1000  \tValid loss: 0.3526133596897125\n",
      "Step: 1100  \tTraining loss: 0.3375603258609772\n",
      "Step: 1100  \tTraining accuracy: 0.8512399196624756\n",
      "Step: 1100  \tValid loss: 0.3491520881652832\n",
      "Step: 1200  \tTraining loss: 0.33345940709114075\n",
      "Step: 1200  \tTraining accuracy: 0.8526736497879028\n",
      "Step: 1200  \tValid loss: 0.34730878472328186\n",
      "Step: 1300  \tTraining loss: 0.330454558134079\n",
      "Step: 1300  \tTraining accuracy: 0.8539062738418579\n",
      "Step: 1300  \tValid loss: 0.34600067138671875\n",
      "Step: 1400  \tTraining loss: 0.3279881477355957\n",
      "Step: 1400  \tTraining accuracy: 0.8551003932952881\n",
      "Step: 1400  \tValid loss: 0.34472352266311646\n",
      "Step: 1500  \tTraining loss: 0.32584148645401\n",
      "Step: 1500  \tTraining accuracy: 0.8562639355659485\n",
      "Step: 1500  \tValid loss: 0.343799889087677\n",
      "Step: 1600  \tTraining loss: 0.32384178042411804\n",
      "Step: 1600  \tTraining accuracy: 0.8573172688484192\n",
      "Step: 1600  \tValid loss: 0.34319379925727844\n",
      "Step: 1700  \tTraining loss: 0.3218938708305359\n",
      "Step: 1700  \tTraining accuracy: 0.8582535982131958\n",
      "Step: 1700  \tValid loss: 0.3427446484565735\n",
      "Step: 1800  \tTraining loss: 0.3200732171535492\n",
      "Step: 1800  \tTraining accuracy: 0.859012246131897\n",
      "Step: 1800  \tValid loss: 0.3424220681190491\n",
      "Step: 1900  \tTraining loss: 0.31835073232650757\n",
      "Step: 1900  \tTraining accuracy: 0.8596171736717224\n",
      "Step: 1900  \tValid loss: 0.3419235646724701\n",
      "Step: 2000  \tTraining loss: 0.31611141562461853\n",
      "Step: 2000  \tTraining accuracy: 0.8602008819580078\n",
      "Step: 2000  \tValid loss: 0.3410027027130127\n",
      "Step: 2100  \tTraining loss: 0.31435713171958923\n",
      "Step: 2100  \tTraining accuracy: 0.8607621192932129\n",
      "Step: 2100  \tValid loss: 0.34101226925849915\n",
      "Step: 2200  \tTraining loss: 0.313268780708313\n",
      "Step: 2200  \tTraining accuracy: 0.8612505793571472\n",
      "Step: 2200  \tValid loss: 0.341190904378891\n",
      "Step: 2300  \tTraining loss: 0.3124101459980011\n",
      "Step: 2300  \tTraining accuracy: 0.8617153167724609\n",
      "Step: 2300  \tValid loss: 0.34152713418006897\n",
      "Step: 2400  \tTraining loss: 0.3116762638092041\n",
      "Step: 2400  \tTraining accuracy: 0.8621743321418762\n",
      "Step: 2400  \tValid loss: 0.34187403321266174\n",
      "Step: 2500  \tTraining loss: 0.3110511600971222\n",
      "Step: 2500  \tTraining accuracy: 0.8626067042350769\n",
      "Step: 2500  \tValid loss: 0.3421177268028259\n",
      "Step: 2600  \tTraining loss: 0.31050413846969604\n",
      "Step: 2600  \tTraining accuracy: 0.8630120754241943\n",
      "Step: 2600  \tValid loss: 0.34223854541778564\n",
      "Step: 2700  \tTraining loss: 0.3100316524505615\n",
      "Step: 2700  \tTraining accuracy: 0.8634035587310791\n",
      "Step: 2700  \tValid loss: 0.3423342704772949\n",
      "Step: 2800  \tTraining loss: 0.30960142612457275\n",
      "Step: 2800  \tTraining accuracy: 0.8637955188751221\n",
      "Step: 2800  \tValid loss: 0.34245288372039795\n",
      "Step: 2900  \tTraining loss: 0.309198260307312\n",
      "Step: 2900  \tTraining accuracy: 0.8641940951347351\n",
      "Step: 2900  \tValid loss: 0.34243282675743103\n",
      "Step: 3000  \tTraining loss: 0.30880847573280334\n",
      "Step: 3000  \tTraining accuracy: 0.86457759141922\n",
      "Step: 3000  \tValid loss: 0.34234514832496643\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8649447\n",
      "Precision: 0.8521959\n",
      "Recall: 0.85544723\n",
      "F1 score: 0.8665915\n",
      "AUC: 0.874629\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.864945   0.852196  0.855447  0.866592  0.874629  0.308742       0.86473   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.340771         0.8647   0.306309      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3015.0  \n",
      "7\n",
      "(4930, 8)\n",
      "(4930, 1)\n",
      "(2720, 8)\n",
      "(2720, 1)\n",
      "(2210, 8)\n",
      "(2210, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.644347608089447\n",
      "Step: 100  \tTraining accuracy: 0.6168357133865356\n",
      "Step: 100  \tValid loss: 0.6274144053459167\n",
      "Step: 200  \tTraining loss: 0.5583990812301636\n",
      "Step: 200  \tTraining accuracy: 0.6548343300819397\n",
      "Step: 200  \tValid loss: 0.5538362860679626\n",
      "Step: 300  \tTraining loss: 0.5254440903663635\n",
      "Step: 300  \tTraining accuracy: 0.68405681848526\n",
      "Step: 300  \tValid loss: 0.530711829662323\n",
      "Step: 400  \tTraining loss: 0.5194811224937439\n",
      "Step: 400  \tTraining accuracy: 0.7000289559364319\n",
      "Step: 400  \tValid loss: 0.5278096199035645\n",
      "Step: 500  \tTraining loss: 0.5162373781204224\n",
      "Step: 500  \tTraining accuracy: 0.7094883918762207\n",
      "Step: 500  \tValid loss: 0.5249394774436951\n",
      "Step: 600  \tTraining loss: 0.5133307576179504\n",
      "Step: 600  \tTraining accuracy: 0.7158768177032471\n",
      "Step: 600  \tValid loss: 0.5218123197555542\n",
      "Step: 700  \tTraining loss: 0.5107613205909729\n",
      "Step: 700  \tTraining accuracy: 0.7203620076179504\n",
      "Step: 700  \tValid loss: 0.5190683603286743\n",
      "Step: 800  \tTraining loss: 0.508666455745697\n",
      "Step: 800  \tTraining accuracy: 0.7238404154777527\n",
      "Step: 800  \tValid loss: 0.5173144936561584\n",
      "Step: 900  \tTraining loss: 0.5068546533584595\n",
      "Step: 900  \tTraining accuracy: 0.7266674637794495\n",
      "Step: 900  \tValid loss: 0.5159304738044739\n",
      "Step: 1000  \tTraining loss: 0.5053386092185974\n",
      "Step: 1000  \tTraining accuracy: 0.7290807962417603\n",
      "Step: 1000  \tValid loss: 0.5145540237426758\n",
      "Step: 1100  \tTraining loss: 0.5041823983192444\n",
      "Step: 1100  \tTraining accuracy: 0.7309185862541199\n",
      "Step: 1100  \tValid loss: 0.5136887431144714\n",
      "Step: 1200  \tTraining loss: 0.5030392408370972\n",
      "Step: 1200  \tTraining accuracy: 0.7327365875244141\n",
      "Step: 1200  \tValid loss: 0.5129164457321167\n",
      "Step: 1300  \tTraining loss: 0.502001941204071\n",
      "Step: 1300  \tTraining accuracy: 0.7345151901245117\n",
      "Step: 1300  \tValid loss: 0.512008786201477\n",
      "Step: 1400  \tTraining loss: 0.5010082721710205\n",
      "Step: 1400  \tTraining accuracy: 0.7362557053565979\n",
      "Step: 1400  \tValid loss: 0.5113107562065125\n",
      "Step: 1500  \tTraining loss: 0.49973031878471375\n",
      "Step: 1500  \tTraining accuracy: 0.7377631664276123\n",
      "Step: 1500  \tValid loss: 0.5109608173370361\n",
      "Step: 1600  \tTraining loss: 0.49882104992866516\n",
      "Step: 1600  \tTraining accuracy: 0.7389518022537231\n",
      "Step: 1600  \tValid loss: 0.510607123374939\n",
      "Step: 1700  \tTraining loss: 0.49807989597320557\n",
      "Step: 1700  \tTraining accuracy: 0.7399901747703552\n",
      "Step: 1700  \tValid loss: 0.5100914239883423\n",
      "Step: 1800  \tTraining loss: 0.49741944670677185\n",
      "Step: 1800  \tTraining accuracy: 0.7410084009170532\n",
      "Step: 1800  \tValid loss: 0.5097129940986633\n",
      "Step: 1900  \tTraining loss: 0.49674466252326965\n",
      "Step: 1900  \tTraining accuracy: 0.7419439554214478\n",
      "Step: 1900  \tValid loss: 0.5094559788703918\n",
      "Step: 2000  \tTraining loss: 0.49599456787109375\n",
      "Step: 2000  \tTraining accuracy: 0.7428044080734253\n",
      "Step: 2000  \tValid loss: 0.509270429611206\n",
      "Step: 2100  \tTraining loss: 0.495378315448761\n",
      "Step: 2100  \tTraining accuracy: 0.7436847686767578\n",
      "Step: 2100  \tValid loss: 0.5091701745986938\n",
      "Step: 2200  \tTraining loss: 0.4947853088378906\n",
      "Step: 2200  \tTraining accuracy: 0.7445021271705627\n",
      "Step: 2200  \tValid loss: 0.5089705586433411\n",
      "Step: 2300  \tTraining loss: 0.4941653907299042\n",
      "Step: 2300  \tTraining accuracy: 0.7452197670936584\n",
      "Step: 2300  \tValid loss: 0.5088870525360107\n",
      "Step: 2400  \tTraining loss: 0.4936167001724243\n",
      "Step: 2400  \tTraining accuracy: 0.7458892464637756\n",
      "Step: 2400  \tValid loss: 0.5087924003601074\n",
      "Step: 2500  \tTraining loss: 0.49308422207832336\n",
      "Step: 2500  \tTraining accuracy: 0.7465124130249023\n",
      "Step: 2500  \tValid loss: 0.5087184906005859\n",
      "Step: 2600  \tTraining loss: 0.49253469705581665\n",
      "Step: 2600  \tTraining accuracy: 0.7470866441726685\n",
      "Step: 2600  \tValid loss: 0.508638858795166\n",
      "Step: 2700  \tTraining loss: 0.4920240342617035\n",
      "Step: 2700  \tTraining accuracy: 0.7476099133491516\n",
      "Step: 2700  \tValid loss: 0.5086197853088379\n",
      "Step: 2800  \tTraining loss: 0.4915233254432678\n",
      "Step: 2800  \tTraining accuracy: 0.7480951547622681\n",
      "Step: 2800  \tValid loss: 0.508496880531311\n",
      "Step: 2900  \tTraining loss: 0.49100008606910706\n",
      "Step: 2900  \tTraining accuracy: 0.7485498785972595\n",
      "Step: 2900  \tValid loss: 0.5084956884384155\n",
      "Step: 3000  \tTraining loss: 0.4904099702835083\n",
      "Step: 3000  \tTraining accuracy: 0.7489668726921082\n",
      "Step: 3000  \tValid loss: 0.5083236694335938\n",
      "Step: 3100  \tTraining loss: 0.4898415803909302\n",
      "Step: 3100  \tTraining accuracy: 0.7493997812271118\n",
      "Step: 3100  \tValid loss: 0.5081908106803894\n",
      "Step: 3200  \tTraining loss: 0.48929139971733093\n",
      "Step: 3200  \tTraining accuracy: 0.7498663663864136\n",
      "Step: 3200  \tValid loss: 0.5080447196960449\n",
      "Step: 3300  \tTraining loss: 0.488701730966568\n",
      "Step: 3300  \tTraining accuracy: 0.7503260970115662\n",
      "Step: 3300  \tValid loss: 0.5079936981201172\n",
      "Step: 3400  \tTraining loss: 0.4880494773387909\n",
      "Step: 3400  \tTraining accuracy: 0.7507311105728149\n",
      "Step: 3400  \tValid loss: 0.5079407691955566\n",
      "Step: 3500  \tTraining loss: 0.4874646067619324\n",
      "Step: 3500  \tTraining accuracy: 0.7511361837387085\n",
      "Step: 3500  \tValid loss: 0.5081446170806885\n",
      "Step: 3600  \tTraining loss: 0.4868599772453308\n",
      "Step: 3600  \tTraining accuracy: 0.7515384554862976\n",
      "Step: 3600  \tValid loss: 0.5082467198371887\n",
      "Step: 3700  \tTraining loss: 0.4863138496875763\n",
      "Step: 3700  \tTraining accuracy: 0.7518991827964783\n",
      "Step: 3700  \tValid loss: 0.5083226561546326\n",
      "Step: 3800  \tTraining loss: 0.48566919565200806\n",
      "Step: 3800  \tTraining accuracy: 0.7522109746932983\n",
      "Step: 3800  \tValid loss: 0.5080273151397705\n",
      "Step: 3900  \tTraining loss: 0.48509785532951355\n",
      "Step: 3900  \tTraining accuracy: 0.7524906992912292\n",
      "Step: 3900  \tValid loss: 0.5079911351203918\n",
      "Step: 4000  \tTraining loss: 0.48456689715385437\n",
      "Step: 4000  \tTraining accuracy: 0.7527177929878235\n",
      "Step: 4000  \tValid loss: 0.5081620812416077\n",
      "Step: 4100  \tTraining loss: 0.4840969443321228\n",
      "Step: 4100  \tTraining accuracy: 0.7529011368751526\n",
      "Step: 4100  \tValid loss: 0.508345365524292\n",
      "Step: 4200  \tTraining loss: 0.4836380183696747\n",
      "Step: 4200  \tTraining accuracy: 0.7530413866043091\n",
      "Step: 4200  \tValid loss: 0.508216142654419\n",
      "Step: 4300  \tTraining loss: 0.4832249581813812\n",
      "Step: 4300  \tTraining accuracy: 0.7531869411468506\n",
      "Step: 4300  \tValid loss: 0.5084238052368164\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.75334454\n",
      "Precision: 0.717302\n",
      "Recall: 0.6474325\n",
      "F1 score: 0.714776\n",
      "AUC: 0.744466\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.753345   0.717302  0.647433  0.714776  0.744466  0.482874      0.753184   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.507855       0.753133   0.526849      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  4375.0  \n",
      "8\n",
      "(13340, 8)\n",
      "(13340, 1)\n",
      "(7360, 8)\n",
      "(7360, 1)\n",
      "(5980, 8)\n",
      "(5980, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5926355719566345\n",
      "Step: 100  \tTraining accuracy: 0.730434775352478\n",
      "Step: 100  \tValid loss: 0.5894840955734253\n",
      "Step: 200  \tTraining loss: 0.44504985213279724\n",
      "Step: 200  \tTraining accuracy: 0.7715892195701599\n",
      "Step: 200  \tValid loss: 0.43358850479125977\n",
      "Step: 300  \tTraining loss: 0.37034034729003906\n",
      "Step: 300  \tTraining accuracy: 0.8068965673446655\n",
      "Step: 300  \tValid loss: 0.353275865316391\n",
      "Step: 400  \tTraining loss: 0.3399108052253723\n",
      "Step: 400  \tTraining accuracy: 0.8238487839698792\n",
      "Step: 400  \tValid loss: 0.31966838240623474\n",
      "Step: 500  \tTraining loss: 0.3271574378013611\n",
      "Step: 500  \tTraining accuracy: 0.8333083391189575\n",
      "Step: 500  \tValid loss: 0.3047550916671753\n",
      "Step: 600  \tTraining loss: 0.32125067710876465\n",
      "Step: 600  \tTraining accuracy: 0.8393349051475525\n",
      "Step: 600  \tValid loss: 0.29763028025627136\n",
      "Step: 700  \tTraining loss: 0.31822484731674194\n",
      "Step: 700  \tTraining accuracy: 0.8436166644096375\n",
      "Step: 700  \tValid loss: 0.2938302755355835\n",
      "Step: 800  \tTraining loss: 0.31623750925064087\n",
      "Step: 800  \tTraining accuracy: 0.8468515872955322\n",
      "Step: 800  \tValid loss: 0.2912546694278717\n",
      "Step: 900  \tTraining loss: 0.31476786732673645\n",
      "Step: 900  \tTraining accuracy: 0.849378228187561\n",
      "Step: 900  \tValid loss: 0.2894173562526703\n",
      "Step: 1000  \tTraining loss: 0.31355738639831543\n",
      "Step: 1000  \tTraining accuracy: 0.8514637351036072\n",
      "Step: 1000  \tValid loss: 0.28798410296440125\n",
      "Step: 1100  \tTraining loss: 0.3124632239341736\n",
      "Step: 1100  \tTraining accuracy: 0.8532341122627258\n",
      "Step: 1100  \tValid loss: 0.2868559658527374\n",
      "Step: 1200  \tTraining loss: 0.31143274903297424\n",
      "Step: 1200  \tTraining accuracy: 0.8547943234443665\n",
      "Step: 1200  \tValid loss: 0.2858608067035675\n",
      "Step: 1300  \tTraining loss: 0.3104192912578583\n",
      "Step: 1300  \tTraining accuracy: 0.8561829328536987\n",
      "Step: 1300  \tValid loss: 0.28498369455337524\n",
      "Step: 1400  \tTraining loss: 0.3094302713871002\n",
      "Step: 1400  \tTraining accuracy: 0.8573518991470337\n",
      "Step: 1400  \tValid loss: 0.28419071435928345\n",
      "Step: 1500  \tTraining loss: 0.3085358440876007\n",
      "Step: 1500  \tTraining accuracy: 0.8583544492721558\n",
      "Step: 1500  \tValid loss: 0.2835312485694885\n",
      "Step: 1600  \tTraining loss: 0.30775782465934753\n",
      "Step: 1600  \tTraining accuracy: 0.8592566847801208\n",
      "Step: 1600  \tValid loss: 0.2829650938510895\n",
      "Step: 1700  \tTraining loss: 0.30708637833595276\n",
      "Step: 1700  \tTraining accuracy: 0.8600495457649231\n",
      "Step: 1700  \tValid loss: 0.282470703125\n",
      "Step: 1800  \tTraining loss: 0.30646586418151855\n",
      "Step: 1800  \tTraining accuracy: 0.8607324957847595\n",
      "Step: 1800  \tValid loss: 0.2819650173187256\n",
      "Step: 1900  \tTraining loss: 0.30590206384658813\n",
      "Step: 1900  \tTraining accuracy: 0.8613497018814087\n",
      "Step: 1900  \tValid loss: 0.28150200843811035\n",
      "Step: 2000  \tTraining loss: 0.30536898970603943\n",
      "Step: 2000  \tTraining accuracy: 0.8618959784507751\n",
      "Step: 2000  \tValid loss: 0.2810633182525635\n",
      "Step: 2100  \tTraining loss: 0.30485260486602783\n",
      "Step: 2100  \tTraining accuracy: 0.8623926043510437\n",
      "Step: 2100  \tValid loss: 0.28060248494148254\n",
      "Step: 2200  \tTraining loss: 0.3043364882469177\n",
      "Step: 2200  \tTraining accuracy: 0.8628412485122681\n",
      "Step: 2200  \tValid loss: 0.28013893961906433\n",
      "Step: 2300  \tTraining loss: 0.3038206994533539\n",
      "Step: 2300  \tTraining accuracy: 0.8632250428199768\n",
      "Step: 2300  \tValid loss: 0.279677152633667\n",
      "Step: 2400  \tTraining loss: 0.3033001720905304\n",
      "Step: 2400  \tTraining accuracy: 0.8635522723197937\n",
      "Step: 2400  \tValid loss: 0.2792116105556488\n",
      "Step: 2500  \tTraining loss: 0.30277785658836365\n",
      "Step: 2500  \tTraining accuracy: 0.8638619184494019\n",
      "Step: 2500  \tValid loss: 0.278743177652359\n",
      "Step: 2600  \tTraining loss: 0.30225634574890137\n",
      "Step: 2600  \tTraining accuracy: 0.8641341328620911\n",
      "Step: 2600  \tValid loss: 0.2782810926437378\n",
      "Step: 2700  \tTraining loss: 0.301736980676651\n",
      "Step: 2700  \tTraining accuracy: 0.864364504814148\n",
      "Step: 2700  \tValid loss: 0.27782630920410156\n",
      "Step: 2800  \tTraining loss: 0.3012278974056244\n",
      "Step: 2800  \tTraining accuracy: 0.8645836114883423\n",
      "Step: 2800  \tValid loss: 0.2773575186729431\n",
      "Step: 2900  \tTraining loss: 0.30073943734169006\n",
      "Step: 2900  \tTraining accuracy: 0.8647978901863098\n",
      "Step: 2900  \tValid loss: 0.27691495418548584\n",
      "Step: 3000  \tTraining loss: 0.30027079582214355\n",
      "Step: 3000  \tTraining accuracy: 0.8650178909301758\n",
      "Step: 3000  \tValid loss: 0.27649274468421936\n",
      "Step: 3100  \tTraining loss: 0.2998279631137848\n",
      "Step: 3100  \tTraining accuracy: 0.8652419447898865\n",
      "Step: 3100  \tValid loss: 0.2761261761188507\n",
      "Step: 3200  \tTraining loss: 0.2994122803211212\n",
      "Step: 3200  \tTraining accuracy: 0.865470826625824\n",
      "Step: 3200  \tValid loss: 0.27578628063201904\n",
      "Step: 3300  \tTraining loss: 0.2990220785140991\n",
      "Step: 3300  \tTraining accuracy: 0.8656890988349915\n",
      "Step: 3300  \tValid loss: 0.2754700481891632\n",
      "Step: 3400  \tTraining loss: 0.29865723848342896\n",
      "Step: 3400  \tTraining accuracy: 0.8658887147903442\n",
      "Step: 3400  \tValid loss: 0.2751842439174652\n",
      "Step: 3500  \tTraining loss: 0.2983163893222809\n",
      "Step: 3500  \tTraining accuracy: 0.8660767674446106\n",
      "Step: 3500  \tValid loss: 0.2749190032482147\n",
      "Step: 3600  \tTraining loss: 0.2979974150657654\n",
      "Step: 3600  \tTraining accuracy: 0.8662562966346741\n",
      "Step: 3600  \tValid loss: 0.27467796206474304\n",
      "Step: 3700  \tTraining loss: 0.2976871132850647\n",
      "Step: 3700  \tTraining accuracy: 0.8664209246635437\n",
      "Step: 3700  \tValid loss: 0.27443066239356995\n",
      "Step: 3800  \tTraining loss: 0.2973997890949249\n",
      "Step: 3800  \tTraining accuracy: 0.8665717244148254\n",
      "Step: 3800  \tValid loss: 0.2742408812046051\n",
      "Step: 3900  \tTraining loss: 0.29712599515914917\n",
      "Step: 3900  \tTraining accuracy: 0.866715669631958\n",
      "Step: 3900  \tValid loss: 0.2740587890148163\n",
      "Step: 4000  \tTraining loss: 0.2968669533729553\n",
      "Step: 4000  \tTraining accuracy: 0.8668627738952637\n",
      "Step: 4000  \tValid loss: 0.27389341592788696\n",
      "Step: 4100  \tTraining loss: 0.29662346839904785\n",
      "Step: 4100  \tTraining accuracy: 0.8670072555541992\n",
      "Step: 4100  \tValid loss: 0.2737460136413574\n",
      "Step: 4200  \tTraining loss: 0.29639366269111633\n",
      "Step: 4200  \tTraining accuracy: 0.8671402335166931\n",
      "Step: 4200  \tValid loss: 0.27361106872558594\n",
      "Step: 4300  \tTraining loss: 0.2961752414703369\n",
      "Step: 4300  \tTraining accuracy: 0.8672687411308289\n",
      "Step: 4300  \tValid loss: 0.2734799385070801\n",
      "Step: 4400  \tTraining loss: 0.29596394300460815\n",
      "Step: 4400  \tTraining accuracy: 0.8674007654190063\n",
      "Step: 4400  \tValid loss: 0.2733554244041443\n",
      "Step: 4500  \tTraining loss: 0.29575613141059875\n",
      "Step: 4500  \tTraining accuracy: 0.8675201535224915\n",
      "Step: 4500  \tValid loss: 0.27323758602142334\n",
      "Step: 4600  \tTraining loss: 0.29554101824760437\n",
      "Step: 4600  \tTraining accuracy: 0.8676310181617737\n",
      "Step: 4600  \tValid loss: 0.27310433983802795\n",
      "Step: 4700  \tTraining loss: 0.29529088735580444\n",
      "Step: 4700  \tTraining accuracy: 0.8677330613136292\n",
      "Step: 4700  \tValid loss: 0.2729245722293854\n",
      "Step: 4800  \tTraining loss: 0.2950098216533661\n",
      "Step: 4800  \tTraining accuracy: 0.8678331971168518\n",
      "Step: 4800  \tValid loss: 0.2727183699607849\n",
      "Step: 4900  \tTraining loss: 0.29475536942481995\n",
      "Step: 4900  \tTraining accuracy: 0.867926836013794\n",
      "Step: 4900  \tValid loss: 0.27257663011550903\n",
      "Step: 5000  \tTraining loss: 0.29451945424079895\n",
      "Step: 5000  \tTraining accuracy: 0.8680213093757629\n",
      "Step: 5000  \tValid loss: 0.2724634110927582\n",
      "Step: 5100  \tTraining loss: 0.29429617524147034\n",
      "Step: 5100  \tTraining accuracy: 0.8681246042251587\n",
      "Step: 5100  \tValid loss: 0.27236899733543396\n",
      "Step: 5200  \tTraining loss: 0.29394766688346863\n",
      "Step: 5200  \tTraining accuracy: 0.8682304620742798\n",
      "Step: 5200  \tValid loss: 0.2721642255783081\n",
      "Step: 5300  \tTraining loss: 0.2934703826904297\n",
      "Step: 5300  \tTraining accuracy: 0.8683451414108276\n",
      "Step: 5300  \tValid loss: 0.27178242802619934\n",
      "Step: 5400  \tTraining loss: 0.2929818034172058\n",
      "Step: 5400  \tTraining accuracy: 0.868467390537262\n",
      "Step: 5400  \tValid loss: 0.27133625745773315\n",
      "Step: 5500  \tTraining loss: 0.2924220860004425\n",
      "Step: 5500  \tTraining accuracy: 0.8685852289199829\n",
      "Step: 5500  \tValid loss: 0.27078840136528015\n",
      "Step: 5600  \tTraining loss: 0.2920084297657013\n",
      "Step: 5600  \tTraining accuracy: 0.8687014579772949\n",
      "Step: 5600  \tValid loss: 0.27041521668434143\n",
      "Step: 5700  \tTraining loss: 0.29166415333747864\n",
      "Step: 5700  \tTraining accuracy: 0.8688142895698547\n",
      "Step: 5700  \tValid loss: 0.2701621949672699\n",
      "Step: 5800  \tTraining loss: 0.2913648784160614\n",
      "Step: 5800  \tTraining accuracy: 0.8689231276512146\n",
      "Step: 5800  \tValid loss: 0.2699827253818512\n",
      "Step: 5900  \tTraining loss: 0.2911047637462616\n",
      "Step: 5900  \tTraining accuracy: 0.8690257668495178\n",
      "Step: 5900  \tValid loss: 0.26982223987579346\n",
      "Step: 6000  \tTraining loss: 0.29087430238723755\n",
      "Step: 6000  \tTraining accuracy: 0.8691217303276062\n",
      "Step: 6000  \tValid loss: 0.2696940004825592\n",
      "Step: 6100  \tTraining loss: 0.2906688451766968\n",
      "Step: 6100  \tTraining accuracy: 0.8692182898521423\n",
      "Step: 6100  \tValid loss: 0.2695766091346741\n",
      "Step: 6200  \tTraining loss: 0.29048240184783936\n",
      "Step: 6200  \tTraining accuracy: 0.869306206703186\n",
      "Step: 6200  \tValid loss: 0.26949363946914673\n",
      "Step: 6300  \tTraining loss: 0.290311723947525\n",
      "Step: 6300  \tTraining accuracy: 0.8693907260894775\n",
      "Step: 6300  \tValid loss: 0.2694181799888611\n",
      "Step: 6400  \tTraining loss: 0.290149062871933\n",
      "Step: 6400  \tTraining accuracy: 0.8694760799407959\n",
      "Step: 6400  \tValid loss: 0.2693585455417633\n",
      "Step: 6500  \tTraining loss: 0.28999367356300354\n",
      "Step: 6500  \tTraining accuracy: 0.8695605397224426\n",
      "Step: 6500  \tValid loss: 0.2693108022212982\n",
      "Step: 6600  \tTraining loss: 0.2898435592651367\n",
      "Step: 6600  \tTraining accuracy: 0.8696482181549072\n",
      "Step: 6600  \tValid loss: 0.2692525088787079\n",
      "Step: 6700  \tTraining loss: 0.2896982431411743\n",
      "Step: 6700  \tTraining accuracy: 0.869742214679718\n",
      "Step: 6700  \tValid loss: 0.26917505264282227\n",
      "Step: 6800  \tTraining loss: 0.28954577445983887\n",
      "Step: 6800  \tTraining accuracy: 0.869846761226654\n",
      "Step: 6800  \tValid loss: 0.26910170912742615\n",
      "Step: 6900  \tTraining loss: 0.2894074022769928\n",
      "Step: 6900  \tTraining accuracy: 0.8699488043785095\n",
      "Step: 6900  \tValid loss: 0.2690685987472534\n",
      "Step: 7000  \tTraining loss: 0.289277583360672\n",
      "Step: 7000  \tTraining accuracy: 0.8700354695320129\n",
      "Step: 7000  \tValid loss: 0.2690322697162628\n",
      "Step: 7100  \tTraining loss: 0.289151668548584\n",
      "Step: 7100  \tTraining accuracy: 0.8701112270355225\n",
      "Step: 7100  \tValid loss: 0.26897624135017395\n",
      "Step: 7200  \tTraining loss: 0.2890244126319885\n",
      "Step: 7200  \tTraining accuracy: 0.8701843023300171\n",
      "Step: 7200  \tValid loss: 0.2689691483974457\n",
      "Step: 7300  \tTraining loss: 0.288904070854187\n",
      "Step: 7300  \tTraining accuracy: 0.870252788066864\n",
      "Step: 7300  \tValid loss: 0.26892876625061035\n",
      "Step: 7400  \tTraining loss: 0.28878551721572876\n",
      "Step: 7400  \tTraining accuracy: 0.8703199625015259\n",
      "Step: 7400  \tValid loss: 0.26888105273246765\n",
      "Step: 7500  \tTraining loss: 0.2886669933795929\n",
      "Step: 7500  \tTraining accuracy: 0.8703852891921997\n",
      "Step: 7500  \tValid loss: 0.2688724100589752\n",
      "Step: 7600  \tTraining loss: 0.2885517179965973\n",
      "Step: 7600  \tTraining accuracy: 0.8704493641853333\n",
      "Step: 7600  \tValid loss: 0.26883837580680847\n",
      "Step: 7700  \tTraining loss: 0.2884395718574524\n",
      "Step: 7700  \tTraining accuracy: 0.8705112934112549\n",
      "Step: 7700  \tValid loss: 0.2687917649745941\n",
      "Step: 7800  \tTraining loss: 0.28832805156707764\n",
      "Step: 7800  \tTraining accuracy: 0.8705716729164124\n",
      "Step: 7800  \tValid loss: 0.2687520682811737\n",
      "Step: 7900  \tTraining loss: 0.28821057081222534\n",
      "Step: 7900  \tTraining accuracy: 0.8706309199333191\n",
      "Step: 7900  \tValid loss: 0.2686575651168823\n",
      "Step: 8000  \tTraining loss: 0.2880992591381073\n",
      "Step: 8000  \tTraining accuracy: 0.8706892132759094\n",
      "Step: 8000  \tValid loss: 0.2686169743537903\n",
      "Step: 8100  \tTraining loss: 0.28798991441726685\n",
      "Step: 8100  \tTraining accuracy: 0.8707460165023804\n",
      "Step: 8100  \tValid loss: 0.2685815691947937\n",
      "Step: 8200  \tTraining loss: 0.28788208961486816\n",
      "Step: 8200  \tTraining accuracy: 0.8708027601242065\n",
      "Step: 8200  \tValid loss: 0.26850801706314087\n",
      "Step: 8300  \tTraining loss: 0.2877727746963501\n",
      "Step: 8300  \tTraining accuracy: 0.8708590865135193\n",
      "Step: 8300  \tValid loss: 0.2684803307056427\n",
      "Step: 8400  \tTraining loss: 0.28766530752182007\n",
      "Step: 8400  \tTraining accuracy: 0.870914101600647\n",
      "Step: 8400  \tValid loss: 0.26843389868736267\n",
      "Step: 8500  \tTraining loss: 0.28755995631217957\n",
      "Step: 8500  \tTraining accuracy: 0.8709655404090881\n",
      "Step: 8500  \tValid loss: 0.2683948576450348\n",
      "Step: 8600  \tTraining loss: 0.2874559164047241\n",
      "Step: 8600  \tTraining accuracy: 0.871016263961792\n",
      "Step: 8600  \tValid loss: 0.26833656430244446\n",
      "Step: 8700  \tTraining loss: 0.28735339641571045\n",
      "Step: 8700  \tTraining accuracy: 0.8710640072822571\n",
      "Step: 8700  \tValid loss: 0.26828521490097046\n",
      "Step: 8800  \tTraining loss: 0.28725340962409973\n",
      "Step: 8800  \tTraining accuracy: 0.8711099028587341\n",
      "Step: 8800  \tValid loss: 0.2682327330112457\n",
      "Step: 8900  \tTraining loss: 0.28715232014656067\n",
      "Step: 8900  \tTraining accuracy: 0.8711538314819336\n",
      "Step: 8900  \tValid loss: 0.2681865692138672\n",
      "Step: 9000  \tTraining loss: 0.2870529294013977\n",
      "Step: 9000  \tTraining accuracy: 0.8712013959884644\n",
      "Step: 9000  \tValid loss: 0.26813510060310364\n",
      "Step: 9100  \tTraining loss: 0.28695619106292725\n",
      "Step: 9100  \tTraining accuracy: 0.8712500333786011\n",
      "Step: 9100  \tValid loss: 0.2680901288986206\n",
      "Step: 9200  \tTraining loss: 0.28685957193374634\n",
      "Step: 9200  \tTraining accuracy: 0.8712979555130005\n",
      "Step: 9200  \tValid loss: 0.26803529262542725\n",
      "Step: 9300  \tTraining loss: 0.28676310181617737\n",
      "Step: 9300  \tTraining accuracy: 0.8713428378105164\n",
      "Step: 9300  \tValid loss: 0.2679838538169861\n",
      "Step: 9400  \tTraining loss: 0.2866702973842621\n",
      "Step: 9400  \tTraining accuracy: 0.871383547782898\n",
      "Step: 9400  \tValid loss: 0.2679345905780792\n",
      "Step: 9500  \tTraining loss: 0.28657400608062744\n",
      "Step: 9500  \tTraining accuracy: 0.8714230060577393\n",
      "Step: 9500  \tValid loss: 0.26788848638534546\n",
      "Step: 9600  \tTraining loss: 0.2864815592765808\n",
      "Step: 9600  \tTraining accuracy: 0.8714635968208313\n",
      "Step: 9600  \tValid loss: 0.26783785223960876\n",
      "Step: 9700  \tTraining loss: 0.2863909602165222\n",
      "Step: 9700  \tTraining accuracy: 0.8715029954910278\n",
      "Step: 9700  \tValid loss: 0.2677821218967438\n",
      "Step: 9800  \tTraining loss: 0.28630027174949646\n",
      "Step: 9800  \tTraining accuracy: 0.8715426921844482\n",
      "Step: 9800  \tValid loss: 0.2677302956581116\n",
      "Step: 9900  \tTraining loss: 0.2862115800380707\n",
      "Step: 9900  \tTraining accuracy: 0.8715816140174866\n",
      "Step: 9900  \tValid loss: 0.267673522233963\n",
      "Step: 10000  \tTraining loss: 0.2861279845237732\n",
      "Step: 10000  \tTraining accuracy: 0.8716204762458801\n",
      "Step: 10000  \tValid loss: 0.2676013112068176\n",
      "Step: 10100  \tTraining loss: 0.28603675961494446\n",
      "Step: 10100  \tTraining accuracy: 0.8716596961021423\n",
      "Step: 10100  \tValid loss: 0.26756754517555237\n",
      "Step: 10200  \tTraining loss: 0.28594788908958435\n",
      "Step: 10200  \tTraining accuracy: 0.8717014789581299\n",
      "Step: 10200  \tValid loss: 0.26750603318214417\n",
      "Step: 10300  \tTraining loss: 0.2858579754829407\n",
      "Step: 10300  \tTraining accuracy: 0.8717453479766846\n",
      "Step: 10300  \tValid loss: 0.2674087584018707\n",
      "Step: 10400  \tTraining loss: 0.2857707142829895\n",
      "Step: 10400  \tTraining accuracy: 0.8717913031578064\n",
      "Step: 10400  \tValid loss: 0.2673502266407013\n",
      "Step: 10500  \tTraining loss: 0.2856844961643219\n",
      "Step: 10500  \tTraining accuracy: 0.8718363046646118\n",
      "Step: 10500  \tValid loss: 0.2672940194606781\n",
      "Step: 10600  \tTraining loss: 0.2856021225452423\n",
      "Step: 10600  \tTraining accuracy: 0.8718801736831665\n",
      "Step: 10600  \tValid loss: 0.2672369182109833\n",
      "Step: 10700  \tTraining loss: 0.28551939129829407\n",
      "Step: 10700  \tTraining accuracy: 0.871923565864563\n",
      "Step: 10700  \tValid loss: 0.26717472076416016\n",
      "Step: 10800  \tTraining loss: 0.28543075919151306\n",
      "Step: 10800  \tTraining accuracy: 0.8719667792320251\n",
      "Step: 10800  \tValid loss: 0.2670231759548187\n",
      "Step: 10900  \tTraining loss: 0.2853185534477234\n",
      "Step: 10900  \tTraining accuracy: 0.87200927734375\n",
      "Step: 10900  \tValid loss: 0.26689696311950684\n",
      "Step: 11000  \tTraining loss: 0.2852262258529663\n",
      "Step: 11000  \tTraining accuracy: 0.8720458149909973\n",
      "Step: 11000  \tValid loss: 0.2667935788631439\n",
      "Step: 11100  \tTraining loss: 0.28514617681503296\n",
      "Step: 11100  \tTraining accuracy: 0.8720813989639282\n",
      "Step: 11100  \tValid loss: 0.2667619585990906\n",
      "Step: 11200  \tTraining loss: 0.28506436944007874\n",
      "Step: 11200  \tTraining accuracy: 0.8721153140068054\n",
      "Step: 11200  \tValid loss: 0.2667282819747925\n",
      "Step: 11300  \tTraining loss: 0.2849869728088379\n",
      "Step: 11300  \tTraining accuracy: 0.8721479177474976\n",
      "Step: 11300  \tValid loss: 0.2666758596897125\n",
      "Step: 11400  \tTraining loss: 0.2849118113517761\n",
      "Step: 11400  \tTraining accuracy: 0.8721776604652405\n",
      "Step: 11400  \tValid loss: 0.2666466534137726\n",
      "Step: 11500  \tTraining loss: 0.28483763337135315\n",
      "Step: 11500  \tTraining accuracy: 0.8722082376480103\n",
      "Step: 11500  \tValid loss: 0.26661428809165955\n",
      "Step: 11600  \tTraining loss: 0.2847598195075989\n",
      "Step: 11600  \tTraining accuracy: 0.8722427487373352\n",
      "Step: 11600  \tValid loss: 0.2665444016456604\n",
      "Step: 11700  \tTraining loss: 0.28468388319015503\n",
      "Step: 11700  \tTraining accuracy: 0.8722819089889526\n",
      "Step: 11700  \tValid loss: 0.26651009917259216\n",
      "Step: 11800  \tTraining loss: 0.2846115529537201\n",
      "Step: 11800  \tTraining accuracy: 0.872323215007782\n",
      "Step: 11800  \tValid loss: 0.266478031873703\n",
      "Step: 11900  \tTraining loss: 0.28454142808914185\n",
      "Step: 11900  \tTraining accuracy: 0.8723647594451904\n",
      "Step: 11900  \tValid loss: 0.2664456367492676\n",
      "Step: 12000  \tTraining loss: 0.28447049856185913\n",
      "Step: 12000  \tTraining accuracy: 0.8724053502082825\n",
      "Step: 12000  \tValid loss: 0.2664031982421875\n",
      "Step: 12100  \tTraining loss: 0.2844015657901764\n",
      "Step: 12100  \tTraining accuracy: 0.8724455237388611\n",
      "Step: 12100  \tValid loss: 0.2663789391517639\n",
      "Step: 12200  \tTraining loss: 0.2843347489833832\n",
      "Step: 12200  \tTraining accuracy: 0.8724850416183472\n",
      "Step: 12200  \tValid loss: 0.2663387656211853\n",
      "Step: 12300  \tTraining loss: 0.28426796197891235\n",
      "Step: 12300  \tTraining accuracy: 0.8725239634513855\n",
      "Step: 12300  \tValid loss: 0.2662978768348694\n",
      "Step: 12400  \tTraining loss: 0.284201055765152\n",
      "Step: 12400  \tTraining accuracy: 0.8725640177726746\n",
      "Step: 12400  \tValid loss: 0.2662888169288635\n",
      "Step: 12500  \tTraining loss: 0.2841339111328125\n",
      "Step: 12500  \tTraining accuracy: 0.8726046681404114\n",
      "Step: 12500  \tValid loss: 0.2662472128868103\n",
      "Step: 12600  \tTraining loss: 0.28406867384910583\n",
      "Step: 12600  \tTraining accuracy: 0.8726446628570557\n",
      "Step: 12600  \tValid loss: 0.2662149965763092\n",
      "Step: 12700  \tTraining loss: 0.2840038537979126\n",
      "Step: 12700  \tTraining accuracy: 0.8726840019226074\n",
      "Step: 12700  \tValid loss: 0.26618438959121704\n",
      "Step: 12800  \tTraining loss: 0.28393974900245667\n",
      "Step: 12800  \tTraining accuracy: 0.8727218508720398\n",
      "Step: 12800  \tValid loss: 0.26615431904792786\n",
      "Step: 12900  \tTraining loss: 0.28387758135795593\n",
      "Step: 12900  \tTraining accuracy: 0.872759997844696\n",
      "Step: 12900  \tValid loss: 0.26610058546066284\n",
      "Step: 13000  \tTraining loss: 0.2838180363178253\n",
      "Step: 13000  \tTraining accuracy: 0.8727970123291016\n",
      "Step: 13000  \tValid loss: 0.26606568694114685\n",
      "Step: 13100  \tTraining loss: 0.28375735878944397\n",
      "Step: 13100  \tTraining accuracy: 0.8728334307670593\n",
      "Step: 13100  \tValid loss: 0.266018807888031\n",
      "Step: 13200  \tTraining loss: 0.2836982011795044\n",
      "Step: 13200  \tTraining accuracy: 0.872870683670044\n",
      "Step: 13200  \tValid loss: 0.26598119735717773\n",
      "Step: 13300  \tTraining loss: 0.2836403548717499\n",
      "Step: 13300  \tTraining accuracy: 0.8729079961776733\n",
      "Step: 13300  \tValid loss: 0.2659514248371124\n",
      "Step: 13400  \tTraining loss: 0.2835823893547058\n",
      "Step: 13400  \tTraining accuracy: 0.8729458451271057\n",
      "Step: 13400  \tValid loss: 0.2659190595149994\n",
      "Step: 13500  \tTraining loss: 0.2835261821746826\n",
      "Step: 13500  \tTraining accuracy: 0.8729833960533142\n",
      "Step: 13500  \tValid loss: 0.26586997509002686\n",
      "Step: 13600  \tTraining loss: 0.28347060084342957\n",
      "Step: 13600  \tTraining accuracy: 0.8730195760726929\n",
      "Step: 13600  \tValid loss: 0.26583507657051086\n",
      "Step: 13700  \tTraining loss: 0.28341639041900635\n",
      "Step: 13700  \tTraining accuracy: 0.8730546832084656\n",
      "Step: 13700  \tValid loss: 0.2658022344112396\n",
      "Step: 13800  \tTraining loss: 0.2833631634712219\n",
      "Step: 13800  \tTraining accuracy: 0.8730887174606323\n",
      "Step: 13800  \tValid loss: 0.26575884222984314\n",
      "Step: 13900  \tTraining loss: 0.28330835700035095\n",
      "Step: 13900  \tTraining accuracy: 0.8731220364570618\n",
      "Step: 13900  \tValid loss: 0.2657316327095032\n",
      "Step: 14000  \tTraining loss: 0.2832571864128113\n",
      "Step: 14000  \tTraining accuracy: 0.8731551170349121\n",
      "Step: 14000  \tValid loss: 0.2656938433647156\n",
      "Step: 14100  \tTraining loss: 0.2832029461860657\n",
      "Step: 14100  \tTraining accuracy: 0.8731882572174072\n",
      "Step: 14100  \tValid loss: 0.2656519114971161\n",
      "Step: 14200  \tTraining loss: 0.28315040469169617\n",
      "Step: 14200  \tTraining accuracy: 0.8732200860977173\n",
      "Step: 14200  \tValid loss: 0.26561200618743896\n",
      "Step: 14300  \tTraining loss: 0.2830981910228729\n",
      "Step: 14300  \tTraining accuracy: 0.8732517957687378\n",
      "Step: 14300  \tValid loss: 0.26556509733200073\n",
      "Step: 14400  \tTraining loss: 0.28304705023765564\n",
      "Step: 14400  \tTraining accuracy: 0.873281717300415\n",
      "Step: 14400  \tValid loss: 0.2655203640460968\n",
      "Step: 14500  \tTraining loss: 0.28299662470817566\n",
      "Step: 14500  \tTraining accuracy: 0.8733122944831848\n",
      "Step: 14500  \tValid loss: 0.26547032594680786\n",
      "Step: 14600  \tTraining loss: 0.2829461693763733\n",
      "Step: 14600  \tTraining accuracy: 0.8733426928520203\n",
      "Step: 14600  \tValid loss: 0.26541972160339355\n",
      "Step: 14700  \tTraining loss: 0.2828969657421112\n",
      "Step: 14700  \tTraining accuracy: 0.8733721971511841\n",
      "Step: 14700  \tValid loss: 0.26540011167526245\n",
      "Step: 14800  \tTraining loss: 0.2828465700149536\n",
      "Step: 14800  \tTraining accuracy: 0.8734007477760315\n",
      "Step: 14800  \tValid loss: 0.2653577923774719\n",
      "Step: 14900  \tTraining loss: 0.2827984094619751\n",
      "Step: 14900  \tTraining accuracy: 0.8734297156333923\n",
      "Step: 14900  \tValid loss: 0.26531925797462463\n",
      "Step: 15000  \tTraining loss: 0.282748818397522\n",
      "Step: 15000  \tTraining accuracy: 0.8734577298164368\n",
      "Step: 15000  \tValid loss: 0.2652831971645355\n",
      "Step: 15100  \tTraining loss: 0.28269702196121216\n",
      "Step: 15100  \tTraining accuracy: 0.8734866976737976\n",
      "Step: 15100  \tValid loss: 0.2652251720428467\n",
      "Step: 15200  \tTraining loss: 0.2826484441757202\n",
      "Step: 15200  \tTraining accuracy: 0.8735162019729614\n",
      "Step: 15200  \tValid loss: 0.2651819884777069\n",
      "Step: 15300  \tTraining loss: 0.28260132670402527\n",
      "Step: 15300  \tTraining accuracy: 0.8735461235046387\n",
      "Step: 15300  \tValid loss: 0.26515188813209534\n",
      "Step: 15400  \tTraining loss: 0.28255271911621094\n",
      "Step: 15400  \tTraining accuracy: 0.8735777735710144\n",
      "Step: 15400  \tValid loss: 0.2651229798793793\n",
      "Step: 15500  \tTraining loss: 0.2825055718421936\n",
      "Step: 15500  \tTraining accuracy: 0.8736088275909424\n",
      "Step: 15500  \tValid loss: 0.26508620381355286\n",
      "Step: 15600  \tTraining loss: 0.28245964646339417\n",
      "Step: 15600  \tTraining accuracy: 0.8736394643783569\n",
      "Step: 15600  \tValid loss: 0.265059232711792\n",
      "Step: 15700  \tTraining loss: 0.28241437673568726\n",
      "Step: 15700  \tTraining accuracy: 0.8736697435379028\n",
      "Step: 15700  \tValid loss: 0.2650270164012909\n",
      "Step: 15800  \tTraining loss: 0.2823683023452759\n",
      "Step: 15800  \tTraining accuracy: 0.8737003207206726\n",
      "Step: 15800  \tValid loss: 0.2649909555912018\n",
      "Step: 15900  \tTraining loss: 0.2823232114315033\n",
      "Step: 15900  \tTraining accuracy: 0.87373286485672\n",
      "Step: 15900  \tValid loss: 0.26496705412864685\n",
      "Step: 16000  \tTraining loss: 0.2822786271572113\n",
      "Step: 16000  \tTraining accuracy: 0.8737671375274658\n",
      "Step: 16000  \tValid loss: 0.2649395763874054\n",
      "Step: 16100  \tTraining loss: 0.28223374485969543\n",
      "Step: 16100  \tTraining accuracy: 0.8738004565238953\n",
      "Step: 16100  \tValid loss: 0.2649149000644684\n",
      "Step: 16200  \tTraining loss: 0.2821893095970154\n",
      "Step: 16200  \tTraining accuracy: 0.873833417892456\n",
      "Step: 16200  \tValid loss: 0.2648901045322418\n",
      "Step: 16300  \tTraining loss: 0.28214573860168457\n",
      "Step: 16300  \tTraining accuracy: 0.8738659620285034\n",
      "Step: 16300  \tValid loss: 0.2648703157901764\n",
      "Step: 16400  \tTraining loss: 0.28210219740867615\n",
      "Step: 16400  \tTraining accuracy: 0.8738981485366821\n",
      "Step: 16400  \tValid loss: 0.26484617590904236\n",
      "Step: 16500  \tTraining loss: 0.2820579409599304\n",
      "Step: 16500  \tTraining accuracy: 0.8739312887191772\n",
      "Step: 16500  \tValid loss: 0.26484414935112\n",
      "Step: 16600  \tTraining loss: 0.28201693296432495\n",
      "Step: 16600  \tTraining accuracy: 0.873964250087738\n",
      "Step: 16600  \tValid loss: 0.26480019092559814\n",
      "Step: 16700  \tTraining loss: 0.28197434544563293\n",
      "Step: 16700  \tTraining accuracy: 0.8739970326423645\n",
      "Step: 16700  \tValid loss: 0.26477715373039246\n",
      "Step: 16800  \tTraining loss: 0.2819293439388275\n",
      "Step: 16800  \tTraining accuracy: 0.8740291595458984\n",
      "Step: 16800  \tValid loss: 0.2647548317909241\n",
      "Step: 16900  \tTraining loss: 0.28188958764076233\n",
      "Step: 16900  \tTraining accuracy: 0.8740614056587219\n",
      "Step: 16900  \tValid loss: 0.26474684476852417\n",
      "Step: 17000  \tTraining loss: 0.28184598684310913\n",
      "Step: 17000  \tTraining accuracy: 0.8740939497947693\n",
      "Step: 17000  \tValid loss: 0.2647453248500824\n",
      "Step: 17100  \tTraining loss: 0.2818014919757843\n",
      "Step: 17100  \tTraining accuracy: 0.8741264939308167\n",
      "Step: 17100  \tValid loss: 0.2647130489349365\n",
      "Step: 17200  \tTraining loss: 0.281759113073349\n",
      "Step: 17200  \tTraining accuracy: 0.8741586804389954\n",
      "Step: 17200  \tValid loss: 0.264692485332489\n",
      "Step: 17300  \tTraining loss: 0.2817172706127167\n",
      "Step: 17300  \tTraining accuracy: 0.874190092086792\n",
      "Step: 17300  \tValid loss: 0.26468008756637573\n",
      "Step: 17400  \tTraining loss: 0.28167685866355896\n",
      "Step: 17400  \tTraining accuracy: 0.8742217421531677\n",
      "Step: 17400  \tValid loss: 0.2646521031856537\n",
      "Step: 17500  \tTraining loss: 0.28163665533065796\n",
      "Step: 17500  \tTraining accuracy: 0.874253511428833\n",
      "Step: 17500  \tValid loss: 0.2646474242210388\n",
      "Step: 17600  \tTraining loss: 0.28159573674201965\n",
      "Step: 17600  \tTraining accuracy: 0.8742855191230774\n",
      "Step: 17600  \tValid loss: 0.26460421085357666\n",
      "Step: 17700  \tTraining loss: 0.28155648708343506\n",
      "Step: 17700  \tTraining accuracy: 0.8743182420730591\n",
      "Step: 17700  \tValid loss: 0.26459527015686035\n",
      "Step: 17800  \tTraining loss: 0.2815169394016266\n",
      "Step: 17800  \tTraining accuracy: 0.8743505477905273\n",
      "Step: 17800  \tValid loss: 0.26456189155578613\n",
      "Step: 17900  \tTraining loss: 0.28147807717323303\n",
      "Step: 17900  \tTraining accuracy: 0.8743821382522583\n",
      "Step: 17900  \tValid loss: 0.264545202255249\n",
      "Step: 18000  \tTraining loss: 0.28143933415412903\n",
      "Step: 18000  \tTraining accuracy: 0.8744133710861206\n",
      "Step: 18000  \tValid loss: 0.2645331919193268\n",
      "Step: 18100  \tTraining loss: 0.2814027667045593\n",
      "Step: 18100  \tTraining accuracy: 0.8744435906410217\n",
      "Step: 18100  \tValid loss: 0.2645284831523895\n",
      "Step: 18200  \tTraining loss: 0.28136348724365234\n",
      "Step: 18200  \tTraining accuracy: 0.8744739294052124\n",
      "Step: 18200  \tValid loss: 0.2644835412502289\n",
      "Step: 18300  \tTraining loss: 0.28132590651512146\n",
      "Step: 18300  \tTraining accuracy: 0.8745045065879822\n",
      "Step: 18300  \tValid loss: 0.264488160610199\n",
      "Step: 18400  \tTraining loss: 0.2812879681587219\n",
      "Step: 18400  \tTraining accuracy: 0.8745347857475281\n",
      "Step: 18400  \tValid loss: 0.2644631564617157\n",
      "Step: 18500  \tTraining loss: 0.28125089406967163\n",
      "Step: 18500  \tTraining accuracy: 0.8745647668838501\n",
      "Step: 18500  \tValid loss: 0.26444754004478455\n",
      "Step: 18600  \tTraining loss: 0.28121456503868103\n",
      "Step: 18600  \tTraining accuracy: 0.8745943903923035\n",
      "Step: 18600  \tValid loss: 0.26442399621009827\n",
      "Step: 18700  \tTraining loss: 0.2811800539493561\n",
      "Step: 18700  \tTraining accuracy: 0.8746234774589539\n",
      "Step: 18700  \tValid loss: 0.26439332962036133\n",
      "Step: 18800  \tTraining loss: 0.2811431884765625\n",
      "Step: 18800  \tTraining accuracy: 0.8746516704559326\n",
      "Step: 18800  \tValid loss: 0.26438644528388977\n",
      "Step: 18900  \tTraining loss: 0.2811059355735779\n",
      "Step: 18900  \tTraining accuracy: 0.8746811747550964\n",
      "Step: 18900  \tValid loss: 0.264377623796463\n",
      "Step: 19000  \tTraining loss: 0.2810707092285156\n",
      "Step: 19000  \tTraining accuracy: 0.8747105598449707\n",
      "Step: 19000  \tValid loss: 0.2643478810787201\n",
      "Step: 19100  \tTraining loss: 0.28103509545326233\n",
      "Step: 19100  \tTraining accuracy: 0.874739408493042\n",
      "Step: 19100  \tValid loss: 0.26434847712516785\n",
      "Step: 19200  \tTraining loss: 0.28100067377090454\n",
      "Step: 19200  \tTraining accuracy: 0.8747671842575073\n",
      "Step: 19200  \tValid loss: 0.264325350522995\n",
      "Step: 19300  \tTraining loss: 0.2809670567512512\n",
      "Step: 19300  \tTraining accuracy: 0.8747941255569458\n",
      "Step: 19300  \tValid loss: 0.26430726051330566\n",
      "Step: 19400  \tTraining loss: 0.28093111515045166\n",
      "Step: 19400  \tTraining accuracy: 0.8748193979263306\n",
      "Step: 19400  \tValid loss: 0.2642894685268402\n",
      "Step: 19500  \tTraining loss: 0.28089720010757446\n",
      "Step: 19500  \tTraining accuracy: 0.8748443722724915\n",
      "Step: 19500  \tValid loss: 0.264276921749115\n",
      "Step: 19600  \tTraining loss: 0.28086355328559875\n",
      "Step: 19600  \tTraining accuracy: 0.8748693466186523\n",
      "Step: 19600  \tValid loss: 0.2642439901828766\n",
      "Step: 19700  \tTraining loss: 0.2808303236961365\n",
      "Step: 19700  \tTraining accuracy: 0.8748947978019714\n",
      "Step: 19700  \tValid loss: 0.2642238140106201\n",
      "Step: 19800  \tTraining loss: 0.2807973027229309\n",
      "Step: 19800  \tTraining accuracy: 0.8749206066131592\n",
      "Step: 19800  \tValid loss: 0.26420867443084717\n",
      "Step: 19900  \tTraining loss: 0.28076523542404175\n",
      "Step: 19900  \tTraining accuracy: 0.874945342540741\n",
      "Step: 19900  \tValid loss: 0.26420217752456665\n",
      "Step: 20000  \tTraining loss: 0.28073155879974365\n",
      "Step: 20000  \tTraining accuracy: 0.8749688863754272\n",
      "Step: 20000  \tValid loss: 0.26416850090026855\n",
      "Step: 20100  \tTraining loss: 0.2806987166404724\n",
      "Step: 20100  \tTraining accuracy: 0.8749924302101135\n",
      "Step: 20100  \tValid loss: 0.26415395736694336\n",
      "Step: 20200  \tTraining loss: 0.280666321516037\n",
      "Step: 20200  \tTraining accuracy: 0.8750157356262207\n",
      "Step: 20200  \tValid loss: 0.2641333341598511\n",
      "Step: 20300  \tTraining loss: 0.28063729405403137\n",
      "Step: 20300  \tTraining accuracy: 0.8750388026237488\n",
      "Step: 20300  \tValid loss: 0.26412081718444824\n",
      "Step: 20400  \tTraining loss: 0.28060242533683777\n",
      "Step: 20400  \tTraining accuracy: 0.875060498714447\n",
      "Step: 20400  \tValid loss: 0.2641099989414215\n",
      "Step: 20500  \tTraining loss: 0.2805747091770172\n",
      "Step: 20500  \tTraining accuracy: 0.8750820159912109\n",
      "Step: 20500  \tValid loss: 0.2640902101993561\n",
      "Step: 20600  \tTraining loss: 0.28054073452949524\n",
      "Step: 20600  \tTraining accuracy: 0.8751051425933838\n",
      "Step: 20600  \tValid loss: 0.2640566825866699\n",
      "Step: 20700  \tTraining loss: 0.2805095314979553\n",
      "Step: 20700  \tTraining accuracy: 0.875128984451294\n",
      "Step: 20700  \tValid loss: 0.264048308134079\n",
      "Step: 20800  \tTraining loss: 0.2804816961288452\n",
      "Step: 20800  \tTraining accuracy: 0.8751512765884399\n",
      "Step: 20800  \tValid loss: 0.26401636004447937\n",
      "Step: 20900  \tTraining loss: 0.28045037388801575\n",
      "Step: 20900  \tTraining accuracy: 0.8751737475395203\n",
      "Step: 20900  \tValid loss: 0.2639833390712738\n",
      "Step: 21000  \tTraining loss: 0.2804189622402191\n",
      "Step: 21000  \tTraining accuracy: 0.8751972317695618\n",
      "Step: 21000  \tValid loss: 0.2639647424221039\n",
      "Step: 21100  \tTraining loss: 0.2803882658481598\n",
      "Step: 21100  \tTraining accuracy: 0.8752210736274719\n",
      "Step: 21100  \tValid loss: 0.2639639675617218\n",
      "Step: 21200  \tTraining loss: 0.2803620398044586\n",
      "Step: 21200  \tTraining accuracy: 0.8752446174621582\n",
      "Step: 21200  \tValid loss: 0.2639244496822357\n",
      "Step: 21300  \tTraining loss: 0.2803294360637665\n",
      "Step: 21300  \tTraining accuracy: 0.8752683401107788\n",
      "Step: 21300  \tValid loss: 0.2639217674732208\n",
      "Step: 21400  \tTraining loss: 0.28030022978782654\n",
      "Step: 21400  \tTraining accuracy: 0.8752920627593994\n",
      "Step: 21400  \tValid loss: 0.26388803124427795\n",
      "Step: 21500  \tTraining loss: 0.2802720367908478\n",
      "Step: 21500  \tTraining accuracy: 0.8753151297569275\n",
      "Step: 21500  \tValid loss: 0.26387912034988403\n",
      "Step: 21600  \tTraining loss: 0.280245840549469\n",
      "Step: 21600  \tTraining accuracy: 0.8753373026847839\n",
      "Step: 21600  \tValid loss: 0.2638415992259979\n",
      "Step: 21700  \tTraining loss: 0.2802150547504425\n",
      "Step: 21700  \tTraining accuracy: 0.8753591179847717\n",
      "Step: 21700  \tValid loss: 0.26382383704185486\n",
      "Step: 21800  \tTraining loss: 0.2801862955093384\n",
      "Step: 21800  \tTraining accuracy: 0.8753814697265625\n",
      "Step: 21800  \tValid loss: 0.26379749178886414\n",
      "Step: 21900  \tTraining loss: 0.28015652298927307\n",
      "Step: 21900  \tTraining accuracy: 0.8754037022590637\n",
      "Step: 21900  \tValid loss: 0.2637885510921478\n",
      "Step: 22000  \tTraining loss: 0.28012844920158386\n",
      "Step: 22000  \tTraining accuracy: 0.8754251003265381\n",
      "Step: 22000  \tValid loss: 0.2637537121772766\n",
      "Step: 22100  \tTraining loss: 0.2801007926464081\n",
      "Step: 22100  \tTraining accuracy: 0.8754461407661438\n",
      "Step: 22100  \tValid loss: 0.2637105882167816\n",
      "Step: 22200  \tTraining loss: 0.28007274866104126\n",
      "Step: 22200  \tTraining accuracy: 0.8754675984382629\n",
      "Step: 22200  \tValid loss: 0.2637038230895996\n",
      "Step: 22300  \tTraining loss: 0.28004583716392517\n",
      "Step: 22300  \tTraining accuracy: 0.8754897713661194\n",
      "Step: 22300  \tValid loss: 0.26369112730026245\n",
      "Step: 22400  \tTraining loss: 0.28001734614372253\n",
      "Step: 22400  \tTraining accuracy: 0.8755120635032654\n",
      "Step: 22400  \tValid loss: 0.26366502046585083\n",
      "Step: 22500  \tTraining loss: 0.27999016642570496\n",
      "Step: 22500  \tTraining accuracy: 0.8755339980125427\n",
      "Step: 22500  \tValid loss: 0.2636411786079407\n",
      "Step: 22600  \tTraining loss: 0.27996334433555603\n",
      "Step: 22600  \tTraining accuracy: 0.8755553960800171\n",
      "Step: 22600  \tValid loss: 0.2636227607727051\n",
      "Step: 22700  \tTraining loss: 0.27993884682655334\n",
      "Step: 22700  \tTraining accuracy: 0.8755772709846497\n",
      "Step: 22700  \tValid loss: 0.2635997533798218\n",
      "Step: 22800  \tTraining loss: 0.2799120545387268\n",
      "Step: 22800  \tTraining accuracy: 0.8755991458892822\n",
      "Step: 22800  \tValid loss: 0.26358580589294434\n",
      "Step: 22900  \tTraining loss: 0.2798841893672943\n",
      "Step: 22900  \tTraining accuracy: 0.8756203055381775\n",
      "Step: 22900  \tValid loss: 0.2635551691055298\n",
      "Step: 23000  \tTraining loss: 0.27985766530036926\n",
      "Step: 23000  \tTraining accuracy: 0.8756412863731384\n",
      "Step: 23000  \tValid loss: 0.26355019211769104\n",
      "Step: 23100  \tTraining loss: 0.27983197569847107\n",
      "Step: 23100  \tTraining accuracy: 0.8756626844406128\n",
      "Step: 23100  \tValid loss: 0.26352572441101074\n",
      "Step: 23200  \tTraining loss: 0.2798054814338684\n",
      "Step: 23200  \tTraining accuracy: 0.8756841421127319\n",
      "Step: 23200  \tValid loss: 0.26352065801620483\n",
      "Step: 23300  \tTraining loss: 0.27977997064590454\n",
      "Step: 23300  \tTraining accuracy: 0.875705361366272\n",
      "Step: 23300  \tValid loss: 0.2634899914264679\n",
      "Step: 23400  \tTraining loss: 0.27975448966026306\n",
      "Step: 23400  \tTraining accuracy: 0.8757262825965881\n",
      "Step: 23400  \tValid loss: 0.2634764015674591\n",
      "Step: 23500  \tTraining loss: 0.2797304093837738\n",
      "Step: 23500  \tTraining accuracy: 0.8757461905479431\n",
      "Step: 23500  \tValid loss: 0.2634497582912445\n",
      "Step: 23600  \tTraining loss: 0.27970418334007263\n",
      "Step: 23600  \tTraining accuracy: 0.8757659196853638\n",
      "Step: 23600  \tValid loss: 0.2634345293045044\n",
      "Step: 23700  \tTraining loss: 0.2796792685985565\n",
      "Step: 23700  \tTraining accuracy: 0.875785231590271\n",
      "Step: 23700  \tValid loss: 0.2634163498878479\n",
      "Step: 23800  \tTraining loss: 0.27965593338012695\n",
      "Step: 23800  \tTraining accuracy: 0.875804603099823\n",
      "Step: 23800  \tValid loss: 0.26338842511177063\n",
      "Step: 23900  \tTraining loss: 0.2796308994293213\n",
      "Step: 23900  \tTraining accuracy: 0.8758237361907959\n",
      "Step: 23900  \tValid loss: 0.2633805572986603\n",
      "Step: 24000  \tTraining loss: 0.2796025276184082\n",
      "Step: 24000  \tTraining accuracy: 0.8758417367935181\n",
      "Step: 24000  \tValid loss: 0.2633562982082367\n",
      "Step: 24100  \tTraining loss: 0.27957433462142944\n",
      "Step: 24100  \tTraining accuracy: 0.8758594393730164\n",
      "Step: 24100  \tValid loss: 0.2633267641067505\n",
      "Step: 24200  \tTraining loss: 0.2795458137989044\n",
      "Step: 24200  \tTraining accuracy: 0.8758774399757385\n",
      "Step: 24200  \tValid loss: 0.26329734921455383\n",
      "Step: 24300  \tTraining loss: 0.2795177698135376\n",
      "Step: 24300  \tTraining accuracy: 0.8758949637413025\n",
      "Step: 24300  \tValid loss: 0.2632942795753479\n",
      "Step: 24400  \tTraining loss: 0.27949267625808716\n",
      "Step: 24400  \tTraining accuracy: 0.8759128451347351\n",
      "Step: 24400  \tValid loss: 0.2632638216018677\n",
      "Step: 24500  \tTraining loss: 0.27946507930755615\n",
      "Step: 24500  \tTraining accuracy: 0.8759306073188782\n",
      "Step: 24500  \tValid loss: 0.2632414996623993\n",
      "Step: 24600  \tTraining loss: 0.2794394791126251\n",
      "Step: 24600  \tTraining accuracy: 0.8759477138519287\n",
      "Step: 24600  \tValid loss: 0.2632124423980713\n",
      "Step: 24700  \tTraining loss: 0.27941441535949707\n",
      "Step: 24700  \tTraining accuracy: 0.8759649991989136\n",
      "Step: 24700  \tValid loss: 0.26319366693496704\n",
      "Step: 24800  \tTraining loss: 0.27938881516456604\n",
      "Step: 24800  \tTraining accuracy: 0.8759822845458984\n",
      "Step: 24800  \tValid loss: 0.26318982243537903\n",
      "Step: 24900  \tTraining loss: 0.2793642282485962\n",
      "Step: 24900  \tTraining accuracy: 0.8759994506835938\n",
      "Step: 24900  \tValid loss: 0.26316118240356445\n",
      "Step: 25000  \tTraining loss: 0.2793388068675995\n",
      "Step: 25000  \tTraining accuracy: 0.8760167956352234\n",
      "Step: 25000  \tValid loss: 0.2631368339061737\n",
      "Step: 25100  \tTraining loss: 0.27931392192840576\n",
      "Step: 25100  \tTraining accuracy: 0.8760344386100769\n",
      "Step: 25100  \tValid loss: 0.2631218731403351\n",
      "Step: 25200  \tTraining loss: 0.27929121255874634\n",
      "Step: 25200  \tTraining accuracy: 0.8760522603988647\n",
      "Step: 25200  \tValid loss: 0.263110876083374\n",
      "Step: 25300  \tTraining loss: 0.279266357421875\n",
      "Step: 25300  \tTraining accuracy: 0.8760707974433899\n",
      "Step: 25300  \tValid loss: 0.26306378841400146\n",
      "Step: 25400  \tTraining loss: 0.27924275398254395\n",
      "Step: 25400  \tTraining accuracy: 0.8760894536972046\n",
      "Step: 25400  \tValid loss: 0.26305440068244934\n",
      "Step: 25500  \tTraining loss: 0.2792186141014099\n",
      "Step: 25500  \tTraining accuracy: 0.8761079907417297\n",
      "Step: 25500  \tValid loss: 0.2630443274974823\n",
      "Step: 25600  \tTraining loss: 0.27919501066207886\n",
      "Step: 25600  \tTraining accuracy: 0.8761264085769653\n",
      "Step: 25600  \tValid loss: 0.2630157768726349\n",
      "Step: 25700  \tTraining loss: 0.27917465567588806\n",
      "Step: 25700  \tTraining accuracy: 0.8761452436447144\n",
      "Step: 25700  \tValid loss: 0.2629983425140381\n",
      "Step: 25800  \tTraining loss: 0.2791482210159302\n",
      "Step: 25800  \tTraining accuracy: 0.8761640787124634\n",
      "Step: 25800  \tValid loss: 0.2629864513874054\n",
      "Step: 25900  \tTraining loss: 0.27912524342536926\n",
      "Step: 25900  \tTraining accuracy: 0.8761832118034363\n",
      "Step: 25900  \tValid loss: 0.2629643976688385\n",
      "Step: 26000  \tTraining loss: 0.27910277247428894\n",
      "Step: 26000  \tTraining accuracy: 0.8762025237083435\n",
      "Step: 26000  \tValid loss: 0.2629336714744568\n",
      "Step: 26100  \tTraining loss: 0.27907899022102356\n",
      "Step: 26100  \tTraining accuracy: 0.8762216567993164\n",
      "Step: 26100  \tValid loss: 0.2629196047782898\n",
      "Step: 26200  \tTraining loss: 0.27905669808387756\n",
      "Step: 26200  \tTraining accuracy: 0.876240611076355\n",
      "Step: 26200  \tValid loss: 0.2628934979438782\n",
      "Step: 26300  \tTraining loss: 0.27903270721435547\n",
      "Step: 26300  \tTraining accuracy: 0.876259446144104\n",
      "Step: 26300  \tValid loss: 0.2629041075706482\n",
      "Step: 26400  \tTraining loss: 0.2790106534957886\n",
      "Step: 26400  \tTraining accuracy: 0.8762781620025635\n",
      "Step: 26400  \tValid loss: 0.2628646194934845\n",
      "Step: 26500  \tTraining loss: 0.2789885103702545\n",
      "Step: 26500  \tTraining accuracy: 0.8762966990470886\n",
      "Step: 26500  \tValid loss: 0.26285383105278015\n",
      "Step: 26600  \tTraining loss: 0.2789653539657593\n",
      "Step: 26600  \tTraining accuracy: 0.8763151168823242\n",
      "Step: 26600  \tValid loss: 0.2628399431705475\n",
      "Step: 26700  \tTraining loss: 0.2789417803287506\n",
      "Step: 26700  \tTraining accuracy: 0.8763336539268494\n",
      "Step: 26700  \tValid loss: 0.262832373380661\n",
      "Step: 26800  \tTraining loss: 0.2789190113544464\n",
      "Step: 26800  \tTraining accuracy: 0.8763524889945984\n",
      "Step: 26800  \tValid loss: 0.2628045678138733\n",
      "Step: 26900  \tTraining loss: 0.27889707684516907\n",
      "Step: 26900  \tTraining accuracy: 0.876370906829834\n",
      "Step: 26900  \tValid loss: 0.26279088854789734\n",
      "Step: 27000  \tTraining loss: 0.27887338399887085\n",
      "Step: 27000  \tTraining accuracy: 0.8763895630836487\n",
      "Step: 27000  \tValid loss: 0.2627713978290558\n",
      "Step: 27100  \tTraining loss: 0.27885088324546814\n",
      "Step: 27100  \tTraining accuracy: 0.8764082789421082\n",
      "Step: 27100  \tValid loss: 0.2627583146095276\n",
      "Step: 27200  \tTraining loss: 0.27882885932922363\n",
      "Step: 27200  \tTraining accuracy: 0.8764265775680542\n",
      "Step: 27200  \tValid loss: 0.26274603605270386\n",
      "Step: 27300  \tTraining loss: 0.27880677580833435\n",
      "Step: 27300  \tTraining accuracy: 0.8764446973800659\n",
      "Step: 27300  \tValid loss: 0.2627260684967041\n",
      "Step: 27400  \tTraining loss: 0.27878424525260925\n",
      "Step: 27400  \tTraining accuracy: 0.8764627575874329\n",
      "Step: 27400  \tValid loss: 0.2627130448818207\n",
      "Step: 27500  \tTraining loss: 0.27876415848731995\n",
      "Step: 27500  \tTraining accuracy: 0.8764798045158386\n",
      "Step: 27500  \tValid loss: 0.2626813054084778\n",
      "Step: 27600  \tTraining loss: 0.2787420153617859\n",
      "Step: 27600  \tTraining accuracy: 0.8764968514442444\n",
      "Step: 27600  \tValid loss: 0.2626689672470093\n",
      "Step: 27700  \tTraining loss: 0.27871978282928467\n",
      "Step: 27700  \tTraining accuracy: 0.8765138387680054\n",
      "Step: 27700  \tValid loss: 0.26264986395835876\n",
      "Step: 27800  \tTraining loss: 0.2786976397037506\n",
      "Step: 27800  \tTraining accuracy: 0.876530647277832\n",
      "Step: 27800  \tValid loss: 0.26263198256492615\n",
      "Step: 27900  \tTraining loss: 0.2786758840084076\n",
      "Step: 27900  \tTraining accuracy: 0.876547634601593\n",
      "Step: 27900  \tValid loss: 0.2626112699508667\n",
      "Step: 28000  \tTraining loss: 0.27865371108055115\n",
      "Step: 28000  \tTraining accuracy: 0.8765636682510376\n",
      "Step: 28000  \tValid loss: 0.26259589195251465\n",
      "Step: 28100  \tTraining loss: 0.2786337435245514\n",
      "Step: 28100  \tTraining accuracy: 0.8765788078308105\n",
      "Step: 28100  \tValid loss: 0.26256874203681946\n",
      "Step: 28200  \tTraining loss: 0.2786102592945099\n",
      "Step: 28200  \tTraining accuracy: 0.8765951991081238\n",
      "Step: 28200  \tValid loss: 0.26257088780403137\n",
      "Step: 28300  \tTraining loss: 0.27858981490135193\n",
      "Step: 28300  \tTraining accuracy: 0.8766108751296997\n",
      "Step: 28300  \tValid loss: 0.2625419497489929\n",
      "Step: 28400  \tTraining loss: 0.2785678207874298\n",
      "Step: 28400  \tTraining accuracy: 0.8766270279884338\n",
      "Step: 28400  \tValid loss: 0.2625333070755005\n",
      "Step: 28500  \tTraining loss: 0.2785482704639435\n",
      "Step: 28500  \tTraining accuracy: 0.8766417503356934\n",
      "Step: 28500  \tValid loss: 0.2624949514865875\n",
      "Step: 28600  \tTraining loss: 0.27852663397789\n",
      "Step: 28600  \tTraining accuracy: 0.8766556978225708\n",
      "Step: 28600  \tValid loss: 0.26249656081199646\n",
      "Step: 28700  \tTraining loss: 0.2785048186779022\n",
      "Step: 28700  \tTraining accuracy: 0.8766695261001587\n",
      "Step: 28700  \tValid loss: 0.2624649405479431\n",
      "Step: 28800  \tTraining loss: 0.2784842848777771\n",
      "Step: 28800  \tTraining accuracy: 0.8766832947731018\n",
      "Step: 28800  \tValid loss: 0.26245802640914917\n",
      "Step: 28900  \tTraining loss: 0.27846166491508484\n",
      "Step: 28900  \tTraining accuracy: 0.8766981959342957\n",
      "Step: 28900  \tValid loss: 0.2624496817588806\n",
      "Step: 29000  \tTraining loss: 0.2784405052661896\n",
      "Step: 29000  \tTraining accuracy: 0.8767127990722656\n",
      "Step: 29000  \tValid loss: 0.26243436336517334\n",
      "Step: 29100  \tTraining loss: 0.278419554233551\n",
      "Step: 29100  \tTraining accuracy: 0.8767275810241699\n",
      "Step: 29100  \tValid loss: 0.2624036371707916\n",
      "Step: 29200  \tTraining loss: 0.27839821577072144\n",
      "Step: 29200  \tTraining accuracy: 0.8767428398132324\n",
      "Step: 29200  \tValid loss: 0.2624059021472931\n",
      "Step: 29300  \tTraining loss: 0.2783786356449127\n",
      "Step: 29300  \tTraining accuracy: 0.8767571449279785\n",
      "Step: 29300  \tValid loss: 0.26237213611602783\n",
      "Step: 29400  \tTraining loss: 0.27835819125175476\n",
      "Step: 29400  \tTraining accuracy: 0.8767709136009216\n",
      "Step: 29400  \tValid loss: 0.2623535692691803\n",
      "Step: 29500  \tTraining loss: 0.2783395051956177\n",
      "Step: 29500  \tTraining accuracy: 0.8767846822738647\n",
      "Step: 29500  \tValid loss: 0.2623482942581177\n",
      "Step: 29600  \tTraining loss: 0.27831587195396423\n",
      "Step: 29600  \tTraining accuracy: 0.8767982721328735\n",
      "Step: 29600  \tValid loss: 0.26232659816741943\n",
      "Step: 29700  \tTraining loss: 0.2782955467700958\n",
      "Step: 29700  \tTraining accuracy: 0.8768120408058167\n",
      "Step: 29700  \tValid loss: 0.262326717376709\n",
      "Step: 29800  \tTraining loss: 0.27827444672584534\n",
      "Step: 29800  \tTraining accuracy: 0.8768261075019836\n",
      "Step: 29800  \tValid loss: 0.2623140215873718\n",
      "Step: 29900  \tTraining loss: 0.27825528383255005\n",
      "Step: 29900  \tTraining accuracy: 0.8768401145935059\n",
      "Step: 29900  \tValid loss: 0.26228460669517517\n",
      "Step: 30000  \tTraining loss: 0.27823373675346375\n",
      "Step: 30000  \tTraining accuracy: 0.8768540024757385\n",
      "Step: 30000  \tValid loss: 0.2622653543949127\n",
      "Step: 30100  \tTraining loss: 0.2782127857208252\n",
      "Step: 30100  \tTraining accuracy: 0.8768677711486816\n",
      "Step: 30100  \tValid loss: 0.26225745677948\n",
      "Step: 30200  \tTraining loss: 0.27819201350212097\n",
      "Step: 30200  \tTraining accuracy: 0.87688148021698\n",
      "Step: 30200  \tValid loss: 0.26224154233932495\n",
      "Step: 30300  \tTraining loss: 0.278172105550766\n",
      "Step: 30300  \tTraining accuracy: 0.8768950700759888\n",
      "Step: 30300  \tValid loss: 0.26222386956214905\n",
      "Step: 30400  \tTraining loss: 0.2781524956226349\n",
      "Step: 30400  \tTraining accuracy: 0.8769084811210632\n",
      "Step: 30400  \tValid loss: 0.26220226287841797\n",
      "Step: 30500  \tTraining loss: 0.27813342213630676\n",
      "Step: 30500  \tTraining accuracy: 0.8769208788871765\n",
      "Step: 30500  \tValid loss: 0.2622041404247284\n",
      "Step: 30600  \tTraining loss: 0.27811288833618164\n",
      "Step: 30600  \tTraining accuracy: 0.8769323825836182\n",
      "Step: 30600  \tValid loss: 0.26217907667160034\n",
      "Step: 30700  \tTraining loss: 0.2780918478965759\n",
      "Step: 30700  \tTraining accuracy: 0.876943826675415\n",
      "Step: 30700  \tValid loss: 0.26216378808021545\n",
      "Step: 30800  \tTraining loss: 0.27807193994522095\n",
      "Step: 30800  \tTraining accuracy: 0.8769551515579224\n",
      "Step: 30800  \tValid loss: 0.2621544897556305\n",
      "Step: 30900  \tTraining loss: 0.2780519723892212\n",
      "Step: 30900  \tTraining accuracy: 0.8769664764404297\n",
      "Step: 30900  \tValid loss: 0.2621387839317322\n",
      "Step: 31000  \tTraining loss: 0.2780328691005707\n",
      "Step: 31000  \tTraining accuracy: 0.8769775629043579\n",
      "Step: 31000  \tValid loss: 0.26212653517723083\n",
      "Step: 31100  \tTraining loss: 0.2780137360095978\n",
      "Step: 31100  \tTraining accuracy: 0.8769880533218384\n",
      "Step: 31100  \tValid loss: 0.26210471987724304\n",
      "Step: 31200  \tTraining loss: 0.2779957056045532\n",
      "Step: 31200  \tTraining accuracy: 0.8769985437393188\n",
      "Step: 31200  \tValid loss: 0.2620975375175476\n",
      "Step: 31300  \tTraining loss: 0.2779728174209595\n",
      "Step: 31300  \tTraining accuracy: 0.8770089149475098\n",
      "Step: 31300  \tValid loss: 0.26209238171577454\n",
      "Step: 31400  \tTraining loss: 0.2779536247253418\n",
      "Step: 31400  \tTraining accuracy: 0.8770192861557007\n",
      "Step: 31400  \tValid loss: 0.2620774805545807\n",
      "Step: 31500  \tTraining loss: 0.2779345214366913\n",
      "Step: 31500  \tTraining accuracy: 0.877029538154602\n",
      "Step: 31500  \tValid loss: 0.26206016540527344\n",
      "Step: 31600  \tTraining loss: 0.2779153883457184\n",
      "Step: 31600  \tTraining accuracy: 0.8770398497581482\n",
      "Step: 31600  \tValid loss: 0.2620411515235901\n",
      "Step: 31700  \tTraining loss: 0.27789637446403503\n",
      "Step: 31700  \tTraining accuracy: 0.8770504593849182\n",
      "Step: 31700  \tValid loss: 0.26204273104667664\n",
      "Step: 31800  \tTraining loss: 0.27787747979164124\n",
      "Step: 31800  \tTraining accuracy: 0.8770605325698853\n",
      "Step: 31800  \tValid loss: 0.26204320788383484\n",
      "Step: 31900  \tTraining loss: 0.2778550386428833\n",
      "Step: 31900  \tTraining accuracy: 0.8770706653594971\n",
      "Step: 31900  \tValid loss: 0.26200637221336365\n",
      "Step: 32000  \tTraining loss: 0.27783429622650146\n",
      "Step: 32000  \tTraining accuracy: 0.8770815134048462\n",
      "Step: 32000  \tValid loss: 0.2620135545730591\n",
      "Step: 32100  \tTraining loss: 0.2778145968914032\n",
      "Step: 32100  \tTraining accuracy: 0.877092719078064\n",
      "Step: 32100  \tValid loss: 0.2619975805282593\n",
      "Step: 32200  \tTraining loss: 0.2777949571609497\n",
      "Step: 32200  \tTraining accuracy: 0.8771042823791504\n",
      "Step: 32200  \tValid loss: 0.26199623942375183\n",
      "Step: 32300  \tTraining loss: 0.27777522802352905\n",
      "Step: 32300  \tTraining accuracy: 0.8771157264709473\n",
      "Step: 32300  \tValid loss: 0.26198476552963257\n",
      "Step: 32400  \tTraining loss: 0.2777559161186218\n",
      "Step: 32400  \tTraining accuracy: 0.8771271705627441\n",
      "Step: 32400  \tValid loss: 0.2619728446006775\n",
      "Step: 32500  \tTraining loss: 0.27773672342300415\n",
      "Step: 32500  \tTraining accuracy: 0.877138078212738\n",
      "Step: 32500  \tValid loss: 0.2619725167751312\n",
      "Step: 32600  \tTraining loss: 0.27771785855293274\n",
      "Step: 32600  \tTraining accuracy: 0.8771487474441528\n",
      "Step: 32600  \tValid loss: 0.2619585394859314\n",
      "Step: 32700  \tTraining loss: 0.2776981294155121\n",
      "Step: 32700  \tTraining accuracy: 0.8771594762802124\n",
      "Step: 32700  \tValid loss: 0.2619553208351135\n",
      "Step: 32800  \tTraining loss: 0.27767953276634216\n",
      "Step: 32800  \tTraining accuracy: 0.8771710991859436\n",
      "Step: 32800  \tValid loss: 0.26194655895233154\n",
      "Step: 32900  \tTraining loss: 0.2776627838611603\n",
      "Step: 32900  \tTraining accuracy: 0.8771836757659912\n",
      "Step: 32900  \tValid loss: 0.26193466782569885\n",
      "Step: 33000  \tTraining loss: 0.27764222025871277\n",
      "Step: 33000  \tTraining accuracy: 0.8771963715553284\n",
      "Step: 33000  \tValid loss: 0.26193010807037354\n",
      "Step: 33100  \tTraining loss: 0.27762219309806824\n",
      "Step: 33100  \tTraining accuracy: 0.8772085309028625\n",
      "Step: 33100  \tValid loss: 0.2619170546531677\n",
      "Step: 33200  \tTraining loss: 0.27760380506515503\n",
      "Step: 33200  \tTraining accuracy: 0.8772198557853699\n",
      "Step: 33200  \tValid loss: 0.26190364360809326\n",
      "Step: 33300  \tTraining loss: 0.2775851786136627\n",
      "Step: 33300  \tTraining accuracy: 0.8772308826446533\n",
      "Step: 33300  \tValid loss: 0.2618967890739441\n",
      "Step: 33400  \tTraining loss: 0.2775667607784271\n",
      "Step: 33400  \tTraining accuracy: 0.8772416114807129\n",
      "Step: 33400  \tValid loss: 0.26189735531806946\n",
      "Step: 33500  \tTraining loss: 0.2775484323501587\n",
      "Step: 33500  \tTraining accuracy: 0.8772522807121277\n",
      "Step: 33500  \tValid loss: 0.2618752717971802\n",
      "Step: 33600  \tTraining loss: 0.2775298058986664\n",
      "Step: 33600  \tTraining accuracy: 0.8772630095481873\n",
      "Step: 33600  \tValid loss: 0.26187726855278015\n",
      "Step: 33700  \tTraining loss: 0.2775115370750427\n",
      "Step: 33700  \tTraining accuracy: 0.8772740960121155\n",
      "Step: 33700  \tValid loss: 0.2618604302406311\n",
      "Step: 33800  \tTraining loss: 0.27749451994895935\n",
      "Step: 33800  \tTraining accuracy: 0.8772851228713989\n",
      "Step: 33800  \tValid loss: 0.26185324788093567\n",
      "Step: 33900  \tTraining loss: 0.27747541666030884\n",
      "Step: 33900  \tTraining accuracy: 0.8772960901260376\n",
      "Step: 33900  \tValid loss: 0.26184943318367004\n",
      "Step: 34000  \tTraining loss: 0.2774581015110016\n",
      "Step: 34000  \tTraining accuracy: 0.8773069977760315\n",
      "Step: 34000  \tValid loss: 0.26182821393013\n",
      "Step: 34100  \tTraining loss: 0.27743953466415405\n",
      "Step: 34100  \tTraining accuracy: 0.8773178458213806\n",
      "Step: 34100  \tValid loss: 0.261820524930954\n",
      "Step: 34200  \tTraining loss: 0.2774215042591095\n",
      "Step: 34200  \tTraining accuracy: 0.877328634262085\n",
      "Step: 34200  \tValid loss: 0.26181378960609436\n",
      "Step: 34300  \tTraining loss: 0.2774048447608948\n",
      "Step: 34300  \tTraining accuracy: 0.8773393034934998\n",
      "Step: 34300  \tValid loss: 0.2618098258972168\n",
      "Step: 34400  \tTraining loss: 0.2773858904838562\n",
      "Step: 34400  \tTraining accuracy: 0.8773497343063354\n",
      "Step: 34400  \tValid loss: 0.261791855096817\n",
      "Step: 34500  \tTraining loss: 0.2773682475090027\n",
      "Step: 34500  \tTraining accuracy: 0.8773598074913025\n",
      "Step: 34500  \tValid loss: 0.261781245470047\n",
      "Step: 34600  \tTraining loss: 0.27735158801078796\n",
      "Step: 34600  \tTraining accuracy: 0.87736976146698\n",
      "Step: 34600  \tValid loss: 0.2617875933647156\n",
      "Step: 34700  \tTraining loss: 0.277333527803421\n",
      "Step: 34700  \tTraining accuracy: 0.8773793578147888\n",
      "Step: 34700  \tValid loss: 0.26177045702934265\n",
      "Step: 34800  \tTraining loss: 0.2773151099681854\n",
      "Step: 34800  \tTraining accuracy: 0.8773887157440186\n",
      "Step: 34800  \tValid loss: 0.26176032423973083\n",
      "Step: 34900  \tTraining loss: 0.27729713916778564\n",
      "Step: 34900  \tTraining accuracy: 0.8773980140686035\n",
      "Step: 34900  \tValid loss: 0.26178157329559326\n",
      "Step: 35000  \tTraining loss: 0.277279794216156\n",
      "Step: 35000  \tTraining accuracy: 0.8774075508117676\n",
      "Step: 35000  \tValid loss: 0.2617414593696594\n",
      "Step: 35100  \tTraining loss: 0.27726301550865173\n",
      "Step: 35100  \tTraining accuracy: 0.8774169087409973\n",
      "Step: 35100  \tValid loss: 0.2617497444152832\n",
      "Step: 35200  \tTraining loss: 0.2772452235221863\n",
      "Step: 35200  \tTraining accuracy: 0.877426028251648\n",
      "Step: 35200  \tValid loss: 0.2617364525794983\n",
      "Step: 35300  \tTraining loss: 0.2772272229194641\n",
      "Step: 35300  \tTraining accuracy: 0.8774352073669434\n",
      "Step: 35300  \tValid loss: 0.2617250978946686\n",
      "Step: 35400  \tTraining loss: 0.27720996737480164\n",
      "Step: 35400  \tTraining accuracy: 0.8774442076683044\n",
      "Step: 35400  \tValid loss: 0.2617103159427643\n",
      "Step: 35500  \tTraining loss: 0.2771936357021332\n",
      "Step: 35500  \tTraining accuracy: 0.8774532079696655\n",
      "Step: 35500  \tValid loss: 0.26170191168785095\n",
      "Step: 35600  \tTraining loss: 0.27717533707618713\n",
      "Step: 35600  \tTraining accuracy: 0.8774620890617371\n",
      "Step: 35600  \tValid loss: 0.2616986036300659\n",
      "Step: 35700  \tTraining loss: 0.2771576941013336\n",
      "Step: 35700  \tTraining accuracy: 0.8774709701538086\n",
      "Step: 35700  \tValid loss: 0.2616875171661377\n",
      "Step: 35800  \tTraining loss: 0.2771398425102234\n",
      "Step: 35800  \tTraining accuracy: 0.8774794936180115\n",
      "Step: 35800  \tValid loss: 0.26169314980506897\n",
      "Step: 35900  \tTraining loss: 0.2771225571632385\n",
      "Step: 35900  \tTraining accuracy: 0.87748783826828\n",
      "Step: 35900  \tValid loss: 0.2616753876209259\n",
      "Step: 36000  \tTraining loss: 0.27710750699043274\n",
      "Step: 36000  \tTraining accuracy: 0.8774959444999695\n",
      "Step: 36000  \tValid loss: 0.2616645097732544\n",
      "Step: 36100  \tTraining loss: 0.2770880460739136\n",
      "Step: 36100  \tTraining accuracy: 0.8775038719177246\n",
      "Step: 36100  \tValid loss: 0.26165539026260376\n",
      "Step: 36200  \tTraining loss: 0.27707117795944214\n",
      "Step: 36200  \tTraining accuracy: 0.8775111436843872\n",
      "Step: 36200  \tValid loss: 0.26165300607681274\n",
      "Step: 36300  \tTraining loss: 0.27705296874046326\n",
      "Step: 36300  \tTraining accuracy: 0.8775184750556946\n",
      "Step: 36300  \tValid loss: 0.2616542875766754\n",
      "Step: 36400  \tTraining loss: 0.27703601121902466\n",
      "Step: 36400  \tTraining accuracy: 0.8775259852409363\n",
      "Step: 36400  \tValid loss: 0.26163846254348755\n",
      "Step: 36500  \tTraining loss: 0.2770192623138428\n",
      "Step: 36500  \tTraining accuracy: 0.8775334358215332\n",
      "Step: 36500  \tValid loss: 0.2616279125213623\n",
      "Step: 36600  \tTraining loss: 0.27700191736221313\n",
      "Step: 36600  \tTraining accuracy: 0.877540647983551\n",
      "Step: 36600  \tValid loss: 0.2616298496723175\n",
      "Step: 36700  \tTraining loss: 0.2769867777824402\n",
      "Step: 36700  \tTraining accuracy: 0.877547562122345\n",
      "Step: 36700  \tValid loss: 0.2616221606731415\n",
      "Step: 36800  \tTraining loss: 0.27696818113327026\n",
      "Step: 36800  \tTraining accuracy: 0.8775543570518494\n",
      "Step: 36800  \tValid loss: 0.2616283595561981\n",
      "Step: 36900  \tTraining loss: 0.2769519090652466\n",
      "Step: 36900  \tTraining accuracy: 0.8775609731674194\n",
      "Step: 36900  \tValid loss: 0.2616085112094879\n",
      "Step: 37000  \tTraining loss: 0.27693450450897217\n",
      "Step: 37000  \tTraining accuracy: 0.8775672316551208\n",
      "Step: 37000  \tValid loss: 0.26159971952438354\n",
      "Step: 37100  \tTraining loss: 0.2769183814525604\n",
      "Step: 37100  \tTraining accuracy: 0.8775734305381775\n",
      "Step: 37100  \tValid loss: 0.2615812420845032\n",
      "Step: 37200  \tTraining loss: 0.2768993675708771\n",
      "Step: 37200  \tTraining accuracy: 0.8775796294212341\n",
      "Step: 37200  \tValid loss: 0.2615871727466583\n",
      "Step: 37300  \tTraining loss: 0.2768843173980713\n",
      "Step: 37300  \tTraining accuracy: 0.8775857090950012\n",
      "Step: 37300  \tValid loss: 0.2615823745727539\n",
      "Step: 37400  \tTraining loss: 0.27686524391174316\n",
      "Step: 37400  \tTraining accuracy: 0.8775913119316101\n",
      "Step: 37400  \tValid loss: 0.26158007979393005\n",
      "Step: 37500  \tTraining loss: 0.27684879302978516\n",
      "Step: 37500  \tTraining accuracy: 0.877596914768219\n",
      "Step: 37500  \tValid loss: 0.2615610659122467\n",
      "Step: 37600  \tTraining loss: 0.2768315076828003\n",
      "Step: 37600  \tTraining accuracy: 0.8776019811630249\n",
      "Step: 37600  \tValid loss: 0.26158058643341064\n",
      "Step: 37700  \tTraining loss: 0.27681466937065125\n",
      "Step: 37700  \tTraining accuracy: 0.877606987953186\n",
      "Step: 37700  \tValid loss: 0.2615590989589691\n",
      "Step: 37800  \tTraining loss: 0.27679750323295593\n",
      "Step: 37800  \tTraining accuracy: 0.8776119947433472\n",
      "Step: 37800  \tValid loss: 0.26156193017959595\n",
      "Step: 37900  \tTraining loss: 0.2767822742462158\n",
      "Step: 37900  \tTraining accuracy: 0.8776170015335083\n",
      "Step: 37900  \tValid loss: 0.2615499496459961\n",
      "Step: 38000  \tTraining loss: 0.27676448225975037\n",
      "Step: 38000  \tTraining accuracy: 0.8776217699050903\n",
      "Step: 38000  \tValid loss: 0.26155146956443787\n",
      "Step: 38100  \tTraining loss: 0.27674660086631775\n",
      "Step: 38100  \tTraining accuracy: 0.8776264786720276\n",
      "Step: 38100  \tValid loss: 0.2615429162979126\n",
      "Step: 38200  \tTraining loss: 0.27672943472862244\n",
      "Step: 38200  \tTraining accuracy: 0.877631425857544\n",
      "Step: 38200  \tValid loss: 0.26153117418289185\n",
      "Step: 38300  \tTraining loss: 0.2767128050327301\n",
      "Step: 38300  \tTraining accuracy: 0.8776360750198364\n",
      "Step: 38300  \tValid loss: 0.26153838634490967\n",
      "Step: 38400  \tTraining loss: 0.276695191860199\n",
      "Step: 38400  \tTraining accuracy: 0.877640426158905\n",
      "Step: 38400  \tValid loss: 0.26153305172920227\n",
      "Step: 38500  \tTraining loss: 0.27667802572250366\n",
      "Step: 38500  \tTraining accuracy: 0.8776447772979736\n",
      "Step: 38500  \tValid loss: 0.2615257203578949\n",
      "Step: 38600  \tTraining loss: 0.276661217212677\n",
      "Step: 38600  \tTraining accuracy: 0.8776491284370422\n",
      "Step: 38600  \tValid loss: 0.26151883602142334\n",
      "Step: 38700  \tTraining loss: 0.2766450047492981\n",
      "Step: 38700  \tTraining accuracy: 0.8776534199714661\n",
      "Step: 38700  \tValid loss: 0.26150915026664734\n",
      "Step: 38800  \tTraining loss: 0.2766280174255371\n",
      "Step: 38800  \tTraining accuracy: 0.8776578903198242\n",
      "Step: 38800  \tValid loss: 0.2615126371383667\n",
      "Step: 38900  \tTraining loss: 0.2766094207763672\n",
      "Step: 38900  \tTraining accuracy: 0.8776625990867615\n",
      "Step: 38900  \tValid loss: 0.26150932908058167\n",
      "Step: 39000  \tTraining loss: 0.2765776216983795\n",
      "Step: 39000  \tTraining accuracy: 0.8776671290397644\n",
      "Step: 39000  \tValid loss: 0.2614964246749878\n",
      "Step: 39100  \tTraining loss: 0.2765274941921234\n",
      "Step: 39100  \tTraining accuracy: 0.8776710629463196\n",
      "Step: 39100  \tValid loss: 0.2614898979663849\n",
      "Step: 39200  \tTraining loss: 0.27650266885757446\n",
      "Step: 39200  \tTraining accuracy: 0.8776752352714539\n",
      "Step: 39200  \tValid loss: 0.261491984128952\n",
      "Step: 39300  \tTraining loss: 0.2764829993247986\n",
      "Step: 39300  \tTraining accuracy: 0.8776807188987732\n",
      "Step: 39300  \tValid loss: 0.2614978849887848\n",
      "Step: 39400  \tTraining loss: 0.27646419405937195\n",
      "Step: 39400  \tTraining accuracy: 0.8776863217353821\n",
      "Step: 39400  \tValid loss: 0.26149553060531616\n",
      "Step: 39500  \tTraining loss: 0.27644583582878113\n",
      "Step: 39500  \tTraining accuracy: 0.8776914477348328\n",
      "Step: 39500  \tValid loss: 0.26150524616241455\n",
      "Step: 39600  \tTraining loss: 0.2764282524585724\n",
      "Step: 39600  \tTraining accuracy: 0.8776963949203491\n",
      "Step: 39600  \tValid loss: 0.2614848017692566\n",
      "Step: 39700  \tTraining loss: 0.2764087915420532\n",
      "Step: 39700  \tTraining accuracy: 0.8777009844779968\n",
      "Step: 39700  \tValid loss: 0.2614907920360565\n",
      "Step: 39800  \tTraining loss: 0.2763917148113251\n",
      "Step: 39800  \tTraining accuracy: 0.8777055740356445\n",
      "Step: 39800  \tValid loss: 0.26147663593292236\n",
      "Step: 39900  \tTraining loss: 0.27637243270874023\n",
      "Step: 39900  \tTraining accuracy: 0.87771075963974\n",
      "Step: 39900  \tValid loss: 0.2614762485027313\n",
      "Step: 40000  \tTraining loss: 0.2763545513153076\n",
      "Step: 40000  \tTraining accuracy: 0.8777158856391907\n",
      "Step: 40000  \tValid loss: 0.2614617943763733\n",
      "Step: 40100  \tTraining loss: 0.2763376832008362\n",
      "Step: 40100  \tTraining accuracy: 0.8777207732200623\n",
      "Step: 40100  \tValid loss: 0.26145288348197937\n",
      "Step: 40200  \tTraining loss: 0.2763197422027588\n",
      "Step: 40200  \tTraining accuracy: 0.8777256608009338\n",
      "Step: 40200  \tValid loss: 0.26146259903907776\n",
      "Step: 40300  \tTraining loss: 0.27630001306533813\n",
      "Step: 40300  \tTraining accuracy: 0.8777300715446472\n",
      "Step: 40300  \tValid loss: 0.26144739985466003\n",
      "Step: 40400  \tTraining loss: 0.2762819528579712\n",
      "Step: 40400  \tTraining accuracy: 0.8777340650558472\n",
      "Step: 40400  \tValid loss: 0.2614477276802063\n",
      "Step: 40500  \tTraining loss: 0.276263564825058\n",
      "Step: 40500  \tTraining accuracy: 0.877737820148468\n",
      "Step: 40500  \tValid loss: 0.2614392340183258\n",
      "Step: 40600  \tTraining loss: 0.2762448191642761\n",
      "Step: 40600  \tTraining accuracy: 0.8777412176132202\n",
      "Step: 40600  \tValid loss: 0.26144710183143616\n",
      "Step: 40700  \tTraining loss: 0.2762269079685211\n",
      "Step: 40700  \tTraining accuracy: 0.8777446150779724\n",
      "Step: 40700  \tValid loss: 0.2614487409591675\n",
      "Step: 40800  \tTraining loss: 0.2762080132961273\n",
      "Step: 40800  \tTraining accuracy: 0.8777483701705933\n",
      "Step: 40800  \tValid loss: 0.2614227831363678\n",
      "Step: 40900  \tTraining loss: 0.2761900722980499\n",
      "Step: 40900  \tTraining accuracy: 0.8777518272399902\n",
      "Step: 40900  \tValid loss: 0.2614317238330841\n",
      "Step: 41000  \tTraining loss: 0.2761702537536621\n",
      "Step: 41000  \tTraining accuracy: 0.8777549862861633\n",
      "Step: 41000  \tValid loss: 0.2614377737045288\n",
      "Step: 41100  \tTraining loss: 0.27615147829055786\n",
      "Step: 41100  \tTraining accuracy: 0.8777578473091125\n",
      "Step: 41100  \tValid loss: 0.2614312171936035\n",
      "Step: 41200  \tTraining loss: 0.27613377571105957\n",
      "Step: 41200  \tTraining accuracy: 0.8777610659599304\n",
      "Step: 41200  \tValid loss: 0.26142212748527527\n",
      "Step: 41300  \tTraining loss: 0.276114284992218\n",
      "Step: 41300  \tTraining accuracy: 0.8777639269828796\n",
      "Step: 41300  \tValid loss: 0.26143020391464233\n",
      "Step: 41400  \tTraining loss: 0.27609527111053467\n",
      "Step: 41400  \tTraining accuracy: 0.8777663111686707\n",
      "Step: 41400  \tValid loss: 0.2614257037639618\n",
      "Step: 41500  \tTraining loss: 0.276077538728714\n",
      "Step: 41500  \tTraining accuracy: 0.877768874168396\n",
      "Step: 41500  \tValid loss: 0.26141253113746643\n",
      "Step: 41600  \tTraining loss: 0.2760583460330963\n",
      "Step: 41600  \tTraining accuracy: 0.8777716755867004\n",
      "Step: 41600  \tValid loss: 0.2614201605319977\n",
      "Step: 41700  \tTraining loss: 0.2760382294654846\n",
      "Step: 41700  \tTraining accuracy: 0.8777744770050049\n",
      "Step: 41700  \tValid loss: 0.261429101228714\n",
      "Step: 41800  \tTraining loss: 0.276020348072052\n",
      "Step: 41800  \tTraining accuracy: 0.8777772784233093\n",
      "Step: 41800  \tValid loss: 0.2614138126373291\n",
      "Step: 41900  \tTraining loss: 0.27600112557411194\n",
      "Step: 41900  \tTraining accuracy: 0.877780020236969\n",
      "Step: 41900  \tValid loss: 0.261415958404541\n",
      "Step: 42000  \tTraining loss: 0.2759808897972107\n",
      "Step: 42000  \tTraining accuracy: 0.87778240442276\n",
      "Step: 42000  \tValid loss: 0.261432409286499\n",
      "Step: 42100  \tTraining loss: 0.2759631276130676\n",
      "Step: 42100  \tTraining accuracy: 0.8777840733528137\n",
      "Step: 42100  \tValid loss: 0.2614101469516754\n",
      "Step: 42200  \tTraining loss: 0.2759425640106201\n",
      "Step: 42200  \tTraining accuracy: 0.8777862191200256\n",
      "Step: 42200  \tValid loss: 0.2614186108112335\n",
      "Step: 42300  \tTraining loss: 0.27592289447784424\n",
      "Step: 42300  \tTraining accuracy: 0.8777879476547241\n",
      "Step: 42300  \tValid loss: 0.2614205777645111\n",
      "Step: 42400  \tTraining loss: 0.27590394020080566\n",
      "Step: 42400  \tTraining accuracy: 0.877789318561554\n",
      "Step: 42400  \tValid loss: 0.26140567660331726\n",
      "Step: 42500  \tTraining loss: 0.27588382363319397\n",
      "Step: 42500  \tTraining accuracy: 0.8777905106544495\n",
      "Step: 42500  \tValid loss: 0.2614190876483917\n",
      "Step: 42600  \tTraining loss: 0.27586477994918823\n",
      "Step: 42600  \tTraining accuracy: 0.8777914643287659\n",
      "Step: 42600  \tValid loss: 0.2614099681377411\n",
      "Step: 42700  \tTraining loss: 0.2758452892303467\n",
      "Step: 42700  \tTraining accuracy: 0.8777923583984375\n",
      "Step: 42700  \tValid loss: 0.2614056468009949\n",
      "Step: 42800  \tTraining loss: 0.27582499384880066\n",
      "Step: 42800  \tTraining accuracy: 0.8777933120727539\n",
      "Step: 42800  \tValid loss: 0.2614116072654724\n",
      "Step: 42900  \tTraining loss: 0.27580395340919495\n",
      "Step: 42900  \tTraining accuracy: 0.8777943849563599\n",
      "Step: 42900  \tValid loss: 0.26142460107803345\n",
      "Step: 43000  \tTraining loss: 0.2757842540740967\n",
      "Step: 43000  \tTraining accuracy: 0.8777957558631897\n",
      "Step: 43000  \tValid loss: 0.26141104102134705\n",
      "Step: 43100  \tTraining loss: 0.2757647931575775\n",
      "Step: 43100  \tTraining accuracy: 0.8777970671653748\n",
      "Step: 43100  \tValid loss: 0.2614103853702545\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.87779844\n",
      "Precision: 0.8955391\n",
      "Recall: 0.8811227\n",
      "F1 score: 0.8706756\n",
      "AUC: 0.8790495\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.877798   0.895539  0.881123  0.870676  0.879049  0.275762      0.877797   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.261395        0.87779   0.317834      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  43108.0  \n",
      "9\n",
      "(3770, 8)\n",
      "(3770, 1)\n",
      "(2000, 8)\n",
      "(2000, 1)\n",
      "(1625, 8)\n",
      "(1625, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6192120313644409\n",
      "Step: 100  \tTraining accuracy: 0.7456233501434326\n",
      "Step: 100  \tValid loss: 0.6302227973937988\n",
      "Step: 200  \tTraining loss: 0.5858554244041443\n",
      "Step: 200  \tTraining accuracy: 0.7565606832504272\n",
      "Step: 200  \tValid loss: 0.6077165603637695\n",
      "Step: 300  \tTraining loss: 0.5655428767204285\n",
      "Step: 300  \tTraining accuracy: 0.7643857598304749\n",
      "Step: 300  \tValid loss: 0.5877016186714172\n",
      "Step: 400  \tTraining loss: 0.5447300672531128\n",
      "Step: 400  \tTraining accuracy: 0.7688306570053101\n",
      "Step: 400  \tValid loss: 0.566424548625946\n",
      "Step: 500  \tTraining loss: 0.5283345580101013\n",
      "Step: 500  \tTraining accuracy: 0.7715142369270325\n",
      "Step: 500  \tValid loss: 0.549199104309082\n",
      "Step: 600  \tTraining loss: 0.5146909952163696\n",
      "Step: 600  \tTraining accuracy: 0.7732236981391907\n",
      "Step: 600  \tValid loss: 0.5340457558631897\n",
      "Step: 700  \tTraining loss: 0.5050150752067566\n",
      "Step: 700  \tTraining accuracy: 0.7744079828262329\n",
      "Step: 700  \tValid loss: 0.5236628651618958\n",
      "Step: 800  \tTraining loss: 0.4959720969200134\n",
      "Step: 800  \tTraining accuracy: 0.7752768397331238\n",
      "Step: 800  \tValid loss: 0.5166118144989014\n",
      "Step: 900  \tTraining loss: 0.48738792538642883\n",
      "Step: 900  \tTraining accuracy: 0.7759415507316589\n",
      "Step: 900  \tValid loss: 0.5102656483650208\n",
      "Step: 1000  \tTraining loss: 0.4787193238735199\n",
      "Step: 1000  \tTraining accuracy: 0.7765944004058838\n",
      "Step: 1000  \tValid loss: 0.5022275447845459\n",
      "Step: 1100  \tTraining loss: 0.47054457664489746\n",
      "Step: 1100  \tTraining accuracy: 0.7779078483581543\n",
      "Step: 1100  \tValid loss: 0.4948499798774719\n",
      "Step: 1200  \tTraining loss: 0.46341758966445923\n",
      "Step: 1200  \tTraining accuracy: 0.7795453071594238\n",
      "Step: 1200  \tValid loss: 0.48790305852890015\n",
      "Step: 1300  \tTraining loss: 0.45837685465812683\n",
      "Step: 1300  \tTraining accuracy: 0.7810074687004089\n",
      "Step: 1300  \tValid loss: 0.48345980048179626\n",
      "Step: 1400  \tTraining loss: 0.45480743050575256\n",
      "Step: 1400  \tTraining accuracy: 0.7822731733322144\n",
      "Step: 1400  \tValid loss: 0.4803106188774109\n",
      "Step: 1500  \tTraining loss: 0.45195111632347107\n",
      "Step: 1500  \tTraining accuracy: 0.7834669351577759\n",
      "Step: 1500  \tValid loss: 0.4767535924911499\n",
      "Step: 1600  \tTraining loss: 0.44965076446533203\n",
      "Step: 1600  \tTraining accuracy: 0.7844631671905518\n",
      "Step: 1600  \tValid loss: 0.47461551427841187\n",
      "Step: 1700  \tTraining loss: 0.446942001581192\n",
      "Step: 1700  \tTraining accuracy: 0.7853468656539917\n",
      "Step: 1700  \tValid loss: 0.47170621156692505\n",
      "Step: 1800  \tTraining loss: 0.4448893964290619\n",
      "Step: 1800  \tTraining accuracy: 0.7862609624862671\n",
      "Step: 1800  \tValid loss: 0.4700726866722107\n",
      "Step: 1900  \tTraining loss: 0.4429594576358795\n",
      "Step: 1900  \tTraining accuracy: 0.7871201038360596\n",
      "Step: 1900  \tValid loss: 0.46859410405158997\n",
      "Step: 2000  \tTraining loss: 0.441646546125412\n",
      "Step: 2000  \tTraining accuracy: 0.7878704071044922\n",
      "Step: 2000  \tValid loss: 0.4674838185310364\n",
      "Step: 2100  \tTraining loss: 0.440500944852829\n",
      "Step: 2100  \tTraining accuracy: 0.7885211110115051\n",
      "Step: 2100  \tValid loss: 0.46661117672920227\n",
      "Step: 2200  \tTraining loss: 0.4394412636756897\n",
      "Step: 2200  \tTraining accuracy: 0.789092481136322\n",
      "Step: 2200  \tValid loss: 0.4655870199203491\n",
      "Step: 2300  \tTraining loss: 0.4384634792804718\n",
      "Step: 2300  \tTraining accuracy: 0.7896371483802795\n",
      "Step: 2300  \tValid loss: 0.4647102653980255\n",
      "Step: 2400  \tTraining loss: 0.4375368654727936\n",
      "Step: 2400  \tTraining accuracy: 0.79017573595047\n",
      "Step: 2400  \tValid loss: 0.4636979401111603\n",
      "Step: 2500  \tTraining loss: 0.436619371175766\n",
      "Step: 2500  \tTraining accuracy: 0.7907145023345947\n",
      "Step: 2500  \tValid loss: 0.46262145042419434\n",
      "Step: 2600  \tTraining loss: 0.4357156455516815\n",
      "Step: 2600  \tTraining accuracy: 0.7912269234657288\n",
      "Step: 2600  \tValid loss: 0.4616507887840271\n",
      "Step: 2700  \tTraining loss: 0.43479570746421814\n",
      "Step: 2700  \tTraining accuracy: 0.7917006611824036\n",
      "Step: 2700  \tValid loss: 0.4606774151325226\n",
      "Step: 2800  \tTraining loss: 0.4337620139122009\n",
      "Step: 2800  \tTraining accuracy: 0.7921448945999146\n",
      "Step: 2800  \tValid loss: 0.4595804512500763\n",
      "Step: 2900  \tTraining loss: 0.4323681592941284\n",
      "Step: 2900  \tTraining accuracy: 0.7925674915313721\n",
      "Step: 2900  \tValid loss: 0.45739930868148804\n",
      "Step: 3000  \tTraining loss: 0.4312949776649475\n",
      "Step: 3000  \tTraining accuracy: 0.7930071949958801\n",
      "Step: 3000  \tValid loss: 0.45600879192352295\n",
      "Step: 3100  \tTraining loss: 0.43015849590301514\n",
      "Step: 3100  \tTraining accuracy: 0.7934136986732483\n",
      "Step: 3100  \tValid loss: 0.45439356565475464\n",
      "Step: 3200  \tTraining loss: 0.4291991591453552\n",
      "Step: 3200  \tTraining accuracy: 0.7937986850738525\n",
      "Step: 3200  \tValid loss: 0.4531092643737793\n",
      "Step: 3300  \tTraining loss: 0.4282757043838501\n",
      "Step: 3300  \tTraining accuracy: 0.7941641211509705\n",
      "Step: 3300  \tValid loss: 0.4519835412502289\n",
      "Step: 3400  \tTraining loss: 0.42745891213417053\n",
      "Step: 3400  \tTraining accuracy: 0.7945037484169006\n",
      "Step: 3400  \tValid loss: 0.45097771286964417\n",
      "Step: 3500  \tTraining loss: 0.42670881748199463\n",
      "Step: 3500  \tTraining accuracy: 0.7948315143585205\n",
      "Step: 3500  \tValid loss: 0.4500483274459839\n",
      "Step: 3600  \tTraining loss: 0.42600640654563904\n",
      "Step: 3600  \tTraining accuracy: 0.7951560616493225\n",
      "Step: 3600  \tValid loss: 0.449243426322937\n",
      "Step: 3700  \tTraining loss: 0.42533302307128906\n",
      "Step: 3700  \tTraining accuracy: 0.7954701781272888\n",
      "Step: 3700  \tValid loss: 0.4485316276550293\n",
      "Step: 3800  \tTraining loss: 0.42464128136634827\n",
      "Step: 3800  \tTraining accuracy: 0.7957928776741028\n",
      "Step: 3800  \tValid loss: 0.4478607475757599\n",
      "Step: 3900  \tTraining loss: 0.4239496886730194\n",
      "Step: 3900  \tTraining accuracy: 0.7961092591285706\n",
      "Step: 3900  \tValid loss: 0.44729089736938477\n",
      "Step: 4000  \tTraining loss: 0.423334002494812\n",
      "Step: 4000  \tTraining accuracy: 0.7964233756065369\n",
      "Step: 4000  \tValid loss: 0.4466921389102936\n",
      "Step: 4100  \tTraining loss: 0.4227742850780487\n",
      "Step: 4100  \tTraining accuracy: 0.7967453598976135\n",
      "Step: 4100  \tValid loss: 0.44618719816207886\n",
      "Step: 4200  \tTraining loss: 0.42222556471824646\n",
      "Step: 4200  \tTraining accuracy: 0.7970713376998901\n",
      "Step: 4200  \tValid loss: 0.44581717252731323\n",
      "Step: 4300  \tTraining loss: 0.42170071601867676\n",
      "Step: 4300  \tTraining accuracy: 0.7974074482917786\n",
      "Step: 4300  \tValid loss: 0.44542646408081055\n",
      "Step: 4400  \tTraining loss: 0.4211971163749695\n",
      "Step: 4400  \tTraining accuracy: 0.7977156639099121\n",
      "Step: 4400  \tValid loss: 0.4450303912162781\n",
      "Step: 4500  \tTraining loss: 0.4207071363925934\n",
      "Step: 4500  \tTraining accuracy: 0.7980100512504578\n",
      "Step: 4500  \tValid loss: 0.44469738006591797\n",
      "Step: 4600  \tTraining loss: 0.4202047884464264\n",
      "Step: 4600  \tTraining accuracy: 0.7983033657073975\n",
      "Step: 4600  \tValid loss: 0.4443095624446869\n",
      "Step: 4700  \tTraining loss: 0.419547438621521\n",
      "Step: 4700  \tTraining accuracy: 0.7985898852348328\n",
      "Step: 4700  \tValid loss: 0.44395458698272705\n",
      "Step: 4800  \tTraining loss: 0.4189673364162445\n",
      "Step: 4800  \tTraining accuracy: 0.7988443970680237\n",
      "Step: 4800  \tValid loss: 0.4435456395149231\n",
      "Step: 4900  \tTraining loss: 0.41813135147094727\n",
      "Step: 4900  \tTraining accuracy: 0.79908287525177\n",
      "Step: 4900  \tValid loss: 0.44279253482818604\n",
      "Step: 5000  \tTraining loss: 0.4175865352153778\n",
      "Step: 5000  \tTraining accuracy: 0.7993089556694031\n",
      "Step: 5000  \tValid loss: 0.4425674378871918\n",
      "Step: 5100  \tTraining loss: 0.4171276390552521\n",
      "Step: 5100  \tTraining accuracy: 0.7995368242263794\n",
      "Step: 5100  \tValid loss: 0.4424072802066803\n",
      "Step: 5200  \tTraining loss: 0.41668128967285156\n",
      "Step: 5200  \tTraining accuracy: 0.7997637391090393\n",
      "Step: 5200  \tValid loss: 0.4421536326408386\n",
      "Step: 5300  \tTraining loss: 0.4162561595439911\n",
      "Step: 5300  \tTraining accuracy: 0.799974262714386\n",
      "Step: 5300  \tValid loss: 0.4419806897640228\n",
      "Step: 5400  \tTraining loss: 0.4158613979816437\n",
      "Step: 5400  \tTraining accuracy: 0.8001920580863953\n",
      "Step: 5400  \tValid loss: 0.4417518973350525\n",
      "Step: 5500  \tTraining loss: 0.4154907464981079\n",
      "Step: 5500  \tTraining accuracy: 0.8003969192504883\n",
      "Step: 5500  \tValid loss: 0.44152459502220154\n",
      "Step: 5600  \tTraining loss: 0.4151185154914856\n",
      "Step: 5600  \tTraining accuracy: 0.8005700707435608\n",
      "Step: 5600  \tValid loss: 0.4413544535636902\n",
      "Step: 5700  \tTraining loss: 0.41477951407432556\n",
      "Step: 5700  \tTraining accuracy: 0.8007466197013855\n",
      "Step: 5700  \tValid loss: 0.4411809742450714\n",
      "Step: 5800  \tTraining loss: 0.41447150707244873\n",
      "Step: 5800  \tTraining accuracy: 0.8009193539619446\n",
      "Step: 5800  \tValid loss: 0.44107311964035034\n",
      "Step: 5900  \tTraining loss: 0.41415297985076904\n",
      "Step: 5900  \tTraining accuracy: 0.8010908961296082\n",
      "Step: 5900  \tValid loss: 0.4409009516239166\n",
      "Step: 6000  \tTraining loss: 0.4138478934764862\n",
      "Step: 6000  \tTraining accuracy: 0.8012679815292358\n",
      "Step: 6000  \tValid loss: 0.44073304533958435\n",
      "Step: 6100  \tTraining loss: 0.4135081470012665\n",
      "Step: 6100  \tTraining accuracy: 0.8014347553253174\n",
      "Step: 6100  \tValid loss: 0.44060394167900085\n",
      "Step: 6200  \tTraining loss: 0.4131750762462616\n",
      "Step: 6200  \tTraining accuracy: 0.801600456237793\n",
      "Step: 6200  \tValid loss: 0.4404100775718689\n",
      "Step: 6300  \tTraining loss: 0.412864625453949\n",
      "Step: 6300  \tTraining accuracy: 0.8017652630805969\n",
      "Step: 6300  \tValid loss: 0.4402674436569214\n",
      "Step: 6400  \tTraining loss: 0.41256821155548096\n",
      "Step: 6400  \tTraining accuracy: 0.8019333481788635\n",
      "Step: 6400  \tValid loss: 0.4401847720146179\n",
      "Step: 6500  \tTraining loss: 0.412278950214386\n",
      "Step: 6500  \tTraining accuracy: 0.8020920157432556\n",
      "Step: 6500  \tValid loss: 0.4401273727416992\n",
      "Step: 6600  \tTraining loss: 0.41197460889816284\n",
      "Step: 6600  \tTraining accuracy: 0.8022520542144775\n",
      "Step: 6600  \tValid loss: 0.4400065839290619\n",
      "Step: 6700  \tTraining loss: 0.41169023513793945\n",
      "Step: 6700  \tTraining accuracy: 0.8024194836616516\n",
      "Step: 6700  \tValid loss: 0.4398946762084961\n",
      "Step: 6800  \tTraining loss: 0.4114135801792145\n",
      "Step: 6800  \tTraining accuracy: 0.8025819659233093\n",
      "Step: 6800  \tValid loss: 0.4397934675216675\n",
      "Step: 6900  \tTraining loss: 0.4111385941505432\n",
      "Step: 6900  \tTraining accuracy: 0.8027475476264954\n",
      "Step: 6900  \tValid loss: 0.43963518738746643\n",
      "Step: 7000  \tTraining loss: 0.4108813405036926\n",
      "Step: 7000  \tTraining accuracy: 0.8029084205627441\n",
      "Step: 7000  \tValid loss: 0.4396066963672638\n",
      "Step: 7100  \tTraining loss: 0.4106285870075226\n",
      "Step: 7100  \tTraining accuracy: 0.8030627965927124\n",
      "Step: 7100  \tValid loss: 0.4394286870956421\n",
      "Step: 7200  \tTraining loss: 0.4103543758392334\n",
      "Step: 7200  \tTraining accuracy: 0.8032185435295105\n",
      "Step: 7200  \tValid loss: 0.43917882442474365\n",
      "Step: 7300  \tTraining loss: 0.4100806415081024\n",
      "Step: 7300  \tTraining accuracy: 0.803371787071228\n",
      "Step: 7300  \tValid loss: 0.43903788924217224\n",
      "Step: 7400  \tTraining loss: 0.4098241329193115\n",
      "Step: 7400  \tTraining accuracy: 0.8035080432891846\n",
      "Step: 7400  \tValid loss: 0.43887147307395935\n",
      "Step: 7500  \tTraining loss: 0.40955498814582825\n",
      "Step: 7500  \tTraining accuracy: 0.8036406636238098\n",
      "Step: 7500  \tValid loss: 0.4386383295059204\n",
      "Step: 7600  \tTraining loss: 0.409305214881897\n",
      "Step: 7600  \tTraining accuracy: 0.8037751317024231\n",
      "Step: 7600  \tValid loss: 0.43844276666641235\n",
      "Step: 7700  \tTraining loss: 0.40900933742523193\n",
      "Step: 7700  \tTraining accuracy: 0.8039095997810364\n",
      "Step: 7700  \tValid loss: 0.4378560185432434\n",
      "Step: 7800  \tTraining loss: 0.4087374806404114\n",
      "Step: 7800  \tTraining accuracy: 0.8040632605552673\n",
      "Step: 7800  \tValid loss: 0.43768224120140076\n",
      "Step: 7900  \tTraining loss: 0.4084862470626831\n",
      "Step: 7900  \tTraining accuracy: 0.8042095899581909\n",
      "Step: 7900  \tValid loss: 0.43749818205833435\n",
      "Step: 8000  \tTraining loss: 0.40823906660079956\n",
      "Step: 8000  \tTraining accuracy: 0.8043573498725891\n",
      "Step: 8000  \tValid loss: 0.43729013204574585\n",
      "Step: 8100  \tTraining loss: 0.4080018997192383\n",
      "Step: 8100  \tTraining accuracy: 0.804498016834259\n",
      "Step: 8100  \tValid loss: 0.43714639544487\n",
      "Step: 8200  \tTraining loss: 0.40773290395736694\n",
      "Step: 8200  \tTraining accuracy: 0.8046369552612305\n",
      "Step: 8200  \tValid loss: 0.4369620978832245\n",
      "Step: 8300  \tTraining loss: 0.40735188126564026\n",
      "Step: 8300  \tTraining accuracy: 0.8047757744789124\n",
      "Step: 8300  \tValid loss: 0.4368194043636322\n",
      "Step: 8400  \tTraining loss: 0.40700605511665344\n",
      "Step: 8400  \tTraining accuracy: 0.8049015998840332\n",
      "Step: 8400  \tValid loss: 0.4366350769996643\n",
      "Step: 8500  \tTraining loss: 0.4066671133041382\n",
      "Step: 8500  \tTraining accuracy: 0.8050147891044617\n",
      "Step: 8500  \tValid loss: 0.43644121289253235\n",
      "Step: 8600  \tTraining loss: 0.4063616991043091\n",
      "Step: 8600  \tTraining accuracy: 0.805126965045929\n",
      "Step: 8600  \tValid loss: 0.4362125098705292\n",
      "Step: 8700  \tTraining loss: 0.40607020258903503\n",
      "Step: 8700  \tTraining accuracy: 0.8052411675453186\n",
      "Step: 8700  \tValid loss: 0.435970276594162\n",
      "Step: 8800  \tTraining loss: 0.4057924151420593\n",
      "Step: 8800  \tTraining accuracy: 0.8053574562072754\n",
      "Step: 8800  \tValid loss: 0.4357469975948334\n",
      "Step: 8900  \tTraining loss: 0.4055355191230774\n",
      "Step: 8900  \tTraining accuracy: 0.8054757118225098\n",
      "Step: 8900  \tValid loss: 0.43551522493362427\n",
      "Step: 9000  \tTraining loss: 0.405294269323349\n",
      "Step: 9000  \tTraining accuracy: 0.805594265460968\n",
      "Step: 9000  \tValid loss: 0.4353494942188263\n",
      "Step: 9100  \tTraining loss: 0.40506502985954285\n",
      "Step: 9100  \tTraining accuracy: 0.8057102560997009\n",
      "Step: 9100  \tValid loss: 0.43511682748794556\n",
      "Step: 9200  \tTraining loss: 0.4048486649990082\n",
      "Step: 9200  \tTraining accuracy: 0.8058266639709473\n",
      "Step: 9200  \tValid loss: 0.4348975718021393\n",
      "Step: 9300  \tTraining loss: 0.40464040637016296\n",
      "Step: 9300  \tTraining accuracy: 0.8059449791908264\n",
      "Step: 9300  \tValid loss: 0.43473780155181885\n",
      "Step: 9400  \tTraining loss: 0.4044341444969177\n",
      "Step: 9400  \tTraining accuracy: 0.8060592412948608\n",
      "Step: 9400  \tValid loss: 0.43449148535728455\n",
      "Step: 9500  \tTraining loss: 0.4042346775531769\n",
      "Step: 9500  \tTraining accuracy: 0.8061725497245789\n",
      "Step: 9500  \tValid loss: 0.4342840015888214\n",
      "Step: 9600  \tTraining loss: 0.40404340624809265\n",
      "Step: 9600  \tTraining accuracy: 0.8062834739685059\n",
      "Step: 9600  \tValid loss: 0.4340590834617615\n",
      "Step: 9700  \tTraining loss: 0.40385326743125916\n",
      "Step: 9700  \tTraining accuracy: 0.8063977360725403\n",
      "Step: 9700  \tValid loss: 0.43386608362197876\n",
      "Step: 9800  \tTraining loss: 0.4036668837070465\n",
      "Step: 9800  \tTraining accuracy: 0.8065110445022583\n",
      "Step: 9800  \tValid loss: 0.4335920214653015\n",
      "Step: 9900  \tTraining loss: 0.4034889340400696\n",
      "Step: 9900  \tTraining accuracy: 0.8066220283508301\n",
      "Step: 9900  \tValid loss: 0.43339595198631287\n",
      "Step: 10000  \tTraining loss: 0.4033082127571106\n",
      "Step: 10000  \tTraining accuracy: 0.8067334890365601\n",
      "Step: 10000  \tValid loss: 0.4331530034542084\n",
      "Step: 10100  \tTraining loss: 0.4031331539154053\n",
      "Step: 10100  \tTraining accuracy: 0.8068386912345886\n",
      "Step: 10100  \tValid loss: 0.43293580412864685\n",
      "Step: 10200  \tTraining loss: 0.40295758843421936\n",
      "Step: 10200  \tTraining accuracy: 0.806936502456665\n",
      "Step: 10200  \tValid loss: 0.43269461393356323\n",
      "Step: 10300  \tTraining loss: 0.40278854966163635\n",
      "Step: 10300  \tTraining accuracy: 0.8070337176322937\n",
      "Step: 10300  \tValid loss: 0.4324781894683838\n",
      "Step: 10400  \tTraining loss: 0.4026198387145996\n",
      "Step: 10400  \tTraining accuracy: 0.8071212768554688\n",
      "Step: 10400  \tValid loss: 0.43223047256469727\n",
      "Step: 10500  \tTraining loss: 0.4024535119533539\n",
      "Step: 10500  \tTraining accuracy: 0.8072083592414856\n",
      "Step: 10500  \tValid loss: 0.43194788694381714\n",
      "Step: 10600  \tTraining loss: 0.4022815525531769\n",
      "Step: 10600  \tTraining accuracy: 0.807299017906189\n",
      "Step: 10600  \tValid loss: 0.4317070245742798\n",
      "Step: 10700  \tTraining loss: 0.40211230516433716\n",
      "Step: 10700  \tTraining accuracy: 0.8073866367340088\n",
      "Step: 10700  \tValid loss: 0.43147391080856323\n",
      "Step: 10800  \tTraining loss: 0.4019466042518616\n",
      "Step: 10800  \tTraining accuracy: 0.8074650764465332\n",
      "Step: 10800  \tValid loss: 0.4312032461166382\n",
      "Step: 10900  \tTraining loss: 0.4017984867095947\n",
      "Step: 10900  \tTraining accuracy: 0.8075445890426636\n",
      "Step: 10900  \tValid loss: 0.43102389574050903\n",
      "Step: 11000  \tTraining loss: 0.40161368250846863\n",
      "Step: 11000  \tTraining accuracy: 0.807617723941803\n",
      "Step: 11000  \tValid loss: 0.43070852756500244\n",
      "Step: 11100  \tTraining loss: 0.4014575481414795\n",
      "Step: 11100  \tTraining accuracy: 0.8076870441436768\n",
      "Step: 11100  \tValid loss: 0.43051353096961975\n",
      "Step: 11200  \tTraining loss: 0.4012814164161682\n",
      "Step: 11200  \tTraining accuracy: 0.807751476764679\n",
      "Step: 11200  \tValid loss: 0.4301976263523102\n",
      "Step: 11300  \tTraining loss: 0.40111956000328064\n",
      "Step: 11300  \tTraining accuracy: 0.8078196048736572\n",
      "Step: 11300  \tValid loss: 0.42994555830955505\n",
      "Step: 11400  \tTraining loss: 0.4009511172771454\n",
      "Step: 11400  \tTraining accuracy: 0.80788654088974\n",
      "Step: 11400  \tValid loss: 0.4296768605709076\n",
      "Step: 11500  \tTraining loss: 0.40078267455101013\n",
      "Step: 11500  \tTraining accuracy: 0.8079522848129272\n",
      "Step: 11500  \tValid loss: 0.4294201135635376\n",
      "Step: 11600  \tTraining loss: 0.4006366431713104\n",
      "Step: 11600  \tTraining accuracy: 0.8080168962478638\n",
      "Step: 11600  \tValid loss: 0.4291490614414215\n",
      "Step: 11700  \tTraining loss: 0.4004442095756531\n",
      "Step: 11700  \tTraining accuracy: 0.8080804347991943\n",
      "Step: 11700  \tValid loss: 0.4289063513278961\n",
      "Step: 11800  \tTraining loss: 0.40028730034828186\n",
      "Step: 11800  \tTraining accuracy: 0.8081428408622742\n",
      "Step: 11800  \tValid loss: 0.428707093000412\n",
      "Step: 11900  \tTraining loss: 0.40010756254196167\n",
      "Step: 11900  \tTraining accuracy: 0.8082064986228943\n",
      "Step: 11900  \tValid loss: 0.4284118711948395\n",
      "Step: 12000  \tTraining loss: 0.3999394178390503\n",
      "Step: 12000  \tTraining accuracy: 0.8082758784294128\n",
      "Step: 12000  \tValid loss: 0.42808347940444946\n",
      "Step: 12100  \tTraining loss: 0.3997705280780792\n",
      "Step: 12100  \tTraining accuracy: 0.8083463311195374\n",
      "Step: 12100  \tValid loss: 0.4279034435749054\n",
      "Step: 12200  \tTraining loss: 0.3996022641658783\n",
      "Step: 12200  \tTraining accuracy: 0.8084156513214111\n",
      "Step: 12200  \tValid loss: 0.4276587665081024\n",
      "Step: 12300  \tTraining loss: 0.3994397819042206\n",
      "Step: 12300  \tTraining accuracy: 0.8084838390350342\n",
      "Step: 12300  \tValid loss: 0.4273565411567688\n",
      "Step: 12400  \tTraining loss: 0.39926251769065857\n",
      "Step: 12400  \tTraining accuracy: 0.8085508942604065\n",
      "Step: 12400  \tValid loss: 0.4271845817565918\n",
      "Step: 12500  \tTraining loss: 0.3991089463233948\n",
      "Step: 12500  \tTraining accuracy: 0.8086168766021729\n",
      "Step: 12500  \tValid loss: 0.42692506313323975\n",
      "Step: 12600  \tTraining loss: 0.3989420533180237\n",
      "Step: 12600  \tTraining accuracy: 0.808681845664978\n",
      "Step: 12600  \tValid loss: 0.42680251598358154\n",
      "Step: 12700  \tTraining loss: 0.39875268936157227\n",
      "Step: 12700  \tTraining accuracy: 0.8087468147277832\n",
      "Step: 12700  \tValid loss: 0.42656293511390686\n",
      "Step: 12800  \tTraining loss: 0.3985845148563385\n",
      "Step: 12800  \tTraining accuracy: 0.8088139891624451\n",
      "Step: 12800  \tValid loss: 0.42634129524230957\n",
      "Step: 12900  \tTraining loss: 0.3984116017818451\n",
      "Step: 12900  \tTraining accuracy: 0.8088758587837219\n",
      "Step: 12900  \tValid loss: 0.4260987937450409\n",
      "Step: 13000  \tTraining loss: 0.3982468545436859\n",
      "Step: 13000  \tTraining accuracy: 0.8089367747306824\n",
      "Step: 13000  \tValid loss: 0.42593875527381897\n",
      "Step: 13100  \tTraining loss: 0.3980812430381775\n",
      "Step: 13100  \tTraining accuracy: 0.8089967966079712\n",
      "Step: 13100  \tValid loss: 0.4257156252861023\n",
      "Step: 13200  \tTraining loss: 0.39792999625205994\n",
      "Step: 13200  \tTraining accuracy: 0.8090579509735107\n",
      "Step: 13200  \tValid loss: 0.4254923164844513\n",
      "Step: 13300  \tTraining loss: 0.3977765738964081\n",
      "Step: 13300  \tTraining accuracy: 0.8091212511062622\n",
      "Step: 13300  \tValid loss: 0.4252917468547821\n",
      "Step: 13400  \tTraining loss: 0.3975941836833954\n",
      "Step: 13400  \tTraining accuracy: 0.8091835975646973\n",
      "Step: 13400  \tValid loss: 0.425090491771698\n",
      "Step: 13500  \tTraining loss: 0.3974493145942688\n",
      "Step: 13500  \tTraining accuracy: 0.8092449903488159\n",
      "Step: 13500  \tValid loss: 0.4249034523963928\n",
      "Step: 13600  \tTraining loss: 0.3972795009613037\n",
      "Step: 13600  \tTraining accuracy: 0.8093035221099854\n",
      "Step: 13600  \tValid loss: 0.42467203736305237\n",
      "Step: 13700  \tTraining loss: 0.39712780714035034\n",
      "Step: 13700  \tTraining accuracy: 0.8093581795692444\n",
      "Step: 13700  \tValid loss: 0.4244730472564697\n",
      "Step: 13800  \tTraining loss: 0.3969637453556061\n",
      "Step: 13800  \tTraining accuracy: 0.8094120621681213\n",
      "Step: 13800  \tValid loss: 0.4243113696575165\n",
      "Step: 13900  \tTraining loss: 0.3968367278575897\n",
      "Step: 13900  \tTraining accuracy: 0.8094651699066162\n",
      "Step: 13900  \tValid loss: 0.4241258502006531\n",
      "Step: 14000  \tTraining loss: 0.39667391777038574\n",
      "Step: 14000  \tTraining accuracy: 0.809517502784729\n",
      "Step: 14000  \tValid loss: 0.42385241389274597\n",
      "Step: 14100  \tTraining loss: 0.39650800824165344\n",
      "Step: 14100  \tTraining accuracy: 0.8095691204071045\n",
      "Step: 14100  \tValid loss: 0.4236376881599426\n",
      "Step: 14200  \tTraining loss: 0.3963530361652374\n",
      "Step: 14200  \tTraining accuracy: 0.809616208076477\n",
      "Step: 14200  \tValid loss: 0.4234810173511505\n",
      "Step: 14300  \tTraining loss: 0.3962040841579437\n",
      "Step: 14300  \tTraining accuracy: 0.8096654415130615\n",
      "Step: 14300  \tValid loss: 0.423239529132843\n",
      "Step: 14400  \tTraining loss: 0.3960418105125427\n",
      "Step: 14400  \tTraining accuracy: 0.8097149133682251\n",
      "Step: 14400  \tValid loss: 0.4230397045612335\n",
      "Step: 14500  \tTraining loss: 0.3958778977394104\n",
      "Step: 14500  \tTraining accuracy: 0.8097665309906006\n",
      "Step: 14500  \tValid loss: 0.42284324765205383\n",
      "Step: 14600  \tTraining loss: 0.39569658041000366\n",
      "Step: 14600  \tTraining accuracy: 0.8098192811012268\n",
      "Step: 14600  \tValid loss: 0.4226672351360321\n",
      "Step: 14700  \tTraining loss: 0.3955441415309906\n",
      "Step: 14700  \tTraining accuracy: 0.8098713159561157\n",
      "Step: 14700  \tValid loss: 0.422529935836792\n",
      "Step: 14800  \tTraining loss: 0.39535507559776306\n",
      "Step: 14800  \tTraining accuracy: 0.8099235892295837\n",
      "Step: 14800  \tValid loss: 0.42236724495887756\n",
      "Step: 14900  \tTraining loss: 0.39520972967147827\n",
      "Step: 14900  \tTraining accuracy: 0.8099787831306458\n",
      "Step: 14900  \tValid loss: 0.42221400141716003\n",
      "Step: 15000  \tTraining loss: 0.3950643837451935\n",
      "Step: 15000  \tTraining accuracy: 0.8100332617759705\n",
      "Step: 15000  \tValid loss: 0.4219817519187927\n",
      "Step: 15100  \tTraining loss: 0.3949202597141266\n",
      "Step: 15100  \tTraining accuracy: 0.8100869655609131\n",
      "Step: 15100  \tValid loss: 0.4217979311943054\n",
      "Step: 15200  \tTraining loss: 0.39477527141571045\n",
      "Step: 15200  \tTraining accuracy: 0.8101400136947632\n",
      "Step: 15200  \tValid loss: 0.4216821491718292\n",
      "Step: 15300  \tTraining loss: 0.3946402966976166\n",
      "Step: 15300  \tTraining accuracy: 0.810192346572876\n",
      "Step: 15300  \tValid loss: 0.4215177893638611\n",
      "Step: 15400  \tTraining loss: 0.39450910687446594\n",
      "Step: 15400  \tTraining accuracy: 0.8102439641952515\n",
      "Step: 15400  \tValid loss: 0.421317458152771\n",
      "Step: 15500  \tTraining loss: 0.3943822383880615\n",
      "Step: 15500  \tTraining accuracy: 0.8102949261665344\n",
      "Step: 15500  \tValid loss: 0.42122766375541687\n",
      "Step: 15600  \tTraining loss: 0.39425888657569885\n",
      "Step: 15600  \tTraining accuracy: 0.8103452920913696\n",
      "Step: 15600  \tValid loss: 0.4210413098335266\n",
      "Step: 15700  \tTraining loss: 0.39413031935691833\n",
      "Step: 15700  \tTraining accuracy: 0.8103949427604675\n",
      "Step: 15700  \tValid loss: 0.420960396528244\n",
      "Step: 15800  \tTraining loss: 0.39403724670410156\n",
      "Step: 15800  \tTraining accuracy: 0.8104448318481445\n",
      "Step: 15800  \tValid loss: 0.4208495318889618\n",
      "Step: 15900  \tTraining loss: 0.39387133717536926\n",
      "Step: 15900  \tTraining accuracy: 0.8104966878890991\n",
      "Step: 15900  \tValid loss: 0.4207233190536499\n",
      "Step: 16000  \tTraining loss: 0.3937431871891022\n",
      "Step: 16000  \tTraining accuracy: 0.8105453252792358\n",
      "Step: 16000  \tValid loss: 0.42055338621139526\n",
      "Step: 16100  \tTraining loss: 0.3936215043067932\n",
      "Step: 16100  \tTraining accuracy: 0.8105967044830322\n",
      "Step: 16100  \tValid loss: 0.4204445779323578\n",
      "Step: 16200  \tTraining loss: 0.39350631833076477\n",
      "Step: 16200  \tTraining accuracy: 0.8106474876403809\n",
      "Step: 16200  \tValid loss: 0.4203336834907532\n",
      "Step: 16300  \tTraining loss: 0.39340388774871826\n",
      "Step: 16300  \tTraining accuracy: 0.810697615146637\n",
      "Step: 16300  \tValid loss: 0.4202595353126526\n",
      "Step: 16400  \tTraining loss: 0.39326798915863037\n",
      "Step: 16400  \tTraining accuracy: 0.8107471466064453\n",
      "Step: 16400  \tValid loss: 0.42013654112815857\n",
      "Step: 16500  \tTraining loss: 0.39315101504325867\n",
      "Step: 16500  \tTraining accuracy: 0.8107960820198059\n",
      "Step: 16500  \tValid loss: 0.41998374462127686\n",
      "Step: 16600  \tTraining loss: 0.3930552005767822\n",
      "Step: 16600  \tTraining accuracy: 0.8108468651771545\n",
      "Step: 16600  \tValid loss: 0.41981884837150574\n",
      "Step: 16700  \tTraining loss: 0.3929339051246643\n",
      "Step: 16700  \tTraining accuracy: 0.8108987212181091\n",
      "Step: 16700  \tValid loss: 0.4197468161582947\n",
      "Step: 16800  \tTraining loss: 0.3928042948246002\n",
      "Step: 16800  \tTraining accuracy: 0.8109498620033264\n",
      "Step: 16800  \tValid loss: 0.4196207821369171\n",
      "Step: 16900  \tTraining loss: 0.3926942050457001\n",
      "Step: 16900  \tTraining accuracy: 0.8109964728355408\n",
      "Step: 16900  \tValid loss: 0.4195214807987213\n",
      "Step: 17000  \tTraining loss: 0.392583966255188\n",
      "Step: 17000  \tTraining accuracy: 0.8110432624816895\n",
      "Step: 17000  \tValid loss: 0.4194186329841614\n",
      "Step: 17100  \tTraining loss: 0.3925204873085022\n",
      "Step: 17100  \tTraining accuracy: 0.8110902905464172\n",
      "Step: 17100  \tValid loss: 0.4194014072418213\n",
      "Step: 17200  \tTraining loss: 0.3923758566379547\n",
      "Step: 17200  \tTraining accuracy: 0.8111391663551331\n",
      "Step: 17200  \tValid loss: 0.4191613495349884\n",
      "Step: 17300  \tTraining loss: 0.39226213097572327\n",
      "Step: 17300  \tTraining accuracy: 0.8111875057220459\n",
      "Step: 17300  \tValid loss: 0.4190675616264343\n",
      "Step: 17400  \tTraining loss: 0.3921513259410858\n",
      "Step: 17400  \tTraining accuracy: 0.8112329244613647\n",
      "Step: 17400  \tValid loss: 0.41892170906066895\n",
      "Step: 17500  \tTraining loss: 0.3920401930809021\n",
      "Step: 17500  \tTraining accuracy: 0.8112785816192627\n",
      "Step: 17500  \tValid loss: 0.41882288455963135\n",
      "Step: 17600  \tTraining loss: 0.39193660020828247\n",
      "Step: 17600  \tTraining accuracy: 0.8113252520561218\n",
      "Step: 17600  \tValid loss: 0.41873258352279663\n",
      "Step: 17700  \tTraining loss: 0.39183130860328674\n",
      "Step: 17700  \tTraining accuracy: 0.8113714456558228\n",
      "Step: 17700  \tValid loss: 0.41864117980003357\n",
      "Step: 17800  \tTraining loss: 0.39173606038093567\n",
      "Step: 17800  \tTraining accuracy: 0.8114170432090759\n",
      "Step: 17800  \tValid loss: 0.41858428716659546\n",
      "Step: 17900  \tTraining loss: 0.3916385769844055\n",
      "Step: 17900  \tTraining accuracy: 0.8114621639251709\n",
      "Step: 17900  \tValid loss: 0.41844961047172546\n",
      "Step: 18000  \tTraining loss: 0.3915337026119232\n",
      "Step: 18000  \tTraining accuracy: 0.8115068078041077\n",
      "Step: 18000  \tValid loss: 0.4183763861656189\n",
      "Step: 18100  \tTraining loss: 0.39142751693725586\n",
      "Step: 18100  \tTraining accuracy: 0.8115509152412415\n",
      "Step: 18100  \tValid loss: 0.4182743430137634\n",
      "Step: 18200  \tTraining loss: 0.3913290202617645\n",
      "Step: 18200  \tTraining accuracy: 0.8115975856781006\n",
      "Step: 18200  \tValid loss: 0.41818371415138245\n",
      "Step: 18300  \tTraining loss: 0.3912317156791687\n",
      "Step: 18300  \tTraining accuracy: 0.8116414546966553\n",
      "Step: 18300  \tValid loss: 0.4180958867073059\n",
      "Step: 18400  \tTraining loss: 0.3911350667476654\n",
      "Step: 18400  \tTraining accuracy: 0.8116870522499084\n",
      "Step: 18400  \tValid loss: 0.41799119114875793\n",
      "Step: 18500  \tTraining loss: 0.3910352885723114\n",
      "Step: 18500  \tTraining accuracy: 0.8117329478263855\n",
      "Step: 18500  \tValid loss: 0.41790154576301575\n",
      "Step: 18600  \tTraining loss: 0.39096200466156006\n",
      "Step: 18600  \tTraining accuracy: 0.8117805123329163\n",
      "Step: 18600  \tValid loss: 0.4178553521633148\n",
      "Step: 18700  \tTraining loss: 0.3908710479736328\n",
      "Step: 18700  \tTraining accuracy: 0.8118239045143127\n",
      "Step: 18700  \tValid loss: 0.4176912009716034\n",
      "Step: 18800  \tTraining loss: 0.39074447751045227\n",
      "Step: 18800  \tTraining accuracy: 0.8118646740913391\n",
      "Step: 18800  \tValid loss: 0.41769275069236755\n",
      "Step: 18900  \tTraining loss: 0.3906814455986023\n",
      "Step: 18900  \tTraining accuracy: 0.811907947063446\n",
      "Step: 18900  \tValid loss: 0.4175523817539215\n",
      "Step: 19000  \tTraining loss: 0.39055952429771423\n",
      "Step: 19000  \tTraining accuracy: 0.8119485378265381\n",
      "Step: 19000  \tValid loss: 0.4174686372280121\n",
      "Step: 19100  \tTraining loss: 0.3904804289340973\n",
      "Step: 19100  \tTraining accuracy: 0.8119908571243286\n",
      "Step: 19100  \tValid loss: 0.4173761308193207\n",
      "Step: 19200  \tTraining loss: 0.3903748095035553\n",
      "Step: 19200  \tTraining accuracy: 0.812033474445343\n",
      "Step: 19200  \tValid loss: 0.4173446595668793\n",
      "Step: 19300  \tTraining loss: 0.39028510451316833\n",
      "Step: 19300  \tTraining accuracy: 0.8120756149291992\n",
      "Step: 19300  \tValid loss: 0.4171936511993408\n",
      "Step: 19400  \tTraining loss: 0.39018118381500244\n",
      "Step: 19400  \tTraining accuracy: 0.812117338180542\n",
      "Step: 19400  \tValid loss: 0.4171546399593353\n",
      "Step: 19500  \tTraining loss: 0.39009082317352295\n",
      "Step: 19500  \tTraining accuracy: 0.8121586441993713\n",
      "Step: 19500  \tValid loss: 0.4170685410499573\n",
      "Step: 19600  \tTraining loss: 0.390001505613327\n",
      "Step: 19600  \tTraining accuracy: 0.8121995329856873\n",
      "Step: 19600  \tValid loss: 0.4170133173465729\n",
      "Step: 19700  \tTraining loss: 0.3899243175983429\n",
      "Step: 19700  \tTraining accuracy: 0.8122434020042419\n",
      "Step: 19700  \tValid loss: 0.416901171207428\n",
      "Step: 19800  \tTraining loss: 0.38983598351478577\n",
      "Step: 19800  \tTraining accuracy: 0.8122868537902832\n",
      "Step: 19800  \tValid loss: 0.416807621717453\n",
      "Step: 19900  \tTraining loss: 0.3897339105606079\n",
      "Step: 19900  \tTraining accuracy: 0.8123312592506409\n",
      "Step: 19900  \tValid loss: 0.4167358875274658\n",
      "Step: 20000  \tTraining loss: 0.3896559476852417\n",
      "Step: 20000  \tTraining accuracy: 0.8123772144317627\n",
      "Step: 20000  \tValid loss: 0.4166108965873718\n",
      "Step: 20100  \tTraining loss: 0.38957083225250244\n",
      "Step: 20100  \tTraining accuracy: 0.8124226927757263\n",
      "Step: 20100  \tValid loss: 0.41653040051460266\n",
      "Step: 20200  \tTraining loss: 0.3894905149936676\n",
      "Step: 20200  \tTraining accuracy: 0.8124677538871765\n",
      "Step: 20200  \tValid loss: 0.4164513945579529\n",
      "Step: 20300  \tTraining loss: 0.38937291502952576\n",
      "Step: 20300  \tTraining accuracy: 0.8125123381614685\n",
      "Step: 20300  \tValid loss: 0.4163757264614105\n",
      "Step: 20400  \tTraining loss: 0.3893240690231323\n",
      "Step: 20400  \tTraining accuracy: 0.8125558495521545\n",
      "Step: 20400  \tValid loss: 0.4163019359111786\n",
      "Step: 20500  \tTraining loss: 0.3892248272895813\n",
      "Step: 20500  \tTraining accuracy: 0.8125962615013123\n",
      "Step: 20500  \tValid loss: 0.41623660922050476\n",
      "Step: 20600  \tTraining loss: 0.38914862275123596\n",
      "Step: 20600  \tTraining accuracy: 0.8126363158226013\n",
      "Step: 20600  \tValid loss: 0.41619378328323364\n",
      "Step: 20700  \tTraining loss: 0.3890290856361389\n",
      "Step: 20700  \tTraining accuracy: 0.812675952911377\n",
      "Step: 20700  \tValid loss: 0.4161212742328644\n",
      "Step: 20800  \tTraining loss: 0.3889508843421936\n",
      "Step: 20800  \tTraining accuracy: 0.8127152323722839\n",
      "Step: 20800  \tValid loss: 0.4159698188304901\n",
      "Step: 20900  \tTraining loss: 0.38888975977897644\n",
      "Step: 20900  \tTraining accuracy: 0.8127541542053223\n",
      "Step: 20900  \tValid loss: 0.4159807860851288\n",
      "Step: 21000  \tTraining loss: 0.3887861371040344\n",
      "Step: 21000  \tTraining accuracy: 0.8127926588058472\n",
      "Step: 21000  \tValid loss: 0.4158283472061157\n",
      "Step: 21100  \tTraining loss: 0.38868871331214905\n",
      "Step: 21100  \tTraining accuracy: 0.8128308057785034\n",
      "Step: 21100  \tValid loss: 0.41571465134620667\n",
      "Step: 21200  \tTraining loss: 0.38860228657722473\n",
      "Step: 21200  \tTraining accuracy: 0.812868595123291\n",
      "Step: 21200  \tValid loss: 0.41567176580429077\n",
      "Step: 21300  \tTraining loss: 0.38851454854011536\n",
      "Step: 21300  \tTraining accuracy: 0.8129041790962219\n",
      "Step: 21300  \tValid loss: 0.41554731130599976\n",
      "Step: 21400  \tTraining loss: 0.38842812180519104\n",
      "Step: 21400  \tTraining accuracy: 0.8129380941390991\n",
      "Step: 21400  \tValid loss: 0.4154791235923767\n",
      "Step: 21500  \tTraining loss: 0.38835880160331726\n",
      "Step: 21500  \tTraining accuracy: 0.8129717111587524\n",
      "Step: 21500  \tValid loss: 0.41538140177726746\n",
      "Step: 21600  \tTraining loss: 0.3882634937763214\n",
      "Step: 21600  \tTraining accuracy: 0.8130062818527222\n",
      "Step: 21600  \tValid loss: 0.4153176248073578\n",
      "Step: 21700  \tTraining loss: 0.3882104456424713\n",
      "Step: 21700  \tTraining accuracy: 0.8130423426628113\n",
      "Step: 21700  \tValid loss: 0.4152155816555023\n",
      "Step: 21800  \tTraining loss: 0.3880910277366638\n",
      "Step: 21800  \tTraining accuracy: 0.8130781650543213\n",
      "Step: 21800  \tValid loss: 0.4151294529438019\n",
      "Step: 21900  \tTraining loss: 0.3880140781402588\n",
      "Step: 21900  \tTraining accuracy: 0.8131135702133179\n",
      "Step: 21900  \tValid loss: 0.415045827627182\n",
      "Step: 22000  \tTraining loss: 0.3879246413707733\n",
      "Step: 22000  \tTraining accuracy: 0.8131487369537354\n",
      "Step: 22000  \tValid loss: 0.414926141500473\n",
      "Step: 22100  \tTraining loss: 0.3878409266471863\n",
      "Step: 22100  \tTraining accuracy: 0.8131835460662842\n",
      "Step: 22100  \tValid loss: 0.41482558846473694\n",
      "Step: 22200  \tTraining loss: 0.3877682387828827\n",
      "Step: 22200  \tTraining accuracy: 0.8132180571556091\n",
      "Step: 22200  \tValid loss: 0.41481539607048035\n",
      "Step: 22300  \tTraining loss: 0.3877031207084656\n",
      "Step: 22300  \tTraining accuracy: 0.8132522106170654\n",
      "Step: 22300  \tValid loss: 0.4147331118583679\n",
      "Step: 22400  \tTraining loss: 0.3876197636127472\n",
      "Step: 22400  \tTraining accuracy: 0.8132861256599426\n",
      "Step: 22400  \tValid loss: 0.4146699011325836\n",
      "Step: 22500  \tTraining loss: 0.38750794529914856\n",
      "Step: 22500  \tTraining accuracy: 0.8133196830749512\n",
      "Step: 22500  \tValid loss: 0.4145330488681793\n",
      "Step: 22600  \tTraining loss: 0.38745376467704773\n",
      "Step: 22600  \tTraining accuracy: 0.8133529424667358\n",
      "Step: 22600  \tValid loss: 0.41448378562927246\n",
      "Step: 22700  \tTraining loss: 0.38734546303749084\n",
      "Step: 22700  \tTraining accuracy: 0.8133859634399414\n",
      "Step: 22700  \tValid loss: 0.4143671989440918\n",
      "Step: 22800  \tTraining loss: 0.3872818946838379\n",
      "Step: 22800  \tTraining accuracy: 0.8134186267852783\n",
      "Step: 22800  \tValid loss: 0.4142850339412689\n",
      "Step: 22900  \tTraining loss: 0.38718387484550476\n",
      "Step: 22900  \tTraining accuracy: 0.8134522438049316\n",
      "Step: 22900  \tValid loss: 0.41417524218559265\n",
      "Step: 23000  \tTraining loss: 0.3871157765388489\n",
      "Step: 23000  \tTraining accuracy: 0.8134872913360596\n",
      "Step: 23000  \tValid loss: 0.41409289836883545\n",
      "Step: 23100  \tTraining loss: 0.38700634241104126\n",
      "Step: 23100  \tTraining accuracy: 0.8135221004486084\n",
      "Step: 23100  \tValid loss: 0.41407087445259094\n",
      "Step: 23200  \tTraining loss: 0.3869313895702362\n",
      "Step: 23200  \tTraining accuracy: 0.8135565519332886\n",
      "Step: 23200  \tValid loss: 0.4139821231365204\n",
      "Step: 23300  \tTraining loss: 0.38685065507888794\n",
      "Step: 23300  \tTraining accuracy: 0.8135907053947449\n",
      "Step: 23300  \tValid loss: 0.41387712955474854\n",
      "Step: 23400  \tTraining loss: 0.3867553472518921\n",
      "Step: 23400  \tTraining accuracy: 0.8136245608329773\n",
      "Step: 23400  \tValid loss: 0.41383886337280273\n",
      "Step: 23500  \tTraining loss: 0.3867042660713196\n",
      "Step: 23500  \tTraining accuracy: 0.8136581778526306\n",
      "Step: 23500  \tValid loss: 0.41371095180511475\n",
      "Step: 23600  \tTraining loss: 0.38660430908203125\n",
      "Step: 23600  \tTraining accuracy: 0.8136926293373108\n",
      "Step: 23600  \tValid loss: 0.41361451148986816\n",
      "Step: 23700  \tTraining loss: 0.38650843501091003\n",
      "Step: 23700  \tTraining accuracy: 0.8137285113334656\n",
      "Step: 23700  \tValid loss: 0.413532018661499\n",
      "Step: 23800  \tTraining loss: 0.3864375352859497\n",
      "Step: 23800  \tTraining accuracy: 0.8137634992599487\n",
      "Step: 23800  \tValid loss: 0.41346466541290283\n",
      "Step: 23900  \tTraining loss: 0.3863319456577301\n",
      "Step: 23900  \tTraining accuracy: 0.8137947916984558\n",
      "Step: 23900  \tValid loss: 0.4133846163749695\n",
      "Step: 24000  \tTraining loss: 0.38624730706214905\n",
      "Step: 24000  \tTraining accuracy: 0.8138241767883301\n",
      "Step: 24000  \tValid loss: 0.41331711411476135\n",
      "Step: 24100  \tTraining loss: 0.38614150881767273\n",
      "Step: 24100  \tTraining accuracy: 0.8138532638549805\n",
      "Step: 24100  \tValid loss: 0.41323840618133545\n",
      "Step: 24200  \tTraining loss: 0.38607943058013916\n",
      "Step: 24200  \tTraining accuracy: 0.8138832449913025\n",
      "Step: 24200  \tValid loss: 0.4130948483943939\n",
      "Step: 24300  \tTraining loss: 0.3859633803367615\n",
      "Step: 24300  \tTraining accuracy: 0.8139146566390991\n",
      "Step: 24300  \tValid loss: 0.41298454999923706\n",
      "Step: 24400  \tTraining loss: 0.385875403881073\n",
      "Step: 24400  \tTraining accuracy: 0.8139458298683167\n",
      "Step: 24400  \tValid loss: 0.41292938590049744\n",
      "Step: 24500  \tTraining loss: 0.38578274846076965\n",
      "Step: 24500  \tTraining accuracy: 0.8139767050743103\n",
      "Step: 24500  \tValid loss: 0.4127943813800812\n",
      "Step: 24600  \tTraining loss: 0.38566839694976807\n",
      "Step: 24600  \tTraining accuracy: 0.8140073418617249\n",
      "Step: 24600  \tValid loss: 0.4127238690853119\n",
      "Step: 24700  \tTraining loss: 0.3855724036693573\n",
      "Step: 24700  \tTraining accuracy: 0.8140377402305603\n",
      "Step: 24700  \tValid loss: 0.41263312101364136\n",
      "Step: 24800  \tTraining loss: 0.38549554347991943\n",
      "Step: 24800  \tTraining accuracy: 0.8140679001808167\n",
      "Step: 24800  \tValid loss: 0.41259005665779114\n",
      "Step: 24900  \tTraining loss: 0.38538679480552673\n",
      "Step: 24900  \tTraining accuracy: 0.8140978217124939\n",
      "Step: 24900  \tValid loss: 0.41237711906433105\n",
      "Step: 25000  \tTraining loss: 0.3852929174900055\n",
      "Step: 25000  \tTraining accuracy: 0.814127504825592\n",
      "Step: 25000  \tValid loss: 0.4123048186302185\n",
      "Step: 25100  \tTraining loss: 0.3851895034313202\n",
      "Step: 25100  \tTraining accuracy: 0.8141569495201111\n",
      "Step: 25100  \tValid loss: 0.41215887665748596\n",
      "Step: 25200  \tTraining loss: 0.38508766889572144\n",
      "Step: 25200  \tTraining accuracy: 0.814186155796051\n",
      "Step: 25200  \tValid loss: 0.41213977336883545\n",
      "Step: 25300  \tTraining loss: 0.38502874970436096\n",
      "Step: 25300  \tTraining accuracy: 0.8142151236534119\n",
      "Step: 25300  \tValid loss: 0.4119642972946167\n",
      "Step: 25400  \tTraining loss: 0.3848609924316406\n",
      "Step: 25400  \tTraining accuracy: 0.8142438530921936\n",
      "Step: 25400  \tValid loss: 0.4119100868701935\n",
      "Step: 25500  \tTraining loss: 0.38477519154548645\n",
      "Step: 25500  \tTraining accuracy: 0.8142697215080261\n",
      "Step: 25500  \tValid loss: 0.41179361939430237\n",
      "Step: 25600  \tTraining loss: 0.384704053401947\n",
      "Step: 25600  \tTraining accuracy: 0.8142927289009094\n",
      "Step: 25600  \tValid loss: 0.41162511706352234\n",
      "Step: 25700  \tTraining loss: 0.38459280133247375\n",
      "Step: 25700  \tTraining accuracy: 0.8143155574798584\n",
      "Step: 25700  \tValid loss: 0.4115348756313324\n",
      "Step: 25800  \tTraining loss: 0.3845699429512024\n",
      "Step: 25800  \tTraining accuracy: 0.814338207244873\n",
      "Step: 25800  \tValid loss: 0.4114513695240021\n",
      "Step: 25900  \tTraining loss: 0.38444530963897705\n",
      "Step: 25900  \tTraining accuracy: 0.8143606781959534\n",
      "Step: 25900  \tValid loss: 0.41139018535614014\n",
      "Step: 26000  \tTraining loss: 0.3843511939048767\n",
      "Step: 26000  \tTraining accuracy: 0.8143829703330994\n",
      "Step: 26000  \tValid loss: 0.41122645139694214\n",
      "Step: 26100  \tTraining loss: 0.3842584788799286\n",
      "Step: 26100  \tTraining accuracy: 0.814406156539917\n",
      "Step: 26100  \tValid loss: 0.4111635684967041\n",
      "Step: 26200  \tTraining loss: 0.38419923186302185\n",
      "Step: 26200  \tTraining accuracy: 0.8144307136535645\n",
      "Step: 26200  \tValid loss: 0.4110572338104248\n",
      "Step: 26300  \tTraining loss: 0.3840973675251007\n",
      "Step: 26300  \tTraining accuracy: 0.8144550323486328\n",
      "Step: 26300  \tValid loss: 0.4110233187675476\n",
      "Step: 26400  \tTraining loss: 0.3840472996234894\n",
      "Step: 26400  \tTraining accuracy: 0.8144792318344116\n",
      "Step: 26400  \tValid loss: 0.41097021102905273\n",
      "Step: 26500  \tTraining loss: 0.38395264744758606\n",
      "Step: 26500  \tTraining accuracy: 0.8145031929016113\n",
      "Step: 26500  \tValid loss: 0.41085025668144226\n",
      "Step: 26600  \tTraining loss: 0.3839007318019867\n",
      "Step: 26600  \tTraining accuracy: 0.8145260214805603\n",
      "Step: 26600  \tValid loss: 0.41080421209335327\n",
      "Step: 26700  \tTraining loss: 0.3837980329990387\n",
      "Step: 26700  \tTraining accuracy: 0.8145491480827332\n",
      "Step: 26700  \tValid loss: 0.4107336401939392\n",
      "Step: 26800  \tTraining loss: 0.38373449444770813\n",
      "Step: 26800  \tTraining accuracy: 0.8145725727081299\n",
      "Step: 26800  \tValid loss: 0.41065356135368347\n",
      "Step: 26900  \tTraining loss: 0.3836703300476074\n",
      "Step: 26900  \tTraining accuracy: 0.8145958781242371\n",
      "Step: 26900  \tValid loss: 0.4105335474014282\n",
      "Step: 27000  \tTraining loss: 0.38360702991485596\n",
      "Step: 27000  \tTraining accuracy: 0.8146190047264099\n",
      "Step: 27000  \tValid loss: 0.41054245829582214\n",
      "Step: 27100  \tTraining loss: 0.38350963592529297\n",
      "Step: 27100  \tTraining accuracy: 0.8146419525146484\n",
      "Step: 27100  \tValid loss: 0.41048234701156616\n",
      "Step: 27200  \tTraining loss: 0.38343140482902527\n",
      "Step: 27200  \tTraining accuracy: 0.8146647214889526\n",
      "Step: 27200  \tValid loss: 0.4103338122367859\n",
      "Step: 27300  \tTraining loss: 0.3833753764629364\n",
      "Step: 27300  \tTraining accuracy: 0.8146873116493225\n",
      "Step: 27300  \tValid loss: 0.4103180468082428\n",
      "Step: 27400  \tTraining loss: 0.3832845687866211\n",
      "Step: 27400  \tTraining accuracy: 0.8147097229957581\n",
      "Step: 27400  \tValid loss: 0.4102131426334381\n",
      "Step: 27500  \tTraining loss: 0.38323017954826355\n",
      "Step: 27500  \tTraining accuracy: 0.814732015132904\n",
      "Step: 27500  \tValid loss: 0.41017669439315796\n",
      "Step: 27600  \tTraining loss: 0.38317087292671204\n",
      "Step: 27600  \tTraining accuracy: 0.8147541284561157\n",
      "Step: 27600  \tValid loss: 0.4100838601589203\n",
      "Step: 27700  \tTraining loss: 0.38308826088905334\n",
      "Step: 27700  \tTraining accuracy: 0.8147780299186707\n",
      "Step: 27700  \tValid loss: 0.41008302569389343\n",
      "Step: 27800  \tTraining loss: 0.38301774859428406\n",
      "Step: 27800  \tTraining accuracy: 0.8148007988929749\n",
      "Step: 27800  \tValid loss: 0.4100099503993988\n",
      "Step: 27900  \tTraining loss: 0.38296037912368774\n",
      "Step: 27900  \tTraining accuracy: 0.8148229122161865\n",
      "Step: 27900  \tValid loss: 0.4098358154296875\n",
      "Step: 28000  \tTraining loss: 0.3828786313533783\n",
      "Step: 28000  \tTraining accuracy: 0.8148468136787415\n",
      "Step: 28000  \tValid loss: 0.40984225273132324\n",
      "Step: 28100  \tTraining loss: 0.38283541798591614\n",
      "Step: 28100  \tTraining accuracy: 0.8148705363273621\n",
      "Step: 28100  \tValid loss: 0.40978601574897766\n",
      "Step: 28200  \tTraining loss: 0.3827504813671112\n",
      "Step: 28200  \tTraining accuracy: 0.814895510673523\n",
      "Step: 28200  \tValid loss: 0.4097995460033417\n",
      "Step: 28300  \tTraining loss: 0.38268330693244934\n",
      "Step: 28300  \tTraining accuracy: 0.8149213194847107\n",
      "Step: 28300  \tValid loss: 0.4097276031970978\n",
      "Step: 28400  \tTraining loss: 0.38262003660202026\n",
      "Step: 28400  \tTraining accuracy: 0.8149468898773193\n",
      "Step: 28400  \tValid loss: 0.4096219837665558\n",
      "Step: 28500  \tTraining loss: 0.38255593180656433\n",
      "Step: 28500  \tTraining accuracy: 0.8149723410606384\n",
      "Step: 28500  \tValid loss: 0.4095659852027893\n",
      "Step: 28600  \tTraining loss: 0.3824944496154785\n",
      "Step: 28600  \tTraining accuracy: 0.814998984336853\n",
      "Step: 28600  \tValid loss: 0.40949079394340515\n",
      "Step: 28700  \tTraining loss: 0.38243386149406433\n",
      "Step: 28700  \tTraining accuracy: 0.8150264024734497\n",
      "Step: 28700  \tValid loss: 0.4094945788383484\n",
      "Step: 28800  \tTraining loss: 0.3823760449886322\n",
      "Step: 28800  \tTraining accuracy: 0.8150526881217957\n",
      "Step: 28800  \tValid loss: 0.4093377888202667\n",
      "Step: 28900  \tTraining loss: 0.382312536239624\n",
      "Step: 28900  \tTraining accuracy: 0.8150773644447327\n",
      "Step: 28900  \tValid loss: 0.4094012975692749\n",
      "Step: 29000  \tTraining loss: 0.38225629925727844\n",
      "Step: 29000  \tTraining accuracy: 0.8151018619537354\n",
      "Step: 29000  \tValid loss: 0.40931418538093567\n",
      "Step: 29100  \tTraining loss: 0.38218796253204346\n",
      "Step: 29100  \tTraining accuracy: 0.8151262402534485\n",
      "Step: 29100  \tValid loss: 0.4092782735824585\n",
      "Step: 29200  \tTraining loss: 0.38213762640953064\n",
      "Step: 29200  \tTraining accuracy: 0.8151504397392273\n",
      "Step: 29200  \tValid loss: 0.409251868724823\n",
      "Step: 29300  \tTraining loss: 0.38209161162376404\n",
      "Step: 29300  \tTraining accuracy: 0.8151744604110718\n",
      "Step: 29300  \tValid loss: 0.4091547131538391\n",
      "Step: 29400  \tTraining loss: 0.3820491135120392\n",
      "Step: 29400  \tTraining accuracy: 0.8151983022689819\n",
      "Step: 29400  \tValid loss: 0.40915626287460327\n",
      "Step: 29500  \tTraining loss: 0.38196903467178345\n",
      "Step: 29500  \tTraining accuracy: 0.8152220249176025\n",
      "Step: 29500  \tValid loss: 0.40911033749580383\n",
      "Step: 29600  \tTraining loss: 0.381899893283844\n",
      "Step: 29600  \tTraining accuracy: 0.815245509147644\n",
      "Step: 29600  \tValid loss: 0.4090729057788849\n",
      "Step: 29700  \tTraining loss: 0.3818424642086029\n",
      "Step: 29700  \tTraining accuracy: 0.8152689337730408\n",
      "Step: 29700  \tValid loss: 0.4090617597103119\n",
      "Step: 29800  \tTraining loss: 0.3817873001098633\n",
      "Step: 29800  \tTraining accuracy: 0.8152921199798584\n",
      "Step: 29800  \tValid loss: 0.4090266823768616\n",
      "Step: 29900  \tTraining loss: 0.3817526698112488\n",
      "Step: 29900  \tTraining accuracy: 0.8153151869773865\n",
      "Step: 29900  \tValid loss: 0.40896716713905334\n",
      "Step: 30000  \tTraining loss: 0.3816763460636139\n",
      "Step: 30000  \tTraining accuracy: 0.815338134765625\n",
      "Step: 30000  \tValid loss: 0.4089158773422241\n",
      "Step: 30100  \tTraining loss: 0.3816152811050415\n",
      "Step: 30100  \tTraining accuracy: 0.8153608441352844\n",
      "Step: 30100  \tValid loss: 0.4089086353778839\n",
      "Step: 30200  \tTraining loss: 0.38156309723854065\n",
      "Step: 30200  \tTraining accuracy: 0.8153834939002991\n",
      "Step: 30200  \tValid loss: 0.4088604748249054\n",
      "Step: 30300  \tTraining loss: 0.38150274753570557\n",
      "Step: 30300  \tTraining accuracy: 0.8154059052467346\n",
      "Step: 30300  \tValid loss: 0.4087960124015808\n",
      "Step: 30400  \tTraining loss: 0.38145142793655396\n",
      "Step: 30400  \tTraining accuracy: 0.8154282569885254\n",
      "Step: 30400  \tValid loss: 0.4087390601634979\n",
      "Step: 30500  \tTraining loss: 0.3813970685005188\n",
      "Step: 30500  \tTraining accuracy: 0.8154504299163818\n",
      "Step: 30500  \tValid loss: 0.4087521433830261\n",
      "Step: 30600  \tTraining loss: 0.38134628534317017\n",
      "Step: 30600  \tTraining accuracy: 0.815472424030304\n",
      "Step: 30600  \tValid loss: 0.40873709321022034\n",
      "Step: 30700  \tTraining loss: 0.38130849599838257\n",
      "Step: 30700  \tTraining accuracy: 0.8154942989349365\n",
      "Step: 30700  \tValid loss: 0.40871700644493103\n",
      "Step: 30800  \tTraining loss: 0.3812565505504608\n",
      "Step: 30800  \tTraining accuracy: 0.8155160546302795\n",
      "Step: 30800  \tValid loss: 0.4087032377719879\n",
      "Step: 30900  \tTraining loss: 0.38119029998779297\n",
      "Step: 30900  \tTraining accuracy: 0.8155376315116882\n",
      "Step: 30900  \tValid loss: 0.4086291790008545\n",
      "Step: 31000  \tTraining loss: 0.3811470866203308\n",
      "Step: 31000  \tTraining accuracy: 0.8155590891838074\n",
      "Step: 31000  \tValid loss: 0.4085794985294342\n",
      "Step: 31100  \tTraining loss: 0.3810889422893524\n",
      "Step: 31100  \tTraining accuracy: 0.8155803680419922\n",
      "Step: 31100  \tValid loss: 0.40855810046195984\n",
      "Step: 31200  \tTraining loss: 0.38102632761001587\n",
      "Step: 31200  \tTraining accuracy: 0.8156015872955322\n",
      "Step: 31200  \tValid loss: 0.4085276424884796\n",
      "Step: 31300  \tTraining loss: 0.38099220395088196\n",
      "Step: 31300  \tTraining accuracy: 0.8156226277351379\n",
      "Step: 31300  \tValid loss: 0.4085119664669037\n",
      "Step: 31400  \tTraining loss: 0.3809269964694977\n",
      "Step: 31400  \tTraining accuracy: 0.8156434893608093\n",
      "Step: 31400  \tValid loss: 0.40854546427726746\n",
      "Step: 31500  \tTraining loss: 0.3808666169643402\n",
      "Step: 31500  \tTraining accuracy: 0.8156625628471375\n",
      "Step: 31500  \tValid loss: 0.40849652886390686\n",
      "Step: 31600  \tTraining loss: 0.3808099329471588\n",
      "Step: 31600  \tTraining accuracy: 0.8156810998916626\n",
      "Step: 31600  \tValid loss: 0.40843474864959717\n",
      "Step: 31700  \tTraining loss: 0.3807471990585327\n",
      "Step: 31700  \tTraining accuracy: 0.8156994581222534\n",
      "Step: 31700  \tValid loss: 0.40843963623046875\n",
      "Step: 31800  \tTraining loss: 0.38069868087768555\n",
      "Step: 31800  \tTraining accuracy: 0.8157172799110413\n",
      "Step: 31800  \tValid loss: 0.4084063172340393\n",
      "Step: 31900  \tTraining loss: 0.38063696026802063\n",
      "Step: 31900  \tTraining accuracy: 0.8157333135604858\n",
      "Step: 31900  \tValid loss: 0.40834760665893555\n",
      "Step: 32000  \tTraining loss: 0.3806042969226837\n",
      "Step: 32000  \tTraining accuracy: 0.8157492876052856\n",
      "Step: 32000  \tValid loss: 0.40834036469459534\n",
      "Step: 32100  \tTraining loss: 0.3805323839187622\n",
      "Step: 32100  \tTraining accuracy: 0.8157651424407959\n",
      "Step: 32100  \tValid loss: 0.40836766362190247\n",
      "Step: 32200  \tTraining loss: 0.3804953992366791\n",
      "Step: 32200  \tTraining accuracy: 0.8157808780670166\n",
      "Step: 32200  \tValid loss: 0.40831732749938965\n",
      "Step: 32300  \tTraining loss: 0.38043519854545593\n",
      "Step: 32300  \tTraining accuracy: 0.8157964944839478\n",
      "Step: 32300  \tValid loss: 0.40825995802879333\n",
      "Step: 32400  \tTraining loss: 0.38039547204971313\n",
      "Step: 32400  \tTraining accuracy: 0.8158120512962341\n",
      "Step: 32400  \tValid loss: 0.4082503318786621\n",
      "Step: 32500  \tTraining loss: 0.3803316354751587\n",
      "Step: 32500  \tTraining accuracy: 0.8158262372016907\n",
      "Step: 32500  \tValid loss: 0.4082085192203522\n",
      "Step: 32600  \tTraining loss: 0.3802734613418579\n",
      "Step: 32600  \tTraining accuracy: 0.8158395290374756\n",
      "Step: 32600  \tValid loss: 0.4082302451133728\n",
      "Step: 32700  \tTraining loss: 0.3802201747894287\n",
      "Step: 32700  \tTraining accuracy: 0.815852701663971\n",
      "Step: 32700  \tValid loss: 0.4081033766269684\n",
      "Step: 32800  \tTraining loss: 0.38016143441200256\n",
      "Step: 32800  \tTraining accuracy: 0.8158658146858215\n",
      "Step: 32800  \tValid loss: 0.4079558253288269\n",
      "Step: 32900  \tTraining loss: 0.38010531663894653\n",
      "Step: 32900  \tTraining accuracy: 0.8158788681030273\n",
      "Step: 32900  \tValid loss: 0.4079166650772095\n",
      "Step: 33000  \tTraining loss: 0.3800489902496338\n",
      "Step: 33000  \tTraining accuracy: 0.8158918023109436\n",
      "Step: 33000  \tValid loss: 0.4079051613807678\n",
      "Step: 33100  \tTraining loss: 0.379986435174942\n",
      "Step: 33100  \tTraining accuracy: 0.8159046769142151\n",
      "Step: 33100  \tValid loss: 0.4078606069087982\n",
      "Step: 33200  \tTraining loss: 0.3799150884151459\n",
      "Step: 33200  \tTraining accuracy: 0.8159174919128418\n",
      "Step: 33200  \tValid loss: 0.40784716606140137\n",
      "Step: 33300  \tTraining loss: 0.3798549473285675\n",
      "Step: 33300  \tTraining accuracy: 0.8159302473068237\n",
      "Step: 33300  \tValid loss: 0.40772154927253723\n",
      "Step: 33400  \tTraining loss: 0.37977132201194763\n",
      "Step: 33400  \tTraining accuracy: 0.8159428834915161\n",
      "Step: 33400  \tValid loss: 0.4077606797218323\n",
      "Step: 33500  \tTraining loss: 0.3797052204608917\n",
      "Step: 33500  \tTraining accuracy: 0.8159550428390503\n",
      "Step: 33500  \tValid loss: 0.4077368378639221\n",
      "Step: 33600  \tTraining loss: 0.3796573281288147\n",
      "Step: 33600  \tTraining accuracy: 0.8159655332565308\n",
      "Step: 33600  \tValid loss: 0.4078240394592285\n",
      "Step: 33700  \tTraining loss: 0.3795810341835022\n",
      "Step: 33700  \tTraining accuracy: 0.8159759640693665\n",
      "Step: 33700  \tValid loss: 0.40774816274642944\n",
      "Step: 33800  \tTraining loss: 0.37952327728271484\n",
      "Step: 33800  \tTraining accuracy: 0.8159863352775574\n",
      "Step: 33800  \tValid loss: 0.40771326422691345\n",
      "Step: 33900  \tTraining loss: 0.37946629524230957\n",
      "Step: 33900  \tTraining accuracy: 0.8159965872764587\n",
      "Step: 33900  \tValid loss: 0.4076983630657196\n",
      "Step: 34000  \tTraining loss: 0.37943148612976074\n",
      "Step: 34000  \tTraining accuracy: 0.8160068392753601\n",
      "Step: 34000  \tValid loss: 0.40762951970100403\n",
      "Step: 34100  \tTraining loss: 0.37935367226600647\n",
      "Step: 34100  \tTraining accuracy: 0.8160170316696167\n",
      "Step: 34100  \tValid loss: 0.4076892137527466\n",
      "Step: 34200  \tTraining loss: 0.37929093837738037\n",
      "Step: 34200  \tTraining accuracy: 0.8160271644592285\n",
      "Step: 34200  \tValid loss: 0.4076601564884186\n",
      "Step: 34300  \tTraining loss: 0.37921464443206787\n",
      "Step: 34300  \tTraining accuracy: 0.8160371780395508\n",
      "Step: 34300  \tValid loss: 0.4076458811759949\n",
      "Step: 34400  \tTraining loss: 0.3791508078575134\n",
      "Step: 34400  \tTraining accuracy: 0.816047191619873\n",
      "Step: 34400  \tValid loss: 0.40751171112060547\n",
      "Step: 34500  \tTraining loss: 0.3790664076805115\n",
      "Step: 34500  \tTraining accuracy: 0.8160571455955505\n",
      "Step: 34500  \tValid loss: 0.4074854552745819\n",
      "Step: 34600  \tTraining loss: 0.3790028691291809\n",
      "Step: 34600  \tTraining accuracy: 0.8160670399665833\n",
      "Step: 34600  \tValid loss: 0.4075022339820862\n",
      "Step: 34700  \tTraining loss: 0.3789568543434143\n",
      "Step: 34700  \tTraining accuracy: 0.8160768747329712\n",
      "Step: 34700  \tValid loss: 0.40748870372772217\n",
      "Step: 34800  \tTraining loss: 0.37888890504837036\n",
      "Step: 34800  \tTraining accuracy: 0.8160866498947144\n",
      "Step: 34800  \tValid loss: 0.40745991468429565\n",
      "Step: 34900  \tTraining loss: 0.3788236081600189\n",
      "Step: 34900  \tTraining accuracy: 0.8160963654518127\n",
      "Step: 34900  \tValid loss: 0.40749138593673706\n",
      "Step: 35000  \tTraining loss: 0.37878531217575073\n",
      "Step: 35000  \tTraining accuracy: 0.8161060214042664\n",
      "Step: 35000  \tValid loss: 0.40749669075012207\n",
      "Step: 35100  \tTraining loss: 0.3787195086479187\n",
      "Step: 35100  \tTraining accuracy: 0.8161156177520752\n",
      "Step: 35100  \tValid loss: 0.4075016975402832\n",
      "Step: 35200  \tTraining loss: 0.3786548972129822\n",
      "Step: 35200  \tTraining accuracy: 0.8161251544952393\n",
      "Step: 35200  \tValid loss: 0.40751776099205017\n",
      "Step: 35300  \tTraining loss: 0.3785942494869232\n",
      "Step: 35300  \tTraining accuracy: 0.8161346912384033\n",
      "Step: 35300  \tValid loss: 0.40749049186706543\n",
      "Step: 35400  \tTraining loss: 0.37852200865745544\n",
      "Step: 35400  \tTraining accuracy: 0.8161441087722778\n",
      "Step: 35400  \tValid loss: 0.4073489308357239\n",
      "Step: 35500  \tTraining loss: 0.37845945358276367\n",
      "Step: 35500  \tTraining accuracy: 0.8161534667015076\n",
      "Step: 35500  \tValid loss: 0.4074529707431793\n",
      "Step: 35600  \tTraining loss: 0.3784027099609375\n",
      "Step: 35600  \tTraining accuracy: 0.816163957118988\n",
      "Step: 35600  \tValid loss: 0.40743890404701233\n",
      "Step: 35700  \tTraining loss: 0.37832385301589966\n",
      "Step: 35700  \tTraining accuracy: 0.8161740303039551\n",
      "Step: 35700  \tValid loss: 0.4074409306049347\n",
      "Step: 35800  \tTraining loss: 0.37824907898902893\n",
      "Step: 35800  \tTraining accuracy: 0.8161832094192505\n",
      "Step: 35800  \tValid loss: 0.4073733389377594\n",
      "Step: 35900  \tTraining loss: 0.378185510635376\n",
      "Step: 35900  \tTraining accuracy: 0.8161923885345459\n",
      "Step: 35900  \tValid loss: 0.40737414360046387\n",
      "Step: 36000  \tTraining loss: 0.3781532347202301\n",
      "Step: 36000  \tTraining accuracy: 0.8162026405334473\n",
      "Step: 36000  \tValid loss: 0.40738940238952637\n",
      "Step: 36100  \tTraining loss: 0.378073126077652\n",
      "Step: 36100  \tTraining accuracy: 0.8162128329277039\n",
      "Step: 36100  \tValid loss: 0.4072660803794861\n",
      "Step: 36200  \tTraining loss: 0.37802639603614807\n",
      "Step: 36200  \tTraining accuracy: 0.8162237405776978\n",
      "Step: 36200  \tValid loss: 0.40730029344558716\n",
      "Step: 36300  \tTraining loss: 0.3779560625553131\n",
      "Step: 36300  \tTraining accuracy: 0.8162345886230469\n",
      "Step: 36300  \tValid loss: 0.40730318427085876\n",
      "Step: 36400  \tTraining loss: 0.3779124617576599\n",
      "Step: 36400  \tTraining accuracy: 0.8162464499473572\n",
      "Step: 36400  \tValid loss: 0.40731385350227356\n",
      "Step: 36500  \tTraining loss: 0.3778368830680847\n",
      "Step: 36500  \tTraining accuracy: 0.816257894039154\n",
      "Step: 36500  \tValid loss: 0.40727537870407104\n",
      "Step: 36600  \tTraining loss: 0.3777799606323242\n",
      "Step: 36600  \tTraining accuracy: 0.8162685632705688\n",
      "Step: 36600  \tValid loss: 0.40727630257606506\n",
      "Step: 36700  \tTraining loss: 0.37772974371910095\n",
      "Step: 36700  \tTraining accuracy: 0.8162791132926941\n",
      "Step: 36700  \tValid loss: 0.40730345249176025\n",
      "Step: 36800  \tTraining loss: 0.3776780664920807\n",
      "Step: 36800  \tTraining accuracy: 0.8162896633148193\n",
      "Step: 36800  \tValid loss: 0.40724021196365356\n",
      "Step: 36900  \tTraining loss: 0.3776218593120575\n",
      "Step: 36900  \tTraining accuracy: 0.8163001537322998\n",
      "Step: 36900  \tValid loss: 0.4072883129119873\n",
      "Step: 37000  \tTraining loss: 0.37757372856140137\n",
      "Step: 37000  \tTraining accuracy: 0.8163105845451355\n",
      "Step: 37000  \tValid loss: 0.40727904438972473\n",
      "Step: 37100  \tTraining loss: 0.37751513719558716\n",
      "Step: 37100  \tTraining accuracy: 0.8163208961486816\n",
      "Step: 37100  \tValid loss: 0.4072939455509186\n",
      "Step: 37200  \tTraining loss: 0.37745970487594604\n",
      "Step: 37200  \tTraining accuracy: 0.8163312077522278\n",
      "Step: 37200  \tValid loss: 0.4072876274585724\n",
      "Step: 37300  \tTraining loss: 0.37741509079933167\n",
      "Step: 37300  \tTraining accuracy: 0.8163418173789978\n",
      "Step: 37300  \tValid loss: 0.40735289454460144\n",
      "Step: 37400  \tTraining loss: 0.3773707449436188\n",
      "Step: 37400  \tTraining accuracy: 0.8163527250289917\n",
      "Step: 37400  \tValid loss: 0.40732601284980774\n",
      "Step: 37500  \tTraining loss: 0.3773000240325928\n",
      "Step: 37500  \tTraining accuracy: 0.8163636326789856\n",
      "Step: 37500  \tValid loss: 0.4074135720729828\n",
      "Step: 37600  \tTraining loss: 0.377261221408844\n",
      "Step: 37600  \tTraining accuracy: 0.8163747787475586\n",
      "Step: 37600  \tValid loss: 0.40736454725265503\n",
      "Step: 37700  \tTraining loss: 0.3772198557853699\n",
      "Step: 37700  \tTraining accuracy: 0.8163865804672241\n",
      "Step: 37700  \tValid loss: 0.4073408544063568\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8163976\n",
      "Precision: 0.8338082\n",
      "Recall: 0.8499516\n",
      "F1 score: 0.82422507\n",
      "AUC: 0.8222763\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.816398   0.833808  0.849952  0.824225  0.822276  0.377153      0.816386   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.407149       0.816386   0.398141      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  37792.0  \n",
      "10\n",
      "(4785, 8)\n",
      "(4785, 1)\n",
      "(2640, 8)\n",
      "(2640, 1)\n",
      "(2145, 8)\n",
      "(2145, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6219577789306641\n",
      "Step: 100  \tTraining accuracy: 0.6664576530456543\n",
      "Step: 100  \tValid loss: 0.6205527186393738\n",
      "Step: 200  \tTraining loss: 0.5332381129264832\n",
      "Step: 200  \tTraining accuracy: 0.7005921006202698\n",
      "Step: 200  \tValid loss: 0.5325019955635071\n",
      "Step: 300  \tTraining loss: 0.42040205001831055\n",
      "Step: 300  \tTraining accuracy: 0.739811897277832\n",
      "Step: 300  \tValid loss: 0.41054216027259827\n",
      "Step: 400  \tTraining loss: 0.37406909465789795\n",
      "Step: 400  \tTraining accuracy: 0.7669204473495483\n",
      "Step: 400  \tValid loss: 0.35604724287986755\n",
      "Step: 500  \tTraining loss: 0.36273396015167236\n",
      "Step: 500  \tTraining accuracy: 0.7837919592857361\n",
      "Step: 500  \tValid loss: 0.3410284221172333\n",
      "Step: 600  \tTraining loss: 0.3587310016155243\n",
      "Step: 600  \tTraining accuracy: 0.7953453063964844\n",
      "Step: 600  \tValid loss: 0.3360253572463989\n",
      "Step: 700  \tTraining loss: 0.35630208253860474\n",
      "Step: 700  \tTraining accuracy: 0.8035849332809448\n",
      "Step: 700  \tValid loss: 0.33342745900154114\n",
      "Step: 800  \tTraining loss: 0.35430607199668884\n",
      "Step: 800  \tTraining accuracy: 0.8095994591712952\n",
      "Step: 800  \tValid loss: 0.33153751492500305\n",
      "Step: 900  \tTraining loss: 0.35248273611068726\n",
      "Step: 900  \tTraining accuracy: 0.8141987919807434\n",
      "Step: 900  \tValid loss: 0.32976433634757996\n",
      "Step: 1000  \tTraining loss: 0.3504810929298401\n",
      "Step: 1000  \tTraining accuracy: 0.8176208734512329\n",
      "Step: 1000  \tValid loss: 0.32818374037742615\n",
      "Step: 1100  \tTraining loss: 0.3482498526573181\n",
      "Step: 1100  \tTraining accuracy: 0.8203214406967163\n",
      "Step: 1100  \tValid loss: 0.32644668221473694\n",
      "Step: 1200  \tTraining loss: 0.3464200496673584\n",
      "Step: 1200  \tTraining accuracy: 0.8227340579032898\n",
      "Step: 1200  \tValid loss: 0.32515081763267517\n",
      "Step: 1300  \tTraining loss: 0.3449428379535675\n",
      "Step: 1300  \tTraining accuracy: 0.8247857689857483\n",
      "Step: 1300  \tValid loss: 0.32429268956184387\n",
      "Step: 1400  \tTraining loss: 0.3436717689037323\n",
      "Step: 1400  \tTraining accuracy: 0.8264716267585754\n",
      "Step: 1400  \tValid loss: 0.32346004247665405\n",
      "Step: 1500  \tTraining loss: 0.3424902856349945\n",
      "Step: 1500  \tTraining accuracy: 0.8279104828834534\n",
      "Step: 1500  \tValid loss: 0.32269808650016785\n",
      "Step: 1600  \tTraining loss: 0.34136027097702026\n",
      "Step: 1600  \tTraining accuracy: 0.8290895819664001\n",
      "Step: 1600  \tValid loss: 0.321919709444046\n",
      "Step: 1700  \tTraining loss: 0.3402753472328186\n",
      "Step: 1700  \tTraining accuracy: 0.8300687074661255\n",
      "Step: 1700  \tValid loss: 0.32113245129585266\n",
      "Step: 1800  \tTraining loss: 0.3392505943775177\n",
      "Step: 1800  \tTraining accuracy: 0.8310016393661499\n",
      "Step: 1800  \tValid loss: 0.3204173743724823\n",
      "Step: 1900  \tTraining loss: 0.33829835057258606\n",
      "Step: 1900  \tTraining accuracy: 0.8318619728088379\n",
      "Step: 1900  \tValid loss: 0.3197053074836731\n",
      "Step: 2000  \tTraining loss: 0.33742210268974304\n",
      "Step: 2000  \tTraining accuracy: 0.832757294178009\n",
      "Step: 2000  \tValid loss: 0.3191060721874237\n",
      "Step: 2100  \tTraining loss: 0.3366110324859619\n",
      "Step: 2100  \tTraining accuracy: 0.8336519002914429\n",
      "Step: 2100  \tValid loss: 0.3184787631034851\n",
      "Step: 2200  \tTraining loss: 0.33583948016166687\n",
      "Step: 2200  \tTraining accuracy: 0.8343806862831116\n",
      "Step: 2200  \tValid loss: 0.31788375973701477\n",
      "Step: 2300  \tTraining loss: 0.33508753776550293\n",
      "Step: 2300  \tTraining accuracy: 0.8350772261619568\n",
      "Step: 2300  \tValid loss: 0.3172557055950165\n",
      "Step: 2400  \tTraining loss: 0.3343302607536316\n",
      "Step: 2400  \tTraining accuracy: 0.83583003282547\n",
      "Step: 2400  \tValid loss: 0.3167206645011902\n",
      "Step: 2500  \tTraining loss: 0.3334462344646454\n",
      "Step: 2500  \tTraining accuracy: 0.8365726470947266\n",
      "Step: 2500  \tValid loss: 0.31639736890792847\n",
      "Step: 2600  \tTraining loss: 0.33252424001693726\n",
      "Step: 2600  \tTraining accuracy: 0.8372528553009033\n",
      "Step: 2600  \tValid loss: 0.3158611059188843\n",
      "Step: 2700  \tTraining loss: 0.3316461443901062\n",
      "Step: 2700  \tTraining accuracy: 0.837952733039856\n",
      "Step: 2700  \tValid loss: 0.31530579924583435\n",
      "Step: 2800  \tTraining loss: 0.33075159788131714\n",
      "Step: 2800  \tTraining accuracy: 0.8386929035186768\n",
      "Step: 2800  \tValid loss: 0.3147898018360138\n",
      "Step: 2900  \tTraining loss: 0.3298427164554596\n",
      "Step: 2900  \tTraining accuracy: 0.8393847942352295\n",
      "Step: 2900  \tValid loss: 0.31417316198349\n",
      "Step: 3000  \tTraining loss: 0.3288596272468567\n",
      "Step: 3000  \tTraining accuracy: 0.8400474786758423\n",
      "Step: 3000  \tValid loss: 0.31341803073883057\n",
      "Step: 3100  \tTraining loss: 0.3278283178806305\n",
      "Step: 3100  \tTraining accuracy: 0.840670108795166\n",
      "Step: 3100  \tValid loss: 0.31248319149017334\n",
      "Step: 3200  \tTraining loss: 0.326709508895874\n",
      "Step: 3200  \tTraining accuracy: 0.8412665128707886\n",
      "Step: 3200  \tValid loss: 0.31152066588401794\n",
      "Step: 3300  \tTraining loss: 0.3255433738231659\n",
      "Step: 3300  \tTraining accuracy: 0.8418165445327759\n",
      "Step: 3300  \tValid loss: 0.31058627367019653\n",
      "Step: 3400  \tTraining loss: 0.3243563175201416\n",
      "Step: 3400  \tTraining accuracy: 0.8423525094985962\n",
      "Step: 3400  \tValid loss: 0.309679239988327\n",
      "Step: 3500  \tTraining loss: 0.3232108950614929\n",
      "Step: 3500  \tTraining accuracy: 0.8428755402565002\n",
      "Step: 3500  \tValid loss: 0.30862653255462646\n",
      "Step: 3600  \tTraining loss: 0.32212188839912415\n",
      "Step: 3600  \tTraining accuracy: 0.843348503112793\n",
      "Step: 3600  \tValid loss: 0.30775055289268494\n",
      "Step: 3700  \tTraining loss: 0.3210866153240204\n",
      "Step: 3700  \tTraining accuracy: 0.84378981590271\n",
      "Step: 3700  \tValid loss: 0.3068735897541046\n",
      "Step: 3800  \tTraining loss: 0.3200341463088989\n",
      "Step: 3800  \tTraining accuracy: 0.8442047834396362\n",
      "Step: 3800  \tValid loss: 0.30643177032470703\n",
      "Step: 3900  \tTraining loss: 0.31911808252334595\n",
      "Step: 3900  \tTraining accuracy: 0.8445873856544495\n",
      "Step: 3900  \tValid loss: 0.3056151866912842\n",
      "Step: 4000  \tTraining loss: 0.3182622194290161\n",
      "Step: 4000  \tTraining accuracy: 0.8449373841285706\n",
      "Step: 4000  \tValid loss: 0.3048297166824341\n",
      "Step: 4100  \tTraining loss: 0.31745296716690063\n",
      "Step: 4100  \tTraining accuracy: 0.8452752232551575\n",
      "Step: 4100  \tValid loss: 0.3041413426399231\n",
      "Step: 4200  \tTraining loss: 0.31667885184288025\n",
      "Step: 4200  \tTraining accuracy: 0.8456270098686218\n",
      "Step: 4200  \tValid loss: 0.30337953567504883\n",
      "Step: 4300  \tTraining loss: 0.3159371614456177\n",
      "Step: 4300  \tTraining accuracy: 0.8459892868995667\n",
      "Step: 4300  \tValid loss: 0.3027579188346863\n",
      "Step: 4400  \tTraining loss: 0.315216064453125\n",
      "Step: 4400  \tTraining accuracy: 0.8463493585586548\n",
      "Step: 4400  \tValid loss: 0.30224838852882385\n",
      "Step: 4500  \tTraining loss: 0.31450578570365906\n",
      "Step: 4500  \tTraining accuracy: 0.8466790914535522\n",
      "Step: 4500  \tValid loss: 0.3017665147781372\n",
      "Step: 4600  \tTraining loss: 0.31380581855773926\n",
      "Step: 4600  \tTraining accuracy: 0.8469760417938232\n",
      "Step: 4600  \tValid loss: 0.30132970213890076\n",
      "Step: 4700  \tTraining loss: 0.313110888004303\n",
      "Step: 4700  \tTraining accuracy: 0.8472601175308228\n",
      "Step: 4700  \tValid loss: 0.300927072763443\n",
      "Step: 4800  \tTraining loss: 0.31243035197257996\n",
      "Step: 4800  \tTraining accuracy: 0.847547709941864\n",
      "Step: 4800  \tValid loss: 0.30052343010902405\n",
      "Step: 4900  \tTraining loss: 0.31176748871803284\n",
      "Step: 4900  \tTraining accuracy: 0.8478255867958069\n",
      "Step: 4900  \tValid loss: 0.30011674761772156\n",
      "Step: 5000  \tTraining loss: 0.3111218512058258\n",
      "Step: 5000  \tTraining accuracy: 0.8480964303016663\n",
      "Step: 5000  \tValid loss: 0.29975438117980957\n",
      "Step: 5100  \tTraining loss: 0.3104917109012604\n",
      "Step: 5100  \tTraining accuracy: 0.8483482599258423\n",
      "Step: 5100  \tValid loss: 0.2994116544723511\n",
      "Step: 5200  \tTraining loss: 0.30987465381622314\n",
      "Step: 5200  \tTraining accuracy: 0.8485944271087646\n",
      "Step: 5200  \tValid loss: 0.29912105202674866\n",
      "Step: 5300  \tTraining loss: 0.30927205085754395\n",
      "Step: 5300  \tTraining accuracy: 0.848857045173645\n",
      "Step: 5300  \tValid loss: 0.2988567352294922\n",
      "Step: 5400  \tTraining loss: 0.3086852729320526\n",
      "Step: 5400  \tTraining accuracy: 0.8491255044937134\n",
      "Step: 5400  \tValid loss: 0.2986011803150177\n",
      "Step: 5500  \tTraining loss: 0.30811160802841187\n",
      "Step: 5500  \tTraining accuracy: 0.8493878841400146\n",
      "Step: 5500  \tValid loss: 0.29835784435272217\n",
      "Step: 5600  \tTraining loss: 0.30754995346069336\n",
      "Step: 5600  \tTraining accuracy: 0.8496370911598206\n",
      "Step: 5600  \tValid loss: 0.2981536090373993\n",
      "Step: 5700  \tTraining loss: 0.30700135231018066\n",
      "Step: 5700  \tTraining accuracy: 0.849892258644104\n",
      "Step: 5700  \tValid loss: 0.2979208827018738\n",
      "Step: 5800  \tTraining loss: 0.30645817518234253\n",
      "Step: 5800  \tTraining accuracy: 0.8501513004302979\n",
      "Step: 5800  \tValid loss: 0.2978692650794983\n",
      "Step: 5900  \tTraining loss: 0.30593177676200867\n",
      "Step: 5900  \tTraining accuracy: 0.8503889441490173\n",
      "Step: 5900  \tValid loss: 0.2976076006889343\n",
      "Step: 6000  \tTraining loss: 0.3054217994213104\n",
      "Step: 6000  \tTraining accuracy: 0.8506203889846802\n",
      "Step: 6000  \tValid loss: 0.2974504828453064\n",
      "Step: 6100  \tTraining loss: 0.3049279451370239\n",
      "Step: 6100  \tTraining accuracy: 0.8508562445640564\n",
      "Step: 6100  \tValid loss: 0.2972490191459656\n",
      "Step: 6200  \tTraining loss: 0.30444902181625366\n",
      "Step: 6200  \tTraining accuracy: 0.8511047959327698\n",
      "Step: 6200  \tValid loss: 0.29713189601898193\n",
      "Step: 6300  \tTraining loss: 0.30398666858673096\n",
      "Step: 6300  \tTraining accuracy: 0.8513604998588562\n",
      "Step: 6300  \tValid loss: 0.29695838689804077\n",
      "Step: 6400  \tTraining loss: 0.3035392463207245\n",
      "Step: 6400  \tTraining accuracy: 0.8516080975532532\n",
      "Step: 6400  \tValid loss: 0.2968183159828186\n",
      "Step: 6500  \tTraining loss: 0.3031044900417328\n",
      "Step: 6500  \tTraining accuracy: 0.8518561720848083\n",
      "Step: 6500  \tValid loss: 0.29663318395614624\n",
      "Step: 6600  \tTraining loss: 0.30268433690071106\n",
      "Step: 6600  \tTraining accuracy: 0.8520886898040771\n",
      "Step: 6600  \tValid loss: 0.2964741885662079\n",
      "Step: 6700  \tTraining loss: 0.3022633492946625\n",
      "Step: 6700  \tTraining accuracy: 0.8523000478744507\n",
      "Step: 6700  \tValid loss: 0.29637956619262695\n",
      "Step: 6800  \tTraining loss: 0.30185002088546753\n",
      "Step: 6800  \tTraining accuracy: 0.8524989485740662\n",
      "Step: 6800  \tValid loss: 0.2962131202220917\n",
      "Step: 6900  \tTraining loss: 0.3014475107192993\n",
      "Step: 6900  \tTraining accuracy: 0.8526920080184937\n",
      "Step: 6900  \tValid loss: 0.29608869552612305\n",
      "Step: 7000  \tTraining loss: 0.30105462670326233\n",
      "Step: 7000  \tTraining accuracy: 0.8528841137886047\n",
      "Step: 7000  \tValid loss: 0.29595816135406494\n",
      "Step: 7100  \tTraining loss: 0.30064988136291504\n",
      "Step: 7100  \tTraining accuracy: 0.8530706763267517\n",
      "Step: 7100  \tValid loss: 0.2958782911300659\n",
      "Step: 7200  \tTraining loss: 0.3002772033214569\n",
      "Step: 7200  \tTraining accuracy: 0.8532578945159912\n",
      "Step: 7200  \tValid loss: 0.295749694108963\n",
      "Step: 7300  \tTraining loss: 0.29991668462753296\n",
      "Step: 7300  \tTraining accuracy: 0.8534414172172546\n",
      "Step: 7300  \tValid loss: 0.29559507966041565\n",
      "Step: 7400  \tTraining loss: 0.2995707392692566\n",
      "Step: 7400  \tTraining accuracy: 0.8536157011985779\n",
      "Step: 7400  \tValid loss: 0.2955256998538971\n",
      "Step: 7500  \tTraining loss: 0.29923713207244873\n",
      "Step: 7500  \tTraining accuracy: 0.8537880778312683\n",
      "Step: 7500  \tValid loss: 0.2954100966453552\n",
      "Step: 7600  \tTraining loss: 0.2989157736301422\n",
      "Step: 7600  \tTraining accuracy: 0.853962779045105\n",
      "Step: 7600  \tValid loss: 0.2953169047832489\n",
      "Step: 7700  \tTraining loss: 0.2986079752445221\n",
      "Step: 7700  \tTraining accuracy: 0.8541370630264282\n",
      "Step: 7700  \tValid loss: 0.2952142059803009\n",
      "Step: 7800  \tTraining loss: 0.29830703139305115\n",
      "Step: 7800  \tTraining accuracy: 0.8543000817298889\n",
      "Step: 7800  \tValid loss: 0.2950395345687866\n",
      "Step: 7900  \tTraining loss: 0.2980135679244995\n",
      "Step: 7900  \tTraining accuracy: 0.8544589281082153\n",
      "Step: 7900  \tValid loss: 0.2948744297027588\n",
      "Step: 8000  \tTraining loss: 0.29772672057151794\n",
      "Step: 8000  \tTraining accuracy: 0.8546098470687866\n",
      "Step: 8000  \tValid loss: 0.2947302758693695\n",
      "Step: 8100  \tTraining loss: 0.29744142293930054\n",
      "Step: 8100  \tTraining accuracy: 0.8547531366348267\n",
      "Step: 8100  \tValid loss: 0.2945394217967987\n",
      "Step: 8200  \tTraining loss: 0.29716724157333374\n",
      "Step: 8200  \tTraining accuracy: 0.8548877835273743\n",
      "Step: 8200  \tValid loss: 0.29445508122444153\n",
      "Step: 8300  \tTraining loss: 0.2968960404396057\n",
      "Step: 8300  \tTraining accuracy: 0.8550191521644592\n",
      "Step: 8300  \tValid loss: 0.2942609488964081\n",
      "Step: 8400  \tTraining loss: 0.29663220047950745\n",
      "Step: 8400  \tTraining accuracy: 0.8551536202430725\n",
      "Step: 8400  \tValid loss: 0.2941088378429413\n",
      "Step: 8500  \tTraining loss: 0.2963714599609375\n",
      "Step: 8500  \tTraining accuracy: 0.8552874326705933\n",
      "Step: 8500  \tValid loss: 0.2939956486225128\n",
      "Step: 8600  \tTraining loss: 0.2961134910583496\n",
      "Step: 8600  \tTraining accuracy: 0.8554290533065796\n",
      "Step: 8600  \tValid loss: 0.29390352964401245\n",
      "Step: 8700  \tTraining loss: 0.29586049914360046\n",
      "Step: 8700  \tTraining accuracy: 0.8555794954299927\n",
      "Step: 8700  \tValid loss: 0.2936832904815674\n",
      "Step: 8800  \tTraining loss: 0.2956123650074005\n",
      "Step: 8800  \tTraining accuracy: 0.8557384610176086\n",
      "Step: 8800  \tValid loss: 0.2935790419578552\n",
      "Step: 8900  \tTraining loss: 0.2953653037548065\n",
      "Step: 8900  \tTraining accuracy: 0.8558902740478516\n",
      "Step: 8900  \tValid loss: 0.2934475839138031\n",
      "Step: 9000  \tTraining loss: 0.2951076030731201\n",
      "Step: 9000  \tTraining accuracy: 0.856042206287384\n",
      "Step: 9000  \tValid loss: 0.2932863235473633\n",
      "Step: 9100  \tTraining loss: 0.29486316442489624\n",
      "Step: 9100  \tTraining accuracy: 0.8561988472938538\n",
      "Step: 9100  \tValid loss: 0.293104887008667\n",
      "Step: 9200  \tTraining loss: 0.2946244180202484\n",
      "Step: 9200  \tTraining accuracy: 0.856353223323822\n",
      "Step: 9200  \tValid loss: 0.29301726818084717\n",
      "Step: 9300  \tTraining loss: 0.294390469789505\n",
      "Step: 9300  \tTraining accuracy: 0.8565019965171814\n",
      "Step: 9300  \tValid loss: 0.29290831089019775\n",
      "Step: 9400  \tTraining loss: 0.29416316747665405\n",
      "Step: 9400  \tTraining accuracy: 0.8566420078277588\n",
      "Step: 9400  \tValid loss: 0.2928446829319\n",
      "Step: 9500  \tTraining loss: 0.29393476247787476\n",
      "Step: 9500  \tTraining accuracy: 0.8567823767662048\n",
      "Step: 9500  \tValid loss: 0.29270848631858826\n",
      "Step: 9600  \tTraining loss: 0.2937139570713043\n",
      "Step: 9600  \tTraining accuracy: 0.8569285273551941\n",
      "Step: 9600  \tValid loss: 0.2926153838634491\n",
      "Step: 9700  \tTraining loss: 0.2934959828853607\n",
      "Step: 9700  \tTraining accuracy: 0.8570727705955505\n",
      "Step: 9700  \tValid loss: 0.29253995418548584\n",
      "Step: 9800  \tTraining loss: 0.2932785451412201\n",
      "Step: 9800  \tTraining accuracy: 0.8572129607200623\n",
      "Step: 9800  \tValid loss: 0.2923634946346283\n",
      "Step: 9900  \tTraining loss: 0.2930712103843689\n",
      "Step: 9900  \tTraining accuracy: 0.8573460578918457\n",
      "Step: 9900  \tValid loss: 0.2923451066017151\n",
      "Step: 10000  \tTraining loss: 0.29285988211631775\n",
      "Step: 10000  \tTraining accuracy: 0.85747230052948\n",
      "Step: 10000  \tValid loss: 0.29221245646476746\n",
      "Step: 10100  \tTraining loss: 0.2926572859287262\n",
      "Step: 10100  \tTraining accuracy: 0.8575929403305054\n",
      "Step: 10100  \tValid loss: 0.29218411445617676\n",
      "Step: 10200  \tTraining loss: 0.29245057702064514\n",
      "Step: 10200  \tTraining accuracy: 0.8577090501785278\n",
      "Step: 10200  \tValid loss: 0.2920193672180176\n",
      "Step: 10300  \tTraining loss: 0.29224634170532227\n",
      "Step: 10300  \tTraining accuracy: 0.8578229546546936\n",
      "Step: 10300  \tValid loss: 0.2919498682022095\n",
      "Step: 10400  \tTraining loss: 0.2920451760292053\n",
      "Step: 10400  \tTraining accuracy: 0.857936680316925\n",
      "Step: 10400  \tValid loss: 0.29186296463012695\n",
      "Step: 10500  \tTraining loss: 0.29184800386428833\n",
      "Step: 10500  \tTraining accuracy: 0.8580512404441833\n",
      "Step: 10500  \tValid loss: 0.2918224036693573\n",
      "Step: 10600  \tTraining loss: 0.29164814949035645\n",
      "Step: 10600  \tTraining accuracy: 0.858163595199585\n",
      "Step: 10600  \tValid loss: 0.29172322154045105\n",
      "Step: 10700  \tTraining loss: 0.2914535105228424\n",
      "Step: 10700  \tTraining accuracy: 0.8582689166069031\n",
      "Step: 10700  \tValid loss: 0.29165542125701904\n",
      "Step: 10800  \tTraining loss: 0.2912587821483612\n",
      "Step: 10800  \tTraining accuracy: 0.8583694100379944\n",
      "Step: 10800  \tValid loss: 0.2915845811367035\n",
      "Step: 10900  \tTraining loss: 0.29106566309928894\n",
      "Step: 10900  \tTraining accuracy: 0.8584709167480469\n",
      "Step: 10900  \tValid loss: 0.29151850938796997\n",
      "Step: 11000  \tTraining loss: 0.2908737063407898\n",
      "Step: 11000  \tTraining accuracy: 0.8585677146911621\n",
      "Step: 11000  \tValid loss: 0.2914581894874573\n",
      "Step: 11100  \tTraining loss: 0.2906827926635742\n",
      "Step: 11100  \tTraining accuracy: 0.8586618304252625\n",
      "Step: 11100  \tValid loss: 0.29137590527534485\n",
      "Step: 11200  \tTraining loss: 0.2904927134513855\n",
      "Step: 11200  \tTraining accuracy: 0.8587533235549927\n",
      "Step: 11200  \tValid loss: 0.291340172290802\n",
      "Step: 11300  \tTraining loss: 0.2903038263320923\n",
      "Step: 11300  \tTraining accuracy: 0.8588486909866333\n",
      "Step: 11300  \tValid loss: 0.29126405715942383\n",
      "Step: 11400  \tTraining loss: 0.2901146113872528\n",
      "Step: 11400  \tTraining accuracy: 0.8589479923248291\n",
      "Step: 11400  \tValid loss: 0.2911987900733948\n",
      "Step: 11500  \tTraining loss: 0.28992965817451477\n",
      "Step: 11500  \tTraining accuracy: 0.8590518832206726\n",
      "Step: 11500  \tValid loss: 0.29119470715522766\n",
      "Step: 11600  \tTraining loss: 0.28974059224128723\n",
      "Step: 11600  \tTraining accuracy: 0.8591621518135071\n",
      "Step: 11600  \tValid loss: 0.29110023379325867\n",
      "Step: 11700  \tTraining loss: 0.2895555794239044\n",
      "Step: 11700  \tTraining accuracy: 0.8592696189880371\n",
      "Step: 11700  \tValid loss: 0.29104700684547424\n",
      "Step: 11800  \tTraining loss: 0.2893710136413574\n",
      "Step: 11800  \tTraining accuracy: 0.8593735098838806\n",
      "Step: 11800  \tValid loss: 0.2909829020500183\n",
      "Step: 11900  \tTraining loss: 0.2891883850097656\n",
      "Step: 11900  \tTraining accuracy: 0.8594764471054077\n",
      "Step: 11900  \tValid loss: 0.29098257422447205\n",
      "Step: 12000  \tTraining loss: 0.2890045642852783\n",
      "Step: 12000  \tTraining accuracy: 0.8595768809318542\n",
      "Step: 12000  \tValid loss: 0.2908855080604553\n",
      "Step: 12100  \tTraining loss: 0.2888229787349701\n",
      "Step: 12100  \tTraining accuracy: 0.8596729636192322\n",
      "Step: 12100  \tValid loss: 0.29085707664489746\n",
      "Step: 12200  \tTraining loss: 0.2886403799057007\n",
      "Step: 12200  \tTraining accuracy: 0.8597658276557922\n",
      "Step: 12200  \tValid loss: 0.2908015251159668\n",
      "Step: 12300  \tTraining loss: 0.2884601652622223\n",
      "Step: 12300  \tTraining accuracy: 0.8598571419715881\n",
      "Step: 12300  \tValid loss: 0.2907866835594177\n",
      "Step: 12400  \tTraining loss: 0.28828054666519165\n",
      "Step: 12400  \tTraining accuracy: 0.8599511981010437\n",
      "Step: 12400  \tValid loss: 0.2907477021217346\n",
      "Step: 12500  \tTraining loss: 0.2881002128124237\n",
      "Step: 12500  \tTraining accuracy: 0.8600428700447083\n",
      "Step: 12500  \tValid loss: 0.29074499011039734\n",
      "Step: 12600  \tTraining loss: 0.28792500495910645\n",
      "Step: 12600  \tTraining accuracy: 0.8601298332214355\n",
      "Step: 12600  \tValid loss: 0.2906891703605652\n",
      "Step: 12700  \tTraining loss: 0.2877422273159027\n",
      "Step: 12700  \tTraining accuracy: 0.8602153658866882\n",
      "Step: 12700  \tValid loss: 0.2906148135662079\n",
      "Step: 12800  \tTraining loss: 0.2875676155090332\n",
      "Step: 12800  \tTraining accuracy: 0.8603011965751648\n",
      "Step: 12800  \tValid loss: 0.29061150550842285\n",
      "Step: 12900  \tTraining loss: 0.28739258646965027\n",
      "Step: 12900  \tTraining accuracy: 0.8603881001472473\n",
      "Step: 12900  \tValid loss: 0.29066792130470276\n",
      "Step: 13000  \tTraining loss: 0.28721487522125244\n",
      "Step: 13000  \tTraining accuracy: 0.8604737520217896\n",
      "Step: 13000  \tValid loss: 0.29060959815979004\n",
      "Step: 13100  \tTraining loss: 0.2870384454727173\n",
      "Step: 13100  \tTraining accuracy: 0.8605524301528931\n",
      "Step: 13100  \tValid loss: 0.2905665636062622\n",
      "Step: 13200  \tTraining loss: 0.2868656516075134\n",
      "Step: 13200  \tTraining accuracy: 0.8606235384941101\n",
      "Step: 13200  \tValid loss: 0.2905491590499878\n",
      "Step: 13300  \tTraining loss: 0.28669241070747375\n",
      "Step: 13300  \tTraining accuracy: 0.8606935739517212\n",
      "Step: 13300  \tValid loss: 0.29056936502456665\n",
      "Step: 13400  \tTraining loss: 0.2865169048309326\n",
      "Step: 13400  \tTraining accuracy: 0.8607633709907532\n",
      "Step: 13400  \tValid loss: 0.29048699140548706\n",
      "Step: 13500  \tTraining loss: 0.2863461971282959\n",
      "Step: 13500  \tTraining accuracy: 0.8608298301696777\n",
      "Step: 13500  \tValid loss: 0.2905077040195465\n",
      "Step: 13600  \tTraining loss: 0.2861725091934204\n",
      "Step: 13600  \tTraining accuracy: 0.8608952760696411\n",
      "Step: 13600  \tValid loss: 0.29044049978256226\n",
      "Step: 13700  \tTraining loss: 0.28599920868873596\n",
      "Step: 13700  \tTraining accuracy: 0.8609681725502014\n",
      "Step: 13700  \tValid loss: 0.2904854416847229\n",
      "Step: 13800  \tTraining loss: 0.2858278155326843\n",
      "Step: 13800  \tTraining accuracy: 0.8610430359840393\n",
      "Step: 13800  \tValid loss: 0.2904636263847351\n",
      "Step: 13900  \tTraining loss: 0.28565263748168945\n",
      "Step: 13900  \tTraining accuracy: 0.8611145615577698\n",
      "Step: 13900  \tValid loss: 0.29044920206069946\n",
      "Step: 14000  \tTraining loss: 0.28547224402427673\n",
      "Step: 14000  \tTraining accuracy: 0.8611873388290405\n",
      "Step: 14000  \tValid loss: 0.29042354226112366\n",
      "Step: 14100  \tTraining loss: 0.2852953374385834\n",
      "Step: 14100  \tTraining accuracy: 0.8612605333328247\n",
      "Step: 14100  \tValid loss: 0.29043275117874146\n",
      "Step: 14200  \tTraining loss: 0.2851199209690094\n",
      "Step: 14200  \tTraining accuracy: 0.8613327145576477\n",
      "Step: 14200  \tValid loss: 0.29036277532577515\n",
      "Step: 14300  \tTraining loss: 0.28494882583618164\n",
      "Step: 14300  \tTraining accuracy: 0.8614038825035095\n",
      "Step: 14300  \tValid loss: 0.2904086112976074\n",
      "Step: 14400  \tTraining loss: 0.28477558493614197\n",
      "Step: 14400  \tTraining accuracy: 0.8614740371704102\n",
      "Step: 14400  \tValid loss: 0.2903539836406708\n",
      "Step: 14500  \tTraining loss: 0.28460317850112915\n",
      "Step: 14500  \tTraining accuracy: 0.8615432381629944\n",
      "Step: 14500  \tValid loss: 0.2903665602207184\n",
      "Step: 14600  \tTraining loss: 0.2844334840774536\n",
      "Step: 14600  \tTraining accuracy: 0.8616100549697876\n",
      "Step: 14600  \tValid loss: 0.2903817296028137\n",
      "Step: 14700  \tTraining loss: 0.28426387906074524\n",
      "Step: 14700  \tTraining accuracy: 0.8616716861724854\n",
      "Step: 14700  \tValid loss: 0.2903898358345032\n",
      "Step: 14800  \tTraining loss: 0.2840912342071533\n",
      "Step: 14800  \tTraining accuracy: 0.8617310523986816\n",
      "Step: 14800  \tValid loss: 0.2903715670108795\n",
      "Step: 14900  \tTraining loss: 0.28392288088798523\n",
      "Step: 14900  \tTraining accuracy: 0.861791729927063\n",
      "Step: 14900  \tValid loss: 0.2903272807598114\n",
      "Step: 15000  \tTraining loss: 0.28375309705734253\n",
      "Step: 15000  \tTraining accuracy: 0.8618530035018921\n",
      "Step: 15000  \tValid loss: 0.2903723120689392\n",
      "Step: 15100  \tTraining loss: 0.28358063101768494\n",
      "Step: 15100  \tTraining accuracy: 0.8619134426116943\n",
      "Step: 15100  \tValid loss: 0.29035165905952454\n",
      "Step: 15200  \tTraining loss: 0.2834143042564392\n",
      "Step: 15200  \tTraining accuracy: 0.8619703054428101\n",
      "Step: 15200  \tValid loss: 0.29038748145103455\n",
      "Step: 15300  \tTraining loss: 0.2832428812980652\n",
      "Step: 15300  \tTraining accuracy: 0.8620251417160034\n",
      "Step: 15300  \tValid loss: 0.2903498709201813\n",
      "Step: 15400  \tTraining loss: 0.28307440876960754\n",
      "Step: 15400  \tTraining accuracy: 0.862076461315155\n",
      "Step: 15400  \tValid loss: 0.29035884141921997\n",
      "Step: 15500  \tTraining loss: 0.28290653228759766\n",
      "Step: 15500  \tTraining accuracy: 0.8621291518211365\n",
      "Step: 15500  \tValid loss: 0.2903577983379364\n",
      "Step: 15600  \tTraining loss: 0.28273719549179077\n",
      "Step: 15600  \tTraining accuracy: 0.862179160118103\n",
      "Step: 15600  \tValid loss: 0.2904198467731476\n",
      "Step: 15700  \tTraining loss: 0.282573401927948\n",
      "Step: 15700  \tTraining accuracy: 0.8622238636016846\n",
      "Step: 15700  \tValid loss: 0.29047369956970215\n",
      "Step: 15800  \tTraining loss: 0.2824002504348755\n",
      "Step: 15800  \tTraining accuracy: 0.8622640371322632\n",
      "Step: 15800  \tValid loss: 0.2903900742530823\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8623004\n",
      "Precision: 0.89590687\n",
      "Recall: 0.89464414\n",
      "F1 score: 0.8616923\n",
      "AUC: 0.8715645\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0    0.8623   0.895907  0.894644  0.861692  0.871565  0.282274      0.862257   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.290261        0.86222   0.345146      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  15875.0  \n",
      "11\n",
      "(3770, 8)\n",
      "(3770, 1)\n",
      "(2080, 8)\n",
      "(2080, 1)\n",
      "(1690, 8)\n",
      "(1690, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4981546103954315\n",
      "Step: 100  \tTraining accuracy: 0.7832891345024109\n",
      "Step: 100  \tValid loss: 0.5028655529022217\n",
      "Step: 200  \tTraining loss: 0.37816283106803894\n",
      "Step: 200  \tTraining accuracy: 0.7989389896392822\n",
      "Step: 200  \tValid loss: 0.396840900182724\n",
      "Step: 300  \tTraining loss: 0.34407365322113037\n",
      "Step: 300  \tTraining accuracy: 0.810769259929657\n",
      "Step: 300  \tValid loss: 0.3704192638397217\n",
      "Step: 400  \tTraining loss: 0.33431586623191833\n",
      "Step: 400  \tTraining accuracy: 0.8176960945129395\n",
      "Step: 400  \tValid loss: 0.3628041446208954\n",
      "Step: 500  \tTraining loss: 0.3287336230278015\n",
      "Step: 500  \tTraining accuracy: 0.8217211961746216\n",
      "Step: 500  \tValid loss: 0.35698625445365906\n",
      "Step: 600  \tTraining loss: 0.3240993022918701\n",
      "Step: 600  \tTraining accuracy: 0.8243308663368225\n",
      "Step: 600  \tValid loss: 0.3513966202735901\n",
      "Step: 700  \tTraining loss: 0.31985947489738464\n",
      "Step: 700  \tTraining accuracy: 0.8263415694236755\n",
      "Step: 700  \tValid loss: 0.34616947174072266\n",
      "Step: 800  \tTraining loss: 0.31578904390335083\n",
      "Step: 800  \tTraining accuracy: 0.8278691172599792\n",
      "Step: 800  \tValid loss: 0.3412318229675293\n",
      "Step: 900  \tTraining loss: 0.31177574396133423\n",
      "Step: 900  \tTraining accuracy: 0.8293025493621826\n",
      "Step: 900  \tValid loss: 0.3365238904953003\n",
      "Step: 1000  \tTraining loss: 0.3078928291797638\n",
      "Step: 1000  \tTraining accuracy: 0.830601692199707\n",
      "Step: 1000  \tValid loss: 0.33200445771217346\n",
      "Step: 1100  \tTraining loss: 0.3042154610157013\n",
      "Step: 1100  \tTraining accuracy: 0.8317670822143555\n",
      "Step: 1100  \tValid loss: 0.327837198972702\n",
      "Step: 1200  \tTraining loss: 0.30076146125793457\n",
      "Step: 1200  \tTraining accuracy: 0.8324875831604004\n",
      "Step: 1200  \tValid loss: 0.3239890933036804\n",
      "Step: 1300  \tTraining loss: 0.2975250482559204\n",
      "Step: 1300  \tTraining accuracy: 0.8331140875816345\n",
      "Step: 1300  \tValid loss: 0.32056522369384766\n",
      "Step: 1400  \tTraining loss: 0.2945494055747986\n",
      "Step: 1400  \tTraining accuracy: 0.8336771726608276\n",
      "Step: 1400  \tValid loss: 0.3174356520175934\n",
      "Step: 1500  \tTraining loss: 0.29183340072631836\n",
      "Step: 1500  \tTraining accuracy: 0.834089457988739\n",
      "Step: 1500  \tValid loss: 0.31467607617378235\n",
      "Step: 1600  \tTraining loss: 0.28937727212905884\n",
      "Step: 1600  \tTraining accuracy: 0.834482729434967\n",
      "Step: 1600  \tValid loss: 0.31229788064956665\n",
      "Step: 1700  \tTraining loss: 0.2871677875518799\n",
      "Step: 1700  \tTraining accuracy: 0.8348042964935303\n",
      "Step: 1700  \tValid loss: 0.31025537848472595\n",
      "Step: 1800  \tTraining loss: 0.2852107584476471\n",
      "Step: 1800  \tTraining accuracy: 0.8351497054100037\n",
      "Step: 1800  \tValid loss: 0.30859920382499695\n",
      "Step: 1900  \tTraining loss: 0.283515065908432\n",
      "Step: 1900  \tTraining accuracy: 0.8355007767677307\n",
      "Step: 1900  \tValid loss: 0.3072889447212219\n",
      "Step: 2000  \tTraining loss: 0.2820836007595062\n",
      "Step: 2000  \tTraining accuracy: 0.8359994292259216\n",
      "Step: 2000  \tValid loss: 0.3063030540943146\n",
      "Step: 2100  \tTraining loss: 0.28090330958366394\n",
      "Step: 2100  \tTraining accuracy: 0.8364365696907043\n",
      "Step: 2100  \tValid loss: 0.3055630624294281\n",
      "Step: 2200  \tTraining loss: 0.279941201210022\n",
      "Step: 2200  \tTraining accuracy: 0.8368515372276306\n",
      "Step: 2200  \tValid loss: 0.3050077557563782\n",
      "Step: 2300  \tTraining loss: 0.27915623784065247\n",
      "Step: 2300  \tTraining accuracy: 0.837282657623291\n",
      "Step: 2300  \tValid loss: 0.30458882451057434\n",
      "Step: 2400  \tTraining loss: 0.27850908041000366\n",
      "Step: 2400  \tTraining accuracy: 0.8377278447151184\n",
      "Step: 2400  \tValid loss: 0.3042842149734497\n",
      "Step: 2500  \tTraining loss: 0.27797165513038635\n",
      "Step: 2500  \tTraining accuracy: 0.8381258845329285\n",
      "Step: 2500  \tValid loss: 0.3040512204170227\n",
      "Step: 2600  \tTraining loss: 0.2775106132030487\n",
      "Step: 2600  \tTraining accuracy: 0.8385135531425476\n",
      "Step: 2600  \tValid loss: 0.3038645386695862\n",
      "Step: 2700  \tTraining loss: 0.2770993411540985\n",
      "Step: 2700  \tTraining accuracy: 0.8389520049095154\n",
      "Step: 2700  \tValid loss: 0.30371296405792236\n",
      "Step: 2800  \tTraining loss: 0.27671176195144653\n",
      "Step: 2800  \tTraining accuracy: 0.8393827080726624\n",
      "Step: 2800  \tValid loss: 0.3035210967063904\n",
      "Step: 2900  \tTraining loss: 0.2763097882270813\n",
      "Step: 2900  \tTraining accuracy: 0.839820384979248\n",
      "Step: 2900  \tValid loss: 0.30316832661628723\n",
      "Step: 3000  \tTraining loss: 0.27591532468795776\n",
      "Step: 3000  \tTraining accuracy: 0.8402193784713745\n",
      "Step: 3000  \tValid loss: 0.3029767572879791\n",
      "Step: 3100  \tTraining loss: 0.2755441665649414\n",
      "Step: 3100  \tTraining accuracy: 0.8406096696853638\n",
      "Step: 3100  \tValid loss: 0.30293238162994385\n",
      "Step: 3200  \tTraining loss: 0.2752177119255066\n",
      "Step: 3200  \tTraining accuracy: 0.840983510017395\n",
      "Step: 3200  \tValid loss: 0.30300503969192505\n",
      "Step: 3300  \tTraining loss: 0.2749268114566803\n",
      "Step: 3300  \tTraining accuracy: 0.8413180708885193\n",
      "Step: 3300  \tValid loss: 0.3031122088432312\n",
      "Step: 3400  \tTraining loss: 0.2746606171131134\n",
      "Step: 3400  \tTraining accuracy: 0.8416247963905334\n",
      "Step: 3400  \tValid loss: 0.3031902313232422\n",
      "Step: 3500  \tTraining loss: 0.2744159400463104\n",
      "Step: 3500  \tTraining accuracy: 0.841913640499115\n",
      "Step: 3500  \tValid loss: 0.30324509739875793\n",
      "Step: 3600  \tTraining loss: 0.27419018745422363\n",
      "Step: 3600  \tTraining accuracy: 0.8421825170516968\n",
      "Step: 3600  \tValid loss: 0.3032900094985962\n",
      "Step: 3700  \tTraining loss: 0.2739817798137665\n",
      "Step: 3700  \tTraining accuracy: 0.8424403071403503\n",
      "Step: 3700  \tValid loss: 0.30331966280937195\n",
      "Step: 3800  \tTraining loss: 0.2737884223461151\n",
      "Step: 3800  \tTraining accuracy: 0.8426914215087891\n",
      "Step: 3800  \tValid loss: 0.30335041880607605\n",
      "Step: 3900  \tTraining loss: 0.27360799908638\n",
      "Step: 3900  \tTraining accuracy: 0.8429432511329651\n",
      "Step: 3900  \tValid loss: 0.3033705949783325\n",
      "Step: 4000  \tTraining loss: 0.2734384536743164\n",
      "Step: 4000  \tTraining accuracy: 0.8431555032730103\n",
      "Step: 4000  \tValid loss: 0.30337318778038025\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8433343\n",
      "Precision: 0.68766403\n",
      "Recall: 0.6405868\n",
      "F1 score: 0.81344396\n",
      "AUC: 0.77998173\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.843334   0.687664  0.640587  0.813444  0.779982  0.273308       0.84314   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.302926       0.843082   0.303892      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  4079.0  \n",
      "12\n",
      "(3770, 8)\n",
      "(3770, 1)\n",
      "(2000, 8)\n",
      "(2000, 1)\n",
      "(1625, 8)\n",
      "(1625, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.524355947971344\n",
      "Step: 100  \tTraining accuracy: 0.7281166911125183\n",
      "Step: 100  \tValid loss: 0.5247493982315063\n",
      "Step: 200  \tTraining loss: 0.46142473816871643\n",
      "Step: 200  \tTraining accuracy: 0.7356919050216675\n",
      "Step: 200  \tValid loss: 0.4707183837890625\n",
      "Step: 300  \tTraining loss: 0.4422151744365692\n",
      "Step: 300  \tTraining accuracy: 0.7454741597175598\n",
      "Step: 300  \tValid loss: 0.4543052315711975\n",
      "Step: 400  \tTraining loss: 0.42840853333473206\n",
      "Step: 400  \tTraining accuracy: 0.7543055415153503\n",
      "Step: 400  \tValid loss: 0.4397284686565399\n",
      "Step: 500  \tTraining loss: 0.4121782183647156\n",
      "Step: 500  \tTraining accuracy: 0.7626386880874634\n",
      "Step: 500  \tValid loss: 0.42203307151794434\n",
      "Step: 600  \tTraining loss: 0.3950425982475281\n",
      "Step: 600  \tTraining accuracy: 0.7710393667221069\n",
      "Step: 600  \tValid loss: 0.4037601351737976\n",
      "Step: 700  \tTraining loss: 0.3806714117527008\n",
      "Step: 700  \tTraining accuracy: 0.7788118124008179\n",
      "Step: 700  \tValid loss: 0.3890446424484253\n",
      "Step: 800  \tTraining loss: 0.37038564682006836\n",
      "Step: 800  \tTraining accuracy: 0.7859548330307007\n",
      "Step: 800  \tValid loss: 0.37903285026550293\n",
      "Step: 900  \tTraining loss: 0.36346885561943054\n",
      "Step: 900  \tTraining accuracy: 0.7920228838920593\n",
      "Step: 900  \tValid loss: 0.37258681654930115\n",
      "Step: 1000  \tTraining loss: 0.35895612835884094\n",
      "Step: 1000  \tTraining accuracy: 0.7970280647277832\n",
      "Step: 1000  \tValid loss: 0.36850255727767944\n",
      "Step: 1100  \tTraining loss: 0.3561003506183624\n",
      "Step: 1100  \tTraining accuracy: 0.8014668226242065\n",
      "Step: 1100  \tValid loss: 0.36594322323799133\n",
      "Step: 1200  \tTraining loss: 0.35431814193725586\n",
      "Step: 1200  \tTraining accuracy: 0.805075466632843\n",
      "Step: 1200  \tValid loss: 0.3643111288547516\n",
      "Step: 1300  \tTraining loss: 0.353179395198822\n",
      "Step: 1300  \tTraining accuracy: 0.8076748251914978\n",
      "Step: 1300  \tValid loss: 0.363205224275589\n",
      "Step: 1400  \tTraining loss: 0.3524008095264435\n",
      "Step: 1400  \tTraining accuracy: 0.8098393678665161\n",
      "Step: 1400  \tValid loss: 0.3623811602592468\n",
      "Step: 1500  \tTraining loss: 0.3518105447292328\n",
      "Step: 1500  \tTraining accuracy: 0.8116682171821594\n",
      "Step: 1500  \tValid loss: 0.36170661449432373\n",
      "Step: 1600  \tTraining loss: 0.35131004452705383\n",
      "Step: 1600  \tTraining accuracy: 0.8133484721183777\n",
      "Step: 1600  \tValid loss: 0.36111292243003845\n",
      "Step: 1700  \tTraining loss: 0.3508411943912506\n",
      "Step: 1700  \tTraining accuracy: 0.8149070143699646\n",
      "Step: 1700  \tValid loss: 0.360580176115036\n",
      "Step: 1800  \tTraining loss: 0.35036683082580566\n",
      "Step: 1800  \tTraining accuracy: 0.8162644505500793\n",
      "Step: 1800  \tValid loss: 0.36008885502815247\n",
      "Step: 1900  \tTraining loss: 0.34985777735710144\n",
      "Step: 1900  \tTraining accuracy: 0.8174605369567871\n",
      "Step: 1900  \tValid loss: 0.3596401512622833\n",
      "Step: 2000  \tTraining loss: 0.3493044674396515\n",
      "Step: 2000  \tTraining accuracy: 0.8184993863105774\n",
      "Step: 2000  \tValid loss: 0.35926905274391174\n",
      "Step: 2100  \tTraining loss: 0.34871652722358704\n",
      "Step: 2100  \tTraining accuracy: 0.8194303512573242\n",
      "Step: 2100  \tValid loss: 0.35908061265945435\n",
      "Step: 2200  \tTraining loss: 0.34812045097351074\n",
      "Step: 2200  \tTraining accuracy: 0.8202621340751648\n",
      "Step: 2200  \tValid loss: 0.3589709401130676\n",
      "Step: 2300  \tTraining loss: 0.3475140631198883\n",
      "Step: 2300  \tTraining accuracy: 0.8210321068763733\n",
      "Step: 2300  \tValid loss: 0.3588548004627228\n",
      "Step: 2400  \tTraining loss: 0.3468885123729706\n",
      "Step: 2400  \tTraining accuracy: 0.8217422366142273\n",
      "Step: 2400  \tValid loss: 0.3583538830280304\n",
      "Step: 2500  \tTraining loss: 0.3462390601634979\n",
      "Step: 2500  \tTraining accuracy: 0.8223613500595093\n",
      "Step: 2500  \tValid loss: 0.3579992651939392\n",
      "Step: 2600  \tTraining loss: 0.3455764651298523\n",
      "Step: 2600  \tTraining accuracy: 0.8229107856750488\n",
      "Step: 2600  \tValid loss: 0.357667475938797\n",
      "Step: 2700  \tTraining loss: 0.3448947072029114\n",
      "Step: 2700  \tTraining accuracy: 0.8235207200050354\n",
      "Step: 2700  \tValid loss: 0.35731247067451477\n",
      "Step: 2800  \tTraining loss: 0.34419506788253784\n",
      "Step: 2800  \tTraining accuracy: 0.8240814208984375\n",
      "Step: 2800  \tValid loss: 0.35685765743255615\n",
      "Step: 2900  \tTraining loss: 0.34347912669181824\n",
      "Step: 2900  \tTraining accuracy: 0.8246217370033264\n",
      "Step: 2900  \tValid loss: 0.3564155101776123\n",
      "Step: 3000  \tTraining loss: 0.3427644968032837\n",
      "Step: 3000  \tTraining accuracy: 0.8251254558563232\n",
      "Step: 3000  \tValid loss: 0.3559384346008301\n",
      "Step: 3100  \tTraining loss: 0.3420588970184326\n",
      "Step: 3100  \tTraining accuracy: 0.8256227374076843\n",
      "Step: 3100  \tValid loss: 0.3554131090641022\n",
      "Step: 3200  \tTraining loss: 0.34135881066322327\n",
      "Step: 3200  \tTraining accuracy: 0.8260583877563477\n",
      "Step: 3200  \tValid loss: 0.35497787594795227\n",
      "Step: 3300  \tTraining loss: 0.3406708240509033\n",
      "Step: 3300  \tTraining accuracy: 0.8264880776405334\n",
      "Step: 3300  \tValid loss: 0.35449475049972534\n",
      "Step: 3400  \tTraining loss: 0.33999741077423096\n",
      "Step: 3400  \tTraining accuracy: 0.8269001841545105\n",
      "Step: 3400  \tValid loss: 0.3539223372936249\n",
      "Step: 3500  \tTraining loss: 0.3393392264842987\n",
      "Step: 3500  \tTraining accuracy: 0.8273236751556396\n",
      "Step: 3500  \tValid loss: 0.35341793298721313\n",
      "Step: 3600  \tTraining loss: 0.33870095014572144\n",
      "Step: 3600  \tTraining accuracy: 0.827742338180542\n",
      "Step: 3600  \tValid loss: 0.3529146611690521\n",
      "Step: 3700  \tTraining loss: 0.3380856513977051\n",
      "Step: 3700  \tTraining accuracy: 0.8281232714653015\n",
      "Step: 3700  \tValid loss: 0.3524659276008606\n",
      "Step: 3800  \tTraining loss: 0.33749663829803467\n",
      "Step: 3800  \tTraining accuracy: 0.8284730315208435\n",
      "Step: 3800  \tValid loss: 0.352021187543869\n",
      "Step: 3900  \tTraining loss: 0.33693549036979675\n",
      "Step: 3900  \tTraining accuracy: 0.8287941813468933\n",
      "Step: 3900  \tValid loss: 0.35155004262924194\n",
      "Step: 4000  \tTraining loss: 0.33640289306640625\n",
      "Step: 4000  \tTraining accuracy: 0.8291229605674744\n",
      "Step: 4000  \tValid loss: 0.35114675760269165\n",
      "Step: 4100  \tTraining loss: 0.3358986973762512\n",
      "Step: 4100  \tTraining accuracy: 0.829455554485321\n",
      "Step: 4100  \tValid loss: 0.3508166968822479\n",
      "Step: 4200  \tTraining loss: 0.33542317152023315\n",
      "Step: 4200  \tTraining accuracy: 0.829749345779419\n",
      "Step: 4200  \tValid loss: 0.35046520829200745\n",
      "Step: 4300  \tTraining loss: 0.33497411012649536\n",
      "Step: 4300  \tTraining accuracy: 0.8300387859344482\n",
      "Step: 4300  \tValid loss: 0.3503599166870117\n",
      "Step: 4400  \tTraining loss: 0.33455225825309753\n",
      "Step: 4400  \tTraining accuracy: 0.8302963376045227\n",
      "Step: 4400  \tValid loss: 0.3502143621444702\n",
      "Step: 4500  \tTraining loss: 0.3341551721096039\n",
      "Step: 4500  \tTraining accuracy: 0.830527126789093\n",
      "Step: 4500  \tValid loss: 0.3499755263328552\n",
      "Step: 4600  \tTraining loss: 0.3337787687778473\n",
      "Step: 4600  \tTraining accuracy: 0.8307477235794067\n",
      "Step: 4600  \tValid loss: 0.3499891459941864\n",
      "Step: 4700  \tTraining loss: 0.3334224820137024\n",
      "Step: 4700  \tTraining accuracy: 0.8309705257415771\n",
      "Step: 4700  \tValid loss: 0.34978652000427246\n",
      "Step: 4800  \tTraining loss: 0.3330831527709961\n",
      "Step: 4800  \tTraining accuracy: 0.8311640024185181\n",
      "Step: 4800  \tValid loss: 0.3498024642467499\n",
      "Step: 4900  \tTraining loss: 0.33275946974754333\n",
      "Step: 4900  \tTraining accuracy: 0.8313438892364502\n",
      "Step: 4900  \tValid loss: 0.3497351109981537\n",
      "Step: 5000  \tTraining loss: 0.3324495553970337\n",
      "Step: 5000  \tTraining accuracy: 0.8315165638923645\n",
      "Step: 5000  \tValid loss: 0.3496851623058319\n",
      "Step: 5100  \tTraining loss: 0.33215221762657166\n",
      "Step: 5100  \tTraining accuracy: 0.8316930532455444\n",
      "Step: 5100  \tValid loss: 0.3496599495410919\n",
      "Step: 5200  \tTraining loss: 0.33186596632003784\n",
      "Step: 5200  \tTraining accuracy: 0.8318575024604797\n",
      "Step: 5200  \tValid loss: 0.3496928811073303\n",
      "Step: 5300  \tTraining loss: 0.3315903842449188\n",
      "Step: 5300  \tTraining accuracy: 0.8320233821868896\n",
      "Step: 5300  \tValid loss: 0.34968826174736023\n",
      "Step: 5400  \tTraining loss: 0.33132416009902954\n",
      "Step: 5400  \tTraining accuracy: 0.8321805596351624\n",
      "Step: 5400  \tValid loss: 0.3496464490890503\n",
      "Step: 5500  \tTraining loss: 0.3310665488243103\n",
      "Step: 5500  \tTraining accuracy: 0.8323517441749573\n",
      "Step: 5500  \tValid loss: 0.3496638238430023\n",
      "Step: 5600  \tTraining loss: 0.33081692457199097\n",
      "Step: 5600  \tTraining accuracy: 0.8324973583221436\n",
      "Step: 5600  \tValid loss: 0.3496553599834442\n",
      "Step: 5700  \tTraining loss: 0.33057430386543274\n",
      "Step: 5700  \tTraining accuracy: 0.8326210379600525\n",
      "Step: 5700  \tValid loss: 0.3497394919395447\n",
      "Step: 5800  \tTraining loss: 0.3303396999835968\n",
      "Step: 5800  \tTraining accuracy: 0.8327662348747253\n",
      "Step: 5800  \tValid loss: 0.3496280908584595\n",
      "Step: 5900  \tTraining loss: 0.33010968565940857\n",
      "Step: 5900  \tTraining accuracy: 0.8329088687896729\n",
      "Step: 5900  \tValid loss: 0.34993958473205566\n",
      "Step: 6000  \tTraining loss: 0.32988741993904114\n",
      "Step: 6000  \tTraining accuracy: 0.8330512046813965\n",
      "Step: 6000  \tValid loss: 0.35002267360687256\n",
      "Step: 6100  \tTraining loss: 0.32967159152030945\n",
      "Step: 6100  \tTraining accuracy: 0.833204448223114\n",
      "Step: 6100  \tValid loss: 0.349966824054718\n",
      "Step: 6200  \tTraining loss: 0.32946181297302246\n",
      "Step: 6200  \tTraining accuracy: 0.8333527445793152\n",
      "Step: 6200  \tValid loss: 0.3499898910522461\n",
      "Step: 6300  \tTraining loss: 0.3292563557624817\n",
      "Step: 6300  \tTraining accuracy: 0.8335027694702148\n",
      "Step: 6300  \tValid loss: 0.35026276111602783\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8336566\n",
      "Precision: 0.87153137\n",
      "Recall: 0.92677593\n",
      "F1 score: 0.8592668\n",
      "AUC: 0.78046113\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.833657   0.871531  0.926776  0.859267  0.780461  0.329102      0.833547   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.349432       0.833532   0.339927      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  6377.0  \n",
      "13\n",
      "(4785, 8)\n",
      "(4785, 1)\n",
      "(2640, 8)\n",
      "(2640, 1)\n",
      "(2145, 8)\n",
      "(2145, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6395780444145203\n",
      "Step: 100  \tTraining accuracy: 0.6639498472213745\n",
      "Step: 100  \tValid loss: 0.6427602767944336\n",
      "Step: 200  \tTraining loss: 0.5767392516136169\n",
      "Step: 200  \tTraining accuracy: 0.6707767248153687\n",
      "Step: 200  \tValid loss: 0.5790836215019226\n",
      "Step: 300  \tTraining loss: 0.44923165440559387\n",
      "Step: 300  \tTraining accuracy: 0.7015256285667419\n",
      "Step: 300  \tValid loss: 0.44971367716789246\n",
      "Step: 400  \tTraining loss: 0.3723485469818115\n",
      "Step: 400  \tTraining accuracy: 0.7379608750343323\n",
      "Step: 400  \tValid loss: 0.37368783354759216\n",
      "Step: 500  \tTraining loss: 0.33472275733947754\n",
      "Step: 500  \tTraining accuracy: 0.7624985575675964\n",
      "Step: 500  \tValid loss: 0.338251531124115\n",
      "Step: 600  \tTraining loss: 0.30945494771003723\n",
      "Step: 600  \tTraining accuracy: 0.7805072665214539\n",
      "Step: 600  \tValid loss: 0.3171212077140808\n",
      "Step: 700  \tTraining loss: 0.30316418409347534\n",
      "Step: 700  \tTraining accuracy: 0.7945020198822021\n",
      "Step: 700  \tValid loss: 0.31360405683517456\n",
      "Step: 800  \tTraining loss: 0.30066758394241333\n",
      "Step: 800  \tTraining accuracy: 0.8050992488861084\n",
      "Step: 800  \tValid loss: 0.31229305267333984\n",
      "Step: 900  \tTraining loss: 0.29902374744415283\n",
      "Step: 900  \tTraining accuracy: 0.8133505582809448\n",
      "Step: 900  \tValid loss: 0.31117454171180725\n",
      "Step: 1000  \tTraining loss: 0.29789435863494873\n",
      "Step: 1000  \tTraining accuracy: 0.8202056884765625\n",
      "Step: 1000  \tValid loss: 0.3104837238788605\n",
      "Step: 1100  \tTraining loss: 0.2970637381076813\n",
      "Step: 1100  \tTraining accuracy: 0.8260735273361206\n",
      "Step: 1100  \tValid loss: 0.3100762963294983\n",
      "Step: 1200  \tTraining loss: 0.2964479327201843\n",
      "Step: 1200  \tTraining accuracy: 0.8310117721557617\n",
      "Step: 1200  \tValid loss: 0.30982327461242676\n",
      "Step: 1300  \tTraining loss: 0.29599201679229736\n",
      "Step: 1300  \tTraining accuracy: 0.8352685570716858\n",
      "Step: 1300  \tValid loss: 0.3096334636211395\n",
      "Step: 1400  \tTraining loss: 0.29564177989959717\n",
      "Step: 1400  \tTraining accuracy: 0.8389489054679871\n",
      "Step: 1400  \tValid loss: 0.30945882201194763\n",
      "Step: 1500  \tTraining loss: 0.29534459114074707\n",
      "Step: 1500  \tTraining accuracy: 0.8421143889427185\n",
      "Step: 1500  \tValid loss: 0.30927562713623047\n",
      "Step: 1600  \tTraining loss: 0.2950659394264221\n",
      "Step: 1600  \tTraining accuracy: 0.8448444604873657\n",
      "Step: 1600  \tValid loss: 0.3090641498565674\n",
      "Step: 1700  \tTraining loss: 0.2947738468647003\n",
      "Step: 1700  \tTraining accuracy: 0.84724360704422\n",
      "Step: 1700  \tValid loss: 0.30878692865371704\n",
      "Step: 1800  \tTraining loss: 0.2944847345352173\n",
      "Step: 1800  \tTraining accuracy: 0.8493446707725525\n",
      "Step: 1800  \tValid loss: 0.3085358142852783\n",
      "Step: 1900  \tTraining loss: 0.29420146346092224\n",
      "Step: 1900  \tTraining accuracy: 0.8512186408042908\n",
      "Step: 1900  \tValid loss: 0.3083246052265167\n",
      "Step: 2000  \tTraining loss: 0.2939220070838928\n",
      "Step: 2000  \tTraining accuracy: 0.8528896570205688\n",
      "Step: 2000  \tValid loss: 0.3081147074699402\n",
      "Step: 2100  \tTraining loss: 0.2936515808105469\n",
      "Step: 2100  \tTraining accuracy: 0.8544231057167053\n",
      "Step: 2100  \tValid loss: 0.3079487979412079\n",
      "Step: 2200  \tTraining loss: 0.29337915778160095\n",
      "Step: 2200  \tTraining accuracy: 0.8558090925216675\n",
      "Step: 2200  \tValid loss: 0.30774354934692383\n",
      "Step: 2300  \tTraining loss: 0.2931126058101654\n",
      "Step: 2300  \tTraining accuracy: 0.8570114970207214\n",
      "Step: 2300  \tValid loss: 0.30757060647010803\n",
      "Step: 2400  \tTraining loss: 0.29285159707069397\n",
      "Step: 2400  \tTraining accuracy: 0.8580893278121948\n",
      "Step: 2400  \tValid loss: 0.30740341544151306\n",
      "Step: 2500  \tTraining loss: 0.29259049892425537\n",
      "Step: 2500  \tTraining accuracy: 0.8590450882911682\n",
      "Step: 2500  \tValid loss: 0.3072356879711151\n",
      "Step: 2600  \tTraining loss: 0.2923242449760437\n",
      "Step: 2600  \tTraining accuracy: 0.8599217534065247\n",
      "Step: 2600  \tValid loss: 0.3070736527442932\n",
      "Step: 2700  \tTraining loss: 0.2920432388782501\n",
      "Step: 2700  \tTraining accuracy: 0.8607479929924011\n",
      "Step: 2700  \tValid loss: 0.30682119727134705\n",
      "Step: 2800  \tTraining loss: 0.2917468845844269\n",
      "Step: 2800  \tTraining accuracy: 0.8614913821220398\n",
      "Step: 2800  \tValid loss: 0.3065868318080902\n",
      "Step: 2900  \tTraining loss: 0.29143527150154114\n",
      "Step: 2900  \tTraining accuracy: 0.8621899485588074\n",
      "Step: 2900  \tValid loss: 0.30633705854415894\n",
      "Step: 3000  \tTraining loss: 0.2911016643047333\n",
      "Step: 3000  \tTraining accuracy: 0.8628517985343933\n",
      "Step: 3000  \tValid loss: 0.30606958270072937\n",
      "Step: 3100  \tTraining loss: 0.2907373607158661\n",
      "Step: 3100  \tTraining accuracy: 0.8634667992591858\n",
      "Step: 3100  \tValid loss: 0.305771142244339\n",
      "Step: 3200  \tTraining loss: 0.2903146743774414\n",
      "Step: 3200  \tTraining accuracy: 0.8640526533126831\n",
      "Step: 3200  \tValid loss: 0.30536842346191406\n",
      "Step: 3300  \tTraining loss: 0.2896142601966858\n",
      "Step: 3300  \tTraining accuracy: 0.8645928502082825\n",
      "Step: 3300  \tValid loss: 0.30427390336990356\n",
      "Step: 3400  \tTraining loss: 0.2888641655445099\n",
      "Step: 3400  \tTraining accuracy: 0.8650540113449097\n",
      "Step: 3400  \tValid loss: 0.30313998460769653\n",
      "Step: 3500  \tTraining loss: 0.2882985770702362\n",
      "Step: 3500  \tTraining accuracy: 0.8654975295066833\n",
      "Step: 3500  \tValid loss: 0.30260565876960754\n",
      "Step: 3600  \tTraining loss: 0.28779542446136475\n",
      "Step: 3600  \tTraining accuracy: 0.8659248948097229\n",
      "Step: 3600  \tValid loss: 0.30214470624923706\n",
      "Step: 3700  \tTraining loss: 0.2873346507549286\n",
      "Step: 3700  \tTraining accuracy: 0.8663231134414673\n",
      "Step: 3700  \tValid loss: 0.3017255365848541\n",
      "Step: 3800  \tTraining loss: 0.2869028151035309\n",
      "Step: 3800  \tTraining accuracy: 0.8667029142379761\n",
      "Step: 3800  \tValid loss: 0.30132144689559937\n",
      "Step: 3900  \tTraining loss: 0.28648751974105835\n",
      "Step: 3900  \tTraining accuracy: 0.8670520782470703\n",
      "Step: 3900  \tValid loss: 0.3009246289730072\n",
      "Step: 4000  \tTraining loss: 0.28609129786491394\n",
      "Step: 4000  \tTraining accuracy: 0.867378294467926\n",
      "Step: 4000  \tValid loss: 0.30055949091911316\n",
      "Step: 4100  \tTraining loss: 0.2857058048248291\n",
      "Step: 4100  \tTraining accuracy: 0.8676832318305969\n",
      "Step: 4100  \tValid loss: 0.30022096633911133\n",
      "Step: 4200  \tTraining loss: 0.28533029556274414\n",
      "Step: 4200  \tTraining accuracy: 0.8679759502410889\n",
      "Step: 4200  \tValid loss: 0.2999171018600464\n",
      "Step: 4300  \tTraining loss: 0.284959614276886\n",
      "Step: 4300  \tTraining accuracy: 0.8682549595832825\n",
      "Step: 4300  \tValid loss: 0.2996145486831665\n",
      "Step: 4400  \tTraining loss: 0.284595251083374\n",
      "Step: 4400  \tTraining accuracy: 0.8685091137886047\n",
      "Step: 4400  \tValid loss: 0.2993122935295105\n",
      "Step: 4500  \tTraining loss: 0.2842385768890381\n",
      "Step: 4500  \tTraining accuracy: 0.8687635660171509\n",
      "Step: 4500  \tValid loss: 0.2989922761917114\n",
      "Step: 4600  \tTraining loss: 0.2838820815086365\n",
      "Step: 4600  \tTraining accuracy: 0.8690091371536255\n",
      "Step: 4600  \tValid loss: 0.29866284132003784\n",
      "Step: 4700  \tTraining loss: 0.2835409939289093\n",
      "Step: 4700  \tTraining accuracy: 0.8692486882209778\n",
      "Step: 4700  \tValid loss: 0.2982962727546692\n",
      "Step: 4800  \tTraining loss: 0.28305694460868835\n",
      "Step: 4800  \tTraining accuracy: 0.8694824576377869\n",
      "Step: 4800  \tValid loss: 0.2977065443992615\n",
      "Step: 4900  \tTraining loss: 0.2826673984527588\n",
      "Step: 4900  \tTraining accuracy: 0.8697023391723633\n",
      "Step: 4900  \tValid loss: 0.2970282733440399\n",
      "Step: 5000  \tTraining loss: 0.2823364734649658\n",
      "Step: 5000  \tTraining accuracy: 0.8699218034744263\n",
      "Step: 5000  \tValid loss: 0.29646921157836914\n",
      "Step: 5100  \tTraining loss: 0.28202253580093384\n",
      "Step: 5100  \tTraining accuracy: 0.8701635599136353\n",
      "Step: 5100  \tValid loss: 0.2959965169429779\n",
      "Step: 5200  \tTraining loss: 0.2817212641239166\n",
      "Step: 5200  \tTraining accuracy: 0.8703979849815369\n",
      "Step: 5200  \tValid loss: 0.2955823838710785\n",
      "Step: 5300  \tTraining loss: 0.2814137041568756\n",
      "Step: 5300  \tTraining accuracy: 0.8706274628639221\n",
      "Step: 5300  \tValid loss: 0.2952277660369873\n",
      "Step: 5400  \tTraining loss: 0.28108030557632446\n",
      "Step: 5400  \tTraining accuracy: 0.8708463907241821\n",
      "Step: 5400  \tValid loss: 0.2948816418647766\n",
      "Step: 5500  \tTraining loss: 0.28072336316108704\n",
      "Step: 5500  \tTraining accuracy: 0.8710553646087646\n",
      "Step: 5500  \tValid loss: 0.2945443391799927\n",
      "Step: 5600  \tTraining loss: 0.2803671061992645\n",
      "Step: 5600  \tTraining accuracy: 0.8712511658668518\n",
      "Step: 5600  \tValid loss: 0.2942236363887787\n",
      "Step: 5700  \tTraining loss: 0.28000500798225403\n",
      "Step: 5700  \tTraining accuracy: 0.8714492917060852\n",
      "Step: 5700  \tValid loss: 0.29397040605545044\n",
      "Step: 5800  \tTraining loss: 0.27966052293777466\n",
      "Step: 5800  \tTraining accuracy: 0.8716441988945007\n",
      "Step: 5800  \tValid loss: 0.2936815321445465\n",
      "Step: 5900  \tTraining loss: 0.2793305516242981\n",
      "Step: 5900  \tTraining accuracy: 0.8718252182006836\n",
      "Step: 5900  \tValid loss: 0.2933882772922516\n",
      "Step: 6000  \tTraining loss: 0.27900436520576477\n",
      "Step: 6000  \tTraining accuracy: 0.8719949126243591\n",
      "Step: 6000  \tValid loss: 0.29310742020606995\n",
      "Step: 6100  \tTraining loss: 0.27869728207588196\n",
      "Step: 6100  \tTraining accuracy: 0.8721538782119751\n",
      "Step: 6100  \tValid loss: 0.2928720712661743\n",
      "Step: 6200  \tTraining loss: 0.2783992290496826\n",
      "Step: 6200  \tTraining accuracy: 0.8723075985908508\n",
      "Step: 6200  \tValid loss: 0.29261741042137146\n",
      "Step: 6300  \tTraining loss: 0.27810853719711304\n",
      "Step: 6300  \tTraining accuracy: 0.8724614381790161\n",
      "Step: 6300  \tValid loss: 0.29236194491386414\n",
      "Step: 6400  \tTraining loss: 0.27782687544822693\n",
      "Step: 6400  \tTraining accuracy: 0.8726120591163635\n",
      "Step: 6400  \tValid loss: 0.2921127676963806\n",
      "Step: 6500  \tTraining loss: 0.2775525152683258\n",
      "Step: 6500  \tTraining accuracy: 0.8727677464485168\n",
      "Step: 6500  \tValid loss: 0.2918635904788971\n",
      "Step: 6600  \tTraining loss: 0.2772843539714813\n",
      "Step: 6600  \tTraining accuracy: 0.872917115688324\n",
      "Step: 6600  \tValid loss: 0.291610985994339\n",
      "Step: 6700  \tTraining loss: 0.2770197093486786\n",
      "Step: 6700  \tTraining accuracy: 0.8730619549751282\n",
      "Step: 6700  \tValid loss: 0.2913798689842224\n",
      "Step: 6800  \tTraining loss: 0.27676182985305786\n",
      "Step: 6800  \tTraining accuracy: 0.8732056021690369\n",
      "Step: 6800  \tValid loss: 0.2911396324634552\n",
      "Step: 6900  \tTraining loss: 0.27650654315948486\n",
      "Step: 6900  \tTraining accuracy: 0.8733481168746948\n",
      "Step: 6900  \tValid loss: 0.29090383648872375\n",
      "Step: 7000  \tTraining loss: 0.2762546241283417\n",
      "Step: 7000  \tTraining accuracy: 0.8734865188598633\n",
      "Step: 7000  \tValid loss: 0.2906695306301117\n",
      "Step: 7100  \tTraining loss: 0.27600550651550293\n",
      "Step: 7100  \tTraining accuracy: 0.8736150860786438\n",
      "Step: 7100  \tValid loss: 0.2904384732246399\n",
      "Step: 7200  \tTraining loss: 0.27575695514678955\n",
      "Step: 7200  \tTraining accuracy: 0.873731255531311\n",
      "Step: 7200  \tValid loss: 0.2902095317840576\n",
      "Step: 7300  \tTraining loss: 0.2755093574523926\n",
      "Step: 7300  \tTraining accuracy: 0.8738442659378052\n",
      "Step: 7300  \tValid loss: 0.2899855077266693\n",
      "Step: 7400  \tTraining loss: 0.27526330947875977\n",
      "Step: 7400  \tTraining accuracy: 0.873954176902771\n",
      "Step: 7400  \tValid loss: 0.2897630035877228\n",
      "Step: 7500  \tTraining loss: 0.27501755952835083\n",
      "Step: 7500  \tTraining accuracy: 0.8740653395652771\n",
      "Step: 7500  \tValid loss: 0.2895520329475403\n",
      "Step: 7600  \tTraining loss: 0.2747719883918762\n",
      "Step: 7600  \tTraining accuracy: 0.8741722106933594\n",
      "Step: 7600  \tValid loss: 0.28933775424957275\n",
      "Step: 7700  \tTraining loss: 0.27452749013900757\n",
      "Step: 7700  \tTraining accuracy: 0.8742817044258118\n",
      "Step: 7700  \tValid loss: 0.2891187369823456\n",
      "Step: 7800  \tTraining loss: 0.27428168058395386\n",
      "Step: 7800  \tTraining accuracy: 0.8743924498558044\n",
      "Step: 7800  \tValid loss: 0.28890755772590637\n",
      "Step: 7900  \tTraining loss: 0.27403536438941956\n",
      "Step: 7900  \tTraining accuracy: 0.8745003342628479\n",
      "Step: 7900  \tValid loss: 0.28870460391044617\n",
      "Step: 8000  \tTraining loss: 0.2737865746021271\n",
      "Step: 8000  \tTraining accuracy: 0.8746120929718018\n",
      "Step: 8000  \tValid loss: 0.28850290179252625\n",
      "Step: 8100  \tTraining loss: 0.2735360264778137\n",
      "Step: 8100  \tTraining accuracy: 0.8747301697731018\n",
      "Step: 8100  \tValid loss: 0.2883053719997406\n",
      "Step: 8200  \tTraining loss: 0.2732841372489929\n",
      "Step: 8200  \tTraining accuracy: 0.8748440742492676\n",
      "Step: 8200  \tValid loss: 0.28810620307922363\n",
      "Step: 8300  \tTraining loss: 0.27303028106689453\n",
      "Step: 8300  \tTraining accuracy: 0.8749628067016602\n",
      "Step: 8300  \tValid loss: 0.28791189193725586\n",
      "Step: 8400  \tTraining loss: 0.2727715075016022\n",
      "Step: 8400  \tTraining accuracy: 0.8750836849212646\n",
      "Step: 8400  \tValid loss: 0.2876967191696167\n",
      "Step: 8500  \tTraining loss: 0.27251115441322327\n",
      "Step: 8500  \tTraining accuracy: 0.8752017021179199\n",
      "Step: 8500  \tValid loss: 0.2874808609485626\n",
      "Step: 8600  \tTraining loss: 0.272247850894928\n",
      "Step: 8600  \tTraining accuracy: 0.8753133416175842\n",
      "Step: 8600  \tValid loss: 0.28726571798324585\n",
      "Step: 8700  \tTraining loss: 0.2719804346561432\n",
      "Step: 8700  \tTraining accuracy: 0.8754150867462158\n",
      "Step: 8700  \tValid loss: 0.28704380989074707\n",
      "Step: 8800  \tTraining loss: 0.2717108726501465\n",
      "Step: 8800  \tTraining accuracy: 0.8755133748054504\n",
      "Step: 8800  \tValid loss: 0.2868193984031677\n",
      "Step: 8900  \tTraining loss: 0.271438866853714\n",
      "Step: 8900  \tTraining accuracy: 0.8756093978881836\n",
      "Step: 8900  \tValid loss: 0.2865966856479645\n",
      "Step: 9000  \tTraining loss: 0.271165132522583\n",
      "Step: 9000  \tTraining accuracy: 0.8756939172744751\n",
      "Step: 9000  \tValid loss: 0.28636765480041504\n",
      "Step: 9100  \tTraining loss: 0.27088984847068787\n",
      "Step: 9100  \tTraining accuracy: 0.8757743239402771\n",
      "Step: 9100  \tValid loss: 0.2861239016056061\n",
      "Step: 9200  \tTraining loss: 0.2706104815006256\n",
      "Step: 9200  \tTraining accuracy: 0.8758620619773865\n",
      "Step: 9200  \tValid loss: 0.2858346104621887\n",
      "Step: 9300  \tTraining loss: 0.27033162117004395\n",
      "Step: 9300  \tTraining accuracy: 0.8759546875953674\n",
      "Step: 9300  \tValid loss: 0.28553399443626404\n",
      "Step: 9400  \tTraining loss: 0.27005258202552795\n",
      "Step: 9400  \tTraining accuracy: 0.8760464787483215\n",
      "Step: 9400  \tValid loss: 0.28524690866470337\n",
      "Step: 9500  \tTraining loss: 0.26977303624153137\n",
      "Step: 9500  \tTraining accuracy: 0.876136302947998\n",
      "Step: 9500  \tValid loss: 0.28496599197387695\n",
      "Step: 9600  \tTraining loss: 0.2694931924343109\n",
      "Step: 9600  \tTraining accuracy: 0.8762242197990417\n",
      "Step: 9600  \tValid loss: 0.2846832871437073\n",
      "Step: 9700  \tTraining loss: 0.2692127227783203\n",
      "Step: 9700  \tTraining accuracy: 0.8763146996498108\n",
      "Step: 9700  \tValid loss: 0.2843974232673645\n",
      "Step: 9800  \tTraining loss: 0.26893123984336853\n",
      "Step: 9800  \tTraining accuracy: 0.8764097094535828\n",
      "Step: 9800  \tValid loss: 0.2841126620769501\n",
      "Step: 9900  \tTraining loss: 0.2686486840248108\n",
      "Step: 9900  \tTraining accuracy: 0.8764985799789429\n",
      "Step: 9900  \tValid loss: 0.2838173806667328\n",
      "Step: 10000  \tTraining loss: 0.2683635652065277\n",
      "Step: 10000  \tTraining accuracy: 0.8765856623649597\n",
      "Step: 10000  \tValid loss: 0.283521443605423\n",
      "Step: 10100  \tTraining loss: 0.2680772840976715\n",
      "Step: 10100  \tTraining accuracy: 0.8766751289367676\n",
      "Step: 10100  \tValid loss: 0.2832258641719818\n",
      "Step: 10200  \tTraining loss: 0.2677890360355377\n",
      "Step: 10200  \tTraining accuracy: 0.8767690658569336\n",
      "Step: 10200  \tValid loss: 0.28293174505233765\n",
      "Step: 10300  \tTraining loss: 0.2674999237060547\n",
      "Step: 10300  \tTraining accuracy: 0.8768652081489563\n",
      "Step: 10300  \tValid loss: 0.28263407945632935\n",
      "Step: 10400  \tTraining loss: 0.26720982789993286\n",
      "Step: 10400  \tTraining accuracy: 0.876959502696991\n",
      "Step: 10400  \tValid loss: 0.28234022855758667\n",
      "Step: 10500  \tTraining loss: 0.2669226825237274\n",
      "Step: 10500  \tTraining accuracy: 0.8770520091056824\n",
      "Step: 10500  \tValid loss: 0.28204938769340515\n",
      "Step: 10600  \tTraining loss: 0.26663854718208313\n",
      "Step: 10600  \tTraining accuracy: 0.8771427273750305\n",
      "Step: 10600  \tValid loss: 0.2817500829696655\n",
      "Step: 10700  \tTraining loss: 0.2663598656654358\n",
      "Step: 10700  \tTraining accuracy: 0.8772337436676025\n",
      "Step: 10700  \tValid loss: 0.28146690130233765\n",
      "Step: 10800  \tTraining loss: 0.26608580350875854\n",
      "Step: 10800  \tTraining accuracy: 0.8773298263549805\n",
      "Step: 10800  \tValid loss: 0.28117749094963074\n",
      "Step: 10900  \tTraining loss: 0.26578810811042786\n",
      "Step: 10900  \tTraining accuracy: 0.8774357438087463\n",
      "Step: 10900  \tValid loss: 0.2808205485343933\n",
      "Step: 11000  \tTraining loss: 0.26544857025146484\n",
      "Step: 11000  \tTraining accuracy: 0.8775454163551331\n",
      "Step: 11000  \tValid loss: 0.2803056240081787\n",
      "Step: 11100  \tTraining loss: 0.26513662934303284\n",
      "Step: 11100  \tTraining accuracy: 0.8776597380638123\n",
      "Step: 11100  \tValid loss: 0.27991217374801636\n",
      "Step: 11200  \tTraining loss: 0.2648474872112274\n",
      "Step: 11200  \tTraining accuracy: 0.8777766823768616\n",
      "Step: 11200  \tValid loss: 0.27955809235572815\n",
      "Step: 11300  \tTraining loss: 0.26456761360168457\n",
      "Step: 11300  \tTraining accuracy: 0.8778878450393677\n",
      "Step: 11300  \tValid loss: 0.2792145609855652\n",
      "Step: 11400  \tTraining loss: 0.26429498195648193\n",
      "Step: 11400  \tTraining accuracy: 0.8779961466789246\n",
      "Step: 11400  \tValid loss: 0.2788778245449066\n",
      "Step: 11500  \tTraining loss: 0.2640281319618225\n",
      "Step: 11500  \tTraining accuracy: 0.8781043291091919\n",
      "Step: 11500  \tValid loss: 0.2785433232784271\n",
      "Step: 11600  \tTraining loss: 0.26376673579216003\n",
      "Step: 11600  \tTraining accuracy: 0.8782115578651428\n",
      "Step: 11600  \tValid loss: 0.2782151997089386\n",
      "Step: 11700  \tTraining loss: 0.2635103762149811\n",
      "Step: 11700  \tTraining accuracy: 0.8783196806907654\n",
      "Step: 11700  \tValid loss: 0.2778984308242798\n",
      "Step: 11800  \tTraining loss: 0.26325786113739014\n",
      "Step: 11800  \tTraining accuracy: 0.8784205913543701\n",
      "Step: 11800  \tValid loss: 0.27758365869522095\n",
      "Step: 11900  \tTraining loss: 0.26300790905952454\n",
      "Step: 11900  \tTraining accuracy: 0.8785136342048645\n",
      "Step: 11900  \tValid loss: 0.27727240324020386\n",
      "Step: 12000  \tTraining loss: 0.26275986433029175\n",
      "Step: 12000  \tTraining accuracy: 0.8785989880561829\n",
      "Step: 12000  \tValid loss: 0.27696701884269714\n",
      "Step: 12100  \tTraining loss: 0.26251310110092163\n",
      "Step: 12100  \tTraining accuracy: 0.8786829710006714\n",
      "Step: 12100  \tValid loss: 0.2766673266887665\n",
      "Step: 12200  \tTraining loss: 0.2622673809528351\n",
      "Step: 12200  \tTraining accuracy: 0.8787680864334106\n",
      "Step: 12200  \tValid loss: 0.27636703848838806\n",
      "Step: 12300  \tTraining loss: 0.2620222866535187\n",
      "Step: 12300  \tTraining accuracy: 0.8788509964942932\n",
      "Step: 12300  \tValid loss: 0.2760704755783081\n",
      "Step: 12400  \tTraining loss: 0.26177820563316345\n",
      "Step: 12400  \tTraining accuracy: 0.8789308667182922\n",
      "Step: 12400  \tValid loss: 0.2757803201675415\n",
      "Step: 12500  \tTraining loss: 0.26153451204299927\n",
      "Step: 12500  \tTraining accuracy: 0.8790094256401062\n",
      "Step: 12500  \tValid loss: 0.27549460530281067\n",
      "Step: 12600  \tTraining loss: 0.26129165291786194\n",
      "Step: 12600  \tTraining accuracy: 0.8790867924690247\n",
      "Step: 12600  \tValid loss: 0.27521398663520813\n",
      "Step: 12700  \tTraining loss: 0.26104965806007385\n",
      "Step: 12700  \tTraining accuracy: 0.8791629076004028\n",
      "Step: 12700  \tValid loss: 0.27494192123413086\n",
      "Step: 12800  \tTraining loss: 0.2608104348182678\n",
      "Step: 12800  \tTraining accuracy: 0.8792378306388855\n",
      "Step: 12800  \tValid loss: 0.2746727764606476\n",
      "Step: 12900  \tTraining loss: 0.26057130098342896\n",
      "Step: 12900  \tTraining accuracy: 0.8793107271194458\n",
      "Step: 12900  \tValid loss: 0.27440187335014343\n",
      "Step: 13000  \tTraining loss: 0.2603328824043274\n",
      "Step: 13000  \tTraining accuracy: 0.8793793320655823\n",
      "Step: 13000  \tValid loss: 0.2741408944129944\n",
      "Step: 13100  \tTraining loss: 0.2600971460342407\n",
      "Step: 13100  \tTraining accuracy: 0.8794452548027039\n",
      "Step: 13100  \tValid loss: 0.2738782465457916\n",
      "Step: 13200  \tTraining loss: 0.25986430048942566\n",
      "Step: 13200  \tTraining accuracy: 0.8795077800750732\n",
      "Step: 13200  \tValid loss: 0.2736210823059082\n",
      "Step: 13300  \tTraining loss: 0.25963327288627625\n",
      "Step: 13300  \tTraining accuracy: 0.8795725703239441\n",
      "Step: 13300  \tValid loss: 0.27336764335632324\n",
      "Step: 13400  \tTraining loss: 0.25940465927124023\n",
      "Step: 13400  \tTraining accuracy: 0.8796355724334717\n",
      "Step: 13400  \tValid loss: 0.2731194496154785\n",
      "Step: 13500  \tTraining loss: 0.2591799795627594\n",
      "Step: 13500  \tTraining accuracy: 0.8796968460083008\n",
      "Step: 13500  \tValid loss: 0.27288198471069336\n",
      "Step: 13600  \tTraining loss: 0.25895482301712036\n",
      "Step: 13600  \tTraining accuracy: 0.8797610998153687\n",
      "Step: 13600  \tValid loss: 0.27264970541000366\n",
      "Step: 13700  \tTraining loss: 0.25873202085494995\n",
      "Step: 13700  \tTraining accuracy: 0.8798274397850037\n",
      "Step: 13700  \tValid loss: 0.2724141776561737\n",
      "Step: 13800  \tTraining loss: 0.25850731134414673\n",
      "Step: 13800  \tTraining accuracy: 0.8798936009407043\n",
      "Step: 13800  \tValid loss: 0.27222609519958496\n",
      "Step: 13900  \tTraining loss: 0.2582835555076599\n",
      "Step: 13900  \tTraining accuracy: 0.8799610733985901\n",
      "Step: 13900  \tValid loss: 0.2720136344432831\n",
      "Step: 14000  \tTraining loss: 0.2580684721469879\n",
      "Step: 14000  \tTraining accuracy: 0.8800275921821594\n",
      "Step: 14000  \tValid loss: 0.2717738449573517\n",
      "Step: 14100  \tTraining loss: 0.2578611671924591\n",
      "Step: 14100  \tTraining accuracy: 0.8800901174545288\n",
      "Step: 14100  \tValid loss: 0.2715259790420532\n",
      "Step: 14200  \tTraining loss: 0.257650226354599\n",
      "Step: 14200  \tTraining accuracy: 0.8801481127738953\n",
      "Step: 14200  \tValid loss: 0.271261602640152\n",
      "Step: 14300  \tTraining loss: 0.25744229555130005\n",
      "Step: 14300  \tTraining accuracy: 0.8802045583724976\n",
      "Step: 14300  \tValid loss: 0.2709704339504242\n",
      "Step: 14400  \tTraining loss: 0.2572401165962219\n",
      "Step: 14400  \tTraining accuracy: 0.8802617192268372\n",
      "Step: 14400  \tValid loss: 0.2706679701805115\n",
      "Step: 14500  \tTraining loss: 0.2570488154888153\n",
      "Step: 14500  \tTraining accuracy: 0.8803201913833618\n",
      "Step: 14500  \tValid loss: 0.2704240381717682\n",
      "Step: 14600  \tTraining loss: 0.2568672001361847\n",
      "Step: 14600  \tTraining accuracy: 0.8803757429122925\n",
      "Step: 14600  \tValid loss: 0.2702224552631378\n",
      "Step: 14700  \tTraining loss: 0.2566896677017212\n",
      "Step: 14700  \tTraining accuracy: 0.880432665348053\n",
      "Step: 14700  \tValid loss: 0.27002081274986267\n",
      "Step: 14800  \tTraining loss: 0.2562828063964844\n",
      "Step: 14800  \tTraining accuracy: 0.8804909586906433\n",
      "Step: 14800  \tValid loss: 0.26992255449295044\n",
      "Step: 14900  \tTraining loss: 0.2560752034187317\n",
      "Step: 14900  \tTraining accuracy: 0.8805526494979858\n",
      "Step: 14900  \tValid loss: 0.2697916328907013\n",
      "Step: 15000  \tTraining loss: 0.25588494539260864\n",
      "Step: 15000  \tTraining accuracy: 0.8806142210960388\n",
      "Step: 15000  \tValid loss: 0.2696225345134735\n",
      "Step: 15100  \tTraining loss: 0.2557019293308258\n",
      "Step: 15100  \tTraining accuracy: 0.8806750178337097\n",
      "Step: 15100  \tValid loss: 0.26943439245224\n",
      "Step: 15200  \tTraining loss: 0.2555241882801056\n",
      "Step: 15200  \tTraining accuracy: 0.8807328939437866\n",
      "Step: 15200  \tValid loss: 0.2692267596721649\n",
      "Step: 15300  \tTraining loss: 0.255353718996048\n",
      "Step: 15300  \tTraining accuracy: 0.8807886838912964\n",
      "Step: 15300  \tValid loss: 0.2690125107765198\n",
      "Step: 15400  \tTraining loss: 0.25518742203712463\n",
      "Step: 15400  \tTraining accuracy: 0.8808450698852539\n",
      "Step: 15400  \tValid loss: 0.2687964141368866\n",
      "Step: 15500  \tTraining loss: 0.2550284266471863\n",
      "Step: 15500  \tTraining accuracy: 0.8809007406234741\n",
      "Step: 15500  \tValid loss: 0.26860249042510986\n",
      "Step: 15600  \tTraining loss: 0.25487297773361206\n",
      "Step: 15600  \tTraining accuracy: 0.8809550404548645\n",
      "Step: 15600  \tValid loss: 0.2684105634689331\n",
      "Step: 15700  \tTraining loss: 0.254716694355011\n",
      "Step: 15700  \tTraining accuracy: 0.8810032606124878\n",
      "Step: 15700  \tValid loss: 0.26821649074554443\n",
      "Step: 15800  \tTraining loss: 0.25456586480140686\n",
      "Step: 15800  \tTraining accuracy: 0.8810489177703857\n",
      "Step: 15800  \tValid loss: 0.26803314685821533\n",
      "Step: 15900  \tTraining loss: 0.25441569089889526\n",
      "Step: 15900  \tTraining accuracy: 0.8810939788818359\n",
      "Step: 15900  \tValid loss: 0.26784181594848633\n",
      "Step: 16000  \tTraining loss: 0.25426915287971497\n",
      "Step: 16000  \tTraining accuracy: 0.8811365365982056\n",
      "Step: 16000  \tValid loss: 0.26766660809516907\n",
      "Step: 16100  \tTraining loss: 0.2541269063949585\n",
      "Step: 16100  \tTraining accuracy: 0.8811752796173096\n",
      "Step: 16100  \tValid loss: 0.2674931287765503\n",
      "Step: 16200  \tTraining loss: 0.2539859712123871\n",
      "Step: 16200  \tTraining accuracy: 0.88121098279953\n",
      "Step: 16200  \tValid loss: 0.26732054352760315\n",
      "Step: 16300  \tTraining loss: 0.2538450360298157\n",
      "Step: 16300  \tTraining accuracy: 0.8812474608421326\n",
      "Step: 16300  \tValid loss: 0.26715144515037537\n",
      "Step: 16400  \tTraining loss: 0.25370460748672485\n",
      "Step: 16400  \tTraining accuracy: 0.8812835812568665\n",
      "Step: 16400  \tValid loss: 0.26697659492492676\n",
      "Step: 16500  \tTraining loss: 0.2535668909549713\n",
      "Step: 16500  \tTraining accuracy: 0.8813192248344421\n",
      "Step: 16500  \tValid loss: 0.2668074369430542\n",
      "Step: 16600  \tTraining loss: 0.2534300982952118\n",
      "Step: 16600  \tTraining accuracy: 0.881356954574585\n",
      "Step: 16600  \tValid loss: 0.2666381001472473\n",
      "Step: 16700  \tTraining loss: 0.25329530239105225\n",
      "Step: 16700  \tTraining accuracy: 0.8813948631286621\n",
      "Step: 16700  \tValid loss: 0.2664716839790344\n",
      "Step: 16800  \tTraining loss: 0.25316429138183594\n",
      "Step: 16800  \tTraining accuracy: 0.8814323544502258\n",
      "Step: 16800  \tValid loss: 0.26630955934524536\n",
      "Step: 16900  \tTraining loss: 0.2530321776866913\n",
      "Step: 16900  \tTraining accuracy: 0.8814693689346313\n",
      "Step: 16900  \tValid loss: 0.26614266633987427\n",
      "Step: 17000  \tTraining loss: 0.2529042661190033\n",
      "Step: 17000  \tTraining accuracy: 0.8815065622329712\n",
      "Step: 17000  \tValid loss: 0.26598113775253296\n",
      "Step: 17100  \tTraining loss: 0.2527753412723541\n",
      "Step: 17100  \tTraining accuracy: 0.8815426826477051\n",
      "Step: 17100  \tValid loss: 0.2658204138278961\n",
      "Step: 17200  \tTraining loss: 0.2526487708091736\n",
      "Step: 17200  \tTraining accuracy: 0.8815784454345703\n",
      "Step: 17200  \tValid loss: 0.26565876603126526\n",
      "Step: 17300  \tTraining loss: 0.25253209471702576\n",
      "Step: 17300  \tTraining accuracy: 0.8816161751747131\n",
      "Step: 17300  \tValid loss: 0.26552891731262207\n",
      "Step: 17400  \tTraining loss: 0.2524186372756958\n",
      "Step: 17400  \tTraining accuracy: 0.8816558718681335\n",
      "Step: 17400  \tValid loss: 0.2654058635234833\n",
      "Step: 17500  \tTraining loss: 0.25229522585868835\n",
      "Step: 17500  \tTraining accuracy: 0.881696343421936\n",
      "Step: 17500  \tValid loss: 0.26531967520713806\n",
      "Step: 17600  \tTraining loss: 0.25217074155807495\n",
      "Step: 17600  \tTraining accuracy: 0.8817363381385803\n",
      "Step: 17600  \tValid loss: 0.26525676250457764\n",
      "Step: 17700  \tTraining loss: 0.25205644965171814\n",
      "Step: 17700  \tTraining accuracy: 0.8817746639251709\n",
      "Step: 17700  \tValid loss: 0.26515212655067444\n",
      "Step: 17800  \tTraining loss: 0.25194546580314636\n",
      "Step: 17800  \tTraining accuracy: 0.8818108439445496\n",
      "Step: 17800  \tValid loss: 0.2650415003299713\n",
      "Step: 17900  \tTraining loss: 0.25183823704719543\n",
      "Step: 17900  \tTraining accuracy: 0.8818471431732178\n",
      "Step: 17900  \tValid loss: 0.2649361491203308\n",
      "Step: 18000  \tTraining loss: 0.25173133611679077\n",
      "Step: 18000  \tTraining accuracy: 0.8818854093551636\n",
      "Step: 18000  \tValid loss: 0.2648303508758545\n",
      "Step: 18100  \tTraining loss: 0.25162607431411743\n",
      "Step: 18100  \tTraining accuracy: 0.881923258304596\n",
      "Step: 18100  \tValid loss: 0.26471859216690063\n",
      "Step: 18200  \tTraining loss: 0.25152382254600525\n",
      "Step: 18200  \tTraining accuracy: 0.8819606900215149\n",
      "Step: 18200  \tValid loss: 0.2646147608757019\n",
      "Step: 18300  \tTraining loss: 0.2514234185218811\n",
      "Step: 18300  \tTraining accuracy: 0.8819977045059204\n",
      "Step: 18300  \tValid loss: 0.26450884342193604\n",
      "Step: 18400  \tTraining loss: 0.25132495164871216\n",
      "Step: 18400  \tTraining accuracy: 0.8820343017578125\n",
      "Step: 18400  \tValid loss: 0.2644073963165283\n",
      "Step: 18500  \tTraining loss: 0.2512269914150238\n",
      "Step: 18500  \tTraining accuracy: 0.8820704817771912\n",
      "Step: 18500  \tValid loss: 0.26430508494377136\n",
      "Step: 18600  \tTraining loss: 0.2511318624019623\n",
      "Step: 18600  \tTraining accuracy: 0.8821063041687012\n",
      "Step: 18600  \tValid loss: 0.26421573758125305\n",
      "Step: 18700  \tTraining loss: 0.2510358691215515\n",
      "Step: 18700  \tTraining accuracy: 0.8821417689323425\n",
      "Step: 18700  \tValid loss: 0.2641352415084839\n",
      "Step: 18800  \tTraining loss: 0.2509424686431885\n",
      "Step: 18800  \tTraining accuracy: 0.8821768164634705\n",
      "Step: 18800  \tValid loss: 0.26402905583381653\n",
      "Step: 18900  \tTraining loss: 0.250851035118103\n",
      "Step: 18900  \tTraining accuracy: 0.8822115063667297\n",
      "Step: 18900  \tValid loss: 0.2639361321926117\n",
      "Step: 19000  \tTraining loss: 0.2507595121860504\n",
      "Step: 19000  \tTraining accuracy: 0.8822457790374756\n",
      "Step: 19000  \tValid loss: 0.26385563611984253\n",
      "Step: 19100  \tTraining loss: 0.25066572427749634\n",
      "Step: 19100  \tTraining accuracy: 0.8822797536849976\n",
      "Step: 19100  \tValid loss: 0.2637637257575989\n",
      "Step: 19200  \tTraining loss: 0.2505796253681183\n",
      "Step: 19200  \tTraining accuracy: 0.8823133707046509\n",
      "Step: 19200  \tValid loss: 0.26366737484931946\n",
      "Step: 19300  \tTraining loss: 0.2504945993423462\n",
      "Step: 19300  \tTraining accuracy: 0.8823466300964355\n",
      "Step: 19300  \tValid loss: 0.26357966661453247\n",
      "Step: 19400  \tTraining loss: 0.2504121959209442\n",
      "Step: 19400  \tTraining accuracy: 0.8823811411857605\n",
      "Step: 19400  \tValid loss: 0.2634882628917694\n",
      "Step: 19500  \tTraining loss: 0.25033000111579895\n",
      "Step: 19500  \tTraining accuracy: 0.8824164271354675\n",
      "Step: 19500  \tValid loss: 0.26340654492378235\n",
      "Step: 19600  \tTraining loss: 0.2502480745315552\n",
      "Step: 19600  \tTraining accuracy: 0.8824529051780701\n",
      "Step: 19600  \tValid loss: 0.2633204460144043\n",
      "Step: 19700  \tTraining loss: 0.2501690983772278\n",
      "Step: 19700  \tTraining accuracy: 0.8824900984764099\n",
      "Step: 19700  \tValid loss: 0.26324474811553955\n",
      "Step: 19800  \tTraining loss: 0.2500884532928467\n",
      "Step: 19800  \tTraining accuracy: 0.8825268745422363\n",
      "Step: 19800  \tValid loss: 0.26317036151885986\n",
      "Step: 19900  \tTraining loss: 0.25000834465026855\n",
      "Step: 19900  \tTraining accuracy: 0.8825632929801941\n",
      "Step: 19900  \tValid loss: 0.2631029188632965\n",
      "Step: 20000  \tTraining loss: 0.24993014335632324\n",
      "Step: 20000  \tTraining accuracy: 0.882599413394928\n",
      "Step: 20000  \tValid loss: 0.26303207874298096\n",
      "Step: 20100  \tTraining loss: 0.2498522400856018\n",
      "Step: 20100  \tTraining accuracy: 0.8826372027397156\n",
      "Step: 20100  \tValid loss: 0.2629629075527191\n",
      "Step: 20200  \tTraining loss: 0.2497778981924057\n",
      "Step: 20200  \tTraining accuracy: 0.8826751112937927\n",
      "Step: 20200  \tValid loss: 0.26289626955986023\n",
      "Step: 20300  \tTraining loss: 0.24970290064811707\n",
      "Step: 20300  \tTraining accuracy: 0.882712721824646\n",
      "Step: 20300  \tValid loss: 0.26283392310142517\n",
      "Step: 20400  \tTraining loss: 0.249628946185112\n",
      "Step: 20400  \tTraining accuracy: 0.8827499151229858\n",
      "Step: 20400  \tValid loss: 0.2627691924571991\n",
      "Step: 20500  \tTraining loss: 0.2495557963848114\n",
      "Step: 20500  \tTraining accuracy: 0.882786750793457\n",
      "Step: 20500  \tValid loss: 0.2627110481262207\n",
      "Step: 20600  \tTraining loss: 0.24948322772979736\n",
      "Step: 20600  \tTraining accuracy: 0.8828231692314148\n",
      "Step: 20600  \tValid loss: 0.26264920830726624\n",
      "Step: 20700  \tTraining loss: 0.2494092732667923\n",
      "Step: 20700  \tTraining accuracy: 0.8828592896461487\n",
      "Step: 20700  \tValid loss: 0.2625877261161804\n",
      "Step: 20800  \tTraining loss: 0.24933691322803497\n",
      "Step: 20800  \tTraining accuracy: 0.8828951120376587\n",
      "Step: 20800  \tValid loss: 0.2625396251678467\n",
      "Step: 20900  \tTraining loss: 0.24926245212554932\n",
      "Step: 20900  \tTraining accuracy: 0.8829305171966553\n",
      "Step: 20900  \tValid loss: 0.2624834179878235\n",
      "Step: 21000  \tTraining loss: 0.24918729066848755\n",
      "Step: 21000  \tTraining accuracy: 0.882965624332428\n",
      "Step: 21000  \tValid loss: 0.26242688298225403\n",
      "Step: 21100  \tTraining loss: 0.24911601841449738\n",
      "Step: 21100  \tTraining accuracy: 0.883000373840332\n",
      "Step: 21100  \tValid loss: 0.2623731195926666\n",
      "Step: 21200  \tTraining loss: 0.24904698133468628\n",
      "Step: 21200  \tTraining accuracy: 0.883033812046051\n",
      "Step: 21200  \tValid loss: 0.2623155117034912\n",
      "Step: 21300  \tTraining loss: 0.24898073077201843\n",
      "Step: 21300  \tTraining accuracy: 0.8830679059028625\n",
      "Step: 21300  \tValid loss: 0.2622598707675934\n",
      "Step: 21400  \tTraining loss: 0.24891312420368195\n",
      "Step: 21400  \tTraining accuracy: 0.8831031918525696\n",
      "Step: 21400  \tValid loss: 0.2621985375881195\n",
      "Step: 21500  \tTraining loss: 0.24884629249572754\n",
      "Step: 21500  \tTraining accuracy: 0.8831390738487244\n",
      "Step: 21500  \tValid loss: 0.26213476061820984\n",
      "Step: 21600  \tTraining loss: 0.2487819939851761\n",
      "Step: 21600  \tTraining accuracy: 0.8831746578216553\n",
      "Step: 21600  \tValid loss: 0.2620680034160614\n",
      "Step: 21700  \tTraining loss: 0.24871644377708435\n",
      "Step: 21700  \tTraining accuracy: 0.8832098841667175\n",
      "Step: 21700  \tValid loss: 0.26199862360954285\n",
      "Step: 21800  \tTraining loss: 0.2486533671617508\n",
      "Step: 21800  \tTraining accuracy: 0.8832448124885559\n",
      "Step: 21800  \tValid loss: 0.26193150877952576\n",
      "Step: 21900  \tTraining loss: 0.24858775734901428\n",
      "Step: 21900  \tTraining accuracy: 0.8832793831825256\n",
      "Step: 21900  \tValid loss: 0.26186880469322205\n",
      "Step: 22000  \tTraining loss: 0.24852395057678223\n",
      "Step: 22000  \tTraining accuracy: 0.8833151459693909\n",
      "Step: 22000  \tValid loss: 0.26180386543273926\n",
      "Step: 22100  \tTraining loss: 0.2484610229730606\n",
      "Step: 22100  \tTraining accuracy: 0.8833514451980591\n",
      "Step: 22100  \tValid loss: 0.26174601912498474\n",
      "Step: 22200  \tTraining loss: 0.248396635055542\n",
      "Step: 22200  \tTraining accuracy: 0.883388876914978\n",
      "Step: 22200  \tValid loss: 0.2616806626319885\n",
      "Step: 22300  \tTraining loss: 0.2483324408531189\n",
      "Step: 22300  \tTraining accuracy: 0.8834269046783447\n",
      "Step: 22300  \tValid loss: 0.26162734627723694\n",
      "Step: 22400  \tTraining loss: 0.24826915562152863\n",
      "Step: 22400  \tTraining accuracy: 0.8834645748138428\n",
      "Step: 22400  \tValid loss: 0.26155441999435425\n",
      "Step: 22500  \tTraining loss: 0.24820734560489655\n",
      "Step: 22500  \tTraining accuracy: 0.8835019469261169\n",
      "Step: 22500  \tValid loss: 0.2614956796169281\n",
      "Step: 22600  \tTraining loss: 0.24815014004707336\n",
      "Step: 22600  \tTraining accuracy: 0.8835399150848389\n",
      "Step: 22600  \tValid loss: 0.26144301891326904\n",
      "Step: 22700  \tTraining loss: 0.24809424579143524\n",
      "Step: 22700  \tTraining accuracy: 0.8835779428482056\n",
      "Step: 22700  \tValid loss: 0.2613829970359802\n",
      "Step: 22800  \tTraining loss: 0.2480398565530777\n",
      "Step: 22800  \tTraining accuracy: 0.883615255355835\n",
      "Step: 22800  \tValid loss: 0.261324405670166\n",
      "Step: 22900  \tTraining loss: 0.24797965586185455\n",
      "Step: 22900  \tTraining accuracy: 0.8836535811424255\n",
      "Step: 22900  \tValid loss: 0.2612655758857727\n",
      "Step: 23000  \tTraining loss: 0.24792508780956268\n",
      "Step: 23000  \tTraining accuracy: 0.8836915493011475\n",
      "Step: 23000  \tValid loss: 0.2611983120441437\n",
      "Step: 23100  \tTraining loss: 0.2478693127632141\n",
      "Step: 23100  \tTraining accuracy: 0.8837273716926575\n",
      "Step: 23100  \tValid loss: 0.2611441910266876\n",
      "Step: 23200  \tTraining loss: 0.24781297147274017\n",
      "Step: 23200  \tTraining accuracy: 0.8837611079216003\n",
      "Step: 23200  \tValid loss: 0.2610797882080078\n",
      "Step: 23300  \tTraining loss: 0.24776124954223633\n",
      "Step: 23300  \tTraining accuracy: 0.8837950229644775\n",
      "Step: 23300  \tValid loss: 0.26102522015571594\n",
      "Step: 23400  \tTraining loss: 0.24770867824554443\n",
      "Step: 23400  \tTraining accuracy: 0.8838263750076294\n",
      "Step: 23400  \tValid loss: 0.2609651982784271\n",
      "Step: 23500  \tTraining loss: 0.24765267968177795\n",
      "Step: 23500  \tTraining accuracy: 0.8838561177253723\n",
      "Step: 23500  \tValid loss: 0.26090243458747864\n",
      "Step: 23600  \tTraining loss: 0.2476007342338562\n",
      "Step: 23600  \tTraining accuracy: 0.8838865160942078\n",
      "Step: 23600  \tValid loss: 0.26084399223327637\n",
      "Step: 23700  \tTraining loss: 0.24754716455936432\n",
      "Step: 23700  \tTraining accuracy: 0.8839179873466492\n",
      "Step: 23700  \tValid loss: 0.2607874572277069\n",
      "Step: 23800  \tTraining loss: 0.24749331176280975\n",
      "Step: 23800  \tTraining accuracy: 0.8839483261108398\n",
      "Step: 23800  \tValid loss: 0.26072657108306885\n",
      "Step: 23900  \tTraining loss: 0.24744053184986115\n",
      "Step: 23900  \tTraining accuracy: 0.8839770555496216\n",
      "Step: 23900  \tValid loss: 0.260667085647583\n",
      "Step: 24000  \tTraining loss: 0.24738669395446777\n",
      "Step: 24000  \tTraining accuracy: 0.8840055465698242\n",
      "Step: 24000  \tValid loss: 0.26060912013053894\n",
      "Step: 24100  \tTraining loss: 0.24733799695968628\n",
      "Step: 24100  \tTraining accuracy: 0.8840337991714478\n",
      "Step: 24100  \tValid loss: 0.26055341958999634\n",
      "Step: 24200  \tTraining loss: 0.247286856174469\n",
      "Step: 24200  \tTraining accuracy: 0.884061872959137\n",
      "Step: 24200  \tValid loss: 0.26049813628196716\n",
      "Step: 24300  \tTraining loss: 0.2472321093082428\n",
      "Step: 24300  \tTraining accuracy: 0.8840896487236023\n",
      "Step: 24300  \tValid loss: 0.2604389488697052\n",
      "Step: 24400  \tTraining loss: 0.2471812665462494\n",
      "Step: 24400  \tTraining accuracy: 0.8841172456741333\n",
      "Step: 24400  \tValid loss: 0.26038527488708496\n",
      "Step: 24500  \tTraining loss: 0.24713069200515747\n",
      "Step: 24500  \tTraining accuracy: 0.8841446042060852\n",
      "Step: 24500  \tValid loss: 0.260331928730011\n",
      "Step: 24600  \tTraining loss: 0.2470809817314148\n",
      "Step: 24600  \tTraining accuracy: 0.884171724319458\n",
      "Step: 24600  \tValid loss: 0.26027536392211914\n",
      "Step: 24700  \tTraining loss: 0.24703165888786316\n",
      "Step: 24700  \tTraining accuracy: 0.8841986060142517\n",
      "Step: 24700  \tValid loss: 0.26022252440452576\n",
      "Step: 24800  \tTraining loss: 0.24697938561439514\n",
      "Step: 24800  \tTraining accuracy: 0.8842253088951111\n",
      "Step: 24800  \tValid loss: 0.26016706228256226\n",
      "Step: 24900  \tTraining loss: 0.24693118035793304\n",
      "Step: 24900  \tTraining accuracy: 0.8842530846595764\n",
      "Step: 24900  \tValid loss: 0.2601175904273987\n",
      "Step: 25000  \tTraining loss: 0.2468823790550232\n",
      "Step: 25000  \tTraining accuracy: 0.8842813968658447\n",
      "Step: 25000  \tValid loss: 0.26006561517715454\n",
      "Step: 25100  \tTraining loss: 0.24683313071727753\n",
      "Step: 25100  \tTraining accuracy: 0.8843095302581787\n",
      "Step: 25100  \tValid loss: 0.26000523567199707\n",
      "Step: 25200  \tTraining loss: 0.24678173661231995\n",
      "Step: 25200  \tTraining accuracy: 0.8843374252319336\n",
      "Step: 25200  \tValid loss: 0.2599521577358246\n",
      "Step: 25300  \tTraining loss: 0.24673351645469666\n",
      "Step: 25300  \tTraining accuracy: 0.8843651413917542\n",
      "Step: 25300  \tValid loss: 0.2599068582057953\n",
      "Step: 25400  \tTraining loss: 0.24668535590171814\n",
      "Step: 25400  \tTraining accuracy: 0.8843926191329956\n",
      "Step: 25400  \tValid loss: 0.2598450183868408\n",
      "Step: 25500  \tTraining loss: 0.24663831293582916\n",
      "Step: 25500  \tTraining accuracy: 0.884419858455658\n",
      "Step: 25500  \tValid loss: 0.2598054111003876\n",
      "Step: 25600  \tTraining loss: 0.24658697843551636\n",
      "Step: 25600  \tTraining accuracy: 0.8844468593597412\n",
      "Step: 25600  \tValid loss: 0.25974124670028687\n",
      "Step: 25700  \tTraining loss: 0.2465379536151886\n",
      "Step: 25700  \tTraining accuracy: 0.8844736814498901\n",
      "Step: 25700  \tValid loss: 0.25968262553215027\n",
      "Step: 25800  \tTraining loss: 0.2464887946844101\n",
      "Step: 25800  \tTraining accuracy: 0.8845003247261047\n",
      "Step: 25800  \tValid loss: 0.25962594151496887\n",
      "Step: 25900  \tTraining loss: 0.24644121527671814\n",
      "Step: 25900  \tTraining accuracy: 0.8845267295837402\n",
      "Step: 25900  \tValid loss: 0.2595769166946411\n",
      "Step: 26000  \tTraining loss: 0.2463892698287964\n",
      "Step: 26000  \tTraining accuracy: 0.8845528960227966\n",
      "Step: 26000  \tValid loss: 0.2595199942588806\n",
      "Step: 26100  \tTraining loss: 0.2463381439447403\n",
      "Step: 26100  \tTraining accuracy: 0.8845789432525635\n",
      "Step: 26100  \tValid loss: 0.25947028398513794\n",
      "Step: 26200  \tTraining loss: 0.24627898633480072\n",
      "Step: 26200  \tTraining accuracy: 0.8846047520637512\n",
      "Step: 26200  \tValid loss: 0.25941580533981323\n",
      "Step: 26300  \tTraining loss: 0.24621808528900146\n",
      "Step: 26300  \tTraining accuracy: 0.8846303224563599\n",
      "Step: 26300  \tValid loss: 0.2593371868133545\n",
      "Step: 26400  \tTraining loss: 0.24616767466068268\n",
      "Step: 26400  \tTraining accuracy: 0.884655773639679\n",
      "Step: 26400  \tValid loss: 0.2592889368534088\n",
      "Step: 26500  \tTraining loss: 0.24611617624759674\n",
      "Step: 26500  \tTraining accuracy: 0.884680986404419\n",
      "Step: 26500  \tValid loss: 0.2592433989048004\n",
      "Step: 26600  \tTraining loss: 0.2460654377937317\n",
      "Step: 26600  \tTraining accuracy: 0.8847060203552246\n",
      "Step: 26600  \tValid loss: 0.25920069217681885\n",
      "Step: 26700  \tTraining loss: 0.24601471424102783\n",
      "Step: 26700  \tTraining accuracy: 0.884730875492096\n",
      "Step: 26700  \tValid loss: 0.2591635584831238\n",
      "Step: 26800  \tTraining loss: 0.24596139788627625\n",
      "Step: 26800  \tTraining accuracy: 0.8847554922103882\n",
      "Step: 26800  \tValid loss: 0.2591157853603363\n",
      "Step: 26900  \tTraining loss: 0.24591420590877533\n",
      "Step: 26900  \tTraining accuracy: 0.8847799897193909\n",
      "Step: 26900  \tValid loss: 0.25907468795776367\n",
      "Step: 27000  \tTraining loss: 0.24586522579193115\n",
      "Step: 27000  \tTraining accuracy: 0.8848035335540771\n",
      "Step: 27000  \tValid loss: 0.25901898741722107\n",
      "Step: 27100  \tTraining loss: 0.24581879377365112\n",
      "Step: 27100  \tTraining accuracy: 0.8848257064819336\n",
      "Step: 27100  \tValid loss: 0.25898057222366333\n",
      "Step: 27200  \tTraining loss: 0.2457721084356308\n",
      "Step: 27200  \tTraining accuracy: 0.8848477005958557\n",
      "Step: 27200  \tValid loss: 0.2589336633682251\n",
      "Step: 27300  \tTraining loss: 0.2457275092601776\n",
      "Step: 27300  \tTraining accuracy: 0.8848695755004883\n",
      "Step: 27300  \tValid loss: 0.25889864563941956\n",
      "Step: 27400  \tTraining loss: 0.24568231403827667\n",
      "Step: 27400  \tTraining accuracy: 0.8848912715911865\n",
      "Step: 27400  \tValid loss: 0.25885647535324097\n",
      "Step: 27500  \tTraining loss: 0.24563686549663544\n",
      "Step: 27500  \tTraining accuracy: 0.8849127888679504\n",
      "Step: 27500  \tValid loss: 0.2588193416595459\n",
      "Step: 27600  \tTraining loss: 0.24558934569358826\n",
      "Step: 27600  \tTraining accuracy: 0.8849341869354248\n",
      "Step: 27600  \tValid loss: 0.25876784324645996\n",
      "Step: 27700  \tTraining loss: 0.24554403126239777\n",
      "Step: 27700  \tTraining accuracy: 0.8849554061889648\n",
      "Step: 27700  \tValid loss: 0.2587308883666992\n",
      "Step: 27800  \tTraining loss: 0.2454971969127655\n",
      "Step: 27800  \tTraining accuracy: 0.8849757313728333\n",
      "Step: 27800  \tValid loss: 0.25868672132492065\n",
      "Step: 27900  \tTraining loss: 0.24544642865657806\n",
      "Step: 27900  \tTraining accuracy: 0.8849948048591614\n",
      "Step: 27900  \tValid loss: 0.2586565315723419\n",
      "Step: 28000  \tTraining loss: 0.24539658427238464\n",
      "Step: 28000  \tTraining accuracy: 0.8850137591362\n",
      "Step: 28000  \tValid loss: 0.2586091160774231\n",
      "Step: 28100  \tTraining loss: 0.24535056948661804\n",
      "Step: 28100  \tTraining accuracy: 0.8850325345993042\n",
      "Step: 28100  \tValid loss: 0.25855088233947754\n",
      "Step: 28200  \tTraining loss: 0.245303213596344\n",
      "Step: 28200  \tTraining accuracy: 0.8850511312484741\n",
      "Step: 28200  \tValid loss: 0.25849395990371704\n",
      "Step: 28300  \tTraining loss: 0.24525997042655945\n",
      "Step: 28300  \tTraining accuracy: 0.8850696682929993\n",
      "Step: 28300  \tValid loss: 0.2584426999092102\n",
      "Step: 28400  \tTraining loss: 0.2452147901058197\n",
      "Step: 28400  \tTraining accuracy: 0.8850880861282349\n",
      "Step: 28400  \tValid loss: 0.25838443636894226\n",
      "Step: 28500  \tTraining loss: 0.2451712191104889\n",
      "Step: 28500  \tTraining accuracy: 0.8851063251495361\n",
      "Step: 28500  \tValid loss: 0.258331298828125\n",
      "Step: 28600  \tTraining loss: 0.24512653052806854\n",
      "Step: 28600  \tTraining accuracy: 0.8851262927055359\n",
      "Step: 28600  \tValid loss: 0.25827160477638245\n",
      "Step: 28700  \tTraining loss: 0.24509117007255554\n",
      "Step: 28700  \tTraining accuracy: 0.8851460814476013\n",
      "Step: 28700  \tValid loss: 0.2582169473171234\n",
      "Step: 28800  \tTraining loss: 0.24504335224628448\n",
      "Step: 28800  \tTraining accuracy: 0.8851657509803772\n",
      "Step: 28800  \tValid loss: 0.2581706643104553\n",
      "Step: 28900  \tTraining loss: 0.24500198662281036\n",
      "Step: 28900  \tTraining accuracy: 0.8851860761642456\n",
      "Step: 28900  \tValid loss: 0.25811144709587097\n",
      "Step: 29000  \tTraining loss: 0.24495917558670044\n",
      "Step: 29000  \tTraining accuracy: 0.8852065205574036\n",
      "Step: 29000  \tValid loss: 0.2580583393573761\n",
      "Step: 29100  \tTraining loss: 0.2449069321155548\n",
      "Step: 29100  \tTraining accuracy: 0.8852272629737854\n",
      "Step: 29100  \tValid loss: 0.2581039071083069\n",
      "Step: 29200  \tTraining loss: 0.2448483258485794\n",
      "Step: 29200  \tTraining accuracy: 0.8852481842041016\n",
      "Step: 29200  \tValid loss: 0.25813207030296326\n",
      "Step: 29300  \tTraining loss: 0.24478386342525482\n",
      "Step: 29300  \tTraining accuracy: 0.8852689862251282\n",
      "Step: 29300  \tValid loss: 0.258147269487381\n",
      "Step: 29400  \tTraining loss: 0.24472299218177795\n",
      "Step: 29400  \tTraining accuracy: 0.8852896094322205\n",
      "Step: 29400  \tValid loss: 0.2580910921096802\n",
      "Step: 29500  \tTraining loss: 0.2446761578321457\n",
      "Step: 29500  \tTraining accuracy: 0.8853108286857605\n",
      "Step: 29500  \tValid loss: 0.25801151990890503\n",
      "Step: 29600  \tTraining loss: 0.24462753534317017\n",
      "Step: 29600  \tTraining accuracy: 0.8853329420089722\n",
      "Step: 29600  \tValid loss: 0.2579175531864166\n",
      "Step: 29700  \tTraining loss: 0.24458125233650208\n",
      "Step: 29700  \tTraining accuracy: 0.8853549361228943\n",
      "Step: 29700  \tValid loss: 0.2578562796115875\n",
      "Step: 29800  \tTraining loss: 0.24453754723072052\n",
      "Step: 29800  \tTraining accuracy: 0.8853767514228821\n",
      "Step: 29800  \tValid loss: 0.2577863037586212\n",
      "Step: 29900  \tTraining loss: 0.2444913536310196\n",
      "Step: 29900  \tTraining accuracy: 0.885397732257843\n",
      "Step: 29900  \tValid loss: 0.25773027539253235\n",
      "Step: 30000  \tTraining loss: 0.24444718658924103\n",
      "Step: 30000  \tTraining accuracy: 0.8854175209999084\n",
      "Step: 30000  \tValid loss: 0.25766974687576294\n",
      "Step: 30100  \tTraining loss: 0.2444036900997162\n",
      "Step: 30100  \tTraining accuracy: 0.8854379057884216\n",
      "Step: 30100  \tValid loss: 0.257612019777298\n",
      "Step: 30200  \tTraining loss: 0.24435925483703613\n",
      "Step: 30200  \tTraining accuracy: 0.8854591846466064\n",
      "Step: 30200  \tValid loss: 0.2575591504573822\n",
      "Step: 30300  \tTraining loss: 0.24432019889354706\n",
      "Step: 30300  \tTraining accuracy: 0.8854802846908569\n",
      "Step: 30300  \tValid loss: 0.257507860660553\n",
      "Step: 30400  \tTraining loss: 0.24427425861358643\n",
      "Step: 30400  \tTraining accuracy: 0.8855012655258179\n",
      "Step: 30400  \tValid loss: 0.25745657086372375\n",
      "Step: 30500  \tTraining loss: 0.24423041939735413\n",
      "Step: 30500  \tTraining accuracy: 0.8855221271514893\n",
      "Step: 30500  \tValid loss: 0.25738993287086487\n",
      "Step: 30600  \tTraining loss: 0.24419257044792175\n",
      "Step: 30600  \tTraining accuracy: 0.8855428099632263\n",
      "Step: 30600  \tValid loss: 0.2573476731777191\n",
      "Step: 30700  \tTraining loss: 0.24414511024951935\n",
      "Step: 30700  \tTraining accuracy: 0.8855637311935425\n",
      "Step: 30700  \tValid loss: 0.2572929263114929\n",
      "Step: 30800  \tTraining loss: 0.2441035509109497\n",
      "Step: 30800  \tTraining accuracy: 0.8855859041213989\n",
      "Step: 30800  \tValid loss: 0.257245808839798\n",
      "Step: 30900  \tTraining loss: 0.24406138062477112\n",
      "Step: 30900  \tTraining accuracy: 0.885607898235321\n",
      "Step: 30900  \tValid loss: 0.25718531012535095\n",
      "Step: 31000  \tTraining loss: 0.24401801824569702\n",
      "Step: 31000  \tTraining accuracy: 0.8856304287910461\n",
      "Step: 31000  \tValid loss: 0.25713828206062317\n",
      "Step: 31100  \tTraining loss: 0.2439766377210617\n",
      "Step: 31100  \tTraining accuracy: 0.8856537938117981\n",
      "Step: 31100  \tValid loss: 0.25708651542663574\n",
      "Step: 31200  \tTraining loss: 0.243935689330101\n",
      "Step: 31200  \tTraining accuracy: 0.8856770396232605\n",
      "Step: 31200  \tValid loss: 0.25703734159469604\n",
      "Step: 31300  \tTraining loss: 0.24389469623565674\n",
      "Step: 31300  \tTraining accuracy: 0.8857001662254333\n",
      "Step: 31300  \tValid loss: 0.2569836676120758\n",
      "Step: 31400  \tTraining loss: 0.2438490390777588\n",
      "Step: 31400  \tTraining accuracy: 0.8857231140136719\n",
      "Step: 31400  \tValid loss: 0.25693845748901367\n",
      "Step: 31500  \tTraining loss: 0.24380387365818024\n",
      "Step: 31500  \tTraining accuracy: 0.8857458829879761\n",
      "Step: 31500  \tValid loss: 0.2568906843662262\n",
      "Step: 31600  \tTraining loss: 0.2437574565410614\n",
      "Step: 31600  \tTraining accuracy: 0.8857685327529907\n",
      "Step: 31600  \tValid loss: 0.2568380534648895\n",
      "Step: 31700  \tTraining loss: 0.24371053278446198\n",
      "Step: 31700  \tTraining accuracy: 0.8857910633087158\n",
      "Step: 31700  \tValid loss: 0.2567821145057678\n",
      "Step: 31800  \tTraining loss: 0.24366745352745056\n",
      "Step: 31800  \tTraining accuracy: 0.8858127593994141\n",
      "Step: 31800  \tValid loss: 0.256735235452652\n",
      "Step: 31900  \tTraining loss: 0.24362386763095856\n",
      "Step: 31900  \tTraining accuracy: 0.8858327269554138\n",
      "Step: 31900  \tValid loss: 0.2566843330860138\n",
      "Step: 32000  \tTraining loss: 0.24357552826404572\n",
      "Step: 32000  \tTraining accuracy: 0.8858515620231628\n",
      "Step: 32000  \tValid loss: 0.2566390633583069\n",
      "Step: 32100  \tTraining loss: 0.24352869391441345\n",
      "Step: 32100  \tTraining accuracy: 0.8858702778816223\n",
      "Step: 32100  \tValid loss: 0.2565944194793701\n",
      "Step: 32200  \tTraining loss: 0.24348126351833344\n",
      "Step: 32200  \tTraining accuracy: 0.8858895301818848\n",
      "Step: 32200  \tValid loss: 0.25654804706573486\n",
      "Step: 32300  \tTraining loss: 0.24343422055244446\n",
      "Step: 32300  \tTraining accuracy: 0.8859096169471741\n",
      "Step: 32300  \tValid loss: 0.25650250911712646\n",
      "Step: 32400  \tTraining loss: 0.24338868260383606\n",
      "Step: 32400  \tTraining accuracy: 0.8859295845031738\n",
      "Step: 32400  \tValid loss: 0.25645703077316284\n",
      "Step: 32500  \tTraining loss: 0.2433481514453888\n",
      "Step: 32500  \tTraining accuracy: 0.885949432849884\n",
      "Step: 32500  \tValid loss: 0.25643956661224365\n",
      "Step: 32600  \tTraining loss: 0.24330534040927887\n",
      "Step: 32600  \tTraining accuracy: 0.8859691619873047\n",
      "Step: 32600  \tValid loss: 0.2564227879047394\n",
      "Step: 32700  \tTraining loss: 0.2432648241519928\n",
      "Step: 32700  \tTraining accuracy: 0.8859887719154358\n",
      "Step: 32700  \tValid loss: 0.2563987076282501\n",
      "Step: 32800  \tTraining loss: 0.243222177028656\n",
      "Step: 32800  \tTraining accuracy: 0.8860082626342773\n",
      "Step: 32800  \tValid loss: 0.2563599646091461\n",
      "Step: 32900  \tTraining loss: 0.24318166077136993\n",
      "Step: 32900  \tTraining accuracy: 0.8860276341438293\n",
      "Step: 32900  \tValid loss: 0.25632244348526\n",
      "Step: 33000  \tTraining loss: 0.24314329028129578\n",
      "Step: 33000  \tTraining accuracy: 0.8860468864440918\n",
      "Step: 33000  \tValid loss: 0.2562941908836365\n",
      "Step: 33100  \tTraining loss: 0.24310217797756195\n",
      "Step: 33100  \tTraining accuracy: 0.8860660195350647\n",
      "Step: 33100  \tValid loss: 0.2562515437602997\n",
      "Step: 33200  \tTraining loss: 0.2430626004934311\n",
      "Step: 33200  \tTraining accuracy: 0.8860850930213928\n",
      "Step: 33200  \tValid loss: 0.256206214427948\n",
      "Step: 33300  \tTraining loss: 0.2430230677127838\n",
      "Step: 33300  \tTraining accuracy: 0.8861039876937866\n",
      "Step: 33300  \tValid loss: 0.25617316365242004\n",
      "Step: 33400  \tTraining loss: 0.24298273026943207\n",
      "Step: 33400  \tTraining accuracy: 0.8861227631568909\n",
      "Step: 33400  \tValid loss: 0.25612470507621765\n",
      "Step: 33500  \tTraining loss: 0.24294379353523254\n",
      "Step: 33500  \tTraining accuracy: 0.8861414790153503\n",
      "Step: 33500  \tValid loss: 0.2560900151729584\n",
      "Step: 33600  \tTraining loss: 0.24290543794631958\n",
      "Step: 33600  \tTraining accuracy: 0.8861600160598755\n",
      "Step: 33600  \tValid loss: 0.2560414969921112\n",
      "Step: 33700  \tTraining loss: 0.24286584556102753\n",
      "Step: 33700  \tTraining accuracy: 0.8861784934997559\n",
      "Step: 33700  \tValid loss: 0.2560049891471863\n",
      "Step: 33800  \tTraining loss: 0.24282854795455933\n",
      "Step: 33800  \tTraining accuracy: 0.8861956000328064\n",
      "Step: 33800  \tValid loss: 0.25596415996551514\n",
      "Step: 33900  \tTraining loss: 0.24279017746448517\n",
      "Step: 33900  \tTraining accuracy: 0.8862135410308838\n",
      "Step: 33900  \tValid loss: 0.2559291124343872\n",
      "Step: 34000  \tTraining loss: 0.24275535345077515\n",
      "Step: 34000  \tTraining accuracy: 0.88623046875\n",
      "Step: 34000  \tValid loss: 0.255892276763916\n",
      "Step: 34100  \tTraining loss: 0.2427203208208084\n",
      "Step: 34100  \tTraining accuracy: 0.886246919631958\n",
      "Step: 34100  \tValid loss: 0.25584644079208374\n",
      "Step: 34200  \tTraining loss: 0.24268114566802979\n",
      "Step: 34200  \tTraining accuracy: 0.886263370513916\n",
      "Step: 34200  \tValid loss: 0.2558121681213379\n",
      "Step: 34300  \tTraining loss: 0.24264422059059143\n",
      "Step: 34300  \tTraining accuracy: 0.8862793445587158\n",
      "Step: 34300  \tValid loss: 0.25577887892723083\n",
      "Step: 34400  \tTraining loss: 0.24260841310024261\n",
      "Step: 34400  \tTraining accuracy: 0.8862955570220947\n",
      "Step: 34400  \tValid loss: 0.2557394206523895\n",
      "Step: 34500  \tTraining loss: 0.24257352948188782\n",
      "Step: 34500  \tTraining accuracy: 0.8863111138343811\n",
      "Step: 34500  \tValid loss: 0.2556930482387543\n",
      "Step: 34600  \tTraining loss: 0.24253705143928528\n",
      "Step: 34600  \tTraining accuracy: 0.8863261938095093\n",
      "Step: 34600  \tValid loss: 0.2556561231613159\n",
      "Step: 34700  \tTraining loss: 0.24250026047229767\n",
      "Step: 34700  \tTraining accuracy: 0.8863421678543091\n",
      "Step: 34700  \tValid loss: 0.25562700629234314\n",
      "Step: 34800  \tTraining loss: 0.24246317148208618\n",
      "Step: 34800  \tTraining accuracy: 0.8863574266433716\n",
      "Step: 34800  \tValid loss: 0.25558632612228394\n",
      "Step: 34900  \tTraining loss: 0.24242593348026276\n",
      "Step: 34900  \tTraining accuracy: 0.8863722681999207\n",
      "Step: 34900  \tValid loss: 0.2555568218231201\n",
      "Step: 35000  \tTraining loss: 0.24238541722297668\n",
      "Step: 35000  \tTraining accuracy: 0.8863879442214966\n",
      "Step: 35000  \tValid loss: 0.25552046298980713\n",
      "Step: 35100  \tTraining loss: 0.24234852194786072\n",
      "Step: 35100  \tTraining accuracy: 0.886403501033783\n",
      "Step: 35100  \tValid loss: 0.25548306107521057\n",
      "Step: 35200  \tTraining loss: 0.2423097938299179\n",
      "Step: 35200  \tTraining accuracy: 0.8864195942878723\n",
      "Step: 35200  \tValid loss: 0.25544798374176025\n",
      "Step: 35300  \tTraining loss: 0.24227258563041687\n",
      "Step: 35300  \tTraining accuracy: 0.886435866355896\n",
      "Step: 35300  \tValid loss: 0.25540730357170105\n",
      "Step: 35400  \tTraining loss: 0.24224041402339935\n",
      "Step: 35400  \tTraining accuracy: 0.8864511847496033\n",
      "Step: 35400  \tValid loss: 0.255361944437027\n",
      "Step: 35500  \tTraining loss: 0.24219878017902374\n",
      "Step: 35500  \tTraining accuracy: 0.8864664435386658\n",
      "Step: 35500  \tValid loss: 0.255329430103302\n",
      "Step: 35600  \tTraining loss: 0.24216359853744507\n",
      "Step: 35600  \tTraining accuracy: 0.8864815831184387\n",
      "Step: 35600  \tValid loss: 0.25528764724731445\n",
      "Step: 35700  \tTraining loss: 0.24212591350078583\n",
      "Step: 35700  \tTraining accuracy: 0.8864966630935669\n",
      "Step: 35700  \tValid loss: 0.2552593946456909\n",
      "Step: 35800  \tTraining loss: 0.2420904040336609\n",
      "Step: 35800  \tTraining accuracy: 0.8865116238594055\n",
      "Step: 35800  \tValid loss: 0.2552129626274109\n",
      "Step: 35900  \tTraining loss: 0.24205371737480164\n",
      "Step: 35900  \tTraining accuracy: 0.8865256309509277\n",
      "Step: 35900  \tValid loss: 0.2551831901073456\n",
      "Step: 36000  \tTraining loss: 0.24201767146587372\n",
      "Step: 36000  \tTraining accuracy: 0.8865389823913574\n",
      "Step: 36000  \tValid loss: 0.2551420032978058\n",
      "Step: 36100  \tTraining loss: 0.2419825941324234\n",
      "Step: 36100  \tTraining accuracy: 0.8865522742271423\n",
      "Step: 36100  \tValid loss: 0.25510743260383606\n",
      "Step: 36200  \tTraining loss: 0.24194668233394623\n",
      "Step: 36200  \tTraining accuracy: 0.8865654468536377\n",
      "Step: 36200  \tValid loss: 0.2550729215145111\n",
      "Step: 36300  \tTraining loss: 0.2419138103723526\n",
      "Step: 36300  \tTraining accuracy: 0.8865786194801331\n",
      "Step: 36300  \tValid loss: 0.25503769516944885\n",
      "Step: 36400  \tTraining loss: 0.24187639355659485\n",
      "Step: 36400  \tTraining accuracy: 0.8865916728973389\n",
      "Step: 36400  \tValid loss: 0.2550027072429657\n",
      "Step: 36500  \tTraining loss: 0.24184271693229675\n",
      "Step: 36500  \tTraining accuracy: 0.8866046667098999\n",
      "Step: 36500  \tValid loss: 0.2549823224544525\n",
      "Step: 36600  \tTraining loss: 0.24180641770362854\n",
      "Step: 36600  \tTraining accuracy: 0.8866176009178162\n",
      "Step: 36600  \tValid loss: 0.25494080781936646\n",
      "Step: 36700  \tTraining loss: 0.24176962673664093\n",
      "Step: 36700  \tTraining accuracy: 0.8866304159164429\n",
      "Step: 36700  \tValid loss: 0.2549130618572235\n",
      "Step: 36800  \tTraining loss: 0.24173390865325928\n",
      "Step: 36800  \tTraining accuracy: 0.8866432309150696\n",
      "Step: 36800  \tValid loss: 0.25488513708114624\n",
      "Step: 36900  \tTraining loss: 0.24170033633708954\n",
      "Step: 36900  \tTraining accuracy: 0.8866559267044067\n",
      "Step: 36900  \tValid loss: 0.25485143065452576\n",
      "Step: 37000  \tTraining loss: 0.241664320230484\n",
      "Step: 37000  \tTraining accuracy: 0.8866685628890991\n",
      "Step: 37000  \tValid loss: 0.2548237144947052\n",
      "Step: 37100  \tTraining loss: 0.2416311800479889\n",
      "Step: 37100  \tTraining accuracy: 0.8866811394691467\n",
      "Step: 37100  \tValid loss: 0.2547916769981384\n",
      "Step: 37200  \tTraining loss: 0.2415967434644699\n",
      "Step: 37200  \tTraining accuracy: 0.8866936564445496\n",
      "Step: 37200  \tValid loss: 0.25475913286209106\n",
      "Step: 37300  \tTraining loss: 0.24156400561332703\n",
      "Step: 37300  \tTraining accuracy: 0.8867060542106628\n",
      "Step: 37300  \tValid loss: 0.25473299622535706\n",
      "Step: 37400  \tTraining loss: 0.24153025448322296\n",
      "Step: 37400  \tTraining accuracy: 0.886719286441803\n",
      "Step: 37400  \tValid loss: 0.2547065019607544\n",
      "Step: 37500  \tTraining loss: 0.24149523675441742\n",
      "Step: 37500  \tTraining accuracy: 0.8867329955101013\n",
      "Step: 37500  \tValid loss: 0.2546699643135071\n",
      "Step: 37600  \tTraining loss: 0.24146242439746857\n",
      "Step: 37600  \tTraining accuracy: 0.8867466449737549\n",
      "Step: 37600  \tValid loss: 0.25463584065437317\n",
      "Step: 37700  \tTraining loss: 0.24142961204051971\n",
      "Step: 37700  \tTraining accuracy: 0.8867601752281189\n",
      "Step: 37700  \tValid loss: 0.2546076774597168\n",
      "Step: 37800  \tTraining loss: 0.24139650166034698\n",
      "Step: 37800  \tTraining accuracy: 0.8867736458778381\n",
      "Step: 37800  \tValid loss: 0.2545855939388275\n",
      "Step: 37900  \tTraining loss: 0.2413647621870041\n",
      "Step: 37900  \tTraining accuracy: 0.8867870569229126\n",
      "Step: 37900  \tValid loss: 0.254562109708786\n",
      "Step: 38000  \tTraining loss: 0.24133354425430298\n",
      "Step: 38000  \tTraining accuracy: 0.8868004083633423\n",
      "Step: 38000  \tValid loss: 0.2545336186885834\n",
      "Step: 38100  \tTraining loss: 0.24129609763622284\n",
      "Step: 38100  \tTraining accuracy: 0.8868137001991272\n",
      "Step: 38100  \tValid loss: 0.25449138879776\n",
      "Step: 38200  \tTraining loss: 0.24126262962818146\n",
      "Step: 38200  \tTraining accuracy: 0.8868263363838196\n",
      "Step: 38200  \tValid loss: 0.2544616162776947\n",
      "Step: 38300  \tTraining loss: 0.24123375117778778\n",
      "Step: 38300  \tTraining accuracy: 0.8868380784988403\n",
      "Step: 38300  \tValid loss: 0.2544287145137787\n",
      "Step: 38400  \tTraining loss: 0.24120044708251953\n",
      "Step: 38400  \tTraining accuracy: 0.8868498206138611\n",
      "Step: 38400  \tValid loss: 0.25440526008605957\n",
      "Step: 38500  \tTraining loss: 0.24116872251033783\n",
      "Step: 38500  \tTraining accuracy: 0.8868614435195923\n",
      "Step: 38500  \tValid loss: 0.25437867641448975\n",
      "Step: 38600  \tTraining loss: 0.24113701283931732\n",
      "Step: 38600  \tTraining accuracy: 0.8868730068206787\n",
      "Step: 38600  \tValid loss: 0.25434622168540955\n",
      "Step: 38700  \tTraining loss: 0.24110843241214752\n",
      "Step: 38700  \tTraining accuracy: 0.8868845701217651\n",
      "Step: 38700  \tValid loss: 0.2543269693851471\n",
      "Step: 38800  \tTraining loss: 0.2410752922296524\n",
      "Step: 38800  \tTraining accuracy: 0.886896014213562\n",
      "Step: 38800  \tValid loss: 0.2542940676212311\n",
      "Step: 38900  \tTraining loss: 0.24104389548301697\n",
      "Step: 38900  \tTraining accuracy: 0.8869073987007141\n",
      "Step: 38900  \tValid loss: 0.2542683780193329\n",
      "Step: 39000  \tTraining loss: 0.24101506173610687\n",
      "Step: 39000  \tTraining accuracy: 0.8869177103042603\n",
      "Step: 39000  \tValid loss: 0.25424623489379883\n",
      "Step: 39100  \tTraining loss: 0.2409834861755371\n",
      "Step: 39100  \tTraining accuracy: 0.8869268298149109\n",
      "Step: 39100  \tValid loss: 0.25422030687332153\n",
      "Step: 39200  \tTraining loss: 0.24095399677753448\n",
      "Step: 39200  \tTraining accuracy: 0.8869367241859436\n",
      "Step: 39200  \tValid loss: 0.25419411063194275\n",
      "Step: 39300  \tTraining loss: 0.24092219769954681\n",
      "Step: 39300  \tTraining accuracy: 0.8869466185569763\n",
      "Step: 39300  \tValid loss: 0.2541663348674774\n",
      "Step: 39400  \tTraining loss: 0.24089358747005463\n",
      "Step: 39400  \tTraining accuracy: 0.8869563937187195\n",
      "Step: 39400  \tValid loss: 0.25414612889289856\n",
      "Step: 39500  \tTraining loss: 0.2408652901649475\n",
      "Step: 39500  \tTraining accuracy: 0.8869656324386597\n",
      "Step: 39500  \tValid loss: 0.25411084294319153\n",
      "Step: 39600  \tTraining loss: 0.24083462357521057\n",
      "Step: 39600  \tTraining accuracy: 0.886974573135376\n",
      "Step: 39600  \tValid loss: 0.25408899784088135\n",
      "Step: 39700  \tTraining loss: 0.24080632627010345\n",
      "Step: 39700  \tTraining accuracy: 0.8869842290878296\n",
      "Step: 39700  \tValid loss: 0.254060298204422\n",
      "Step: 39800  \tTraining loss: 0.24077507853507996\n",
      "Step: 39800  \tTraining accuracy: 0.8869938254356384\n",
      "Step: 39800  \tValid loss: 0.25402867794036865\n",
      "Step: 39900  \tTraining loss: 0.24074715375900269\n",
      "Step: 39900  \tTraining accuracy: 0.8870033621788025\n",
      "Step: 39900  \tValid loss: 0.2540023922920227\n",
      "Step: 40000  \tTraining loss: 0.24071668088436127\n",
      "Step: 40000  \tTraining accuracy: 0.8870128989219666\n",
      "Step: 40000  \tValid loss: 0.253971666097641\n",
      "Step: 40100  \tTraining loss: 0.2406884729862213\n",
      "Step: 40100  \tTraining accuracy: 0.8870223760604858\n",
      "Step: 40100  \tValid loss: 0.2539418339729309\n",
      "Step: 40200  \tTraining loss: 0.24065902829170227\n",
      "Step: 40200  \tTraining accuracy: 0.8870317935943604\n",
      "Step: 40200  \tValid loss: 0.2539120614528656\n",
      "Step: 40300  \tTraining loss: 0.24063168466091156\n",
      "Step: 40300  \tTraining accuracy: 0.8870406150817871\n",
      "Step: 40300  \tValid loss: 0.25388869643211365\n",
      "Step: 40400  \tTraining loss: 0.24060428142547607\n",
      "Step: 40400  \tTraining accuracy: 0.8870491981506348\n",
      "Step: 40400  \tValid loss: 0.2538621723651886\n",
      "Step: 40500  \tTraining loss: 0.24057450890541077\n",
      "Step: 40500  \tTraining accuracy: 0.8870584964752197\n",
      "Step: 40500  \tValid loss: 0.25384220480918884\n",
      "Step: 40600  \tTraining loss: 0.24054810404777527\n",
      "Step: 40600  \tTraining accuracy: 0.8870677351951599\n",
      "Step: 40600  \tValid loss: 0.25382813811302185\n",
      "Step: 40700  \tTraining loss: 0.24052093923091888\n",
      "Step: 40700  \tTraining accuracy: 0.8870763778686523\n",
      "Step: 40700  \tValid loss: 0.25379297137260437\n",
      "Step: 40800  \tTraining loss: 0.24048969149589539\n",
      "Step: 40800  \tTraining accuracy: 0.8870840072631836\n",
      "Step: 40800  \tValid loss: 0.2537737190723419\n",
      "Step: 40900  \tTraining loss: 0.24046310782432556\n",
      "Step: 40900  \tTraining accuracy: 0.887091338634491\n",
      "Step: 40900  \tValid loss: 0.2537477910518646\n",
      "Step: 41000  \tTraining loss: 0.24043504893779755\n",
      "Step: 41000  \tTraining accuracy: 0.8870983719825745\n",
      "Step: 41000  \tValid loss: 0.25372442603111267\n",
      "Step: 41100  \tTraining loss: 0.2404070794582367\n",
      "Step: 41100  \tTraining accuracy: 0.8871061205863953\n",
      "Step: 41100  \tValid loss: 0.25369128584861755\n",
      "Step: 41200  \tTraining loss: 0.24038216471672058\n",
      "Step: 41200  \tTraining accuracy: 0.8871133327484131\n",
      "Step: 41200  \tValid loss: 0.2536703944206238\n",
      "Step: 41300  \tTraining loss: 0.2403530776500702\n",
      "Step: 41300  \tTraining accuracy: 0.8871207237243652\n",
      "Step: 41300  \tValid loss: 0.2536512613296509\n",
      "Step: 41400  \tTraining loss: 0.24032659828662872\n",
      "Step: 41400  \tTraining accuracy: 0.8871291279792786\n",
      "Step: 41400  \tValid loss: 0.2536398768424988\n",
      "Step: 41500  \tTraining loss: 0.2403026968240738\n",
      "Step: 41500  \tTraining accuracy: 0.8871367573738098\n",
      "Step: 41500  \tValid loss: 0.2536064088344574\n",
      "Step: 41600  \tTraining loss: 0.24027259647846222\n",
      "Step: 41600  \tTraining accuracy: 0.8871448040008545\n",
      "Step: 41600  \tValid loss: 0.25358808040618896\n",
      "Step: 41700  \tTraining loss: 0.240244522690773\n",
      "Step: 41700  \tTraining accuracy: 0.8871536254882812\n",
      "Step: 41700  \tValid loss: 0.25356829166412354\n",
      "Step: 41800  \tTraining loss: 0.24021980166435242\n",
      "Step: 41800  \tTraining accuracy: 0.8871618509292603\n",
      "Step: 41800  \tValid loss: 0.25354576110839844\n",
      "Step: 41900  \tTraining loss: 0.24018947780132294\n",
      "Step: 41900  \tTraining accuracy: 0.8871697783470154\n",
      "Step: 41900  \tValid loss: 0.2535300552845001\n",
      "Step: 42000  \tTraining loss: 0.24016261100769043\n",
      "Step: 42000  \tTraining accuracy: 0.8871784806251526\n",
      "Step: 42000  \tValid loss: 0.2535060942173004\n",
      "Step: 42100  \tTraining loss: 0.24013549089431763\n",
      "Step: 42100  \tTraining accuracy: 0.887186586856842\n",
      "Step: 42100  \tValid loss: 0.25348782539367676\n",
      "Step: 42200  \tTraining loss: 0.24010835587978363\n",
      "Step: 42200  \tTraining accuracy: 0.8871939182281494\n",
      "Step: 42200  \tValid loss: 0.2534649968147278\n",
      "Step: 42300  \tTraining loss: 0.2400835007429123\n",
      "Step: 42300  \tTraining accuracy: 0.8872012495994568\n",
      "Step: 42300  \tValid loss: 0.2534429728984833\n",
      "Step: 42400  \tTraining loss: 0.24005529284477234\n",
      "Step: 42400  \tTraining accuracy: 0.8872085213661194\n",
      "Step: 42400  \tValid loss: 0.2534257471561432\n",
      "Step: 42500  \tTraining loss: 0.24002860486507416\n",
      "Step: 42500  \tTraining accuracy: 0.887215793132782\n",
      "Step: 42500  \tValid loss: 0.25340622663497925\n",
      "Step: 42600  \tTraining loss: 0.2400021255016327\n",
      "Step: 42600  \tTraining accuracy: 0.887222945690155\n",
      "Step: 42600  \tValid loss: 0.25338485836982727\n",
      "Step: 42700  \tTraining loss: 0.23997855186462402\n",
      "Step: 42700  \tTraining accuracy: 0.8872301578521729\n",
      "Step: 42700  \tValid loss: 0.2533572018146515\n",
      "Step: 42800  \tTraining loss: 0.23994971811771393\n",
      "Step: 42800  \tTraining accuracy: 0.8872373104095459\n",
      "Step: 42800  \tValid loss: 0.25334081053733826\n",
      "Step: 42900  \tTraining loss: 0.23992538452148438\n",
      "Step: 42900  \tTraining accuracy: 0.8872448801994324\n",
      "Step: 42900  \tValid loss: 0.2533259689807892\n",
      "Step: 43000  \tTraining loss: 0.23989783227443695\n",
      "Step: 43000  \tTraining accuracy: 0.8872531652450562\n",
      "Step: 43000  \tValid loss: 0.25330471992492676\n",
      "Step: 43100  \tTraining loss: 0.23987184464931488\n",
      "Step: 43100  \tTraining accuracy: 0.8872601985931396\n",
      "Step: 43100  \tValid loss: 0.2532821595668793\n",
      "Step: 43200  \tTraining loss: 0.23984742164611816\n",
      "Step: 43200  \tTraining accuracy: 0.8872659802436829\n",
      "Step: 43200  \tValid loss: 0.2532619833946228\n",
      "Step: 43300  \tTraining loss: 0.2398225963115692\n",
      "Step: 43300  \tTraining accuracy: 0.8872717618942261\n",
      "Step: 43300  \tValid loss: 0.25324028730392456\n",
      "Step: 43400  \tTraining loss: 0.2397940307855606\n",
      "Step: 43400  \tTraining accuracy: 0.8872774839401245\n",
      "Step: 43400  \tValid loss: 0.2532353699207306\n",
      "Step: 43500  \tTraining loss: 0.23976752161979675\n",
      "Step: 43500  \tTraining accuracy: 0.887283205986023\n",
      "Step: 43500  \tValid loss: 0.25320377945899963\n",
      "Step: 43600  \tTraining loss: 0.23974254727363586\n",
      "Step: 43600  \tTraining accuracy: 0.8872891664505005\n",
      "Step: 43600  \tValid loss: 0.2531769871711731\n",
      "Step: 43700  \tTraining loss: 0.23971520364284515\n",
      "Step: 43700  \tTraining accuracy: 0.8872947692871094\n",
      "Step: 43700  \tValid loss: 0.2531580924987793\n",
      "Step: 43800  \tTraining loss: 0.2396901696920395\n",
      "Step: 43800  \tTraining accuracy: 0.8872992396354675\n",
      "Step: 43800  \tValid loss: 0.25313055515289307\n",
      "Step: 43900  \tTraining loss: 0.2396642565727234\n",
      "Step: 43900  \tTraining accuracy: 0.8873036503791809\n",
      "Step: 43900  \tValid loss: 0.2531106173992157\n",
      "Step: 44000  \tTraining loss: 0.23963885009288788\n",
      "Step: 44000  \tTraining accuracy: 0.8873082995414734\n",
      "Step: 44000  \tValid loss: 0.2530881464481354\n",
      "Step: 44100  \tTraining loss: 0.2396133840084076\n",
      "Step: 44100  \tTraining accuracy: 0.8873138427734375\n",
      "Step: 44100  \tValid loss: 0.2530533969402313\n",
      "Step: 44200  \tTraining loss: 0.23958861827850342\n",
      "Step: 44200  \tTraining accuracy: 0.8873193860054016\n",
      "Step: 44200  \tValid loss: 0.25303784012794495\n",
      "Step: 44300  \tTraining loss: 0.23956353962421417\n",
      "Step: 44300  \tTraining accuracy: 0.8873249292373657\n",
      "Step: 44300  \tValid loss: 0.25301432609558105\n",
      "Step: 44400  \tTraining loss: 0.23953787982463837\n",
      "Step: 44400  \tTraining accuracy: 0.8873304128646851\n",
      "Step: 44400  \tValid loss: 0.252993106842041\n",
      "Step: 44500  \tTraining loss: 0.2395118772983551\n",
      "Step: 44500  \tTraining accuracy: 0.8873358964920044\n",
      "Step: 44500  \tValid loss: 0.25297069549560547\n",
      "Step: 44600  \tTraining loss: 0.23948560655117035\n",
      "Step: 44600  \tTraining accuracy: 0.887341320514679\n",
      "Step: 44600  \tValid loss: 0.252946138381958\n",
      "Step: 44700  \tTraining loss: 0.23946024477481842\n",
      "Step: 44700  \tTraining accuracy: 0.8873467445373535\n",
      "Step: 44700  \tValid loss: 0.25292542576789856\n",
      "Step: 44800  \tTraining loss: 0.2394338846206665\n",
      "Step: 44800  \tTraining accuracy: 0.8873521089553833\n",
      "Step: 44800  \tValid loss: 0.25289982557296753\n",
      "Step: 44900  \tTraining loss: 0.23940907418727875\n",
      "Step: 44900  \tTraining accuracy: 0.8873574733734131\n",
      "Step: 44900  \tValid loss: 0.25287869572639465\n",
      "Step: 45000  \tTraining loss: 0.23938332498073578\n",
      "Step: 45000  \tTraining accuracy: 0.8873628377914429\n",
      "Step: 45000  \tValid loss: 0.2528505325317383\n",
      "Step: 45100  \tTraining loss: 0.239358052611351\n",
      "Step: 45100  \tTraining accuracy: 0.8873681426048279\n",
      "Step: 45100  \tValid loss: 0.2528340220451355\n",
      "Step: 45200  \tTraining loss: 0.23933297395706177\n",
      "Step: 45200  \tTraining accuracy: 0.8873734474182129\n",
      "Step: 45200  \tValid loss: 0.25280290842056274\n",
      "Step: 45300  \tTraining loss: 0.23931020498275757\n",
      "Step: 45300  \tTraining accuracy: 0.8873791694641113\n",
      "Step: 45300  \tValid loss: 0.2528001666069031\n",
      "Step: 45400  \tTraining loss: 0.2392832189798355\n",
      "Step: 45400  \tTraining accuracy: 0.8873851299285889\n",
      "Step: 45400  \tValid loss: 0.25276291370391846\n",
      "Step: 45500  \tTraining loss: 0.23925867676734924\n",
      "Step: 45500  \tTraining accuracy: 0.8873903751373291\n",
      "Step: 45500  \tValid loss: 0.25274190306663513\n",
      "Step: 45600  \tTraining loss: 0.23922008275985718\n",
      "Step: 45600  \tTraining accuracy: 0.8873955607414246\n",
      "Step: 45600  \tValid loss: 0.2527328133583069\n",
      "Step: 45700  \tTraining loss: 0.23919765651226044\n",
      "Step: 45700  \tTraining accuracy: 0.8874002695083618\n",
      "Step: 45700  \tValid loss: 0.252686470746994\n",
      "Step: 45800  \tTraining loss: 0.23916608095169067\n",
      "Step: 45800  \tTraining accuracy: 0.88740473985672\n",
      "Step: 45800  \tValid loss: 0.25265660881996155\n",
      "Step: 45900  \tTraining loss: 0.23913975059986115\n",
      "Step: 45900  \tTraining accuracy: 0.8874094486236572\n",
      "Step: 45900  \tValid loss: 0.25263530015945435\n",
      "Step: 46000  \tTraining loss: 0.2391134649515152\n",
      "Step: 46000  \tTraining accuracy: 0.8874133825302124\n",
      "Step: 46000  \tValid loss: 0.2526124119758606\n",
      "Step: 46100  \tTraining loss: 0.23908928036689758\n",
      "Step: 46100  \tTraining accuracy: 0.8874177932739258\n",
      "Step: 46100  \tValid loss: 0.2525995373725891\n",
      "Step: 46200  \tTraining loss: 0.23906128108501434\n",
      "Step: 46200  \tTraining accuracy: 0.8874229192733765\n",
      "Step: 46200  \tValid loss: 0.2525629699230194\n",
      "Step: 46300  \tTraining loss: 0.239035964012146\n",
      "Step: 46300  \tTraining accuracy: 0.8874279260635376\n",
      "Step: 46300  \tValid loss: 0.2525448799133301\n",
      "Step: 46400  \tTraining loss: 0.23901183903217316\n",
      "Step: 46400  \tTraining accuracy: 0.8874329924583435\n",
      "Step: 46400  \tValid loss: 0.25252702832221985\n",
      "Step: 46500  \tTraining loss: 0.23898793756961823\n",
      "Step: 46500  \tTraining accuracy: 0.8874379992485046\n",
      "Step: 46500  \tValid loss: 0.25250762701034546\n",
      "Step: 46600  \tTraining loss: 0.23896491527557373\n",
      "Step: 46600  \tTraining accuracy: 0.887442946434021\n",
      "Step: 46600  \tValid loss: 0.2524849772453308\n",
      "Step: 46700  \tTraining loss: 0.23894397914409637\n",
      "Step: 46700  \tTraining accuracy: 0.8874479532241821\n",
      "Step: 46700  \tValid loss: 0.2524741291999817\n",
      "Step: 46800  \tTraining loss: 0.23891785740852356\n",
      "Step: 46800  \tTraining accuracy: 0.8874529004096985\n",
      "Step: 46800  \tValid loss: 0.2524488568305969\n",
      "Step: 46900  \tTraining loss: 0.23889537155628204\n",
      "Step: 46900  \tTraining accuracy: 0.8874577879905701\n",
      "Step: 46900  \tValid loss: 0.2524303197860718\n",
      "Step: 47000  \tTraining loss: 0.2388717085123062\n",
      "Step: 47000  \tTraining accuracy: 0.8874627351760864\n",
      "Step: 47000  \tValid loss: 0.25240200757980347\n",
      "Step: 47100  \tTraining loss: 0.2388484627008438\n",
      "Step: 47100  \tTraining accuracy: 0.8874675631523132\n",
      "Step: 47100  \tValid loss: 0.2523824870586395\n",
      "Step: 47200  \tTraining loss: 0.23882661759853363\n",
      "Step: 47200  \tTraining accuracy: 0.8874720335006714\n",
      "Step: 47200  \tValid loss: 0.2523576319217682\n",
      "Step: 47300  \tTraining loss: 0.23880314826965332\n",
      "Step: 47300  \tTraining accuracy: 0.8874762058258057\n",
      "Step: 47300  \tValid loss: 0.25233349204063416\n",
      "Step: 47400  \tTraining loss: 0.23878024518489838\n",
      "Step: 47400  \tTraining accuracy: 0.8874810338020325\n",
      "Step: 47400  \tValid loss: 0.25231578946113586\n",
      "Step: 47500  \tTraining loss: 0.23875820636749268\n",
      "Step: 47500  \tTraining accuracy: 0.8874858021736145\n",
      "Step: 47500  \tValid loss: 0.2522961497306824\n",
      "Step: 47600  \tTraining loss: 0.2387365698814392\n",
      "Step: 47600  \tTraining accuracy: 0.8874905705451965\n",
      "Step: 47600  \tValid loss: 0.2522776424884796\n",
      "Step: 47700  \tTraining loss: 0.23871473968029022\n",
      "Step: 47700  \tTraining accuracy: 0.8874949216842651\n",
      "Step: 47700  \tValid loss: 0.2522592842578888\n",
      "Step: 47800  \tTraining loss: 0.23869259655475616\n",
      "Step: 47800  \tTraining accuracy: 0.8874989748001099\n",
      "Step: 47800  \tValid loss: 0.2522428631782532\n",
      "Step: 47900  \tTraining loss: 0.23867018520832062\n",
      "Step: 47900  \tTraining accuracy: 0.8875041604042053\n",
      "Step: 47900  \tValid loss: 0.2522229254245758\n",
      "Step: 48000  \tTraining loss: 0.23864784836769104\n",
      "Step: 48000  \tTraining accuracy: 0.8875108361244202\n",
      "Step: 48000  \tValid loss: 0.2521999478340149\n",
      "Step: 48100  \tTraining loss: 0.23862679302692413\n",
      "Step: 48100  \tTraining accuracy: 0.8875163793563843\n",
      "Step: 48100  \tValid loss: 0.252174973487854\n",
      "Step: 48200  \tTraining loss: 0.23860636353492737\n",
      "Step: 48200  \tTraining accuracy: 0.8875219225883484\n",
      "Step: 48200  \tValid loss: 0.25215667486190796\n",
      "Step: 48300  \tTraining loss: 0.23858317732810974\n",
      "Step: 48300  \tTraining accuracy: 0.8875280618667603\n",
      "Step: 48300  \tValid loss: 0.2521388530731201\n",
      "Step: 48400  \tTraining loss: 0.23856337368488312\n",
      "Step: 48400  \tTraining accuracy: 0.8875343799591064\n",
      "Step: 48400  \tValid loss: 0.2521202862262726\n",
      "Step: 48500  \tTraining loss: 0.23854126036167145\n",
      "Step: 48500  \tTraining accuracy: 0.8875401020050049\n",
      "Step: 48500  \tValid loss: 0.25210511684417725\n",
      "Step: 48600  \tTraining loss: 0.23851962387561798\n",
      "Step: 48600  \tTraining accuracy: 0.8875457048416138\n",
      "Step: 48600  \tValid loss: 0.25207918882369995\n",
      "Step: 48700  \tTraining loss: 0.2384985089302063\n",
      "Step: 48700  \tTraining accuracy: 0.8875513672828674\n",
      "Step: 48700  \tValid loss: 0.2520610988140106\n",
      "Step: 48800  \tTraining loss: 0.2384769767522812\n",
      "Step: 48800  \tTraining accuracy: 0.8875569701194763\n",
      "Step: 48800  \tValid loss: 0.2520451843738556\n",
      "Step: 48900  \tTraining loss: 0.2384558469057083\n",
      "Step: 48900  \tTraining accuracy: 0.8875634074211121\n",
      "Step: 48900  \tValid loss: 0.252025842666626\n",
      "Step: 49000  \tTraining loss: 0.23843564093112946\n",
      "Step: 49000  \tTraining accuracy: 0.8875706195831299\n",
      "Step: 49000  \tValid loss: 0.2519989013671875\n",
      "Step: 49100  \tTraining loss: 0.23841418325901031\n",
      "Step: 49100  \tTraining accuracy: 0.8875776529312134\n",
      "Step: 49100  \tValid loss: 0.2519911229610443\n",
      "Step: 49200  \tTraining loss: 0.23839548230171204\n",
      "Step: 49200  \tTraining accuracy: 0.8875848650932312\n",
      "Step: 49200  \tValid loss: 0.25197508931159973\n",
      "Step: 49300  \tTraining loss: 0.23837333917617798\n",
      "Step: 49300  \tTraining accuracy: 0.8875914216041565\n",
      "Step: 49300  \tValid loss: 0.2519635856151581\n",
      "Step: 49400  \tTraining loss: 0.2383531928062439\n",
      "Step: 49400  \tTraining accuracy: 0.887597918510437\n",
      "Step: 49400  \tValid loss: 0.25194740295410156\n",
      "Step: 49500  \tTraining loss: 0.2383318692445755\n",
      "Step: 49500  \tTraining accuracy: 0.8876044154167175\n",
      "Step: 49500  \tValid loss: 0.25193122029304504\n",
      "Step: 49600  \tTraining loss: 0.23831120133399963\n",
      "Step: 49600  \tTraining accuracy: 0.8876108527183533\n",
      "Step: 49600  \tValid loss: 0.25191330909729004\n",
      "Step: 49700  \tTraining loss: 0.23829135298728943\n",
      "Step: 49700  \tTraining accuracy: 0.887617290019989\n",
      "Step: 49700  \tValid loss: 0.251899778842926\n",
      "Step: 49800  \tTraining loss: 0.23827093839645386\n",
      "Step: 49800  \tTraining accuracy: 0.8876237273216248\n",
      "Step: 49800  \tValid loss: 0.25187987089157104\n",
      "Step: 49900  \tTraining loss: 0.23825158178806305\n",
      "Step: 49900  \tTraining accuracy: 0.8876301050186157\n",
      "Step: 49900  \tValid loss: 0.2518704831600189\n",
      "Step: 50000  \tTraining loss: 0.23823073506355286\n",
      "Step: 50000  \tTraining accuracy: 0.8876364231109619\n",
      "Step: 50000  \tValid loss: 0.2518502175807953\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.88764274\n",
      "Precision: 0.9121891\n",
      "Recall: 0.93515897\n",
      "F1 score: 0.8986797\n",
      "AUC: 0.8786492\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.887643   0.912189  0.935159   0.89868  0.878649  0.238231      0.887633   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.251846        0.88763   0.314608      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  49999.0  \n",
      "14\n",
      "(8990, 8)\n",
      "(8990, 1)\n",
      "(4960, 8)\n",
      "(4960, 1)\n",
      "(4030, 8)\n",
      "(4030, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5901938676834106\n",
      "Step: 100  \tTraining accuracy: 0.680088996887207\n",
      "Step: 100  \tValid loss: 0.5881088376045227\n",
      "Step: 200  \tTraining loss: 0.5523455142974854\n",
      "Step: 200  \tTraining accuracy: 0.6946607232093811\n",
      "Step: 200  \tValid loss: 0.5502246618270874\n",
      "Step: 300  \tTraining loss: 0.5032894611358643\n",
      "Step: 300  \tTraining accuracy: 0.7108564972877502\n",
      "Step: 300  \tValid loss: 0.4991684854030609\n",
      "Step: 400  \tTraining loss: 0.4495752453804016\n",
      "Step: 400  \tTraining accuracy: 0.7288892269134521\n",
      "Step: 400  \tValid loss: 0.4450036883354187\n",
      "Step: 500  \tTraining loss: 0.42663729190826416\n",
      "Step: 500  \tTraining accuracy: 0.7449511885643005\n",
      "Step: 500  \tValid loss: 0.4213870167732239\n",
      "Step: 600  \tTraining loss: 0.4166095554828644\n",
      "Step: 600  \tTraining accuracy: 0.7572049498558044\n",
      "Step: 600  \tValid loss: 0.41053506731987\n",
      "Step: 700  \tTraining loss: 0.4105238616466522\n",
      "Step: 700  \tTraining accuracy: 0.7667322754859924\n",
      "Step: 700  \tValid loss: 0.40399661660194397\n",
      "Step: 800  \tTraining loss: 0.4067371189594269\n",
      "Step: 800  \tTraining accuracy: 0.7742232084274292\n",
      "Step: 800  \tValid loss: 0.39981138706207275\n",
      "Step: 900  \tTraining loss: 0.4041145145893097\n",
      "Step: 900  \tTraining accuracy: 0.7803310751914978\n",
      "Step: 900  \tValid loss: 0.39695942401885986\n",
      "Step: 1000  \tTraining loss: 0.4020691514015198\n",
      "Step: 1000  \tTraining accuracy: 0.7854750752449036\n",
      "Step: 1000  \tValid loss: 0.3947362005710602\n",
      "Step: 1100  \tTraining loss: 0.4003356993198395\n",
      "Step: 1100  \tTraining accuracy: 0.7897769808769226\n",
      "Step: 1100  \tValid loss: 0.3928462266921997\n",
      "Step: 1200  \tTraining loss: 0.39890363812446594\n",
      "Step: 1200  \tTraining accuracy: 0.7932968735694885\n",
      "Step: 1200  \tValid loss: 0.39124929904937744\n",
      "Step: 1300  \tTraining loss: 0.3976099193096161\n",
      "Step: 1300  \tTraining accuracy: 0.7962847352027893\n",
      "Step: 1300  \tValid loss: 0.3898182511329651\n",
      "Step: 1400  \tTraining loss: 0.39632660150527954\n",
      "Step: 1400  \tTraining accuracy: 0.7988464832305908\n",
      "Step: 1400  \tValid loss: 0.3882726728916168\n",
      "Step: 1500  \tTraining loss: 0.3951723575592041\n",
      "Step: 1500  \tTraining accuracy: 0.8011008501052856\n",
      "Step: 1500  \tValid loss: 0.3870091736316681\n",
      "Step: 1600  \tTraining loss: 0.3941693902015686\n",
      "Step: 1600  \tTraining accuracy: 0.8030391931533813\n",
      "Step: 1600  \tValid loss: 0.38593292236328125\n",
      "Step: 1700  \tTraining loss: 0.393211305141449\n",
      "Step: 1700  \tTraining accuracy: 0.8047628402709961\n",
      "Step: 1700  \tValid loss: 0.38480648398399353\n",
      "Step: 1800  \tTraining loss: 0.39232054352760315\n",
      "Step: 1800  \tTraining accuracy: 0.806299090385437\n",
      "Step: 1800  \tValid loss: 0.38403356075286865\n",
      "Step: 1900  \tTraining loss: 0.39149370789527893\n",
      "Step: 1900  \tTraining accuracy: 0.8076481223106384\n",
      "Step: 1900  \tValid loss: 0.38318267464637756\n",
      "Step: 2000  \tTraining loss: 0.3906814455986023\n",
      "Step: 2000  \tTraining accuracy: 0.8088588714599609\n",
      "Step: 2000  \tValid loss: 0.382305771112442\n",
      "Step: 2100  \tTraining loss: 0.38990989327430725\n",
      "Step: 2100  \tTraining accuracy: 0.809924304485321\n",
      "Step: 2100  \tValid loss: 0.38165438175201416\n",
      "Step: 2200  \tTraining loss: 0.3891630470752716\n",
      "Step: 2200  \tTraining accuracy: 0.8108699321746826\n",
      "Step: 2200  \tValid loss: 0.3809799253940582\n",
      "Step: 2300  \tTraining loss: 0.3884483277797699\n",
      "Step: 2300  \tTraining accuracy: 0.8117538094520569\n",
      "Step: 2300  \tValid loss: 0.3802127540111542\n",
      "Step: 2400  \tTraining loss: 0.38782554864883423\n",
      "Step: 2400  \tTraining accuracy: 0.8125789761543274\n",
      "Step: 2400  \tValid loss: 0.3796514570713043\n",
      "Step: 2500  \tTraining loss: 0.38725408911705017\n",
      "Step: 2500  \tTraining accuracy: 0.8133459091186523\n",
      "Step: 2500  \tValid loss: 0.37911275029182434\n",
      "Step: 2600  \tTraining loss: 0.3866960406303406\n",
      "Step: 2600  \tTraining accuracy: 0.8140308260917664\n",
      "Step: 2600  \tValid loss: 0.37847900390625\n",
      "Step: 2700  \tTraining loss: 0.3861715793609619\n",
      "Step: 2700  \tTraining accuracy: 0.8147018551826477\n",
      "Step: 2700  \tValid loss: 0.377949059009552\n",
      "Step: 2800  \tTraining loss: 0.3856618106365204\n",
      "Step: 2800  \tTraining accuracy: 0.8153079152107239\n",
      "Step: 2800  \tValid loss: 0.3772564232349396\n",
      "Step: 2900  \tTraining loss: 0.38515618443489075\n",
      "Step: 2900  \tTraining accuracy: 0.8158811926841736\n",
      "Step: 2900  \tValid loss: 0.37660592794418335\n",
      "Step: 3000  \tTraining loss: 0.3847138583660126\n",
      "Step: 3000  \tTraining accuracy: 0.8164287805557251\n",
      "Step: 3000  \tValid loss: 0.37610551714897156\n",
      "Step: 3100  \tTraining loss: 0.38431671261787415\n",
      "Step: 3100  \tTraining accuracy: 0.8169550895690918\n",
      "Step: 3100  \tValid loss: 0.3756929039955139\n",
      "Step: 3200  \tTraining loss: 0.38394516706466675\n",
      "Step: 3200  \tTraining accuracy: 0.817454993724823\n",
      "Step: 3200  \tValid loss: 0.3752850890159607\n",
      "Step: 3300  \tTraining loss: 0.38358429074287415\n",
      "Step: 3300  \tTraining accuracy: 0.8179242014884949\n",
      "Step: 3300  \tValid loss: 0.3749082386493683\n",
      "Step: 3400  \tTraining loss: 0.38324153423309326\n",
      "Step: 3400  \tTraining accuracy: 0.8183620572090149\n",
      "Step: 3400  \tValid loss: 0.3745344579219818\n",
      "Step: 3500  \tTraining loss: 0.3829180896282196\n",
      "Step: 3500  \tTraining accuracy: 0.8187922239303589\n",
      "Step: 3500  \tValid loss: 0.3742155432701111\n",
      "Step: 3600  \tTraining loss: 0.3826008439064026\n",
      "Step: 3600  \tTraining accuracy: 0.8192185163497925\n",
      "Step: 3600  \tValid loss: 0.37391600012779236\n",
      "Step: 3700  \tTraining loss: 0.382295161485672\n",
      "Step: 3700  \tTraining accuracy: 0.8196382522583008\n",
      "Step: 3700  \tValid loss: 0.37366169691085815\n",
      "Step: 3800  \tTraining loss: 0.38199204206466675\n",
      "Step: 3800  \tTraining accuracy: 0.8200400471687317\n",
      "Step: 3800  \tValid loss: 0.37342503666877747\n",
      "Step: 3900  \tTraining loss: 0.38168588280677795\n",
      "Step: 3900  \tTraining accuracy: 0.8204166293144226\n",
      "Step: 3900  \tValid loss: 0.37319833040237427\n",
      "Step: 4000  \tTraining loss: 0.38135677576065063\n",
      "Step: 4000  \tTraining accuracy: 0.8207671046257019\n",
      "Step: 4000  \tValid loss: 0.37291842699050903\n",
      "Step: 4100  \tTraining loss: 0.3810168206691742\n",
      "Step: 4100  \tTraining accuracy: 0.8211002349853516\n",
      "Step: 4100  \tValid loss: 0.37268510460853577\n",
      "Step: 4200  \tTraining loss: 0.3806909918785095\n",
      "Step: 4200  \tTraining accuracy: 0.8214187026023865\n",
      "Step: 4200  \tValid loss: 0.37244194746017456\n",
      "Step: 4300  \tTraining loss: 0.3803657591342926\n",
      "Step: 4300  \tTraining accuracy: 0.8217418193817139\n",
      "Step: 4300  \tValid loss: 0.3722213804721832\n",
      "Step: 4400  \tTraining loss: 0.3800382912158966\n",
      "Step: 4400  \tTraining accuracy: 0.8220615386962891\n",
      "Step: 4400  \tValid loss: 0.37200498580932617\n",
      "Step: 4500  \tTraining loss: 0.37971317768096924\n",
      "Step: 4500  \tTraining accuracy: 0.8223719000816345\n",
      "Step: 4500  \tValid loss: 0.3717968463897705\n",
      "Step: 4600  \tTraining loss: 0.379354864358902\n",
      "Step: 4600  \tTraining accuracy: 0.8226796388626099\n",
      "Step: 4600  \tValid loss: 0.3715307414531708\n",
      "Step: 4700  \tTraining loss: 0.3789912462234497\n",
      "Step: 4700  \tTraining accuracy: 0.822995662689209\n",
      "Step: 4700  \tValid loss: 0.3712712228298187\n",
      "Step: 4800  \tTraining loss: 0.37864038348197937\n",
      "Step: 4800  \tTraining accuracy: 0.8233054280281067\n",
      "Step: 4800  \tValid loss: 0.37101104855537415\n",
      "Step: 4900  \tTraining loss: 0.37827903032302856\n",
      "Step: 4900  \tTraining accuracy: 0.8235966563224792\n",
      "Step: 4900  \tValid loss: 0.3707265853881836\n",
      "Step: 5000  \tTraining loss: 0.3779323101043701\n",
      "Step: 5000  \tTraining accuracy: 0.8238738775253296\n",
      "Step: 5000  \tValid loss: 0.3704639673233032\n",
      "Step: 5100  \tTraining loss: 0.37759190797805786\n",
      "Step: 5100  \tTraining accuracy: 0.824141263961792\n",
      "Step: 5100  \tValid loss: 0.37021204829216003\n",
      "Step: 5200  \tTraining loss: 0.3772668242454529\n",
      "Step: 5200  \tTraining accuracy: 0.824409008026123\n",
      "Step: 5200  \tValid loss: 0.3699391484260559\n",
      "Step: 5300  \tTraining loss: 0.3769487142562866\n",
      "Step: 5300  \tTraining accuracy: 0.8246697187423706\n",
      "Step: 5300  \tValid loss: 0.3696633577346802\n",
      "Step: 5400  \tTraining loss: 0.37663811445236206\n",
      "Step: 5400  \tTraining accuracy: 0.8249166011810303\n",
      "Step: 5400  \tValid loss: 0.36939457058906555\n",
      "Step: 5500  \tTraining loss: 0.37632232904434204\n",
      "Step: 5500  \tTraining accuracy: 0.825158417224884\n",
      "Step: 5500  \tValid loss: 0.369128555059433\n",
      "Step: 5600  \tTraining loss: 0.3759957551956177\n",
      "Step: 5600  \tTraining accuracy: 0.8254005908966064\n",
      "Step: 5600  \tValid loss: 0.36886897683143616\n",
      "Step: 5700  \tTraining loss: 0.3756723403930664\n",
      "Step: 5700  \tTraining accuracy: 0.8256469964981079\n",
      "Step: 5700  \tValid loss: 0.36862272024154663\n",
      "Step: 5800  \tTraining loss: 0.3753376007080078\n",
      "Step: 5800  \tTraining accuracy: 0.8258906006813049\n",
      "Step: 5800  \tValid loss: 0.3684016168117523\n",
      "Step: 5900  \tTraining loss: 0.37499895691871643\n",
      "Step: 5900  \tTraining accuracy: 0.8261277675628662\n",
      "Step: 5900  \tValid loss: 0.36817213892936707\n",
      "Step: 6000  \tTraining loss: 0.3746590316295624\n",
      "Step: 6000  \tTraining accuracy: 0.8263504505157471\n",
      "Step: 6000  \tValid loss: 0.3679414987564087\n",
      "Step: 6100  \tTraining loss: 0.37431663274765015\n",
      "Step: 6100  \tTraining accuracy: 0.8265694379806519\n",
      "Step: 6100  \tValid loss: 0.36769986152648926\n",
      "Step: 6200  \tTraining loss: 0.3739573359489441\n",
      "Step: 6200  \tTraining accuracy: 0.8267776966094971\n",
      "Step: 6200  \tValid loss: 0.36745601892471313\n",
      "Step: 6300  \tTraining loss: 0.37360110878944397\n",
      "Step: 6300  \tTraining accuracy: 0.826977550983429\n",
      "Step: 6300  \tValid loss: 0.3672075867652893\n",
      "Step: 6400  \tTraining loss: 0.37325090169906616\n",
      "Step: 6400  \tTraining accuracy: 0.8271728157997131\n",
      "Step: 6400  \tValid loss: 0.3669682741165161\n",
      "Step: 6500  \tTraining loss: 0.37289685010910034\n",
      "Step: 6500  \tTraining accuracy: 0.8273628950119019\n",
      "Step: 6500  \tValid loss: 0.36671364307403564\n",
      "Step: 6600  \tTraining loss: 0.37254175543785095\n",
      "Step: 6600  \tTraining accuracy: 0.8275420665740967\n",
      "Step: 6600  \tValid loss: 0.366459459066391\n",
      "Step: 6700  \tTraining loss: 0.3721872568130493\n",
      "Step: 6700  \tTraining accuracy: 0.8277183771133423\n",
      "Step: 6700  \tValid loss: 0.36620235443115234\n",
      "Step: 6800  \tTraining loss: 0.3718315362930298\n",
      "Step: 6800  \tTraining accuracy: 0.8278902769088745\n",
      "Step: 6800  \tValid loss: 0.36594218015670776\n",
      "Step: 6900  \tTraining loss: 0.37147602438926697\n",
      "Step: 6900  \tTraining accuracy: 0.8280587792396545\n",
      "Step: 6900  \tValid loss: 0.36567768454551697\n",
      "Step: 7000  \tTraining loss: 0.37112298607826233\n",
      "Step: 7000  \tTraining accuracy: 0.828220009803772\n",
      "Step: 7000  \tValid loss: 0.3654302656650543\n",
      "Step: 7100  \tTraining loss: 0.3707740604877472\n",
      "Step: 7100  \tTraining accuracy: 0.8283727169036865\n",
      "Step: 7100  \tValid loss: 0.36513790488243103\n",
      "Step: 7200  \tTraining loss: 0.3704291582107544\n",
      "Step: 7200  \tTraining accuracy: 0.8285118937492371\n",
      "Step: 7200  \tValid loss: 0.36488449573516846\n",
      "Step: 7300  \tTraining loss: 0.3700912296772003\n",
      "Step: 7300  \tTraining accuracy: 0.8286409974098206\n",
      "Step: 7300  \tValid loss: 0.3646087944507599\n",
      "Step: 7400  \tTraining loss: 0.3697599470615387\n",
      "Step: 7400  \tTraining accuracy: 0.8287674188613892\n",
      "Step: 7400  \tValid loss: 0.36433786153793335\n",
      "Step: 7500  \tTraining loss: 0.3694358468055725\n",
      "Step: 7500  \tTraining accuracy: 0.8288896679878235\n",
      "Step: 7500  \tValid loss: 0.36406058073043823\n",
      "Step: 7600  \tTraining loss: 0.36911943554878235\n",
      "Step: 7600  \tTraining accuracy: 0.8289968967437744\n",
      "Step: 7600  \tValid loss: 0.36378100514411926\n",
      "Step: 7700  \tTraining loss: 0.3688122034072876\n",
      "Step: 7700  \tTraining accuracy: 0.8290984034538269\n",
      "Step: 7700  \tValid loss: 0.36352795362472534\n",
      "Step: 7800  \tTraining loss: 0.36851435899734497\n",
      "Step: 7800  \tTraining accuracy: 0.8291929960250854\n",
      "Step: 7800  \tValid loss: 0.36326485872268677\n",
      "Step: 7900  \tTraining loss: 0.3682275414466858\n",
      "Step: 7900  \tTraining accuracy: 0.8292866349220276\n",
      "Step: 7900  \tValid loss: 0.3630066514015198\n",
      "Step: 8000  \tTraining loss: 0.3679514527320862\n",
      "Step: 8000  \tTraining accuracy: 0.8293792605400085\n",
      "Step: 8000  \tValid loss: 0.3627551198005676\n",
      "Step: 8100  \tTraining loss: 0.3676873445510864\n",
      "Step: 8100  \tTraining accuracy: 0.8294689059257507\n",
      "Step: 8100  \tValid loss: 0.3625071942806244\n",
      "Step: 8200  \tTraining loss: 0.3674337565898895\n",
      "Step: 8200  \tTraining accuracy: 0.8295556902885437\n",
      "Step: 8200  \tValid loss: 0.3622499406337738\n",
      "Step: 8300  \tTraining loss: 0.36718806624412537\n",
      "Step: 8300  \tTraining accuracy: 0.829643726348877\n",
      "Step: 8300  \tValid loss: 0.36200833320617676\n",
      "Step: 8400  \tTraining loss: 0.3669428825378418\n",
      "Step: 8400  \tTraining accuracy: 0.8297316431999207\n",
      "Step: 8400  \tValid loss: 0.3617768883705139\n",
      "Step: 8500  \tTraining loss: 0.36671218276023865\n",
      "Step: 8500  \tTraining accuracy: 0.8298174738883972\n",
      "Step: 8500  \tValid loss: 0.36153116822242737\n",
      "Step: 8600  \tTraining loss: 0.36649131774902344\n",
      "Step: 8600  \tTraining accuracy: 0.8299000263214111\n",
      "Step: 8600  \tValid loss: 0.3612980544567108\n",
      "Step: 8700  \tTraining loss: 0.3662780225276947\n",
      "Step: 8700  \tTraining accuracy: 0.8299793601036072\n",
      "Step: 8700  \tValid loss: 0.36105895042419434\n",
      "Step: 8800  \tTraining loss: 0.36607199907302856\n",
      "Step: 8800  \tTraining accuracy: 0.8300575017929077\n",
      "Step: 8800  \tValid loss: 0.36083948612213135\n",
      "Step: 8900  \tTraining loss: 0.3657868504524231\n",
      "Step: 8900  \tTraining accuracy: 0.8301314115524292\n",
      "Step: 8900  \tValid loss: 0.36056965589523315\n",
      "Step: 9000  \tTraining loss: 0.3655441701412201\n",
      "Step: 9000  \tTraining accuracy: 0.8302018046379089\n",
      "Step: 9000  \tValid loss: 0.3602980375289917\n",
      "Step: 9100  \tTraining loss: 0.3653199076652527\n",
      "Step: 9100  \tTraining accuracy: 0.8302724361419678\n",
      "Step: 9100  \tValid loss: 0.3600746989250183\n",
      "Step: 9200  \tTraining loss: 0.3650999069213867\n",
      "Step: 9200  \tTraining accuracy: 0.8303409218788147\n",
      "Step: 9200  \tValid loss: 0.35984349250793457\n",
      "Step: 9300  \tTraining loss: 0.3648902475833893\n",
      "Step: 9300  \tTraining accuracy: 0.830410361289978\n",
      "Step: 9300  \tValid loss: 0.3596310615539551\n",
      "Step: 9400  \tTraining loss: 0.3646893799304962\n",
      "Step: 9400  \tTraining accuracy: 0.8304800987243652\n",
      "Step: 9400  \tValid loss: 0.35944241285324097\n",
      "Step: 9500  \tTraining loss: 0.3644978404045105\n",
      "Step: 9500  \tTraining accuracy: 0.8305495381355286\n",
      "Step: 9500  \tValid loss: 0.35925090312957764\n",
      "Step: 9600  \tTraining loss: 0.36431363224983215\n",
      "Step: 9600  \tTraining accuracy: 0.8306145668029785\n",
      "Step: 9600  \tValid loss: 0.3590734601020813\n",
      "Step: 9700  \tTraining loss: 0.3641306757926941\n",
      "Step: 9700  \tTraining accuracy: 0.8306754231452942\n",
      "Step: 9700  \tValid loss: 0.35888567566871643\n",
      "Step: 9800  \tTraining loss: 0.3639523684978485\n",
      "Step: 9800  \tTraining accuracy: 0.8307315707206726\n",
      "Step: 9800  \tValid loss: 0.35871759057044983\n",
      "Step: 9900  \tTraining loss: 0.3637797236442566\n",
      "Step: 9900  \tTraining accuracy: 0.8307899832725525\n",
      "Step: 9900  \tValid loss: 0.3585747480392456\n",
      "Step: 10000  \tTraining loss: 0.3636121153831482\n",
      "Step: 10000  \tTraining accuracy: 0.8308466672897339\n",
      "Step: 10000  \tValid loss: 0.3584163188934326\n",
      "Step: 10100  \tTraining loss: 0.36344805359840393\n",
      "Step: 10100  \tTraining accuracy: 0.8308994770050049\n",
      "Step: 10100  \tValid loss: 0.35827675461769104\n",
      "Step: 10200  \tTraining loss: 0.3632887601852417\n",
      "Step: 10200  \tTraining accuracy: 0.8309484720230103\n",
      "Step: 10200  \tValid loss: 0.3581392467021942\n",
      "Step: 10300  \tTraining loss: 0.36313173174858093\n",
      "Step: 10300  \tTraining accuracy: 0.8309975862503052\n",
      "Step: 10300  \tValid loss: 0.3580080270767212\n",
      "Step: 10400  \tTraining loss: 0.3629782199859619\n",
      "Step: 10400  \tTraining accuracy: 0.8310457468032837\n",
      "Step: 10400  \tValid loss: 0.35787585377693176\n",
      "Step: 10500  \tTraining loss: 0.3628278374671936\n",
      "Step: 10500  \tTraining accuracy: 0.8310919404029846\n",
      "Step: 10500  \tValid loss: 0.35774022340774536\n",
      "Step: 10600  \tTraining loss: 0.3626782298088074\n",
      "Step: 10600  \tTraining accuracy: 0.8311388492584229\n",
      "Step: 10600  \tValid loss: 0.3576105833053589\n",
      "Step: 10700  \tTraining loss: 0.36253198981285095\n",
      "Step: 10700  \tTraining accuracy: 0.8311880230903625\n",
      "Step: 10700  \tValid loss: 0.3574780821800232\n",
      "Step: 10800  \tTraining loss: 0.3623793125152588\n",
      "Step: 10800  \tTraining accuracy: 0.83123779296875\n",
      "Step: 10800  \tValid loss: 0.3573608994483948\n",
      "Step: 10900  \tTraining loss: 0.3622291386127472\n",
      "Step: 10900  \tTraining accuracy: 0.8312897682189941\n",
      "Step: 10900  \tValid loss: 0.35719814896583557\n",
      "Step: 11000  \tTraining loss: 0.36208242177963257\n",
      "Step: 11000  \tTraining accuracy: 0.8313432931900024\n",
      "Step: 11000  \tValid loss: 0.35704857110977173\n",
      "Step: 11100  \tTraining loss: 0.36193758249282837\n",
      "Step: 11100  \tTraining accuracy: 0.8313968777656555\n",
      "Step: 11100  \tValid loss: 0.3569168150424957\n",
      "Step: 11200  \tTraining loss: 0.3617972731590271\n",
      "Step: 11200  \tTraining accuracy: 0.8314470052719116\n",
      "Step: 11200  \tValid loss: 0.35677486658096313\n",
      "Step: 11300  \tTraining loss: 0.36166083812713623\n",
      "Step: 11300  \tTraining accuracy: 0.8314947485923767\n",
      "Step: 11300  \tValid loss: 0.3566216826438904\n",
      "Step: 11400  \tTraining loss: 0.36152204871177673\n",
      "Step: 11400  \tTraining accuracy: 0.8315441012382507\n",
      "Step: 11400  \tValid loss: 0.3565163016319275\n",
      "Step: 11500  \tTraining loss: 0.36138853430747986\n",
      "Step: 11500  \tTraining accuracy: 0.8315935730934143\n",
      "Step: 11500  \tValid loss: 0.3563457727432251\n",
      "Step: 11600  \tTraining loss: 0.3612547814846039\n",
      "Step: 11600  \tTraining accuracy: 0.831639289855957\n",
      "Step: 11600  \tValid loss: 0.3561975657939911\n",
      "Step: 11700  \tTraining loss: 0.36112287640571594\n",
      "Step: 11700  \tTraining accuracy: 0.8316847085952759\n",
      "Step: 11700  \tValid loss: 0.3560514748096466\n",
      "Step: 11800  \tTraining loss: 0.3609675467014313\n",
      "Step: 11800  \tTraining accuracy: 0.8317359685897827\n",
      "Step: 11800  \tValid loss: 0.35593315958976746\n",
      "Step: 11900  \tTraining loss: 0.36083120107650757\n",
      "Step: 11900  \tTraining accuracy: 0.8317919969558716\n",
      "Step: 11900  \tValid loss: 0.355785071849823\n",
      "Step: 12000  \tTraining loss: 0.3606988489627838\n",
      "Step: 12000  \tTraining accuracy: 0.8318549990653992\n",
      "Step: 12000  \tValid loss: 0.355623722076416\n",
      "Step: 12100  \tTraining loss: 0.3605673909187317\n",
      "Step: 12100  \tTraining accuracy: 0.8319183588027954\n",
      "Step: 12100  \tValid loss: 0.355482816696167\n",
      "Step: 12200  \tTraining loss: 0.360422819852829\n",
      "Step: 12200  \tTraining accuracy: 0.8319761157035828\n",
      "Step: 12200  \tValid loss: 0.3553263545036316\n",
      "Step: 12300  \tTraining loss: 0.36028826236724854\n",
      "Step: 12300  \tTraining accuracy: 0.8320314884185791\n",
      "Step: 12300  \tValid loss: 0.35516998171806335\n",
      "Step: 12400  \tTraining loss: 0.36015164852142334\n",
      "Step: 12400  \tTraining accuracy: 0.832085132598877\n",
      "Step: 12400  \tValid loss: 0.35503628849983215\n",
      "Step: 12500  \tTraining loss: 0.36002129316329956\n",
      "Step: 12500  \tTraining accuracy: 0.8321365714073181\n",
      "Step: 12500  \tValid loss: 0.3548624813556671\n",
      "Step: 12600  \tTraining loss: 0.3598928451538086\n",
      "Step: 12600  \tTraining accuracy: 0.8321849703788757\n",
      "Step: 12600  \tValid loss: 0.3546997010707855\n",
      "Step: 12700  \tTraining loss: 0.3597692847251892\n",
      "Step: 12700  \tTraining accuracy: 0.8322339057922363\n",
      "Step: 12700  \tValid loss: 0.3545476496219635\n",
      "Step: 12800  \tTraining loss: 0.35965070128440857\n",
      "Step: 12800  \tTraining accuracy: 0.8322798609733582\n",
      "Step: 12800  \tValid loss: 0.3543701171875\n",
      "Step: 12900  \tTraining loss: 0.3595326840877533\n",
      "Step: 12900  \tTraining accuracy: 0.8323264718055725\n",
      "Step: 12900  \tValid loss: 0.35423800349235535\n",
      "Step: 13000  \tTraining loss: 0.359418123960495\n",
      "Step: 13000  \tTraining accuracy: 0.8323701620101929\n",
      "Step: 13000  \tValid loss: 0.35407811403274536\n",
      "Step: 13100  \tTraining loss: 0.35930463671684265\n",
      "Step: 13100  \tTraining accuracy: 0.8324140310287476\n",
      "Step: 13100  \tValid loss: 0.3539328873157501\n",
      "Step: 13200  \tTraining loss: 0.359192430973053\n",
      "Step: 13200  \tTraining accuracy: 0.8324572443962097\n",
      "Step: 13200  \tValid loss: 0.35378220677375793\n",
      "Step: 13300  \tTraining loss: 0.3590831756591797\n",
      "Step: 13300  \tTraining accuracy: 0.8324977159500122\n",
      "Step: 13300  \tValid loss: 0.3536132872104645\n",
      "Step: 13400  \tTraining loss: 0.3589743673801422\n",
      "Step: 13400  \tTraining accuracy: 0.8325359225273132\n",
      "Step: 13400  \tValid loss: 0.35344937443733215\n",
      "Step: 13500  \tTraining loss: 0.35886624455451965\n",
      "Step: 13500  \tTraining accuracy: 0.832572340965271\n",
      "Step: 13500  \tValid loss: 0.35330039262771606\n",
      "Step: 13600  \tTraining loss: 0.3587574064731598\n",
      "Step: 13600  \tTraining accuracy: 0.8326089978218079\n",
      "Step: 13600  \tValid loss: 0.3531489372253418\n",
      "Step: 13700  \tTraining loss: 0.3586515188217163\n",
      "Step: 13700  \tTraining accuracy: 0.8326455354690552\n",
      "Step: 13700  \tValid loss: 0.352990061044693\n",
      "Step: 13800  \tTraining loss: 0.35854604840278625\n",
      "Step: 13800  \tTraining accuracy: 0.8326832056045532\n",
      "Step: 13800  \tValid loss: 0.3528345227241516\n",
      "Step: 13900  \tTraining loss: 0.35844138264656067\n",
      "Step: 13900  \tTraining accuracy: 0.832721471786499\n",
      "Step: 13900  \tValid loss: 0.3526891767978668\n",
      "Step: 14000  \tTraining loss: 0.3583386242389679\n",
      "Step: 14000  \tTraining accuracy: 0.8327608108520508\n",
      "Step: 14000  \tValid loss: 0.35252538323402405\n",
      "Step: 14100  \tTraining loss: 0.3582362234592438\n",
      "Step: 14100  \tTraining accuracy: 0.8328003883361816\n",
      "Step: 14100  \tValid loss: 0.35237500071525574\n",
      "Step: 14200  \tTraining loss: 0.3581348657608032\n",
      "Step: 14200  \tTraining accuracy: 0.8328409790992737\n",
      "Step: 14200  \tValid loss: 0.3522295355796814\n",
      "Step: 14300  \tTraining loss: 0.35803213715553284\n",
      "Step: 14300  \tTraining accuracy: 0.8328794240951538\n",
      "Step: 14300  \tValid loss: 0.35208016633987427\n",
      "Step: 14400  \tTraining loss: 0.3579273223876953\n",
      "Step: 14400  \tTraining accuracy: 0.832918107509613\n",
      "Step: 14400  \tValid loss: 0.35192257165908813\n",
      "Step: 14500  \tTraining loss: 0.35782405734062195\n",
      "Step: 14500  \tTraining accuracy: 0.8329566717147827\n",
      "Step: 14500  \tValid loss: 0.3517718017101288\n",
      "Step: 14600  \tTraining loss: 0.3577222228050232\n",
      "Step: 14600  \tTraining accuracy: 0.8329931497573853\n",
      "Step: 14600  \tValid loss: 0.3516136407852173\n",
      "Step: 14700  \tTraining loss: 0.3576194643974304\n",
      "Step: 14700  \tTraining accuracy: 0.8330298662185669\n",
      "Step: 14700  \tValid loss: 0.35148149728775024\n",
      "Step: 14800  \tTraining loss: 0.3575187623500824\n",
      "Step: 14800  \tTraining accuracy: 0.8330672383308411\n",
      "Step: 14800  \tValid loss: 0.35132384300231934\n",
      "Step: 14900  \tTraining loss: 0.35741907358169556\n",
      "Step: 14900  \tTraining accuracy: 0.8331082463264465\n",
      "Step: 14900  \tValid loss: 0.3511773645877838\n",
      "Step: 15000  \tTraining loss: 0.35731983184814453\n",
      "Step: 15000  \tTraining accuracy: 0.8331509232521057\n",
      "Step: 15000  \tValid loss: 0.35105401277542114\n",
      "Step: 15100  \tTraining loss: 0.35722193121910095\n",
      "Step: 15100  \tTraining accuracy: 0.8331937789916992\n",
      "Step: 15100  \tValid loss: 0.3509056866168976\n",
      "Step: 15200  \tTraining loss: 0.3571244478225708\n",
      "Step: 15200  \tTraining accuracy: 0.8332346081733704\n",
      "Step: 15200  \tValid loss: 0.3507607877254486\n",
      "Step: 15300  \tTraining loss: 0.35702717304229736\n",
      "Step: 15300  \tTraining accuracy: 0.8332741260528564\n",
      "Step: 15300  \tValid loss: 0.350626140832901\n",
      "Step: 15400  \tTraining loss: 0.3569319248199463\n",
      "Step: 15400  \tTraining accuracy: 0.8333120942115784\n",
      "Step: 15400  \tValid loss: 0.3504817485809326\n",
      "Step: 15500  \tTraining loss: 0.3568360209465027\n",
      "Step: 15500  \tTraining accuracy: 0.8333513140678406\n",
      "Step: 15500  \tValid loss: 0.35035622119903564\n",
      "Step: 15600  \tTraining loss: 0.35674360394477844\n",
      "Step: 15600  \tTraining accuracy: 0.8333882689476013\n",
      "Step: 15600  \tValid loss: 0.3502264618873596\n",
      "Step: 15700  \tTraining loss: 0.3566480875015259\n",
      "Step: 15700  \tTraining accuracy: 0.8334248065948486\n",
      "Step: 15700  \tValid loss: 0.35014501214027405\n",
      "Step: 15800  \tTraining loss: 0.35655704140663147\n",
      "Step: 15800  \tTraining accuracy: 0.8334583640098572\n",
      "Step: 15800  \tValid loss: 0.3500048816204071\n",
      "Step: 15900  \tTraining loss: 0.3564658761024475\n",
      "Step: 15900  \tTraining accuracy: 0.8334932327270508\n",
      "Step: 15900  \tValid loss: 0.34990301728248596\n",
      "Step: 16000  \tTraining loss: 0.35637643933296204\n",
      "Step: 16000  \tTraining accuracy: 0.833527684211731\n",
      "Step: 16000  \tValid loss: 0.3497893214225769\n",
      "Step: 16100  \tTraining loss: 0.3562867045402527\n",
      "Step: 16100  \tTraining accuracy: 0.8335634469985962\n",
      "Step: 16100  \tValid loss: 0.3496933877468109\n",
      "Step: 16200  \tTraining loss: 0.3561989367008209\n",
      "Step: 16200  \tTraining accuracy: 0.8336014747619629\n",
      "Step: 16200  \tValid loss: 0.34958091378211975\n",
      "Step: 16300  \tTraining loss: 0.3561096787452698\n",
      "Step: 16300  \tTraining accuracy: 0.8336373567581177\n",
      "Step: 16300  \tValid loss: 0.34949830174446106\n",
      "Step: 16400  \tTraining loss: 0.3560214936733246\n",
      "Step: 16400  \tTraining accuracy: 0.8336758613586426\n",
      "Step: 16400  \tValid loss: 0.34940624237060547\n",
      "Step: 16500  \tTraining loss: 0.35593417286872864\n",
      "Step: 16500  \tTraining accuracy: 0.833713948726654\n",
      "Step: 16500  \tValid loss: 0.34929755330085754\n",
      "Step: 16600  \tTraining loss: 0.35584524273872375\n",
      "Step: 16600  \tTraining accuracy: 0.8337525129318237\n",
      "Step: 16600  \tValid loss: 0.3492271304130554\n",
      "Step: 16700  \tTraining loss: 0.35575708746910095\n",
      "Step: 16700  \tTraining accuracy: 0.8337913155555725\n",
      "Step: 16700  \tValid loss: 0.3491387665271759\n",
      "Step: 16800  \tTraining loss: 0.3556680679321289\n",
      "Step: 16800  \tTraining accuracy: 0.8338276147842407\n",
      "Step: 16800  \tValid loss: 0.34906142950057983\n",
      "Step: 16900  \tTraining loss: 0.3555786609649658\n",
      "Step: 16900  \tTraining accuracy: 0.8338635563850403\n",
      "Step: 16900  \tValid loss: 0.3489883840084076\n",
      "Step: 17000  \tTraining loss: 0.35548943281173706\n",
      "Step: 17000  \tTraining accuracy: 0.8339003324508667\n",
      "Step: 17000  \tValid loss: 0.3488942086696625\n",
      "Step: 17100  \tTraining loss: 0.35539907217025757\n",
      "Step: 17100  \tTraining accuracy: 0.8339363932609558\n",
      "Step: 17100  \tValid loss: 0.3488229215145111\n",
      "Step: 17200  \tTraining loss: 0.3553107976913452\n",
      "Step: 17200  \tTraining accuracy: 0.8339710235595703\n",
      "Step: 17200  \tValid loss: 0.3487280011177063\n",
      "Step: 17300  \tTraining loss: 0.3552201986312866\n",
      "Step: 17300  \tTraining accuracy: 0.8340029716491699\n",
      "Step: 17300  \tValid loss: 0.3486692011356354\n",
      "Step: 17400  \tTraining loss: 0.3551309108734131\n",
      "Step: 17400  \tTraining accuracy: 0.8340330123901367\n",
      "Step: 17400  \tValid loss: 0.34863370656967163\n",
      "Step: 17500  \tTraining loss: 0.35504385828971863\n",
      "Step: 17500  \tTraining accuracy: 0.8340613842010498\n",
      "Step: 17500  \tValid loss: 0.34853309392929077\n",
      "Step: 17600  \tTraining loss: 0.3549577295780182\n",
      "Step: 17600  \tTraining accuracy: 0.834089457988739\n",
      "Step: 17600  \tValid loss: 0.34845075011253357\n",
      "Step: 17700  \tTraining loss: 0.35487231612205505\n",
      "Step: 17700  \tTraining accuracy: 0.8341162800788879\n",
      "Step: 17700  \tValid loss: 0.3483683168888092\n",
      "Step: 17800  \tTraining loss: 0.35478758811950684\n",
      "Step: 17800  \tTraining accuracy: 0.8341431021690369\n",
      "Step: 17800  \tValid loss: 0.34828412532806396\n",
      "Step: 17900  \tTraining loss: 0.3547031283378601\n",
      "Step: 17900  \tTraining accuracy: 0.8341705799102783\n",
      "Step: 17900  \tValid loss: 0.34820252656936646\n",
      "Step: 18000  \tTraining loss: 0.3546190559864044\n",
      "Step: 18000  \tTraining accuracy: 0.8341977000236511\n",
      "Step: 18000  \tValid loss: 0.348120778799057\n",
      "Step: 18100  \tTraining loss: 0.35453617572784424\n",
      "Step: 18100  \tTraining accuracy: 0.8342257738113403\n",
      "Step: 18100  \tValid loss: 0.34803223609924316\n",
      "Step: 18200  \tTraining loss: 0.35445258021354675\n",
      "Step: 18200  \tTraining accuracy: 0.8342544436454773\n",
      "Step: 18200  \tValid loss: 0.34795236587524414\n",
      "Step: 18300  \tTraining loss: 0.3543696701526642\n",
      "Step: 18300  \tTraining accuracy: 0.8342819213867188\n",
      "Step: 18300  \tValid loss: 0.34787726402282715\n",
      "Step: 18400  \tTraining loss: 0.35428720712661743\n",
      "Step: 18400  \tTraining accuracy: 0.834309995174408\n",
      "Step: 18400  \tValid loss: 0.3478015959262848\n",
      "Step: 18500  \tTraining loss: 0.3542039394378662\n",
      "Step: 18500  \tTraining accuracy: 0.8343377709388733\n",
      "Step: 18500  \tValid loss: 0.3477289080619812\n",
      "Step: 18600  \tTraining loss: 0.35412195324897766\n",
      "Step: 18600  \tTraining accuracy: 0.8343652486801147\n",
      "Step: 18600  \tValid loss: 0.3476583659648895\n",
      "Step: 18700  \tTraining loss: 0.35403814911842346\n",
      "Step: 18700  \tTraining accuracy: 0.834391176700592\n",
      "Step: 18700  \tValid loss: 0.34759408235549927\n",
      "Step: 18800  \tTraining loss: 0.35395705699920654\n",
      "Step: 18800  \tTraining accuracy: 0.8344172239303589\n",
      "Step: 18800  \tValid loss: 0.34749674797058105\n",
      "Step: 18900  \tTraining loss: 0.35387367010116577\n",
      "Step: 18900  \tTraining accuracy: 0.8344441056251526\n",
      "Step: 18900  \tValid loss: 0.3474295735359192\n",
      "Step: 19000  \tTraining loss: 0.3537900745868683\n",
      "Step: 19000  \tTraining accuracy: 0.8344689607620239\n",
      "Step: 19000  \tValid loss: 0.34734901785850525\n",
      "Step: 19100  \tTraining loss: 0.35370755195617676\n",
      "Step: 19100  \tTraining accuracy: 0.8344950079917908\n",
      "Step: 19100  \tValid loss: 0.34726986289024353\n",
      "Step: 19200  \tTraining loss: 0.3536243736743927\n",
      "Step: 19200  \tTraining accuracy: 0.8345202207565308\n",
      "Step: 19200  \tValid loss: 0.34720703959465027\n",
      "Step: 19300  \tTraining loss: 0.3535382151603699\n",
      "Step: 19300  \tTraining accuracy: 0.8345434069633484\n",
      "Step: 19300  \tValid loss: 0.347160279750824\n",
      "Step: 19400  \tTraining loss: 0.3534514605998993\n",
      "Step: 19400  \tTraining accuracy: 0.8345658183097839\n",
      "Step: 19400  \tValid loss: 0.3470810353755951\n",
      "Step: 19500  \tTraining loss: 0.35335758328437805\n",
      "Step: 19500  \tTraining accuracy: 0.8345888257026672\n",
      "Step: 19500  \tValid loss: 0.34701213240623474\n",
      "Step: 19600  \tTraining loss: 0.3532596230506897\n",
      "Step: 19600  \tTraining accuracy: 0.8346139192581177\n",
      "Step: 19600  \tValid loss: 0.3469667136669159\n",
      "Step: 19700  \tTraining loss: 0.35314926505088806\n",
      "Step: 19700  \tTraining accuracy: 0.8346356153488159\n",
      "Step: 19700  \tValid loss: 0.3469419777393341\n",
      "Step: 19800  \tTraining loss: 0.3530362546443939\n",
      "Step: 19800  \tTraining accuracy: 0.8346554040908813\n",
      "Step: 19800  \tValid loss: 0.3469141721725464\n",
      "Step: 19900  \tTraining loss: 0.3529300391674042\n",
      "Step: 19900  \tTraining accuracy: 0.8346752524375916\n",
      "Step: 19900  \tValid loss: 0.3468714952468872\n",
      "Step: 20000  \tTraining loss: 0.3528297543525696\n",
      "Step: 20000  \tTraining accuracy: 0.8346946239471436\n",
      "Step: 20000  \tValid loss: 0.3468059003353119\n",
      "Step: 20100  \tTraining loss: 0.35273441672325134\n",
      "Step: 20100  \tTraining accuracy: 0.8347168564796448\n",
      "Step: 20100  \tValid loss: 0.346754252910614\n",
      "Step: 20200  \tTraining loss: 0.35264286398887634\n",
      "Step: 20200  \tTraining accuracy: 0.8347394466400146\n",
      "Step: 20200  \tValid loss: 0.34668174386024475\n",
      "Step: 20300  \tTraining loss: 0.35255295038223267\n",
      "Step: 20300  \tTraining accuracy: 0.834764301776886\n",
      "Step: 20300  \tValid loss: 0.346623033285141\n",
      "Step: 20400  \tTraining loss: 0.3524664640426636\n",
      "Step: 20400  \tTraining accuracy: 0.8347877860069275\n",
      "Step: 20400  \tValid loss: 0.3465510606765747\n",
      "Step: 20500  \tTraining loss: 0.35238105058670044\n",
      "Step: 20500  \tTraining accuracy: 0.8348091244697571\n",
      "Step: 20500  \tValid loss: 0.34649163484573364\n",
      "Step: 20600  \tTraining loss: 0.35229751467704773\n",
      "Step: 20600  \tTraining accuracy: 0.8348286151885986\n",
      "Step: 20600  \tValid loss: 0.3464109003543854\n",
      "Step: 20700  \tTraining loss: 0.352213978767395\n",
      "Step: 20700  \tTraining accuracy: 0.8348498344421387\n",
      "Step: 20700  \tValid loss: 0.34636586904525757\n",
      "Step: 20800  \tTraining loss: 0.35213255882263184\n",
      "Step: 20800  \tTraining accuracy: 0.8348695039749146\n",
      "Step: 20800  \tValid loss: 0.3463001847267151\n",
      "Step: 20900  \tTraining loss: 0.35205215215682983\n",
      "Step: 20900  \tTraining accuracy: 0.834889829158783\n",
      "Step: 20900  \tValid loss: 0.34624728560447693\n",
      "Step: 21000  \tTraining loss: 0.35196414589881897\n",
      "Step: 21000  \tTraining accuracy: 0.8349130749702454\n",
      "Step: 21000  \tValid loss: 0.3461301028728485\n",
      "Step: 21100  \tTraining loss: 0.3518705666065216\n",
      "Step: 21100  \tTraining accuracy: 0.8349356055259705\n",
      "Step: 21100  \tValid loss: 0.3460308015346527\n",
      "Step: 21200  \tTraining loss: 0.35177919268608093\n",
      "Step: 21200  \tTraining accuracy: 0.8349581956863403\n",
      "Step: 21200  \tValid loss: 0.3459266424179077\n",
      "Step: 21300  \tTraining loss: 0.35166406631469727\n",
      "Step: 21300  \tTraining accuracy: 0.8349787592887878\n",
      "Step: 21300  \tValid loss: 0.3458271026611328\n",
      "Step: 21400  \tTraining loss: 0.35152143239974976\n",
      "Step: 21400  \tTraining accuracy: 0.8349990844726562\n",
      "Step: 21400  \tValid loss: 0.345670223236084\n",
      "Step: 21500  \tTraining loss: 0.3513985276222229\n",
      "Step: 21500  \tTraining accuracy: 0.8350194692611694\n",
      "Step: 21500  \tValid loss: 0.3455027937889099\n",
      "Step: 21600  \tTraining loss: 0.3512793481349945\n",
      "Step: 21600  \tTraining accuracy: 0.8350417613983154\n",
      "Step: 21600  \tValid loss: 0.34531623125076294\n",
      "Step: 21700  \tTraining loss: 0.35117894411087036\n",
      "Step: 21700  \tTraining accuracy: 0.8350676894187927\n",
      "Step: 21700  \tValid loss: 0.34523528814315796\n",
      "Step: 21800  \tTraining loss: 0.3510800898075104\n",
      "Step: 21800  \tTraining accuracy: 0.8350915908813477\n",
      "Step: 21800  \tValid loss: 0.34518811106681824\n",
      "Step: 21900  \tTraining loss: 0.3509417176246643\n",
      "Step: 21900  \tTraining accuracy: 0.8351143002510071\n",
      "Step: 21900  \tValid loss: 0.34509626030921936\n",
      "Step: 22000  \tTraining loss: 0.35082873702049255\n",
      "Step: 22000  \tTraining accuracy: 0.8351370096206665\n",
      "Step: 22000  \tValid loss: 0.3450165390968323\n",
      "Step: 22100  \tTraining loss: 0.35072779655456543\n",
      "Step: 22100  \tTraining accuracy: 0.8351594805717468\n",
      "Step: 22100  \tValid loss: 0.3449919521808624\n",
      "Step: 22200  \tTraining loss: 0.35063475370407104\n",
      "Step: 22200  \tTraining accuracy: 0.8351810574531555\n",
      "Step: 22200  \tValid loss: 0.34493693709373474\n",
      "Step: 22300  \tTraining loss: 0.3505462110042572\n",
      "Step: 22300  \tTraining accuracy: 0.8352019190788269\n",
      "Step: 22300  \tValid loss: 0.3449096381664276\n",
      "Step: 22400  \tTraining loss: 0.35046032071113586\n",
      "Step: 22400  \tTraining accuracy: 0.835222601890564\n",
      "Step: 22400  \tValid loss: 0.3448878526687622\n",
      "Step: 22500  \tTraining loss: 0.35037705302238464\n",
      "Step: 22500  \tTraining accuracy: 0.8352433443069458\n",
      "Step: 22500  \tValid loss: 0.3448646664619446\n",
      "Step: 22600  \tTraining loss: 0.3502886891365051\n",
      "Step: 22600  \tTraining accuracy: 0.83526211977005\n",
      "Step: 22600  \tValid loss: 0.344854474067688\n",
      "Step: 22700  \tTraining loss: 0.3502022624015808\n",
      "Step: 22700  \tTraining accuracy: 0.8352800607681274\n",
      "Step: 22700  \tValid loss: 0.34484362602233887\n",
      "Step: 22800  \tTraining loss: 0.35011962056159973\n",
      "Step: 22800  \tTraining accuracy: 0.8352973461151123\n",
      "Step: 22800  \tValid loss: 0.3448372781276703\n",
      "Step: 22900  \tTraining loss: 0.35003817081451416\n",
      "Step: 22900  \tTraining accuracy: 0.8353159427642822\n",
      "Step: 22900  \tValid loss: 0.34478047490119934\n",
      "Step: 23000  \tTraining loss: 0.3499530255794525\n",
      "Step: 23000  \tTraining accuracy: 0.8353345990180969\n",
      "Step: 23000  \tValid loss: 0.3447502851486206\n",
      "Step: 23100  \tTraining loss: 0.34984228014945984\n",
      "Step: 23100  \tTraining accuracy: 0.8353525996208191\n",
      "Step: 23100  \tValid loss: 0.3447253108024597\n",
      "Step: 23200  \tTraining loss: 0.3497297763824463\n",
      "Step: 23200  \tTraining accuracy: 0.8353719115257263\n",
      "Step: 23200  \tValid loss: 0.34471848607063293\n",
      "Step: 23300  \tTraining loss: 0.3496318757534027\n",
      "Step: 23300  \tTraining accuracy: 0.8353939056396484\n",
      "Step: 23300  \tValid loss: 0.3447350263595581\n",
      "Step: 23400  \tTraining loss: 0.34954604506492615\n",
      "Step: 23400  \tTraining accuracy: 0.8354161977767944\n",
      "Step: 23400  \tValid loss: 0.3446940779685974\n",
      "Step: 23500  \tTraining loss: 0.3494599461555481\n",
      "Step: 23500  \tTraining accuracy: 0.8354390263557434\n",
      "Step: 23500  \tValid loss: 0.3446701467037201\n",
      "Step: 23600  \tTraining loss: 0.349378377199173\n",
      "Step: 23600  \tTraining accuracy: 0.8354626297950745\n",
      "Step: 23600  \tValid loss: 0.344635009765625\n",
      "Step: 23700  \tTraining loss: 0.3492971360683441\n",
      "Step: 23700  \tTraining accuracy: 0.8354857563972473\n",
      "Step: 23700  \tValid loss: 0.3445984423160553\n",
      "Step: 23800  \tTraining loss: 0.3492150604724884\n",
      "Step: 23800  \tTraining accuracy: 0.8355103135108948\n",
      "Step: 23800  \tValid loss: 0.3445669114589691\n",
      "Step: 23900  \tTraining loss: 0.34913769364356995\n",
      "Step: 23900  \tTraining accuracy: 0.8355342149734497\n",
      "Step: 23900  \tValid loss: 0.3445107340812683\n",
      "Step: 24000  \tTraining loss: 0.3490588963031769\n",
      "Step: 24000  \tTraining accuracy: 0.8355572819709778\n",
      "Step: 24000  \tValid loss: 0.3444922864437103\n",
      "Step: 24100  \tTraining loss: 0.34898266196250916\n",
      "Step: 24100  \tTraining accuracy: 0.8355796337127686\n",
      "Step: 24100  \tValid loss: 0.3444564938545227\n",
      "Step: 24200  \tTraining loss: 0.34890681505203247\n",
      "Step: 24200  \tTraining accuracy: 0.8356019854545593\n",
      "Step: 24200  \tValid loss: 0.3444347381591797\n",
      "Step: 24300  \tTraining loss: 0.34883251786231995\n",
      "Step: 24300  \tTraining accuracy: 0.8356269598007202\n",
      "Step: 24300  \tValid loss: 0.3443892300128937\n",
      "Step: 24400  \tTraining loss: 0.34876105189323425\n",
      "Step: 24400  \tTraining accuracy: 0.8356533646583557\n",
      "Step: 24400  \tValid loss: 0.34433624148368835\n",
      "Step: 24500  \tTraining loss: 0.34868669509887695\n",
      "Step: 24500  \tTraining accuracy: 0.8356810808181763\n",
      "Step: 24500  \tValid loss: 0.3443243205547333\n",
      "Step: 24600  \tTraining loss: 0.3486148416996002\n",
      "Step: 24600  \tTraining accuracy: 0.835709273815155\n",
      "Step: 24600  \tValid loss: 0.3442970812320709\n",
      "Step: 24700  \tTraining loss: 0.34854286909103394\n",
      "Step: 24700  \tTraining accuracy: 0.835737943649292\n",
      "Step: 24700  \tValid loss: 0.344291627407074\n",
      "Step: 24800  \tTraining loss: 0.34847256541252136\n",
      "Step: 24800  \tTraining accuracy: 0.8357676863670349\n",
      "Step: 24800  \tValid loss: 0.3442598283290863\n",
      "Step: 24900  \tTraining loss: 0.34840402007102966\n",
      "Step: 24900  \tTraining accuracy: 0.835797905921936\n",
      "Step: 24900  \tValid loss: 0.3442140519618988\n",
      "Step: 25000  \tTraining loss: 0.34833478927612305\n",
      "Step: 25000  \tTraining accuracy: 0.8358284831047058\n",
      "Step: 25000  \tValid loss: 0.3441896438598633\n",
      "Step: 25100  \tTraining loss: 0.3482666015625\n",
      "Step: 25100  \tTraining accuracy: 0.8358586430549622\n",
      "Step: 25100  \tValid loss: 0.34417808055877686\n",
      "Step: 25200  \tTraining loss: 0.3481999933719635\n",
      "Step: 25200  \tTraining accuracy: 0.8358888030052185\n",
      "Step: 25200  \tValid loss: 0.34414711594581604\n",
      "Step: 25300  \tTraining loss: 0.34813249111175537\n",
      "Step: 25300  \tTraining accuracy: 0.8359202146530151\n",
      "Step: 25300  \tValid loss: 0.3441234827041626\n",
      "Step: 25400  \tTraining loss: 0.3480656147003174\n",
      "Step: 25400  \tTraining accuracy: 0.8359513878822327\n",
      "Step: 25400  \tValid loss: 0.34409815073013306\n",
      "Step: 25500  \tTraining loss: 0.34799930453300476\n",
      "Step: 25500  \tTraining accuracy: 0.8359812498092651\n",
      "Step: 25500  \tValid loss: 0.34409090876579285\n",
      "Step: 25600  \tTraining loss: 0.34793519973754883\n",
      "Step: 25600  \tTraining accuracy: 0.8360117673873901\n",
      "Step: 25600  \tValid loss: 0.3440678119659424\n",
      "Step: 25700  \tTraining loss: 0.3478712737560272\n",
      "Step: 25700  \tTraining accuracy: 0.8360430598258972\n",
      "Step: 25700  \tValid loss: 0.34406372904777527\n",
      "Step: 25800  \tTraining loss: 0.34780940413475037\n",
      "Step: 25800  \tTraining accuracy: 0.8360745906829834\n",
      "Step: 25800  \tValid loss: 0.34403324127197266\n",
      "Step: 25900  \tTraining loss: 0.3477497100830078\n",
      "Step: 25900  \tTraining accuracy: 0.8361058831214905\n",
      "Step: 25900  \tValid loss: 0.3440011441707611\n",
      "Step: 26000  \tTraining loss: 0.3476894795894623\n",
      "Step: 26000  \tTraining accuracy: 0.8361375331878662\n",
      "Step: 26000  \tValid loss: 0.34398284554481506\n",
      "Step: 26100  \tTraining loss: 0.3476305305957794\n",
      "Step: 26100  \tTraining accuracy: 0.8361698389053345\n",
      "Step: 26100  \tValid loss: 0.34396156668663025\n",
      "Step: 26200  \tTraining loss: 0.3475714325904846\n",
      "Step: 26200  \tTraining accuracy: 0.836202085018158\n",
      "Step: 26200  \tValid loss: 0.3439481556415558\n",
      "Step: 26300  \tTraining loss: 0.34751713275909424\n",
      "Step: 26300  \tTraining accuracy: 0.8362343311309814\n",
      "Step: 26300  \tValid loss: 0.3438946306705475\n",
      "Step: 26400  \tTraining loss: 0.34746068716049194\n",
      "Step: 26400  \tTraining accuracy: 0.8362652659416199\n",
      "Step: 26400  \tValid loss: 0.3438824415206909\n",
      "Step: 26500  \tTraining loss: 0.34740564227104187\n",
      "Step: 26500  \tTraining accuracy: 0.8362961411476135\n",
      "Step: 26500  \tValid loss: 0.34387439489364624\n",
      "Step: 26600  \tTraining loss: 0.3473537862300873\n",
      "Step: 26600  \tTraining accuracy: 0.8363276720046997\n",
      "Step: 26600  \tValid loss: 0.34383371472358704\n",
      "Step: 26700  \tTraining loss: 0.3473007380962372\n",
      "Step: 26700  \tTraining accuracy: 0.8363593220710754\n",
      "Step: 26700  \tValid loss: 0.3438290059566498\n",
      "Step: 26800  \tTraining loss: 0.34724828600883484\n",
      "Step: 26800  \tTraining accuracy: 0.8363903760910034\n",
      "Step: 26800  \tValid loss: 0.3438117206096649\n",
      "Step: 26900  \tTraining loss: 0.3471980094909668\n",
      "Step: 26900  \tTraining accuracy: 0.8364211916923523\n",
      "Step: 26900  \tValid loss: 0.34379497170448303\n",
      "Step: 27000  \tTraining loss: 0.3471483588218689\n",
      "Step: 27000  \tTraining accuracy: 0.8364517688751221\n",
      "Step: 27000  \tValid loss: 0.34377098083496094\n",
      "Step: 27100  \tTraining loss: 0.34709882736206055\n",
      "Step: 27100  \tTraining accuracy: 0.8364816904067993\n",
      "Step: 27100  \tValid loss: 0.3437511920928955\n",
      "Step: 27200  \tTraining loss: 0.34705016016960144\n",
      "Step: 27200  \tTraining accuracy: 0.8365116119384766\n",
      "Step: 27200  \tValid loss: 0.3437348008155823\n",
      "Step: 27300  \tTraining loss: 0.3470015227794647\n",
      "Step: 27300  \tTraining accuracy: 0.8365408778190613\n",
      "Step: 27300  \tValid loss: 0.3437258005142212\n",
      "Step: 27400  \tTraining loss: 0.3469539284706116\n",
      "Step: 27400  \tTraining accuracy: 0.8365693688392639\n",
      "Step: 27400  \tValid loss: 0.3437170386314392\n",
      "Step: 27500  \tTraining loss: 0.34690791368484497\n",
      "Step: 27500  \tTraining accuracy: 0.8365976214408875\n",
      "Step: 27500  \tValid loss: 0.343697726726532\n",
      "Step: 27600  \tTraining loss: 0.3468612730503082\n",
      "Step: 27600  \tTraining accuracy: 0.8366256952285767\n",
      "Step: 27600  \tValid loss: 0.34368571639060974\n",
      "Step: 27700  \tTraining loss: 0.3468157649040222\n",
      "Step: 27700  \tTraining accuracy: 0.8366535305976868\n",
      "Step: 27700  \tValid loss: 0.34366917610168457\n",
      "Step: 27800  \tTraining loss: 0.3467704951763153\n",
      "Step: 27800  \tTraining accuracy: 0.8366811871528625\n",
      "Step: 27800  \tValid loss: 0.343658983707428\n",
      "Step: 27900  \tTraining loss: 0.3467264473438263\n",
      "Step: 27900  \tTraining accuracy: 0.836708664894104\n",
      "Step: 27900  \tValid loss: 0.34363889694213867\n",
      "Step: 28000  \tTraining loss: 0.34668195247650146\n",
      "Step: 28000  \tTraining accuracy: 0.8367359042167664\n",
      "Step: 28000  \tValid loss: 0.3436376750469208\n",
      "Step: 28100  \tTraining loss: 0.3466382324695587\n",
      "Step: 28100  \tTraining accuracy: 0.8367629647254944\n",
      "Step: 28100  \tValid loss: 0.34362611174583435\n",
      "Step: 28200  \tTraining loss: 0.3465959131717682\n",
      "Step: 28200  \tTraining accuracy: 0.8367904424667358\n",
      "Step: 28200  \tValid loss: 0.3436061143875122\n",
      "Step: 28300  \tTraining loss: 0.34655359387397766\n",
      "Step: 28300  \tTraining accuracy: 0.8368173241615295\n",
      "Step: 28300  \tValid loss: 0.34359458088874817\n",
      "Step: 28400  \tTraining loss: 0.3465111255645752\n",
      "Step: 28400  \tTraining accuracy: 0.8368428349494934\n",
      "Step: 28400  \tValid loss: 0.34358251094818115\n",
      "Step: 28500  \tTraining loss: 0.34646934270858765\n",
      "Step: 28500  \tTraining accuracy: 0.8368675708770752\n",
      "Step: 28500  \tValid loss: 0.34358271956443787\n",
      "Step: 28600  \tTraining loss: 0.3464282155036926\n",
      "Step: 28600  \tTraining accuracy: 0.8368925452232361\n",
      "Step: 28600  \tValid loss: 0.34356388449668884\n",
      "Step: 28700  \tTraining loss: 0.3463872969150543\n",
      "Step: 28700  \tTraining accuracy: 0.8369172811508179\n",
      "Step: 28700  \tValid loss: 0.3435591161251068\n",
      "Step: 28800  \tTraining loss: 0.34634798765182495\n",
      "Step: 28800  \tTraining accuracy: 0.8369415402412415\n",
      "Step: 28800  \tValid loss: 0.34353333711624146\n",
      "Step: 28900  \tTraining loss: 0.3463080823421478\n",
      "Step: 28900  \tTraining accuracy: 0.8369659781455994\n",
      "Step: 28900  \tValid loss: 0.34352895617485046\n",
      "Step: 29000  \tTraining loss: 0.34626859426498413\n",
      "Step: 29000  \tTraining accuracy: 0.836990237236023\n",
      "Step: 29000  \tValid loss: 0.34351733326911926\n",
      "Step: 29100  \tTraining loss: 0.3462289571762085\n",
      "Step: 29100  \tTraining accuracy: 0.837014377117157\n",
      "Step: 29100  \tValid loss: 0.3435104191303253\n",
      "Step: 29200  \tTraining loss: 0.34619036316871643\n",
      "Step: 29200  \tTraining accuracy: 0.8370382785797119\n",
      "Step: 29200  \tValid loss: 0.34351077675819397\n",
      "Step: 29300  \tTraining loss: 0.34615179896354675\n",
      "Step: 29300  \tTraining accuracy: 0.8370617032051086\n",
      "Step: 29300  \tValid loss: 0.343502014875412\n",
      "Step: 29400  \tTraining loss: 0.34611400961875916\n",
      "Step: 29400  \tTraining accuracy: 0.8370847702026367\n",
      "Step: 29400  \tValid loss: 0.34349575638771057\n",
      "Step: 29500  \tTraining loss: 0.346076637506485\n",
      "Step: 29500  \tTraining accuracy: 0.8371068835258484\n",
      "Step: 29500  \tValid loss: 0.3434823453426361\n",
      "Step: 29600  \tTraining loss: 0.34604009985923767\n",
      "Step: 29600  \tTraining accuracy: 0.8371288776397705\n",
      "Step: 29600  \tValid loss: 0.3434615731239319\n",
      "Step: 29700  \tTraining loss: 0.34600168466567993\n",
      "Step: 29700  \tTraining accuracy: 0.8371501564979553\n",
      "Step: 29700  \tValid loss: 0.34346696734428406\n",
      "Step: 29800  \tTraining loss: 0.34595996141433716\n",
      "Step: 29800  \tTraining accuracy: 0.8371712565422058\n",
      "Step: 29800  \tValid loss: 0.34342578053474426\n",
      "Step: 29900  \tTraining loss: 0.3459133207798004\n",
      "Step: 29900  \tTraining accuracy: 0.8371918797492981\n",
      "Step: 29900  \tValid loss: 0.3434927761554718\n",
      "Step: 30000  \tTraining loss: 0.3458712100982666\n",
      "Step: 30000  \tTraining accuracy: 0.8372120261192322\n",
      "Step: 30000  \tValid loss: 0.34353864192962646\n",
      "Step: 30100  \tTraining loss: 0.3458322286605835\n",
      "Step: 30100  \tTraining accuracy: 0.8372316360473633\n",
      "Step: 30100  \tValid loss: 0.3435364365577698\n",
      "Step: 30200  \tTraining loss: 0.34579503536224365\n",
      "Step: 30200  \tTraining accuracy: 0.8372510671615601\n",
      "Step: 30200  \tValid loss: 0.3435439169406891\n",
      "Step: 30300  \tTraining loss: 0.3457583487033844\n",
      "Step: 30300  \tTraining accuracy: 0.8372704386711121\n",
      "Step: 30300  \tValid loss: 0.3435397148132324\n",
      "Step: 30400  \tTraining loss: 0.3457220196723938\n",
      "Step: 30400  \tTraining accuracy: 0.8372896313667297\n",
      "Step: 30400  \tValid loss: 0.3435414135456085\n",
      "Step: 30500  \tTraining loss: 0.34568649530410767\n",
      "Step: 30500  \tTraining accuracy: 0.8373083472251892\n",
      "Step: 30500  \tValid loss: 0.3435295820236206\n",
      "Step: 30600  \tTraining loss: 0.3456510603427887\n",
      "Step: 30600  \tTraining accuracy: 0.8373270034790039\n",
      "Step: 30600  \tValid loss: 0.34352487325668335\n",
      "Step: 30700  \tTraining loss: 0.3456156551837921\n",
      "Step: 30700  \tTraining accuracy: 0.8373454809188843\n",
      "Step: 30700  \tValid loss: 0.34352099895477295\n",
      "Step: 30800  \tTraining loss: 0.3455808162689209\n",
      "Step: 30800  \tTraining accuracy: 0.8373638391494751\n",
      "Step: 30800  \tValid loss: 0.34351634979248047\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8373817\n",
      "Precision: 0.87125295\n",
      "Recall: 0.8425443\n",
      "F1 score: 0.82335436\n",
      "AUC: 0.850116\n",
      "   accuracy  precision    recall  f1_score       auc     loss  accuracy_val  \\\n",
      "0  0.837382   0.871253  0.842544  0.823354  0.850116  0.34558      0.837362   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.343426       0.837351   0.411147      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  30800.0  \n",
      "15\n",
      "(4350, 8)\n",
      "(4350, 1)\n",
      "(2400, 8)\n",
      "(2400, 1)\n",
      "(1950, 8)\n",
      "(1950, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5739508271217346\n",
      "Step: 100  \tTraining accuracy: 0.7085057497024536\n",
      "Step: 100  \tValid loss: 0.5640411376953125\n",
      "Step: 200  \tTraining loss: 0.5404407978057861\n",
      "Step: 200  \tTraining accuracy: 0.7116475105285645\n",
      "Step: 200  \tValid loss: 0.5392428636550903\n",
      "Step: 300  \tTraining loss: 0.5283986330032349\n",
      "Step: 300  \tTraining accuracy: 0.7149885296821594\n",
      "Step: 300  \tValid loss: 0.5317522287368774\n",
      "Step: 400  \tTraining loss: 0.5102857351303101\n",
      "Step: 400  \tTraining accuracy: 0.7220689654350281\n",
      "Step: 400  \tValid loss: 0.5150642991065979\n",
      "Step: 500  \tTraining loss: 0.4811779260635376\n",
      "Step: 500  \tTraining accuracy: 0.7300894260406494\n",
      "Step: 500  \tValid loss: 0.48197585344314575\n",
      "Step: 600  \tTraining loss: 0.4558405876159668\n",
      "Step: 600  \tTraining accuracy: 0.7374921441078186\n",
      "Step: 600  \tValid loss: 0.4522576332092285\n",
      "Step: 700  \tTraining loss: 0.4391366243362427\n",
      "Step: 700  \tTraining accuracy: 0.7447568774223328\n",
      "Step: 700  \tValid loss: 0.4312981069087982\n",
      "Step: 800  \tTraining loss: 0.4304303526878357\n",
      "Step: 800  \tTraining accuracy: 0.7508965730667114\n",
      "Step: 800  \tValid loss: 0.4193468987941742\n",
      "Step: 900  \tTraining loss: 0.42478427290916443\n",
      "Step: 900  \tTraining accuracy: 0.7559026479721069\n",
      "Step: 900  \tValid loss: 0.4119400680065155\n",
      "Step: 1000  \tTraining loss: 0.41980570554733276\n",
      "Step: 1000  \tTraining accuracy: 0.7600241899490356\n",
      "Step: 1000  \tValid loss: 0.40638282895088196\n",
      "Step: 1100  \tTraining loss: 0.4159799814224243\n",
      "Step: 1100  \tTraining accuracy: 0.7631417512893677\n",
      "Step: 1100  \tValid loss: 0.4024009108543396\n",
      "Step: 1200  \tTraining loss: 0.412833034992218\n",
      "Step: 1200  \tTraining accuracy: 0.7656771540641785\n",
      "Step: 1200  \tValid loss: 0.39940235018730164\n",
      "Step: 1300  \tTraining loss: 0.4097841680049896\n",
      "Step: 1300  \tTraining accuracy: 0.7680827379226685\n",
      "Step: 1300  \tValid loss: 0.3969340920448303\n",
      "Step: 1400  \tTraining loss: 0.4064350724220276\n",
      "Step: 1400  \tTraining accuracy: 0.7703533172607422\n",
      "Step: 1400  \tValid loss: 0.3938112258911133\n",
      "Step: 1500  \tTraining loss: 0.40383511781692505\n",
      "Step: 1500  \tTraining accuracy: 0.772437572479248\n",
      "Step: 1500  \tValid loss: 0.3910441994667053\n",
      "Step: 1600  \tTraining loss: 0.4020182192325592\n",
      "Step: 1600  \tTraining accuracy: 0.7743566632270813\n",
      "Step: 1600  \tValid loss: 0.38911694288253784\n",
      "Step: 1700  \tTraining loss: 0.39959877729415894\n",
      "Step: 1700  \tTraining accuracy: 0.7761616110801697\n",
      "Step: 1700  \tValid loss: 0.3882216811180115\n",
      "Step: 1800  \tTraining loss: 0.3983807861804962\n",
      "Step: 1800  \tTraining accuracy: 0.777642011642456\n",
      "Step: 1800  \tValid loss: 0.3867328464984894\n",
      "Step: 1900  \tTraining loss: 0.39746609330177307\n",
      "Step: 1900  \tTraining accuracy: 0.7789686322212219\n",
      "Step: 1900  \tValid loss: 0.38607004284858704\n",
      "Step: 2000  \tTraining loss: 0.3966940641403198\n",
      "Step: 2000  \tTraining accuracy: 0.780259370803833\n",
      "Step: 2000  \tValid loss: 0.3855153024196625\n",
      "Step: 2100  \tTraining loss: 0.39601948857307434\n",
      "Step: 2100  \tTraining accuracy: 0.7814017534255981\n",
      "Step: 2100  \tValid loss: 0.38508597016334534\n",
      "Step: 2200  \tTraining loss: 0.3954189121723175\n",
      "Step: 2200  \tTraining accuracy: 0.782432496547699\n",
      "Step: 2200  \tValid loss: 0.3846868872642517\n",
      "Step: 2300  \tTraining loss: 0.3948584496974945\n",
      "Step: 2300  \tTraining accuracy: 0.7834023237228394\n",
      "Step: 2300  \tValid loss: 0.3842799961566925\n",
      "Step: 2400  \tTraining loss: 0.394320547580719\n",
      "Step: 2400  \tTraining accuracy: 0.7842455506324768\n",
      "Step: 2400  \tValid loss: 0.3839176297187805\n",
      "Step: 2500  \tTraining loss: 0.3937784731388092\n",
      "Step: 2500  \tTraining accuracy: 0.7850340008735657\n",
      "Step: 2500  \tValid loss: 0.38367024064064026\n",
      "Step: 2600  \tTraining loss: 0.39326053857803345\n",
      "Step: 2600  \tTraining accuracy: 0.7857561707496643\n",
      "Step: 2600  \tValid loss: 0.3834506571292877\n",
      "Step: 2700  \tTraining loss: 0.39274436235427856\n",
      "Step: 2700  \tTraining accuracy: 0.7863543629646301\n",
      "Step: 2700  \tValid loss: 0.3833024203777313\n",
      "Step: 2800  \tTraining loss: 0.39226052165031433\n",
      "Step: 2800  \tTraining accuracy: 0.7869174480438232\n",
      "Step: 2800  \tValid loss: 0.3830837905406952\n",
      "Step: 2900  \tTraining loss: 0.39178943634033203\n",
      "Step: 2900  \tTraining accuracy: 0.7874531149864197\n",
      "Step: 2900  \tValid loss: 0.38284623622894287\n",
      "Step: 3000  \tTraining loss: 0.39131712913513184\n",
      "Step: 3000  \tTraining accuracy: 0.7879524827003479\n",
      "Step: 3000  \tValid loss: 0.3825082778930664\n",
      "Step: 3100  \tTraining loss: 0.39081141352653503\n",
      "Step: 3100  \tTraining accuracy: 0.7884831428527832\n",
      "Step: 3100  \tValid loss: 0.38209450244903564\n",
      "Step: 3200  \tTraining loss: 0.3902323544025421\n",
      "Step: 3200  \tTraining accuracy: 0.7889983654022217\n",
      "Step: 3200  \tValid loss: 0.3815925121307373\n",
      "Step: 3300  \tTraining loss: 0.38958945870399475\n",
      "Step: 3300  \tTraining accuracy: 0.7894606590270996\n",
      "Step: 3300  \tValid loss: 0.38102760910987854\n",
      "Step: 3400  \tTraining loss: 0.3889465034008026\n",
      "Step: 3400  \tTraining accuracy: 0.7898438572883606\n",
      "Step: 3400  \tValid loss: 0.38039737939834595\n",
      "Step: 3500  \tTraining loss: 0.3883112370967865\n",
      "Step: 3500  \tTraining accuracy: 0.7901949286460876\n",
      "Step: 3500  \tValid loss: 0.3797610402107239\n",
      "Step: 3600  \tTraining loss: 0.38767632842063904\n",
      "Step: 3600  \tTraining accuracy: 0.7905261516571045\n",
      "Step: 3600  \tValid loss: 0.3791782557964325\n",
      "Step: 3700  \tTraining loss: 0.38702821731567383\n",
      "Step: 3700  \tTraining accuracy: 0.7908549904823303\n",
      "Step: 3700  \tValid loss: 0.37866827845573425\n",
      "Step: 3800  \tTraining loss: 0.3864099681377411\n",
      "Step: 3800  \tTraining accuracy: 0.7911478877067566\n",
      "Step: 3800  \tValid loss: 0.37839943170547485\n",
      "Step: 3900  \tTraining loss: 0.38581642508506775\n",
      "Step: 3900  \tTraining accuracy: 0.7914375066757202\n",
      "Step: 3900  \tValid loss: 0.3781411647796631\n",
      "Step: 4000  \tTraining loss: 0.38523709774017334\n",
      "Step: 4000  \tTraining accuracy: 0.7917125225067139\n",
      "Step: 4000  \tValid loss: 0.3779016137123108\n",
      "Step: 4100  \tTraining loss: 0.3846692144870758\n",
      "Step: 4100  \tTraining accuracy: 0.7919653654098511\n",
      "Step: 4100  \tValid loss: 0.377640038728714\n",
      "Step: 4200  \tTraining loss: 0.38411152362823486\n",
      "Step: 4200  \tTraining accuracy: 0.7922254800796509\n",
      "Step: 4200  \tValid loss: 0.37739554047584534\n",
      "Step: 4300  \tTraining loss: 0.3835684359073639\n",
      "Step: 4300  \tTraining accuracy: 0.7924976348876953\n",
      "Step: 4300  \tValid loss: 0.37710821628570557\n",
      "Step: 4400  \tTraining loss: 0.3830357491970062\n",
      "Step: 4400  \tTraining accuracy: 0.792765200138092\n",
      "Step: 4400  \tValid loss: 0.37693172693252563\n",
      "Step: 4500  \tTraining loss: 0.3824983239173889\n",
      "Step: 4500  \tTraining accuracy: 0.793041467666626\n",
      "Step: 4500  \tValid loss: 0.3766057789325714\n",
      "Step: 4600  \tTraining loss: 0.38196712732315063\n",
      "Step: 4600  \tTraining accuracy: 0.793305516242981\n",
      "Step: 4600  \tValid loss: 0.37630993127822876\n",
      "Step: 4700  \tTraining loss: 0.38142094016075134\n",
      "Step: 4700  \tTraining accuracy: 0.793585479259491\n",
      "Step: 4700  \tValid loss: 0.3761095702648163\n",
      "Step: 4800  \tTraining loss: 0.38092494010925293\n",
      "Step: 4800  \tTraining accuracy: 0.7938608527183533\n",
      "Step: 4800  \tValid loss: 0.3760366439819336\n",
      "Step: 4900  \tTraining loss: 0.3804492652416229\n",
      "Step: 4900  \tTraining accuracy: 0.7940964698791504\n",
      "Step: 4900  \tValid loss: 0.37590697407722473\n",
      "Step: 5000  \tTraining loss: 0.37998899817466736\n",
      "Step: 5000  \tTraining accuracy: 0.794317901134491\n",
      "Step: 5000  \tValid loss: 0.3757835924625397\n",
      "Step: 5100  \tTraining loss: 0.379528671503067\n",
      "Step: 5100  \tTraining accuracy: 0.794532835483551\n",
      "Step: 5100  \tValid loss: 0.3756796717643738\n",
      "Step: 5200  \tTraining loss: 0.3790867030620575\n",
      "Step: 5200  \tTraining accuracy: 0.794752836227417\n",
      "Step: 5200  \tValid loss: 0.37556058168411255\n",
      "Step: 5300  \tTraining loss: 0.37866178154945374\n",
      "Step: 5300  \tTraining accuracy: 0.794953465461731\n",
      "Step: 5300  \tValid loss: 0.3755158483982086\n",
      "Step: 5400  \tTraining loss: 0.37824746966362\n",
      "Step: 5400  \tTraining accuracy: 0.7951766848564148\n",
      "Step: 5400  \tValid loss: 0.3753921687602997\n",
      "Step: 5500  \tTraining loss: 0.37785106897354126\n",
      "Step: 5500  \tTraining accuracy: 0.7953980565071106\n",
      "Step: 5500  \tValid loss: 0.37519922852516174\n",
      "Step: 5600  \tTraining loss: 0.3774595260620117\n",
      "Step: 5600  \tTraining accuracy: 0.7956115007400513\n",
      "Step: 5600  \tValid loss: 0.3750692307949066\n",
      "Step: 5700  \tTraining loss: 0.37708547711372375\n",
      "Step: 5700  \tTraining accuracy: 0.7958295345306396\n",
      "Step: 5700  \tValid loss: 0.3749525845050812\n",
      "Step: 5800  \tTraining loss: 0.37672677636146545\n",
      "Step: 5800  \tTraining accuracy: 0.7960219979286194\n",
      "Step: 5800  \tValid loss: 0.37485620379447937\n",
      "Step: 5900  \tTraining loss: 0.3763405382633209\n",
      "Step: 5900  \tTraining accuracy: 0.7962255477905273\n",
      "Step: 5900  \tValid loss: 0.37457937002182007\n",
      "Step: 6000  \tTraining loss: 0.3758205473423004\n",
      "Step: 6000  \tTraining accuracy: 0.7964299917221069\n",
      "Step: 6000  \tValid loss: 0.37422439455986023\n",
      "Step: 6100  \tTraining loss: 0.3753250241279602\n",
      "Step: 6100  \tTraining accuracy: 0.7966296076774597\n",
      "Step: 6100  \tValid loss: 0.37417611479759216\n",
      "Step: 6200  \tTraining loss: 0.37469860911369324\n",
      "Step: 6200  \tTraining accuracy: 0.7968526482582092\n",
      "Step: 6200  \tValid loss: 0.3739832937717438\n",
      "Step: 6300  \tTraining loss: 0.3741576373577118\n",
      "Step: 6300  \tTraining accuracy: 0.7970629930496216\n",
      "Step: 6300  \tValid loss: 0.37385275959968567\n",
      "Step: 6400  \tTraining loss: 0.3736743927001953\n",
      "Step: 6400  \tTraining accuracy: 0.7972703576087952\n",
      "Step: 6400  \tValid loss: 0.37398311495780945\n",
      "Step: 6500  \tTraining loss: 0.37324175238609314\n",
      "Step: 6500  \tTraining accuracy: 0.7974801659584045\n",
      "Step: 6500  \tValid loss: 0.3740948438644409\n",
      "Step: 6600  \tTraining loss: 0.3727583885192871\n",
      "Step: 6600  \tTraining accuracy: 0.7976765632629395\n",
      "Step: 6600  \tValid loss: 0.37443456053733826\n",
      "Step: 6700  \tTraining loss: 0.37233951687812805\n",
      "Step: 6700  \tTraining accuracy: 0.797886073589325\n",
      "Step: 6700  \tValid loss: 0.37443339824676514\n",
      "Step: 6800  \tTraining loss: 0.37194952368736267\n",
      "Step: 6800  \tTraining accuracy: 0.7980911135673523\n",
      "Step: 6800  \tValid loss: 0.3744659423828125\n",
      "Step: 6900  \tTraining loss: 0.37153196334838867\n",
      "Step: 6900  \tTraining accuracy: 0.7982985377311707\n",
      "Step: 6900  \tValid loss: 0.37461429834365845\n",
      "Step: 7000  \tTraining loss: 0.37114980816841125\n",
      "Step: 7000  \tTraining accuracy: 0.7985065579414368\n",
      "Step: 7000  \tValid loss: 0.3745112717151642\n",
      "Step: 7100  \tTraining loss: 0.3707863986492157\n",
      "Step: 7100  \tTraining accuracy: 0.798707127571106\n",
      "Step: 7100  \tValid loss: 0.37448978424072266\n",
      "Step: 7200  \tTraining loss: 0.3704356253147125\n",
      "Step: 7200  \tTraining accuracy: 0.7989036440849304\n",
      "Step: 7200  \tValid loss: 0.37451979517936707\n",
      "Step: 7300  \tTraining loss: 0.37010061740875244\n",
      "Step: 7300  \tTraining accuracy: 0.7991042137145996\n",
      "Step: 7300  \tValid loss: 0.37456390261650085\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.79928064\n",
      "Precision: 0.85246915\n",
      "Recall: 0.89617133\n",
      "F1 score: 0.8192563\n",
      "AUC: 0.75959986\n",
      "   accuracy  precision    recall  f1_score     auc      loss  accuracy_val  \\\n",
      "0  0.799281   0.852469  0.896171  0.819256  0.7596  0.369933      0.799162   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.373795       0.799116   0.420426      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  7351.0  \n",
      "16\n",
      "(8120, 8)\n",
      "(8120, 1)\n",
      "(4400, 8)\n",
      "(4400, 1)\n",
      "(3575, 8)\n",
      "(3575, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6075899004936218\n",
      "Step: 100  \tTraining accuracy: 0.6854679584503174\n",
      "Step: 100  \tValid loss: 0.601101279258728\n",
      "Step: 200  \tTraining loss: 0.461857408285141\n",
      "Step: 200  \tTraining accuracy: 0.736981213092804\n",
      "Step: 200  \tValid loss: 0.4502737820148468\n",
      "Step: 300  \tTraining loss: 0.3867042064666748\n",
      "Step: 300  \tTraining accuracy: 0.7785413265228271\n",
      "Step: 300  \tValid loss: 0.36825212836265564\n",
      "Step: 400  \tTraining loss: 0.36431390047073364\n",
      "Step: 400  \tTraining accuracy: 0.7990426421165466\n",
      "Step: 400  \tValid loss: 0.34113436937332153\n",
      "Step: 500  \tTraining loss: 0.3540555536746979\n",
      "Step: 500  \tTraining accuracy: 0.8111310601234436\n",
      "Step: 500  \tValid loss: 0.3294643759727478\n",
      "Step: 600  \tTraining loss: 0.34662652015686035\n",
      "Step: 600  \tTraining accuracy: 0.8195383548736572\n",
      "Step: 600  \tValid loss: 0.322433203458786\n",
      "Step: 700  \tTraining loss: 0.3421327471733093\n",
      "Step: 700  \tTraining accuracy: 0.8260961174964905\n",
      "Step: 700  \tValid loss: 0.3183344304561615\n",
      "Step: 800  \tTraining loss: 0.3395501971244812\n",
      "Step: 800  \tTraining accuracy: 0.830980658531189\n",
      "Step: 800  \tValid loss: 0.31605401635169983\n",
      "Step: 900  \tTraining loss: 0.3378874361515045\n",
      "Step: 900  \tTraining accuracy: 0.8346654176712036\n",
      "Step: 900  \tValid loss: 0.3147444427013397\n",
      "Step: 1000  \tTraining loss: 0.3366946280002594\n",
      "Step: 1000  \tTraining accuracy: 0.8376270532608032\n",
      "Step: 1000  \tValid loss: 0.3140140175819397\n",
      "Step: 1100  \tTraining loss: 0.33585312962532043\n",
      "Step: 1100  \tTraining accuracy: 0.8399657011032104\n",
      "Step: 1100  \tValid loss: 0.31341439485549927\n",
      "Step: 1200  \tTraining loss: 0.33519256114959717\n",
      "Step: 1200  \tTraining accuracy: 0.8419625759124756\n",
      "Step: 1200  \tValid loss: 0.3129318058490753\n",
      "Step: 1300  \tTraining loss: 0.3346336781978607\n",
      "Step: 1300  \tTraining accuracy: 0.843635082244873\n",
      "Step: 1300  \tValid loss: 0.3125442862510681\n",
      "Step: 1400  \tTraining loss: 0.3341406583786011\n",
      "Step: 1400  \tTraining accuracy: 0.8450783491134644\n",
      "Step: 1400  \tValid loss: 0.3122294843196869\n",
      "Step: 1500  \tTraining loss: 0.333689421415329\n",
      "Step: 1500  \tTraining accuracy: 0.846314013004303\n",
      "Step: 1500  \tValid loss: 0.3119919002056122\n",
      "Step: 1600  \tTraining loss: 0.3332553803920746\n",
      "Step: 1600  \tTraining accuracy: 0.847410261631012\n",
      "Step: 1600  \tValid loss: 0.3117924928665161\n",
      "Step: 1700  \tTraining loss: 0.3328098952770233\n",
      "Step: 1700  \tTraining accuracy: 0.848358690738678\n",
      "Step: 1700  \tValid loss: 0.3116365969181061\n",
      "Step: 1800  \tTraining loss: 0.33235156536102295\n",
      "Step: 1800  \tTraining accuracy: 0.8491561412811279\n",
      "Step: 1800  \tValid loss: 0.3114936351776123\n",
      "Step: 1900  \tTraining loss: 0.33192285895347595\n",
      "Step: 1900  \tTraining accuracy: 0.8498237133026123\n",
      "Step: 1900  \tValid loss: 0.31133323907852173\n",
      "Step: 2000  \tTraining loss: 0.3315492272377014\n",
      "Step: 2000  \tTraining accuracy: 0.8503655195236206\n",
      "Step: 2000  \tValid loss: 0.31119850277900696\n",
      "Step: 2100  \tTraining loss: 0.3312290608882904\n",
      "Step: 2100  \tTraining accuracy: 0.8508332967758179\n",
      "Step: 2100  \tValid loss: 0.31111007928848267\n",
      "Step: 2200  \tTraining loss: 0.33095091581344604\n",
      "Step: 2200  \tTraining accuracy: 0.8512372970581055\n",
      "Step: 2200  \tValid loss: 0.3110596835613251\n",
      "Step: 2300  \tTraining loss: 0.33070108294487\n",
      "Step: 2300  \tTraining accuracy: 0.8516219854354858\n",
      "Step: 2300  \tValid loss: 0.31102484464645386\n",
      "Step: 2400  \tTraining loss: 0.3304692804813385\n",
      "Step: 2400  \tTraining accuracy: 0.8519739508628845\n",
      "Step: 2400  \tValid loss: 0.3109872043132782\n",
      "Step: 2500  \tTraining loss: 0.3302503228187561\n",
      "Step: 2500  \tTraining accuracy: 0.8522869944572449\n",
      "Step: 2500  \tValid loss: 0.31095951795578003\n",
      "Step: 2600  \tTraining loss: 0.3300407826900482\n",
      "Step: 2600  \tTraining accuracy: 0.8525901436805725\n",
      "Step: 2600  \tValid loss: 0.31092557311058044\n",
      "Step: 2700  \tTraining loss: 0.3298359811306\n",
      "Step: 2700  \tTraining accuracy: 0.8528774976730347\n",
      "Step: 2700  \tValid loss: 0.31089699268341064\n",
      "Step: 2800  \tTraining loss: 0.32963332533836365\n",
      "Step: 2800  \tTraining accuracy: 0.8531145453453064\n",
      "Step: 2800  \tValid loss: 0.3108650743961334\n",
      "Step: 2900  \tTraining loss: 0.3294302821159363\n",
      "Step: 2900  \tTraining accuracy: 0.8533284068107605\n",
      "Step: 2900  \tValid loss: 0.3108333647251129\n",
      "Step: 3000  \tTraining loss: 0.3292255103588104\n",
      "Step: 3000  \tTraining accuracy: 0.8535193204879761\n",
      "Step: 3000  \tValid loss: 0.3108013868331909\n",
      "Step: 3100  \tTraining loss: 0.3290194272994995\n",
      "Step: 3100  \tTraining accuracy: 0.8537140488624573\n",
      "Step: 3100  \tValid loss: 0.3107750415802002\n",
      "Step: 3200  \tTraining loss: 0.32881107926368713\n",
      "Step: 3200  \tTraining accuracy: 0.8538846373558044\n",
      "Step: 3200  \tValid loss: 0.3107629418373108\n",
      "Step: 3300  \tTraining loss: 0.32859939336776733\n",
      "Step: 3300  \tTraining accuracy: 0.8540427684783936\n",
      "Step: 3300  \tValid loss: 0.3107643127441406\n",
      "Step: 3400  \tTraining loss: 0.3283850848674774\n",
      "Step: 3400  \tTraining accuracy: 0.8542118072509766\n",
      "Step: 3400  \tValid loss: 0.31078284978866577\n",
      "Step: 3500  \tTraining loss: 0.3281678259372711\n",
      "Step: 3500  \tTraining accuracy: 0.8543657064437866\n",
      "Step: 3500  \tValid loss: 0.31080853939056396\n",
      "Step: 3600  \tTraining loss: 0.327945739030838\n",
      "Step: 3600  \tTraining accuracy: 0.8545214533805847\n",
      "Step: 3600  \tValid loss: 0.3108460605144501\n",
      "Step: 3700  \tTraining loss: 0.32771605253219604\n",
      "Step: 3700  \tTraining accuracy: 0.854682207107544\n",
      "Step: 3700  \tValid loss: 0.3108932673931122\n",
      "Step: 3800  \tTraining loss: 0.3274761736392975\n",
      "Step: 3800  \tTraining accuracy: 0.8548576831817627\n",
      "Step: 3800  \tValid loss: 0.31094399094581604\n",
      "Step: 3900  \tTraining loss: 0.32722774147987366\n",
      "Step: 3900  \tTraining accuracy: 0.8550207614898682\n",
      "Step: 3900  \tValid loss: 0.3109864890575409\n",
      "Step: 4000  \tTraining loss: 0.3269771635532379\n",
      "Step: 4000  \tTraining accuracy: 0.8551708459854126\n",
      "Step: 4000  \tValid loss: 0.3110020160675049\n",
      "Step: 4100  \tTraining loss: 0.3267306685447693\n",
      "Step: 4100  \tTraining accuracy: 0.8553012609481812\n",
      "Step: 4100  \tValid loss: 0.310997873544693\n",
      "Step: 4200  \tTraining loss: 0.3264915347099304\n",
      "Step: 4200  \tTraining accuracy: 0.855419397354126\n",
      "Step: 4200  \tValid loss: 0.310981422662735\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.85552907\n",
      "Precision: 0.8480285\n",
      "Recall: 0.8402062\n",
      "F1 score: 0.851565\n",
      "AUC: 0.8575185\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.855529   0.848028  0.840206  0.851565  0.857518  0.326363      0.855486   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0   0.31076       0.855433   0.316829      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  4254.0  \n",
      "17\n",
      "(3915, 8)\n",
      "(3915, 1)\n",
      "(2160, 8)\n",
      "(2160, 1)\n",
      "(1755, 8)\n",
      "(1755, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4387962818145752\n",
      "Step: 100  \tTraining accuracy: 0.8122605085372925\n",
      "Step: 100  \tValid loss: 0.4732010066509247\n",
      "Step: 200  \tTraining loss: 0.42077308893203735\n",
      "Step: 200  \tTraining accuracy: 0.8090251088142395\n",
      "Step: 200  \tValid loss: 0.45926156640052795\n",
      "Step: 300  \tTraining loss: 0.41304048895835876\n",
      "Step: 300  \tTraining accuracy: 0.806845486164093\n",
      "Step: 300  \tValid loss: 0.45044511556625366\n",
      "Step: 400  \tTraining loss: 0.4049280881881714\n",
      "Step: 400  \tTraining accuracy: 0.8068965673446655\n",
      "Step: 400  \tValid loss: 0.4386370778083801\n",
      "Step: 500  \tTraining loss: 0.39563077688217163\n",
      "Step: 500  \tTraining accuracy: 0.8083439469337463\n",
      "Step: 500  \tValid loss: 0.4242612421512604\n",
      "Step: 600  \tTraining loss: 0.3887588083744049\n",
      "Step: 600  \tTraining accuracy: 0.81075119972229\n",
      "Step: 600  \tValid loss: 0.4144155979156494\n",
      "Step: 700  \tTraining loss: 0.3843134045600891\n",
      "Step: 700  \tTraining accuracy: 0.8134590983390808\n",
      "Step: 700  \tValid loss: 0.4093368947505951\n",
      "Step: 800  \tTraining loss: 0.38126543164253235\n",
      "Step: 800  \tTraining accuracy: 0.8159556984901428\n",
      "Step: 800  \tValid loss: 0.4058612287044525\n",
      "Step: 900  \tTraining loss: 0.37897610664367676\n",
      "Step: 900  \tTraining accuracy: 0.8177897930145264\n",
      "Step: 900  \tValid loss: 0.4030967950820923\n",
      "Step: 1000  \tTraining loss: 0.37706977128982544\n",
      "Step: 1000  \tTraining accuracy: 0.8194528222084045\n",
      "Step: 1000  \tValid loss: 0.4008502960205078\n",
      "Step: 1100  \tTraining loss: 0.37536606192588806\n",
      "Step: 1100  \tTraining accuracy: 0.8210910558700562\n",
      "Step: 1100  \tValid loss: 0.3988148868083954\n",
      "Step: 1200  \tTraining loss: 0.3737538456916809\n",
      "Step: 1200  \tTraining accuracy: 0.8224443197250366\n",
      "Step: 1200  \tValid loss: 0.397011935710907\n",
      "Step: 1300  \tTraining loss: 0.3721826672554016\n",
      "Step: 1300  \tTraining accuracy: 0.8238058686256409\n",
      "Step: 1300  \tValid loss: 0.395405650138855\n",
      "Step: 1400  \tTraining loss: 0.3706471621990204\n",
      "Step: 1400  \tTraining accuracy: 0.8249373435974121\n",
      "Step: 1400  \tValid loss: 0.3938365578651428\n",
      "Step: 1500  \tTraining loss: 0.36929312348365784\n",
      "Step: 1500  \tTraining accuracy: 0.8259655833244324\n",
      "Step: 1500  \tValid loss: 0.3925181031227112\n",
      "Step: 1600  \tTraining loss: 0.3680892288684845\n",
      "Step: 1600  \tTraining accuracy: 0.8269105553627014\n",
      "Step: 1600  \tValid loss: 0.39136430621147156\n",
      "Step: 1700  \tTraining loss: 0.3670041859149933\n",
      "Step: 1700  \tTraining accuracy: 0.8278571367263794\n",
      "Step: 1700  \tValid loss: 0.39053621888160706\n",
      "Step: 1800  \tTraining loss: 0.3660160303115845\n",
      "Step: 1800  \tTraining accuracy: 0.8287392854690552\n",
      "Step: 1800  \tValid loss: 0.3898639976978302\n",
      "Step: 1900  \tTraining loss: 0.36510640382766724\n",
      "Step: 1900  \tTraining accuracy: 0.8294915556907654\n",
      "Step: 1900  \tValid loss: 0.38916677236557007\n",
      "Step: 2000  \tTraining loss: 0.36424922943115234\n",
      "Step: 2000  \tTraining accuracy: 0.8301798105239868\n",
      "Step: 2000  \tValid loss: 0.3886173963546753\n",
      "Step: 2100  \tTraining loss: 0.3634692132472992\n",
      "Step: 2100  \tTraining accuracy: 0.8308195471763611\n",
      "Step: 2100  \tValid loss: 0.38815951347351074\n",
      "Step: 2200  \tTraining loss: 0.3627437949180603\n",
      "Step: 2200  \tTraining accuracy: 0.831405758857727\n",
      "Step: 2200  \tValid loss: 0.3877725601196289\n",
      "Step: 2300  \tTraining loss: 0.36207085847854614\n",
      "Step: 2300  \tTraining accuracy: 0.8319512009620667\n",
      "Step: 2300  \tValid loss: 0.38741329312324524\n",
      "Step: 2400  \tTraining loss: 0.36138826608657837\n",
      "Step: 2400  \tTraining accuracy: 0.8324610590934753\n",
      "Step: 2400  \tValid loss: 0.38701653480529785\n",
      "Step: 2500  \tTraining loss: 0.360720157623291\n",
      "Step: 2500  \tTraining accuracy: 0.8329606056213379\n",
      "Step: 2500  \tValid loss: 0.38675040006637573\n",
      "Step: 2600  \tTraining loss: 0.360102117061615\n",
      "Step: 2600  \tTraining accuracy: 0.8334360122680664\n",
      "Step: 2600  \tValid loss: 0.38647952675819397\n",
      "Step: 2700  \tTraining loss: 0.3595268428325653\n",
      "Step: 2700  \tTraining accuracy: 0.8339044451713562\n",
      "Step: 2700  \tValid loss: 0.38622498512268066\n",
      "Step: 2800  \tTraining loss: 0.3589862883090973\n",
      "Step: 2800  \tTraining accuracy: 0.8343573808670044\n",
      "Step: 2800  \tValid loss: 0.3860498368740082\n",
      "Step: 2900  \tTraining loss: 0.35844552516937256\n",
      "Step: 2900  \tTraining accuracy: 0.8348008990287781\n",
      "Step: 2900  \tValid loss: 0.3858198821544647\n",
      "Step: 3000  \tTraining loss: 0.35792651772499084\n",
      "Step: 3000  \tTraining accuracy: 0.8352317214012146\n",
      "Step: 3000  \tValid loss: 0.38564932346343994\n",
      "Step: 3100  \tTraining loss: 0.35741642117500305\n",
      "Step: 3100  \tTraining accuracy: 0.8356384634971619\n",
      "Step: 3100  \tValid loss: 0.3854595720767975\n",
      "Step: 3200  \tTraining loss: 0.35686081647872925\n",
      "Step: 3200  \tTraining accuracy: 0.8360356092453003\n",
      "Step: 3200  \tValid loss: 0.385508269071579\n",
      "Step: 3300  \tTraining loss: 0.3563452959060669\n",
      "Step: 3300  \tTraining accuracy: 0.8364357948303223\n",
      "Step: 3300  \tValid loss: 0.38534995913505554\n",
      "Step: 3400  \tTraining loss: 0.3558398485183716\n",
      "Step: 3400  \tTraining accuracy: 0.8368311524391174\n",
      "Step: 3400  \tValid loss: 0.3851846158504486\n",
      "Step: 3500  \tTraining loss: 0.355325847864151\n",
      "Step: 3500  \tTraining accuracy: 0.837211012840271\n",
      "Step: 3500  \tValid loss: 0.38504135608673096\n",
      "Step: 3600  \tTraining loss: 0.3547992408275604\n",
      "Step: 3600  \tTraining accuracy: 0.8375694751739502\n",
      "Step: 3600  \tValid loss: 0.38493210077285767\n",
      "Step: 3700  \tTraining loss: 0.3542468249797821\n",
      "Step: 3700  \tTraining accuracy: 0.8379048109054565\n",
      "Step: 3700  \tValid loss: 0.38486629724502563\n",
      "Step: 3800  \tTraining loss: 0.3536669909954071\n",
      "Step: 3800  \tTraining accuracy: 0.8382086157798767\n",
      "Step: 3800  \tValid loss: 0.3847837746143341\n",
      "Step: 3900  \tTraining loss: 0.3530508875846863\n",
      "Step: 3900  \tTraining accuracy: 0.8384866714477539\n",
      "Step: 3900  \tValid loss: 0.3846672475337982\n",
      "Step: 4000  \tTraining loss: 0.35240575671195984\n",
      "Step: 4000  \tTraining accuracy: 0.8387377262115479\n",
      "Step: 4000  \tValid loss: 0.38452476263046265\n",
      "Step: 4100  \tTraining loss: 0.351735919713974\n",
      "Step: 4100  \tTraining accuracy: 0.8389700651168823\n",
      "Step: 4100  \tValid loss: 0.38434863090515137\n",
      "Step: 4200  \tTraining loss: 0.3510344624519348\n",
      "Step: 4200  \tTraining accuracy: 0.8392097353935242\n",
      "Step: 4200  \tValid loss: 0.38411033153533936\n",
      "Step: 4300  \tTraining loss: 0.3503093123435974\n",
      "Step: 4300  \tTraining accuracy: 0.8394591212272644\n",
      "Step: 4300  \tValid loss: 0.3838493227958679\n",
      "Step: 4400  \tTraining loss: 0.34954172372817993\n",
      "Step: 4400  \tTraining accuracy: 0.8396823406219482\n",
      "Step: 4400  \tValid loss: 0.383686363697052\n",
      "Step: 4500  \tTraining loss: 0.34864509105682373\n",
      "Step: 4500  \tTraining accuracy: 0.8398955464363098\n",
      "Step: 4500  \tValid loss: 0.3829921782016754\n",
      "Step: 4600  \tTraining loss: 0.34782642126083374\n",
      "Step: 4600  \tTraining accuracy: 0.8401134014129639\n",
      "Step: 4600  \tValid loss: 0.38287851214408875\n",
      "Step: 4700  \tTraining loss: 0.34702587127685547\n",
      "Step: 4700  \tTraining accuracy: 0.8403164148330688\n",
      "Step: 4700  \tValid loss: 0.38276174664497375\n",
      "Step: 4800  \tTraining loss: 0.3462764322757721\n",
      "Step: 4800  \tTraining accuracy: 0.8405108451843262\n",
      "Step: 4800  \tValid loss: 0.3828040659427643\n",
      "Step: 4900  \tTraining loss: 0.3455667495727539\n",
      "Step: 4900  \tTraining accuracy: 0.8407130837440491\n",
      "Step: 4900  \tValid loss: 0.38274940848350525\n",
      "Step: 5000  \tTraining loss: 0.3448792099952698\n",
      "Step: 5000  \tTraining accuracy: 0.8409252166748047\n",
      "Step: 5000  \tValid loss: 0.3826029598712921\n",
      "Step: 5100  \tTraining loss: 0.3442225754261017\n",
      "Step: 5100  \tTraining accuracy: 0.8411365151405334\n",
      "Step: 5100  \tValid loss: 0.38249319791793823\n",
      "Step: 5200  \tTraining loss: 0.3435840904712677\n",
      "Step: 5200  \tTraining accuracy: 0.8413569927215576\n",
      "Step: 5200  \tValid loss: 0.3823038637638092\n",
      "Step: 5300  \tTraining loss: 0.34297022223472595\n",
      "Step: 5300  \tTraining accuracy: 0.8416031002998352\n",
      "Step: 5300  \tValid loss: 0.3820863962173462\n",
      "Step: 5400  \tTraining loss: 0.3423764705657959\n",
      "Step: 5400  \tTraining accuracy: 0.8418400287628174\n",
      "Step: 5400  \tValid loss: 0.38182172179222107\n",
      "Step: 5500  \tTraining loss: 0.3417993187904358\n",
      "Step: 5500  \tTraining accuracy: 0.8420659303665161\n",
      "Step: 5500  \tValid loss: 0.38144001364707947\n",
      "Step: 5600  \tTraining loss: 0.3412296772003174\n",
      "Step: 5600  \tTraining accuracy: 0.8422882556915283\n",
      "Step: 5600  \tValid loss: 0.38099613785743713\n",
      "Step: 5700  \tTraining loss: 0.3406611382961273\n",
      "Step: 5700  \tTraining accuracy: 0.8425027132034302\n",
      "Step: 5700  \tValid loss: 0.3805554211139679\n",
      "Step: 5800  \tTraining loss: 0.34008434414863586\n",
      "Step: 5800  \tTraining accuracy: 0.8427097797393799\n",
      "Step: 5800  \tValid loss: 0.38009029626846313\n",
      "Step: 5900  \tTraining loss: 0.33949241042137146\n",
      "Step: 5900  \tTraining accuracy: 0.8429184556007385\n",
      "Step: 5900  \tValid loss: 0.37963223457336426\n",
      "Step: 6000  \tTraining loss: 0.33887720108032227\n",
      "Step: 6000  \tTraining accuracy: 0.8431243896484375\n",
      "Step: 6000  \tValid loss: 0.37916824221611023\n",
      "Step: 6100  \tTraining loss: 0.3382408618927002\n",
      "Step: 6100  \tTraining accuracy: 0.8433235287666321\n",
      "Step: 6100  \tValid loss: 0.37869757413864136\n",
      "Step: 6200  \tTraining loss: 0.33758434653282166\n",
      "Step: 6200  \tTraining accuracy: 0.8435286283493042\n",
      "Step: 6200  \tValid loss: 0.3782415986061096\n",
      "Step: 6300  \tTraining loss: 0.3369244635105133\n",
      "Step: 6300  \tTraining accuracy: 0.8437333106994629\n",
      "Step: 6300  \tValid loss: 0.37779897451400757\n",
      "Step: 6400  \tTraining loss: 0.33625465631484985\n",
      "Step: 6400  \tTraining accuracy: 0.8439255356788635\n",
      "Step: 6400  \tValid loss: 0.377400666475296\n",
      "Step: 6500  \tTraining loss: 0.3355848491191864\n",
      "Step: 6500  \tTraining accuracy: 0.8441078066825867\n",
      "Step: 6500  \tValid loss: 0.37703704833984375\n",
      "Step: 6600  \tTraining loss: 0.33493202924728394\n",
      "Step: 6600  \tTraining accuracy: 0.8442904353141785\n",
      "Step: 6600  \tValid loss: 0.37665805220603943\n",
      "Step: 6700  \tTraining loss: 0.3343029320240021\n",
      "Step: 6700  \tTraining accuracy: 0.8444809317588806\n",
      "Step: 6700  \tValid loss: 0.3763321340084076\n",
      "Step: 6800  \tTraining loss: 0.33369961380958557\n",
      "Step: 6800  \tTraining accuracy: 0.8446809649467468\n",
      "Step: 6800  \tValid loss: 0.37598633766174316\n",
      "Step: 6900  \tTraining loss: 0.33312490582466125\n",
      "Step: 6900  \tTraining accuracy: 0.8448900580406189\n",
      "Step: 6900  \tValid loss: 0.37571388483047485\n",
      "Step: 7000  \tTraining loss: 0.33252230286598206\n",
      "Step: 7000  \tTraining accuracy: 0.8451077938079834\n",
      "Step: 7000  \tValid loss: 0.3753640651702881\n",
      "Step: 7100  \tTraining loss: 0.33200058341026306\n",
      "Step: 7100  \tTraining accuracy: 0.8453085422515869\n",
      "Step: 7100  \tValid loss: 0.37511953711509705\n",
      "Step: 7200  \tTraining loss: 0.3315103352069855\n",
      "Step: 7200  \tTraining accuracy: 0.8455018997192383\n",
      "Step: 7200  \tValid loss: 0.3749224841594696\n",
      "Step: 7300  \tTraining loss: 0.3310486972332001\n",
      "Step: 7300  \tTraining accuracy: 0.8456881046295166\n",
      "Step: 7300  \tValid loss: 0.3747114837169647\n",
      "Step: 7400  \tTraining loss: 0.3306080996990204\n",
      "Step: 7400  \tTraining accuracy: 0.8458675146102905\n",
      "Step: 7400  \tValid loss: 0.3744948208332062\n",
      "Step: 7500  \tTraining loss: 0.3301888108253479\n",
      "Step: 7500  \tTraining accuracy: 0.8460353016853333\n",
      "Step: 7500  \tValid loss: 0.3743346631526947\n",
      "Step: 7600  \tTraining loss: 0.3297877311706543\n",
      "Step: 7600  \tTraining accuracy: 0.8461952209472656\n",
      "Step: 7600  \tValid loss: 0.37417343258857727\n",
      "Step: 7700  \tTraining loss: 0.3294004797935486\n",
      "Step: 7700  \tTraining accuracy: 0.8463442921638489\n",
      "Step: 7700  \tValid loss: 0.3739777207374573\n",
      "Step: 7800  \tTraining loss: 0.32902324199676514\n",
      "Step: 7800  \tTraining accuracy: 0.8464928269386292\n",
      "Step: 7800  \tValid loss: 0.3738233745098114\n",
      "Step: 7900  \tTraining loss: 0.3286605477333069\n",
      "Step: 7900  \tTraining accuracy: 0.8466391563415527\n",
      "Step: 7900  \tValid loss: 0.3736538290977478\n",
      "Step: 8000  \tTraining loss: 0.3283073604106903\n",
      "Step: 8000  \tTraining accuracy: 0.8467850685119629\n",
      "Step: 8000  \tValid loss: 0.3735194802284241\n",
      "Step: 8100  \tTraining loss: 0.3279638886451721\n",
      "Step: 8100  \tTraining accuracy: 0.8469288945198059\n",
      "Step: 8100  \tValid loss: 0.3733634650707245\n",
      "Step: 8200  \tTraining loss: 0.32762783765792847\n",
      "Step: 8200  \tTraining accuracy: 0.847069263458252\n",
      "Step: 8200  \tValid loss: 0.3732362985610962\n",
      "Step: 8300  \tTraining loss: 0.3272993266582489\n",
      "Step: 8300  \tTraining accuracy: 0.8472077250480652\n",
      "Step: 8300  \tValid loss: 0.3730841279029846\n",
      "Step: 8400  \tTraining loss: 0.32697534561157227\n",
      "Step: 8400  \tTraining accuracy: 0.8473398089408875\n",
      "Step: 8400  \tValid loss: 0.3729562759399414\n",
      "Step: 8500  \tTraining loss: 0.3266356289386749\n",
      "Step: 8500  \tTraining accuracy: 0.8474687933921814\n",
      "Step: 8500  \tValid loss: 0.37307655811309814\n",
      "Step: 8600  \tTraining loss: 0.32631635665893555\n",
      "Step: 8600  \tTraining accuracy: 0.847591757774353\n",
      "Step: 8600  \tValid loss: 0.3730657994747162\n",
      "Step: 8700  \tTraining loss: 0.32600921392440796\n",
      "Step: 8700  \tTraining accuracy: 0.8477118611335754\n",
      "Step: 8700  \tValid loss: 0.372958779335022\n",
      "Step: 8800  \tTraining loss: 0.32570940256118774\n",
      "Step: 8800  \tTraining accuracy: 0.8478175401687622\n",
      "Step: 8800  \tValid loss: 0.37284523248672485\n",
      "Step: 8900  \tTraining loss: 0.325415700674057\n",
      "Step: 8900  \tTraining accuracy: 0.8479179739952087\n",
      "Step: 8900  \tValid loss: 0.37276554107666016\n",
      "Step: 9000  \tTraining loss: 0.3251131474971771\n",
      "Step: 9000  \tTraining accuracy: 0.848019003868103\n",
      "Step: 9000  \tValid loss: 0.37258610129356384\n",
      "Step: 9100  \tTraining loss: 0.3248211741447449\n",
      "Step: 9100  \tTraining accuracy: 0.8481149673461914\n",
      "Step: 9100  \tValid loss: 0.37246641516685486\n",
      "Step: 9200  \tTraining loss: 0.3245324194431305\n",
      "Step: 9200  \tTraining accuracy: 0.8482186198234558\n",
      "Step: 9200  \tValid loss: 0.37232497334480286\n",
      "Step: 9300  \tTraining loss: 0.3242517113685608\n",
      "Step: 9300  \tTraining accuracy: 0.8483200669288635\n",
      "Step: 9300  \tValid loss: 0.37224915623664856\n",
      "Step: 9400  \tTraining loss: 0.32397621870040894\n",
      "Step: 9400  \tTraining accuracy: 0.8484193086624146\n",
      "Step: 9400  \tValid loss: 0.3721783757209778\n",
      "Step: 9500  \tTraining loss: 0.3237079381942749\n",
      "Step: 9500  \tTraining accuracy: 0.8485191464424133\n",
      "Step: 9500  \tValid loss: 0.3720930814743042\n",
      "Step: 9600  \tTraining loss: 0.32344135642051697\n",
      "Step: 9600  \tTraining accuracy: 0.848616898059845\n",
      "Step: 9600  \tValid loss: 0.37199899554252625\n",
      "Step: 9700  \tTraining loss: 0.32318243384361267\n",
      "Step: 9700  \tTraining accuracy: 0.8487205505371094\n",
      "Step: 9700  \tValid loss: 0.3719038665294647\n",
      "Step: 9800  \tTraining loss: 0.3229236900806427\n",
      "Step: 9800  \tTraining accuracy: 0.848824679851532\n",
      "Step: 9800  \tValid loss: 0.3718852996826172\n",
      "Step: 9900  \tTraining loss: 0.32267171144485474\n",
      "Step: 9900  \tTraining accuracy: 0.8489267230033875\n",
      "Step: 9900  \tValid loss: 0.37176313996315\n",
      "Step: 10000  \tTraining loss: 0.3224196135997772\n",
      "Step: 10000  \tTraining accuracy: 0.8490216135978699\n",
      "Step: 10000  \tValid loss: 0.371681809425354\n",
      "Step: 10100  \tTraining loss: 0.32217302918434143\n",
      "Step: 10100  \tTraining accuracy: 0.8491094708442688\n",
      "Step: 10100  \tValid loss: 0.3715934753417969\n",
      "Step: 10200  \tTraining loss: 0.32192814350128174\n",
      "Step: 10200  \tTraining accuracy: 0.8491931557655334\n",
      "Step: 10200  \tValid loss: 0.37146392464637756\n",
      "Step: 10300  \tTraining loss: 0.32168638706207275\n",
      "Step: 10300  \tTraining accuracy: 0.8492751717567444\n",
      "Step: 10300  \tValid loss: 0.37141209840774536\n",
      "Step: 10400  \tTraining loss: 0.32144778966903687\n",
      "Step: 10400  \tTraining accuracy: 0.8493592739105225\n",
      "Step: 10400  \tValid loss: 0.3713226318359375\n",
      "Step: 10500  \tTraining loss: 0.32121360301971436\n",
      "Step: 10500  \tTraining accuracy: 0.8494393229484558\n",
      "Step: 10500  \tValid loss: 0.37124285101890564\n",
      "Step: 10600  \tTraining loss: 0.32098284363746643\n",
      "Step: 10600  \tTraining accuracy: 0.8495215177536011\n",
      "Step: 10600  \tValid loss: 0.3711361289024353\n",
      "Step: 10700  \tTraining loss: 0.32075396180152893\n",
      "Step: 10700  \tTraining accuracy: 0.8495985865592957\n",
      "Step: 10700  \tValid loss: 0.37103191018104553\n",
      "Step: 10800  \tTraining loss: 0.32052773237228394\n",
      "Step: 10800  \tTraining accuracy: 0.8496670722961426\n",
      "Step: 10800  \tValid loss: 0.3709777891635895\n",
      "Step: 10900  \tTraining loss: 0.32026952505111694\n",
      "Step: 10900  \tTraining accuracy: 0.8497307300567627\n",
      "Step: 10900  \tValid loss: 0.3710944354534149\n",
      "Step: 11000  \tTraining loss: 0.32000797986984253\n",
      "Step: 11000  \tTraining accuracy: 0.8497932553291321\n",
      "Step: 11000  \tValid loss: 0.3710046410560608\n",
      "Step: 11100  \tTraining loss: 0.3197557032108307\n",
      "Step: 11100  \tTraining accuracy: 0.849853515625\n",
      "Step: 11100  \tValid loss: 0.37092676758766174\n",
      "Step: 11200  \tTraining loss: 0.31950438022613525\n",
      "Step: 11200  \tTraining accuracy: 0.8499126434326172\n",
      "Step: 11200  \tValid loss: 0.37075158953666687\n",
      "Step: 11300  \tTraining loss: 0.3192553222179413\n",
      "Step: 11300  \tTraining accuracy: 0.8499696254730225\n",
      "Step: 11300  \tValid loss: 0.3706617057323456\n",
      "Step: 11400  \tTraining loss: 0.3190139830112457\n",
      "Step: 11400  \tTraining accuracy: 0.8500255942344666\n",
      "Step: 11400  \tValid loss: 0.3704926669597626\n",
      "Step: 11500  \tTraining loss: 0.3187732398509979\n",
      "Step: 11500  \tTraining accuracy: 0.8500850200653076\n",
      "Step: 11500  \tValid loss: 0.37035173177719116\n",
      "Step: 11600  \tTraining loss: 0.31854259967803955\n",
      "Step: 11600  \tTraining accuracy: 0.8501445651054382\n",
      "Step: 11600  \tValid loss: 0.37024110555648804\n",
      "Step: 11700  \tTraining loss: 0.31831151247024536\n",
      "Step: 11700  \tTraining accuracy: 0.8502030968666077\n",
      "Step: 11700  \tValid loss: 0.3700239956378937\n",
      "Step: 11800  \tTraining loss: 0.318087100982666\n",
      "Step: 11800  \tTraining accuracy: 0.8502606153488159\n",
      "Step: 11800  \tValid loss: 0.3698594272136688\n",
      "Step: 11900  \tTraining loss: 0.3178107738494873\n",
      "Step: 11900  \tTraining accuracy: 0.8503214120864868\n",
      "Step: 11900  \tValid loss: 0.36983421444892883\n",
      "Step: 12000  \tTraining loss: 0.3175029456615448\n",
      "Step: 12000  \tTraining accuracy: 0.8503812551498413\n",
      "Step: 12000  \tValid loss: 0.3695370852947235\n",
      "Step: 12100  \tTraining loss: 0.3172200918197632\n",
      "Step: 12100  \tTraining accuracy: 0.8504358530044556\n",
      "Step: 12100  \tValid loss: 0.3691609501838684\n",
      "Step: 12200  \tTraining loss: 0.3169591426849365\n",
      "Step: 12200  \tTraining accuracy: 0.8504895567893982\n",
      "Step: 12200  \tValid loss: 0.368884414434433\n",
      "Step: 12300  \tTraining loss: 0.31670454144477844\n",
      "Step: 12300  \tTraining accuracy: 0.8505423665046692\n",
      "Step: 12300  \tValid loss: 0.36872947216033936\n",
      "Step: 12400  \tTraining loss: 0.31645435094833374\n",
      "Step: 12400  \tTraining accuracy: 0.8505892157554626\n",
      "Step: 12400  \tValid loss: 0.36850032210350037\n",
      "Step: 12500  \tTraining loss: 0.31621044874191284\n",
      "Step: 12500  \tTraining accuracy: 0.8506383299827576\n",
      "Step: 12500  \tValid loss: 0.3683517575263977\n",
      "Step: 12600  \tTraining loss: 0.31596463918685913\n",
      "Step: 12600  \tTraining accuracy: 0.8506886959075928\n",
      "Step: 12600  \tValid loss: 0.36818012595176697\n",
      "Step: 12700  \tTraining loss: 0.3157254457473755\n",
      "Step: 12700  \tTraining accuracy: 0.8507382869720459\n",
      "Step: 12700  \tValid loss: 0.3680463433265686\n",
      "Step: 12800  \tTraining loss: 0.3154888153076172\n",
      "Step: 12800  \tTraining accuracy: 0.8507870435714722\n",
      "Step: 12800  \tValid loss: 0.36792728304862976\n",
      "Step: 12900  \tTraining loss: 0.3152550756931305\n",
      "Step: 12900  \tTraining accuracy: 0.8508350849151611\n",
      "Step: 12900  \tValid loss: 0.3677746653556824\n",
      "Step: 13000  \tTraining loss: 0.315022736787796\n",
      "Step: 13000  \tTraining accuracy: 0.8508824110031128\n",
      "Step: 13000  \tValid loss: 0.3676629066467285\n",
      "Step: 13100  \tTraining loss: 0.31479379534721375\n",
      "Step: 13100  \tTraining accuracy: 0.8509289622306824\n",
      "Step: 13100  \tValid loss: 0.3675291836261749\n",
      "Step: 13200  \tTraining loss: 0.3145653307437897\n",
      "Step: 13200  \tTraining accuracy: 0.8509787321090698\n",
      "Step: 13200  \tValid loss: 0.3673536479473114\n",
      "Step: 13300  \tTraining loss: 0.3143385946750641\n",
      "Step: 13300  \tTraining accuracy: 0.851032555103302\n",
      "Step: 13300  \tValid loss: 0.36727502942085266\n",
      "Step: 13400  \tTraining loss: 0.31411024928092957\n",
      "Step: 13400  \tTraining accuracy: 0.8510817289352417\n",
      "Step: 13400  \tValid loss: 0.36711111664772034\n",
      "Step: 13500  \tTraining loss: 0.31387874484062195\n",
      "Step: 13500  \tTraining accuracy: 0.8511292338371277\n",
      "Step: 13500  \tValid loss: 0.36687350273132324\n",
      "Step: 13600  \tTraining loss: 0.3136506974697113\n",
      "Step: 13600  \tTraining accuracy: 0.8511769771575928\n",
      "Step: 13600  \tValid loss: 0.366696834564209\n",
      "Step: 13700  \tTraining loss: 0.31342482566833496\n",
      "Step: 13700  \tTraining accuracy: 0.8512221574783325\n",
      "Step: 13700  \tValid loss: 0.3665432035923004\n",
      "Step: 13800  \tTraining loss: 0.31314536929130554\n",
      "Step: 13800  \tTraining accuracy: 0.8512611389160156\n",
      "Step: 13800  \tValid loss: 0.3663693964481354\n",
      "Step: 13900  \tTraining loss: 0.3128693103790283\n",
      "Step: 13900  \tTraining accuracy: 0.8512976765632629\n",
      "Step: 13900  \tValid loss: 0.36622461676597595\n",
      "Step: 14000  \tTraining loss: 0.3126099705696106\n",
      "Step: 14000  \tTraining accuracy: 0.8513428568840027\n",
      "Step: 14000  \tValid loss: 0.3660697638988495\n",
      "Step: 14100  \tTraining loss: 0.3123593330383301\n",
      "Step: 14100  \tTraining accuracy: 0.8513946533203125\n",
      "Step: 14100  \tValid loss: 0.3658641278743744\n",
      "Step: 14200  \tTraining loss: 0.31212055683135986\n",
      "Step: 14200  \tTraining accuracy: 0.8514475226402283\n",
      "Step: 14200  \tValid loss: 0.3656671643257141\n",
      "Step: 14300  \tTraining loss: 0.3118855655193329\n",
      "Step: 14300  \tTraining accuracy: 0.851499617099762\n",
      "Step: 14300  \tValid loss: 0.3654642105102539\n",
      "Step: 14400  \tTraining loss: 0.3116416037082672\n",
      "Step: 14400  \tTraining accuracy: 0.8515554666519165\n",
      "Step: 14400  \tValid loss: 0.3651353716850281\n",
      "Step: 14500  \tTraining loss: 0.3113972246646881\n",
      "Step: 14500  \tTraining accuracy: 0.8516132235527039\n",
      "Step: 14500  \tValid loss: 0.3648889362812042\n",
      "Step: 14600  \tTraining loss: 0.31115609407424927\n",
      "Step: 14600  \tTraining accuracy: 0.8516675233840942\n",
      "Step: 14600  \tValid loss: 0.3646983504295349\n",
      "Step: 14700  \tTraining loss: 0.3109195828437805\n",
      "Step: 14700  \tTraining accuracy: 0.851719319820404\n",
      "Step: 14700  \tValid loss: 0.3645312786102295\n",
      "Step: 14800  \tTraining loss: 0.3105347454547882\n",
      "Step: 14800  \tTraining accuracy: 0.8517730832099915\n",
      "Step: 14800  \tValid loss: 0.36422696709632874\n",
      "Step: 14900  \tTraining loss: 0.31020185351371765\n",
      "Step: 14900  \tTraining accuracy: 0.8518251776695251\n",
      "Step: 14900  \tValid loss: 0.36421671509742737\n",
      "Step: 15000  \tTraining loss: 0.30991560220718384\n",
      "Step: 15000  \tTraining accuracy: 0.8518766164779663\n",
      "Step: 15000  \tValid loss: 0.36405643820762634\n",
      "Step: 15100  \tTraining loss: 0.3096540868282318\n",
      "Step: 15100  \tTraining accuracy: 0.8519256711006165\n",
      "Step: 15100  \tValid loss: 0.3637757897377014\n",
      "Step: 15200  \tTraining loss: 0.3093988597393036\n",
      "Step: 15200  \tTraining accuracy: 0.8519673347473145\n",
      "Step: 15200  \tValid loss: 0.36358124017715454\n",
      "Step: 15300  \tTraining loss: 0.3091493546962738\n",
      "Step: 15300  \tTraining accuracy: 0.8520076274871826\n",
      "Step: 15300  \tValid loss: 0.36335307359695435\n",
      "Step: 15400  \tTraining loss: 0.30890607833862305\n",
      "Step: 15400  \tTraining accuracy: 0.8520440459251404\n",
      "Step: 15400  \tValid loss: 0.3631710410118103\n",
      "Step: 15500  \tTraining loss: 0.3086674213409424\n",
      "Step: 15500  \tTraining accuracy: 0.8520833253860474\n",
      "Step: 15500  \tValid loss: 0.3629955053329468\n",
      "Step: 15600  \tTraining loss: 0.30842745304107666\n",
      "Step: 15600  \tTraining accuracy: 0.8521294593811035\n",
      "Step: 15600  \tValid loss: 0.36272966861724854\n",
      "Step: 15700  \tTraining loss: 0.30818238854408264\n",
      "Step: 15700  \tTraining accuracy: 0.8521774411201477\n",
      "Step: 15700  \tValid loss: 0.3624281585216522\n",
      "Step: 15800  \tTraining loss: 0.30794110894203186\n",
      "Step: 15800  \tTraining accuracy: 0.8522264957427979\n",
      "Step: 15800  \tValid loss: 0.3622465431690216\n",
      "Step: 15900  \tTraining loss: 0.30770498514175415\n",
      "Step: 15900  \tTraining accuracy: 0.8522692322731018\n",
      "Step: 15900  \tValid loss: 0.36203569173812866\n",
      "Step: 16000  \tTraining loss: 0.3074706196784973\n",
      "Step: 16000  \tTraining accuracy: 0.8523074388504028\n",
      "Step: 16000  \tValid loss: 0.36184772849082947\n",
      "Step: 16100  \tTraining loss: 0.30723950266838074\n",
      "Step: 16100  \tTraining accuracy: 0.8523460030555725\n",
      "Step: 16100  \tValid loss: 0.36167484521865845\n",
      "Step: 16200  \tTraining loss: 0.307007759809494\n",
      "Step: 16200  \tTraining accuracy: 0.8523848652839661\n",
      "Step: 16200  \tValid loss: 0.36147627234458923\n",
      "Step: 16300  \tTraining loss: 0.306779682636261\n",
      "Step: 16300  \tTraining accuracy: 0.8524208664894104\n",
      "Step: 16300  \tValid loss: 0.36128848791122437\n",
      "Step: 16400  \tTraining loss: 0.3065531551837921\n",
      "Step: 16400  \tTraining accuracy: 0.8524564504623413\n",
      "Step: 16400  \tValid loss: 0.36108189821243286\n",
      "Step: 16500  \tTraining loss: 0.30632930994033813\n",
      "Step: 16500  \tTraining accuracy: 0.8524884581565857\n",
      "Step: 16500  \tValid loss: 0.3609601557254791\n",
      "Step: 16600  \tTraining loss: 0.3061060607433319\n",
      "Step: 16600  \tTraining accuracy: 0.85251784324646\n",
      "Step: 16600  \tValid loss: 0.360766738653183\n",
      "Step: 16700  \tTraining loss: 0.30588164925575256\n",
      "Step: 16700  \tTraining accuracy: 0.852548360824585\n",
      "Step: 16700  \tValid loss: 0.360638827085495\n",
      "Step: 16800  \tTraining loss: 0.3056602478027344\n",
      "Step: 16800  \tTraining accuracy: 0.8525830507278442\n",
      "Step: 16800  \tValid loss: 0.36043575406074524\n",
      "Step: 16900  \tTraining loss: 0.3054351508617401\n",
      "Step: 16900  \tTraining accuracy: 0.8526173830032349\n",
      "Step: 16900  \tValid loss: 0.36024776101112366\n",
      "Step: 17000  \tTraining loss: 0.30521249771118164\n",
      "Step: 17000  \tTraining accuracy: 0.8526512980461121\n",
      "Step: 17000  \tValid loss: 0.36002880334854126\n",
      "Step: 17100  \tTraining loss: 0.30498987436294556\n",
      "Step: 17100  \tTraining accuracy: 0.8526915311813354\n",
      "Step: 17100  \tValid loss: 0.35986796021461487\n",
      "Step: 17200  \tTraining loss: 0.30476775765419006\n",
      "Step: 17200  \tTraining accuracy: 0.8527350425720215\n",
      "Step: 17200  \tValid loss: 0.35962942242622375\n",
      "Step: 17300  \tTraining loss: 0.30454346537590027\n",
      "Step: 17300  \tTraining accuracy: 0.8527780771255493\n",
      "Step: 17300  \tValid loss: 0.35942214727401733\n",
      "Step: 17400  \tTraining loss: 0.3043200373649597\n",
      "Step: 17400  \tTraining accuracy: 0.8528190851211548\n",
      "Step: 17400  \tValid loss: 0.35922595858573914\n",
      "Step: 17500  \tTraining loss: 0.30409759283065796\n",
      "Step: 17500  \tTraining accuracy: 0.8528574705123901\n",
      "Step: 17500  \tValid loss: 0.35900071263313293\n",
      "Step: 17600  \tTraining loss: 0.30387577414512634\n",
      "Step: 17600  \tTraining accuracy: 0.8528953790664673\n",
      "Step: 17600  \tValid loss: 0.3587755858898163\n",
      "Step: 17700  \tTraining loss: 0.30365321040153503\n",
      "Step: 17700  \tTraining accuracy: 0.852932870388031\n",
      "Step: 17700  \tValid loss: 0.35852330923080444\n",
      "Step: 17800  \tTraining loss: 0.3034253418445587\n",
      "Step: 17800  \tTraining accuracy: 0.8529700040817261\n",
      "Step: 17800  \tValid loss: 0.35826683044433594\n",
      "Step: 17900  \tTraining loss: 0.3031890392303467\n",
      "Step: 17900  \tTraining accuracy: 0.8530066609382629\n",
      "Step: 17900  \tValid loss: 0.3580184876918793\n",
      "Step: 18000  \tTraining loss: 0.30295488238334656\n",
      "Step: 18000  \tTraining accuracy: 0.853044331073761\n",
      "Step: 18000  \tValid loss: 0.3577752709388733\n",
      "Step: 18100  \tTraining loss: 0.3027222156524658\n",
      "Step: 18100  \tTraining accuracy: 0.8530837297439575\n",
      "Step: 18100  \tValid loss: 0.3575442135334015\n",
      "Step: 18200  \tTraining loss: 0.3024911880493164\n",
      "Step: 18200  \tTraining accuracy: 0.8531226515769958\n",
      "Step: 18200  \tValid loss: 0.3573750853538513\n",
      "Step: 18300  \tTraining loss: 0.3022577166557312\n",
      "Step: 18300  \tTraining accuracy: 0.8531611561775208\n",
      "Step: 18300  \tValid loss: 0.3571656048297882\n",
      "Step: 18400  \tTraining loss: 0.3020259141921997\n",
      "Step: 18400  \tTraining accuracy: 0.8531985878944397\n",
      "Step: 18400  \tValid loss: 0.35698339343070984\n",
      "Step: 18500  \tTraining loss: 0.3017849028110504\n",
      "Step: 18500  \tTraining accuracy: 0.8532328009605408\n",
      "Step: 18500  \tValid loss: 0.356777161359787\n",
      "Step: 18600  \tTraining loss: 0.3015446066856384\n",
      "Step: 18600  \tTraining accuracy: 0.8532646298408508\n",
      "Step: 18600  \tValid loss: 0.35662969946861267\n",
      "Step: 18700  \tTraining loss: 0.3011596202850342\n",
      "Step: 18700  \tTraining accuracy: 0.8533015847206116\n",
      "Step: 18700  \tValid loss: 0.3567262887954712\n",
      "Step: 18800  \tTraining loss: 0.30087611079216003\n",
      "Step: 18800  \tTraining accuracy: 0.8533401489257812\n",
      "Step: 18800  \tValid loss: 0.35682472586631775\n",
      "Step: 18900  \tTraining loss: 0.30061307549476624\n",
      "Step: 18900  \tTraining accuracy: 0.8533810377120972\n",
      "Step: 18900  \tValid loss: 0.35678207874298096\n",
      "Step: 19000  \tTraining loss: 0.3003493547439575\n",
      "Step: 19000  \tTraining accuracy: 0.8534228205680847\n",
      "Step: 19000  \tValid loss: 0.3567640483379364\n",
      "Step: 19100  \tTraining loss: 0.30009374022483826\n",
      "Step: 19100  \tTraining accuracy: 0.8534648418426514\n",
      "Step: 19100  \tValid loss: 0.35669219493865967\n",
      "Step: 19200  \tTraining loss: 0.2998451888561249\n",
      "Step: 19200  \tTraining accuracy: 0.8535064458847046\n",
      "Step: 19200  \tValid loss: 0.3566499650478363\n",
      "Step: 19300  \tTraining loss: 0.2996053397655487\n",
      "Step: 19300  \tTraining accuracy: 0.8535502552986145\n",
      "Step: 19300  \tValid loss: 0.356570839881897\n",
      "Step: 19400  \tTraining loss: 0.29937344789505005\n",
      "Step: 19400  \tTraining accuracy: 0.8535943031311035\n",
      "Step: 19400  \tValid loss: 0.356469988822937\n",
      "Step: 19500  \tTraining loss: 0.2991489768028259\n",
      "Step: 19500  \tTraining accuracy: 0.8536385297775269\n",
      "Step: 19500  \tValid loss: 0.35638830065727234\n",
      "Step: 19600  \tTraining loss: 0.29893141984939575\n",
      "Step: 19600  \tTraining accuracy: 0.8536816835403442\n",
      "Step: 19600  \tValid loss: 0.35628461837768555\n",
      "Step: 19700  \tTraining loss: 0.29872047901153564\n",
      "Step: 19700  \tTraining accuracy: 0.8537243604660034\n",
      "Step: 19700  \tValid loss: 0.35619452595710754\n",
      "Step: 19800  \tTraining loss: 0.29851412773132324\n",
      "Step: 19800  \tTraining accuracy: 0.8537666201591492\n",
      "Step: 19800  \tValid loss: 0.35611069202423096\n",
      "Step: 19900  \tTraining loss: 0.2983153462409973\n",
      "Step: 19900  \tTraining accuracy: 0.8538103699684143\n",
      "Step: 19900  \tValid loss: 0.3560112416744232\n",
      "Step: 20000  \tTraining loss: 0.29812091588974\n",
      "Step: 20000  \tTraining accuracy: 0.8538530468940735\n",
      "Step: 20000  \tValid loss: 0.3559780716896057\n",
      "Step: 20100  \tTraining loss: 0.2979310154914856\n",
      "Step: 20100  \tTraining accuracy: 0.8538939952850342\n",
      "Step: 20100  \tValid loss: 0.35585474967956543\n",
      "Step: 20200  \tTraining loss: 0.29774799942970276\n",
      "Step: 20200  \tTraining accuracy: 0.8539358377456665\n",
      "Step: 20200  \tValid loss: 0.3557771146297455\n",
      "Step: 20300  \tTraining loss: 0.29756852984428406\n",
      "Step: 20300  \tTraining accuracy: 0.8539785146713257\n",
      "Step: 20300  \tValid loss: 0.3556913733482361\n",
      "Step: 20400  \tTraining loss: 0.2973938584327698\n",
      "Step: 20400  \tTraining accuracy: 0.8540151715278625\n",
      "Step: 20400  \tValid loss: 0.35561153292655945\n",
      "Step: 20500  \tTraining loss: 0.29722288250923157\n",
      "Step: 20500  \tTraining accuracy: 0.854051411151886\n",
      "Step: 20500  \tValid loss: 0.35554400086402893\n",
      "Step: 20600  \tTraining loss: 0.29703953862190247\n",
      "Step: 20600  \tTraining accuracy: 0.8540872931480408\n",
      "Step: 20600  \tValid loss: 0.35545864701271057\n",
      "Step: 20700  \tTraining loss: 0.2968503534793854\n",
      "Step: 20700  \tTraining accuracy: 0.8541228771209717\n",
      "Step: 20700  \tValid loss: 0.35544976592063904\n",
      "Step: 20800  \tTraining loss: 0.296669602394104\n",
      "Step: 20800  \tTraining accuracy: 0.8541586995124817\n",
      "Step: 20800  \tValid loss: 0.35535332560539246\n",
      "Step: 20900  \tTraining loss: 0.29649636149406433\n",
      "Step: 20900  \tTraining accuracy: 0.8541948199272156\n",
      "Step: 20900  \tValid loss: 0.3552684187889099\n",
      "Step: 21000  \tTraining loss: 0.2963307797908783\n",
      "Step: 21000  \tTraining accuracy: 0.8542293310165405\n",
      "Step: 21000  \tValid loss: 0.35516712069511414\n",
      "Step: 21100  \tTraining loss: 0.2961706221103668\n",
      "Step: 21100  \tTraining accuracy: 0.8542647361755371\n",
      "Step: 21100  \tValid loss: 0.35511770844459534\n",
      "Step: 21200  \tTraining loss: 0.29601627588272095\n",
      "Step: 21200  \tTraining accuracy: 0.8543016910552979\n",
      "Step: 21200  \tValid loss: 0.35503971576690674\n",
      "Step: 21300  \tTraining loss: 0.29586756229400635\n",
      "Step: 21300  \tTraining accuracy: 0.8543400168418884\n",
      "Step: 21300  \tValid loss: 0.3550090491771698\n",
      "Step: 21400  \tTraining loss: 0.2957209050655365\n",
      "Step: 21400  \tTraining accuracy: 0.8543792366981506\n",
      "Step: 21400  \tValid loss: 0.3549262285232544\n",
      "Step: 21500  \tTraining loss: 0.2955797016620636\n",
      "Step: 21500  \tTraining accuracy: 0.8544180393218994\n",
      "Step: 21500  \tValid loss: 0.35484039783477783\n",
      "Step: 21600  \tTraining loss: 0.295441597700119\n",
      "Step: 21600  \tTraining accuracy: 0.854457676410675\n",
      "Step: 21600  \tValid loss: 0.3547973036766052\n",
      "Step: 21700  \tTraining loss: 0.2953057289123535\n",
      "Step: 21700  \tTraining accuracy: 0.8544987440109253\n",
      "Step: 21700  \tValid loss: 0.3547177314758301\n",
      "Step: 21800  \tTraining loss: 0.2951737940311432\n",
      "Step: 21800  \tTraining accuracy: 0.8545393943786621\n",
      "Step: 21800  \tValid loss: 0.35466665029525757\n",
      "Step: 21900  \tTraining loss: 0.2950448989868164\n",
      "Step: 21900  \tTraining accuracy: 0.8545814752578735\n",
      "Step: 21900  \tValid loss: 0.35458406805992126\n",
      "Step: 22000  \tTraining loss: 0.2949175536632538\n",
      "Step: 22000  \tTraining accuracy: 0.854624330997467\n",
      "Step: 22000  \tValid loss: 0.3545283377170563\n",
      "Step: 22100  \tTraining loss: 0.29479122161865234\n",
      "Step: 22100  \tTraining accuracy: 0.8546679019927979\n",
      "Step: 22100  \tValid loss: 0.3544721305370331\n",
      "Step: 22200  \tTraining loss: 0.2946663796901703\n",
      "Step: 22200  \tTraining accuracy: 0.8547129034996033\n",
      "Step: 22200  \tValid loss: 0.35440900921821594\n",
      "Step: 22300  \tTraining loss: 0.29454362392425537\n",
      "Step: 22300  \tTraining accuracy: 0.8547574281692505\n",
      "Step: 22300  \tValid loss: 0.3543570041656494\n",
      "Step: 22400  \tTraining loss: 0.2944236099720001\n",
      "Step: 22400  \tTraining accuracy: 0.8548015356063843\n",
      "Step: 22400  \tValid loss: 0.3542872369289398\n",
      "Step: 22500  \tTraining loss: 0.2943083643913269\n",
      "Step: 22500  \tTraining accuracy: 0.8548436164855957\n",
      "Step: 22500  \tValid loss: 0.3542158603668213\n",
      "Step: 22600  \tTraining loss: 0.29419437050819397\n",
      "Step: 22600  \tTraining accuracy: 0.854884147644043\n",
      "Step: 22600  \tValid loss: 0.35418954491615295\n",
      "Step: 22700  \tTraining loss: 0.2940834164619446\n",
      "Step: 22700  \tTraining accuracy: 0.8549254536628723\n",
      "Step: 22700  \tValid loss: 0.3541298806667328\n",
      "Step: 22800  \tTraining loss: 0.29397404193878174\n",
      "Step: 22800  \tTraining accuracy: 0.8549658060073853\n",
      "Step: 22800  \tValid loss: 0.3541071116924286\n",
      "Step: 22900  \tTraining loss: 0.2938666343688965\n",
      "Step: 22900  \tTraining accuracy: 0.8550041913986206\n",
      "Step: 22900  \tValid loss: 0.3540448248386383\n",
      "Step: 23000  \tTraining loss: 0.293760746717453\n",
      "Step: 23000  \tTraining accuracy: 0.8550405502319336\n",
      "Step: 23000  \tValid loss: 0.3540172278881073\n",
      "Step: 23100  \tTraining loss: 0.2936566472053528\n",
      "Step: 23100  \tTraining accuracy: 0.8550765514373779\n",
      "Step: 23100  \tValid loss: 0.3539591431617737\n",
      "Step: 23200  \tTraining loss: 0.2935542166233063\n",
      "Step: 23200  \tTraining accuracy: 0.8551122546195984\n",
      "Step: 23200  \tValid loss: 0.35393741726875305\n",
      "Step: 23300  \tTraining loss: 0.29345256090164185\n",
      "Step: 23300  \tTraining accuracy: 0.8551493287086487\n",
      "Step: 23300  \tValid loss: 0.35393062233924866\n",
      "Step: 23400  \tTraining loss: 0.29335325956344604\n",
      "Step: 23400  \tTraining accuracy: 0.855187177658081\n",
      "Step: 23400  \tValid loss: 0.35388290882110596\n",
      "Step: 23500  \tTraining loss: 0.2932545244693756\n",
      "Step: 23500  \tTraining accuracy: 0.8552246689796448\n",
      "Step: 23500  \tValid loss: 0.35385093092918396\n",
      "Step: 23600  \tTraining loss: 0.293157696723938\n",
      "Step: 23600  \tTraining accuracy: 0.8552629947662354\n",
      "Step: 23600  \tValid loss: 0.35382747650146484\n",
      "Step: 23700  \tTraining loss: 0.2930624186992645\n",
      "Step: 23700  \tTraining accuracy: 0.8553025722503662\n",
      "Step: 23700  \tValid loss: 0.35379165410995483\n",
      "Step: 23800  \tTraining loss: 0.2929677963256836\n",
      "Step: 23800  \tTraining accuracy: 0.8553417921066284\n",
      "Step: 23800  \tValid loss: 0.3537614047527313\n",
      "Step: 23900  \tTraining loss: 0.29287418723106384\n",
      "Step: 23900  \tTraining accuracy: 0.8553807139396667\n",
      "Step: 23900  \tValid loss: 0.3537278175354004\n",
      "Step: 24000  \tTraining loss: 0.29278287291526794\n",
      "Step: 24000  \tTraining accuracy: 0.8554193377494812\n",
      "Step: 24000  \tValid loss: 0.3537028133869171\n",
      "Step: 24100  \tTraining loss: 0.29269203543663025\n",
      "Step: 24100  \tTraining accuracy: 0.8554602265357971\n",
      "Step: 24100  \tValid loss: 0.3537036180496216\n",
      "Step: 24200  \tTraining loss: 0.29260337352752686\n",
      "Step: 24200  \tTraining accuracy: 0.8555008172988892\n",
      "Step: 24200  \tValid loss: 0.35366711020469666\n",
      "Step: 24300  \tTraining loss: 0.292514443397522\n",
      "Step: 24300  \tTraining accuracy: 0.8555410504341125\n",
      "Step: 24300  \tValid loss: 0.3536457419395447\n",
      "Step: 24400  \tTraining loss: 0.29242831468582153\n",
      "Step: 24400  \tTraining accuracy: 0.8555809855461121\n",
      "Step: 24400  \tValid loss: 0.35360389947891235\n",
      "Step: 24500  \tTraining loss: 0.29234278202056885\n",
      "Step: 24500  \tTraining accuracy: 0.8556205630302429\n",
      "Step: 24500  \tValid loss: 0.35360807180404663\n",
      "Step: 24600  \tTraining loss: 0.2922576069831848\n",
      "Step: 24600  \tTraining accuracy: 0.8556598424911499\n",
      "Step: 24600  \tValid loss: 0.3535926640033722\n",
      "Step: 24700  \tTraining loss: 0.2921735346317291\n",
      "Step: 24700  \tTraining accuracy: 0.855698823928833\n",
      "Step: 24700  \tValid loss: 0.35356950759887695\n",
      "Step: 24800  \tTraining loss: 0.29208993911743164\n",
      "Step: 24800  \tTraining accuracy: 0.8557374477386475\n",
      "Step: 24800  \tValid loss: 0.35353824496269226\n",
      "Step: 24900  \tTraining loss: 0.29200616478919983\n",
      "Step: 24900  \tTraining accuracy: 0.855775773525238\n",
      "Step: 24900  \tValid loss: 0.3534878194332123\n",
      "Step: 25000  \tTraining loss: 0.29188093543052673\n",
      "Step: 25000  \tTraining accuracy: 0.8558138012886047\n",
      "Step: 25000  \tValid loss: 0.3535223603248596\n",
      "Step: 25100  \tTraining loss: 0.2917773425579071\n",
      "Step: 25100  \tTraining accuracy: 0.8558515310287476\n",
      "Step: 25100  \tValid loss: 0.35359328985214233\n",
      "Step: 25200  \tTraining loss: 0.2916845679283142\n",
      "Step: 25200  \tTraining accuracy: 0.8558904528617859\n",
      "Step: 25200  \tValid loss: 0.3536492586135864\n",
      "Step: 25300  \tTraining loss: 0.29158687591552734\n",
      "Step: 25300  \tTraining accuracy: 0.8559300899505615\n",
      "Step: 25300  \tValid loss: 0.35366520285606384\n",
      "Step: 25400  \tTraining loss: 0.2914903461933136\n",
      "Step: 25400  \tTraining accuracy: 0.8559694290161133\n",
      "Step: 25400  \tValid loss: 0.35368582606315613\n",
      "Step: 25500  \tTraining loss: 0.2914007306098938\n",
      "Step: 25500  \tTraining accuracy: 0.8560059666633606\n",
      "Step: 25500  \tValid loss: 0.35366514325141907\n",
      "Step: 25600  \tTraining loss: 0.2913140058517456\n",
      "Step: 25600  \tTraining accuracy: 0.8560421466827393\n",
      "Step: 25600  \tValid loss: 0.3536711633205414\n",
      "Step: 25700  \tTraining loss: 0.2912287712097168\n",
      "Step: 25700  \tTraining accuracy: 0.8560780882835388\n",
      "Step: 25700  \tValid loss: 0.3536880910396576\n",
      "Step: 25800  \tTraining loss: 0.2911446988582611\n",
      "Step: 25800  \tTraining accuracy: 0.8561137914657593\n",
      "Step: 25800  \tValid loss: 0.3536861538887024\n",
      "Step: 25900  \tTraining loss: 0.2910613715648651\n",
      "Step: 25900  \tTraining accuracy: 0.8561491370201111\n",
      "Step: 25900  \tValid loss: 0.3537159562110901\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8561843\n",
      "Precision: 0.88637024\n",
      "Recall: 0.9664132\n",
      "F1 score: 0.8931727\n",
      "AUC: 0.7256309\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.856184    0.88637  0.966413  0.893173  0.725631  0.291036      0.856152   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.353459       0.856173   0.310068      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  25929.0  \n",
      "18\n",
      "(5945, 8)\n",
      "(5945, 1)\n",
      "(3200, 8)\n",
      "(3200, 1)\n",
      "(2600, 8)\n",
      "(2600, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.49792909622192383\n",
      "Step: 100  \tTraining accuracy: 0.7820016741752625\n",
      "Step: 100  \tValid loss: 0.5630130171775818\n",
      "Step: 200  \tTraining loss: 0.45736363530158997\n",
      "Step: 200  \tTraining accuracy: 0.7678914666175842\n",
      "Step: 200  \tValid loss: 0.5183535814285278\n",
      "Step: 300  \tTraining loss: 0.44424566626548767\n",
      "Step: 300  \tTraining accuracy: 0.7712587118148804\n",
      "Step: 300  \tValid loss: 0.5014468431472778\n",
      "Step: 400  \tTraining loss: 0.44173282384872437\n",
      "Step: 400  \tTraining accuracy: 0.7783875465393066\n",
      "Step: 400  \tValid loss: 0.49768248200416565\n",
      "Step: 500  \tTraining loss: 0.43894895911216736\n",
      "Step: 500  \tTraining accuracy: 0.7825602293014526\n",
      "Step: 500  \tValid loss: 0.49379339814186096\n",
      "Step: 600  \tTraining loss: 0.433692067861557\n",
      "Step: 600  \tTraining accuracy: 0.7856502532958984\n",
      "Step: 600  \tValid loss: 0.48733848333358765\n",
      "Step: 700  \tTraining loss: 0.424146831035614\n",
      "Step: 700  \tTraining accuracy: 0.7884577512741089\n",
      "Step: 700  \tValid loss: 0.47685784101486206\n",
      "Step: 800  \tTraining loss: 0.41703686118125916\n",
      "Step: 800  \tTraining accuracy: 0.7910390496253967\n",
      "Step: 800  \tValid loss: 0.47037452459335327\n",
      "Step: 900  \tTraining loss: 0.4130186438560486\n",
      "Step: 900  \tTraining accuracy: 0.7935038208961487\n",
      "Step: 900  \tValid loss: 0.467085063457489\n",
      "Step: 1000  \tTraining loss: 0.41050824522972107\n",
      "Step: 1000  \tTraining accuracy: 0.7957277297973633\n",
      "Step: 1000  \tValid loss: 0.4648516774177551\n",
      "Step: 1100  \tTraining loss: 0.40860265493392944\n",
      "Step: 1100  \tTraining accuracy: 0.7977146506309509\n",
      "Step: 1100  \tValid loss: 0.46288084983825684\n",
      "Step: 1200  \tTraining loss: 0.40689417719841003\n",
      "Step: 1200  \tTraining accuracy: 0.79935622215271\n",
      "Step: 1200  \tValid loss: 0.46091192960739136\n",
      "Step: 1300  \tTraining loss: 0.40544062852859497\n",
      "Step: 1300  \tTraining accuracy: 0.8007080554962158\n",
      "Step: 1300  \tValid loss: 0.4591864049434662\n",
      "Step: 1400  \tTraining loss: 0.404062956571579\n",
      "Step: 1400  \tTraining accuracy: 0.8017650842666626\n",
      "Step: 1400  \tValid loss: 0.4574616253376007\n",
      "Step: 1500  \tTraining loss: 0.4028151035308838\n",
      "Step: 1500  \tTraining accuracy: 0.8026353716850281\n",
      "Step: 1500  \tValid loss: 0.4558781087398529\n",
      "Step: 1600  \tTraining loss: 0.40171414613723755\n",
      "Step: 1600  \tTraining accuracy: 0.8034757375717163\n",
      "Step: 1600  \tValid loss: 0.4546210467815399\n",
      "Step: 1700  \tTraining loss: 0.4007261395454407\n",
      "Step: 1700  \tTraining accuracy: 0.8042813539505005\n",
      "Step: 1700  \tValid loss: 0.4535650610923767\n",
      "Step: 1800  \tTraining loss: 0.39990484714508057\n",
      "Step: 1800  \tTraining accuracy: 0.8049900531768799\n",
      "Step: 1800  \tValid loss: 0.452677458524704\n",
      "Step: 1900  \tTraining loss: 0.3991750180721283\n",
      "Step: 1900  \tTraining accuracy: 0.8056267499923706\n",
      "Step: 1900  \tValid loss: 0.45196449756622314\n",
      "Step: 2000  \tTraining loss: 0.3985004127025604\n",
      "Step: 2000  \tTraining accuracy: 0.8062942028045654\n",
      "Step: 2000  \tValid loss: 0.4513823390007019\n",
      "Step: 2100  \tTraining loss: 0.39786458015441895\n",
      "Step: 2100  \tTraining accuracy: 0.8069173097610474\n",
      "Step: 2100  \tValid loss: 0.4509657621383667\n",
      "Step: 2200  \tTraining loss: 0.39727503061294556\n",
      "Step: 2200  \tTraining accuracy: 0.8075101971626282\n",
      "Step: 2200  \tValid loss: 0.4506545960903168\n",
      "Step: 2300  \tTraining loss: 0.39670875668525696\n",
      "Step: 2300  \tTraining accuracy: 0.808073103427887\n",
      "Step: 2300  \tValid loss: 0.4504709243774414\n",
      "Step: 2400  \tTraining loss: 0.396182656288147\n",
      "Step: 2400  \tTraining accuracy: 0.8086025714874268\n",
      "Step: 2400  \tValid loss: 0.45027464628219604\n",
      "Step: 2500  \tTraining loss: 0.395663857460022\n",
      "Step: 2500  \tTraining accuracy: 0.8091305494308472\n",
      "Step: 2500  \tValid loss: 0.45006638765335083\n",
      "Step: 2600  \tTraining loss: 0.39512327313423157\n",
      "Step: 2600  \tTraining accuracy: 0.8096104264259338\n",
      "Step: 2600  \tValid loss: 0.4497494399547577\n",
      "Step: 2700  \tTraining loss: 0.39456409215927124\n",
      "Step: 2700  \tTraining accuracy: 0.8100765943527222\n",
      "Step: 2700  \tValid loss: 0.44936829805374146\n",
      "Step: 2800  \tTraining loss: 0.3939748704433441\n",
      "Step: 2800  \tTraining accuracy: 0.8105460405349731\n",
      "Step: 2800  \tValid loss: 0.44899827241897583\n",
      "Step: 2900  \tTraining loss: 0.3934020698070526\n",
      "Step: 2900  \tTraining accuracy: 0.8109645843505859\n",
      "Step: 2900  \tValid loss: 0.4486485421657562\n",
      "Step: 3000  \tTraining loss: 0.39286312460899353\n",
      "Step: 3000  \tTraining accuracy: 0.8113749623298645\n",
      "Step: 3000  \tValid loss: 0.4483911991119385\n",
      "Step: 3100  \tTraining loss: 0.39237669110298157\n",
      "Step: 3100  \tTraining accuracy: 0.8117780089378357\n",
      "Step: 3100  \tValid loss: 0.44816628098487854\n",
      "Step: 3200  \tTraining loss: 0.39191383123397827\n",
      "Step: 3200  \tTraining accuracy: 0.812160849571228\n",
      "Step: 3200  \tValid loss: 0.44790634512901306\n",
      "Step: 3300  \tTraining loss: 0.39145800471305847\n",
      "Step: 3300  \tTraining accuracy: 0.8125253915786743\n",
      "Step: 3300  \tValid loss: 0.4476349949836731\n",
      "Step: 3400  \tTraining loss: 0.3910478949546814\n",
      "Step: 3400  \tTraining accuracy: 0.8128605484962463\n",
      "Step: 3400  \tValid loss: 0.44735032320022583\n",
      "Step: 3500  \tTraining loss: 0.3906845450401306\n",
      "Step: 3500  \tTraining accuracy: 0.8131811618804932\n",
      "Step: 3500  \tValid loss: 0.4472864866256714\n",
      "Step: 3600  \tTraining loss: 0.3903520703315735\n",
      "Step: 3600  \tTraining accuracy: 0.8134861588478088\n",
      "Step: 3600  \tValid loss: 0.4471830725669861\n",
      "Step: 3700  \tTraining loss: 0.39004674553871155\n",
      "Step: 3700  \tTraining accuracy: 0.8137814402580261\n",
      "Step: 3700  \tValid loss: 0.44715845584869385\n",
      "Step: 3800  \tTraining loss: 0.3897095322608948\n",
      "Step: 3800  \tTraining accuracy: 0.8140655159950256\n",
      "Step: 3800  \tValid loss: 0.44703781604766846\n",
      "Step: 3900  \tTraining loss: 0.389328271150589\n",
      "Step: 3900  \tTraining accuracy: 0.8143525123596191\n",
      "Step: 3900  \tValid loss: 0.4470706284046173\n",
      "Step: 4000  \tTraining loss: 0.3889673948287964\n",
      "Step: 4000  \tTraining accuracy: 0.814629316329956\n",
      "Step: 4000  \tValid loss: 0.4469470679759979\n",
      "Step: 4100  \tTraining loss: 0.388596773147583\n",
      "Step: 4100  \tTraining accuracy: 0.8148735165596008\n",
      "Step: 4100  \tValid loss: 0.44675594568252563\n",
      "Step: 4200  \tTraining loss: 0.38824138045310974\n",
      "Step: 4200  \tTraining accuracy: 0.8151223659515381\n",
      "Step: 4200  \tValid loss: 0.44660484790802\n",
      "Step: 4300  \tTraining loss: 0.3879213035106659\n",
      "Step: 4300  \tTraining accuracy: 0.815361499786377\n",
      "Step: 4300  \tValid loss: 0.4464443325996399\n",
      "Step: 4400  \tTraining loss: 0.387612521648407\n",
      "Step: 4400  \tTraining accuracy: 0.8155994415283203\n",
      "Step: 4400  \tValid loss: 0.4463467001914978\n",
      "Step: 4500  \tTraining loss: 0.38730689883232117\n",
      "Step: 4500  \tTraining accuracy: 0.8158285617828369\n",
      "Step: 4500  \tValid loss: 0.4462701380252838\n",
      "Step: 4600  \tTraining loss: 0.38700926303863525\n",
      "Step: 4600  \tTraining accuracy: 0.8160495162010193\n",
      "Step: 4600  \tValid loss: 0.4462248980998993\n",
      "Step: 4700  \tTraining loss: 0.38661542534828186\n",
      "Step: 4700  \tTraining accuracy: 0.8162591457366943\n",
      "Step: 4700  \tValid loss: 0.4459434449672699\n",
      "Step: 4800  \tTraining loss: 0.38599613308906555\n",
      "Step: 4800  \tTraining accuracy: 0.8164688944816589\n",
      "Step: 4800  \tValid loss: 0.44545993208885193\n",
      "Step: 4900  \tTraining loss: 0.38557901978492737\n",
      "Step: 4900  \tTraining accuracy: 0.8166753053665161\n",
      "Step: 4900  \tValid loss: 0.44538700580596924\n",
      "Step: 5000  \tTraining loss: 0.38525572419166565\n",
      "Step: 5000  \tTraining accuracy: 0.8168681859970093\n",
      "Step: 5000  \tValid loss: 0.44524911046028137\n",
      "Step: 5100  \tTraining loss: 0.3849535584449768\n",
      "Step: 5100  \tTraining accuracy: 0.8170416355133057\n",
      "Step: 5100  \tValid loss: 0.44510406255722046\n",
      "Step: 5200  \tTraining loss: 0.3846702575683594\n",
      "Step: 5200  \tTraining accuracy: 0.8172017335891724\n",
      "Step: 5200  \tValid loss: 0.4449499547481537\n",
      "Step: 5300  \tTraining loss: 0.3844071626663208\n",
      "Step: 5300  \tTraining accuracy: 0.8173556923866272\n",
      "Step: 5300  \tValid loss: 0.4448207914829254\n",
      "Step: 5400  \tTraining loss: 0.3841543197631836\n",
      "Step: 5400  \tTraining accuracy: 0.8174943923950195\n",
      "Step: 5400  \tValid loss: 0.44475993514060974\n",
      "Step: 5500  \tTraining loss: 0.3839167654514313\n",
      "Step: 5500  \tTraining accuracy: 0.8176357746124268\n",
      "Step: 5500  \tValid loss: 0.4446773827075958\n",
      "Step: 5600  \tTraining loss: 0.38368746638298035\n",
      "Step: 5600  \tTraining accuracy: 0.8177644610404968\n",
      "Step: 5600  \tValid loss: 0.4445981979370117\n",
      "Step: 5700  \tTraining loss: 0.38347145915031433\n",
      "Step: 5700  \tTraining accuracy: 0.8178794980049133\n",
      "Step: 5700  \tValid loss: 0.44455578923225403\n",
      "Step: 5800  \tTraining loss: 0.383263498544693\n",
      "Step: 5800  \tTraining accuracy: 0.8179905414581299\n",
      "Step: 5800  \tValid loss: 0.4444694519042969\n",
      "Step: 5900  \tTraining loss: 0.38306644558906555\n",
      "Step: 5900  \tTraining accuracy: 0.8181006908416748\n",
      "Step: 5900  \tValid loss: 0.4443861246109009\n",
      "Step: 6000  \tTraining loss: 0.3828810751438141\n",
      "Step: 6000  \tTraining accuracy: 0.8182029128074646\n",
      "Step: 6000  \tValid loss: 0.44435256719589233\n",
      "Step: 6100  \tTraining loss: 0.3827058970928192\n",
      "Step: 6100  \tTraining accuracy: 0.8183059096336365\n",
      "Step: 6100  \tValid loss: 0.44427749514579773\n",
      "Step: 6200  \tTraining loss: 0.3825375735759735\n",
      "Step: 6200  \tTraining accuracy: 0.8184041976928711\n",
      "Step: 6200  \tValid loss: 0.44421809911727905\n",
      "Step: 6300  \tTraining loss: 0.38237470388412476\n",
      "Step: 6300  \tTraining accuracy: 0.8184979557991028\n",
      "Step: 6300  \tValid loss: 0.4441300928592682\n",
      "Step: 6400  \tTraining loss: 0.38221439719200134\n",
      "Step: 6400  \tTraining accuracy: 0.8185887932777405\n",
      "Step: 6400  \tValid loss: 0.44411879777908325\n",
      "Step: 6500  \tTraining loss: 0.382057249546051\n",
      "Step: 6500  \tTraining accuracy: 0.818676769733429\n",
      "Step: 6500  \tValid loss: 0.4440726339817047\n",
      "Step: 6600  \tTraining loss: 0.3819043040275574\n",
      "Step: 6600  \tTraining accuracy: 0.8187581896781921\n",
      "Step: 6600  \tValid loss: 0.4440234303474426\n",
      "Step: 6700  \tTraining loss: 0.3817543089389801\n",
      "Step: 6700  \tTraining accuracy: 0.8188384771347046\n",
      "Step: 6700  \tValid loss: 0.44399893283843994\n",
      "Step: 6800  \tTraining loss: 0.3816070258617401\n",
      "Step: 6800  \tTraining accuracy: 0.8189289569854736\n",
      "Step: 6800  \tValid loss: 0.4439421594142914\n",
      "Step: 6900  \tTraining loss: 0.38146132230758667\n",
      "Step: 6900  \tTraining accuracy: 0.8190205097198486\n",
      "Step: 6900  \tValid loss: 0.44393062591552734\n",
      "Step: 7000  \tTraining loss: 0.3813112676143646\n",
      "Step: 7000  \tTraining accuracy: 0.8191069960594177\n",
      "Step: 7000  \tValid loss: 0.44377246499061584\n",
      "Step: 7100  \tTraining loss: 0.3811657428741455\n",
      "Step: 7100  \tTraining accuracy: 0.8191946744918823\n",
      "Step: 7100  \tValid loss: 0.44368991255760193\n",
      "Step: 7200  \tTraining loss: 0.3809988498687744\n",
      "Step: 7200  \tTraining accuracy: 0.8192762732505798\n",
      "Step: 7200  \tValid loss: 0.44364356994628906\n",
      "Step: 7300  \tTraining loss: 0.38084930181503296\n",
      "Step: 7300  \tTraining accuracy: 0.8193591833114624\n",
      "Step: 7300  \tValid loss: 0.4435821771621704\n",
      "Step: 7400  \tTraining loss: 0.3807061016559601\n",
      "Step: 7400  \tTraining accuracy: 0.8194456100463867\n",
      "Step: 7400  \tValid loss: 0.443548321723938\n",
      "Step: 7500  \tTraining loss: 0.38056254386901855\n",
      "Step: 7500  \tTraining accuracy: 0.8195263147354126\n",
      "Step: 7500  \tValid loss: 0.44348305463790894\n",
      "Step: 7600  \tTraining loss: 0.38042566180229187\n",
      "Step: 7600  \tTraining accuracy: 0.8196048736572266\n",
      "Step: 7600  \tValid loss: 0.44342857599258423\n",
      "Step: 7700  \tTraining loss: 0.3802899420261383\n",
      "Step: 7700  \tTraining accuracy: 0.8196847438812256\n",
      "Step: 7700  \tValid loss: 0.4434330463409424\n",
      "Step: 7800  \tTraining loss: 0.38015520572662354\n",
      "Step: 7800  \tTraining accuracy: 0.8197668790817261\n",
      "Step: 7800  \tValid loss: 0.4434014558792114\n",
      "Step: 7900  \tTraining loss: 0.3800197243690491\n",
      "Step: 7900  \tTraining accuracy: 0.8198469877243042\n",
      "Step: 7900  \tValid loss: 0.44337350130081177\n",
      "Step: 8000  \tTraining loss: 0.3798874020576477\n",
      "Step: 8000  \tTraining accuracy: 0.8199250102043152\n",
      "Step: 8000  \tValid loss: 0.4433518350124359\n",
      "Step: 8100  \tTraining loss: 0.3797592520713806\n",
      "Step: 8100  \tTraining accuracy: 0.8200011849403381\n",
      "Step: 8100  \tValid loss: 0.44336608052253723\n",
      "Step: 8200  \tTraining loss: 0.3796200752258301\n",
      "Step: 8200  \tTraining accuracy: 0.8200733065605164\n",
      "Step: 8200  \tValid loss: 0.44316184520721436\n",
      "Step: 8300  \tTraining loss: 0.3794916272163391\n",
      "Step: 8300  \tTraining accuracy: 0.8201478719711304\n",
      "Step: 8300  \tValid loss: 0.4431988596916199\n",
      "Step: 8400  \tTraining loss: 0.37936365604400635\n",
      "Step: 8400  \tTraining accuracy: 0.8202216625213623\n",
      "Step: 8400  \tValid loss: 0.44319164752960205\n",
      "Step: 8500  \tTraining loss: 0.3792354464530945\n",
      "Step: 8500  \tTraining accuracy: 0.8202967047691345\n",
      "Step: 8500  \tValid loss: 0.44324687123298645\n",
      "Step: 8600  \tTraining loss: 0.3791077136993408\n",
      "Step: 8600  \tTraining accuracy: 0.8203700184822083\n",
      "Step: 8600  \tValid loss: 0.4433017373085022\n",
      "Step: 8700  \tTraining loss: 0.37898197770118713\n",
      "Step: 8700  \tTraining accuracy: 0.8204435706138611\n",
      "Step: 8700  \tValid loss: 0.44333404302597046\n",
      "Step: 8800  \tTraining loss: 0.37885722517967224\n",
      "Step: 8800  \tTraining accuracy: 0.820518434047699\n",
      "Step: 8800  \tValid loss: 0.4433593451976776\n",
      "Step: 8900  \tTraining loss: 0.378733366727829\n",
      "Step: 8900  \tTraining accuracy: 0.8205915093421936\n",
      "Step: 8900  \tValid loss: 0.44338828325271606\n",
      "Step: 9000  \tTraining loss: 0.3786117136478424\n",
      "Step: 9000  \tTraining accuracy: 0.8206630349159241\n",
      "Step: 9000  \tValid loss: 0.44340232014656067\n",
      "Step: 9100  \tTraining loss: 0.3784914016723633\n",
      "Step: 9100  \tTraining accuracy: 0.8207329511642456\n",
      "Step: 9100  \tValid loss: 0.4433823823928833\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8208013\n",
      "Precision: 0.8566467\n",
      "Recall: 0.9550441\n",
      "F1 score: 0.86538106\n",
      "AUC: 0.69087076\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.820801   0.856647  0.955044  0.865381  0.690871  0.378423      0.820695   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0   0.44311       0.820759   0.388325      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  9156.0  \n",
      "19\n",
      "(4495, 8)\n",
      "(4495, 1)\n",
      "(2400, 8)\n",
      "(2400, 1)\n",
      "(1950, 8)\n",
      "(1950, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4391610324382782\n",
      "Step: 100  \tTraining accuracy: 0.8335928916931152\n",
      "Step: 100  \tValid loss: 0.452732652425766\n",
      "Step: 200  \tTraining loss: 0.43458986282348633\n",
      "Step: 200  \tTraining accuracy: 0.8318590521812439\n",
      "Step: 200  \tValid loss: 0.44755634665489197\n",
      "Step: 300  \tTraining loss: 0.4321216344833374\n",
      "Step: 300  \tTraining accuracy: 0.8315078020095825\n",
      "Step: 300  \tValid loss: 0.4455445408821106\n",
      "Step: 400  \tTraining loss: 0.42903873324394226\n",
      "Step: 400  \tTraining accuracy: 0.8313567638397217\n",
      "Step: 400  \tValid loss: 0.44373422861099243\n",
      "Step: 500  \tTraining loss: 0.42561081051826477\n",
      "Step: 500  \tTraining accuracy: 0.8313479423522949\n",
      "Step: 500  \tValid loss: 0.442055344581604\n",
      "Step: 600  \tTraining loss: 0.42261719703674316\n",
      "Step: 600  \tTraining accuracy: 0.831691324710846\n",
      "Step: 600  \tValid loss: 0.44070759415626526\n",
      "Step: 700  \tTraining loss: 0.42008623480796814\n",
      "Step: 700  \tTraining accuracy: 0.8319464921951294\n",
      "Step: 700  \tValid loss: 0.43938854336738586\n",
      "Step: 800  \tTraining loss: 0.41790497303009033\n",
      "Step: 800  \tTraining accuracy: 0.8320132493972778\n",
      "Step: 800  \tValid loss: 0.43792593479156494\n",
      "Step: 900  \tTraining loss: 0.41617801785469055\n",
      "Step: 900  \tTraining accuracy: 0.8320111632347107\n",
      "Step: 900  \tValid loss: 0.43678081035614014\n",
      "Step: 1000  \tTraining loss: 0.4147716164588928\n",
      "Step: 1000  \tTraining accuracy: 0.8319738507270813\n",
      "Step: 1000  \tValid loss: 0.43581482768058777\n",
      "Step: 1100  \tTraining loss: 0.4135039150714874\n",
      "Step: 1100  \tTraining accuracy: 0.8318682909011841\n",
      "Step: 1100  \tValid loss: 0.4349191188812256\n",
      "Step: 1200  \tTraining loss: 0.4123072922229767\n",
      "Step: 1200  \tTraining accuracy: 0.8317614793777466\n",
      "Step: 1200  \tValid loss: 0.4340921938419342\n",
      "Step: 1300  \tTraining loss: 0.41108545660972595\n",
      "Step: 1300  \tTraining accuracy: 0.8316988348960876\n",
      "Step: 1300  \tValid loss: 0.4332943856716156\n",
      "Step: 1400  \tTraining loss: 0.4098028838634491\n",
      "Step: 1400  \tTraining accuracy: 0.8316789269447327\n",
      "Step: 1400  \tValid loss: 0.43252918124198914\n",
      "Step: 1500  \tTraining loss: 0.4082716107368469\n",
      "Step: 1500  \tTraining accuracy: 0.8317241668701172\n",
      "Step: 1500  \tValid loss: 0.4314732849597931\n",
      "Step: 1600  \tTraining loss: 0.4069484770298004\n",
      "Step: 1600  \tTraining accuracy: 0.8318072557449341\n",
      "Step: 1600  \tValid loss: 0.43063628673553467\n",
      "Step: 1700  \tTraining loss: 0.4057851731777191\n",
      "Step: 1700  \tTraining accuracy: 0.831921398639679\n",
      "Step: 1700  \tValid loss: 0.43020516633987427\n",
      "Step: 1800  \tTraining loss: 0.40471214056015015\n",
      "Step: 1800  \tTraining accuracy: 0.832003116607666\n",
      "Step: 1800  \tValid loss: 0.42966943979263306\n",
      "Step: 1900  \tTraining loss: 0.403679221868515\n",
      "Step: 1900  \tTraining accuracy: 0.8320576548576355\n",
      "Step: 1900  \tValid loss: 0.42903757095336914\n",
      "Step: 2000  \tTraining loss: 0.4024820625782013\n",
      "Step: 2000  \tTraining accuracy: 0.8321356177330017\n",
      "Step: 2000  \tValid loss: 0.4282764792442322\n",
      "Step: 2100  \tTraining loss: 0.4013484716415405\n",
      "Step: 2100  \tTraining accuracy: 0.832239031791687\n",
      "Step: 2100  \tValid loss: 0.42742839455604553\n",
      "Step: 2200  \tTraining loss: 0.4003818929195404\n",
      "Step: 2200  \tTraining accuracy: 0.8323118090629578\n",
      "Step: 2200  \tValid loss: 0.4267737865447998\n",
      "Step: 2300  \tTraining loss: 0.3995969295501709\n",
      "Step: 2300  \tTraining accuracy: 0.8323630690574646\n",
      "Step: 2300  \tValid loss: 0.4262244701385498\n",
      "Step: 2400  \tTraining loss: 0.3989216387271881\n",
      "Step: 2400  \tTraining accuracy: 0.8324003219604492\n",
      "Step: 2400  \tValid loss: 0.42569077014923096\n",
      "Step: 2500  \tTraining loss: 0.39832907915115356\n",
      "Step: 2500  \tTraining accuracy: 0.8324345350265503\n",
      "Step: 2500  \tValid loss: 0.4252329468727112\n",
      "Step: 2600  \tTraining loss: 0.3977677524089813\n",
      "Step: 2600  \tTraining accuracy: 0.8324660658836365\n",
      "Step: 2600  \tValid loss: 0.4247734546661377\n",
      "Step: 2700  \tTraining loss: 0.3972485661506653\n",
      "Step: 2700  \tTraining accuracy: 0.8325165510177612\n",
      "Step: 2700  \tValid loss: 0.4242618978023529\n",
      "Step: 2800  \tTraining loss: 0.3967501223087311\n",
      "Step: 2800  \tTraining accuracy: 0.8325634002685547\n",
      "Step: 2800  \tValid loss: 0.42378613352775574\n",
      "Step: 2900  \tTraining loss: 0.396316796541214\n",
      "Step: 2900  \tTraining accuracy: 0.8325989842414856\n",
      "Step: 2900  \tValid loss: 0.4234740436077118\n",
      "Step: 3000  \tTraining loss: 0.3959415555000305\n",
      "Step: 3000  \tTraining accuracy: 0.8326206803321838\n",
      "Step: 3000  \tValid loss: 0.4231748878955841\n",
      "Step: 3100  \tTraining loss: 0.39558786153793335\n",
      "Step: 3100  \tTraining accuracy: 0.8326520919799805\n",
      "Step: 3100  \tValid loss: 0.4228501617908478\n",
      "Step: 3200  \tTraining loss: 0.39527422189712524\n",
      "Step: 3200  \tTraining accuracy: 0.8326886296272278\n",
      "Step: 3200  \tValid loss: 0.42259690165519714\n",
      "Step: 3300  \tTraining loss: 0.39499422907829285\n",
      "Step: 3300  \tTraining accuracy: 0.8327229619026184\n",
      "Step: 3300  \tValid loss: 0.42241621017456055\n",
      "Step: 3400  \tTraining loss: 0.39470571279525757\n",
      "Step: 3400  \tTraining accuracy: 0.8327451348304749\n",
      "Step: 3400  \tValid loss: 0.4223179817199707\n",
      "Step: 3500  \tTraining loss: 0.3944157361984253\n",
      "Step: 3500  \tTraining accuracy: 0.8327430486679077\n",
      "Step: 3500  \tValid loss: 0.4222274720668793\n",
      "Step: 3600  \tTraining loss: 0.39415642619132996\n",
      "Step: 3600  \tTraining accuracy: 0.8327410817146301\n",
      "Step: 3600  \tValid loss: 0.42203646898269653\n",
      "Step: 3700  \tTraining loss: 0.3939066529273987\n",
      "Step: 3700  \tTraining accuracy: 0.8327454328536987\n",
      "Step: 3700  \tValid loss: 0.4218789339065552\n",
      "Step: 3800  \tTraining loss: 0.39366787672042847\n",
      "Step: 3800  \tTraining accuracy: 0.8327586054801941\n",
      "Step: 3800  \tValid loss: 0.4218224883079529\n",
      "Step: 3900  \tTraining loss: 0.3933936357498169\n",
      "Step: 3900  \tTraining accuracy: 0.8327740430831909\n",
      "Step: 3900  \tValid loss: 0.421775221824646\n",
      "Step: 4000  \tTraining loss: 0.3931126594543457\n",
      "Step: 4000  \tTraining accuracy: 0.8328287601470947\n",
      "Step: 4000  \tValid loss: 0.42159780859947205\n",
      "Step: 4100  \tTraining loss: 0.3929081857204437\n",
      "Step: 4100  \tTraining accuracy: 0.8328946828842163\n",
      "Step: 4100  \tValid loss: 0.42157769203186035\n",
      "Step: 4200  \tTraining loss: 0.3927100598812103\n",
      "Step: 4200  \tTraining accuracy: 0.8329574465751648\n",
      "Step: 4200  \tValid loss: 0.4215199649333954\n",
      "Step: 4300  \tTraining loss: 0.3925107717514038\n",
      "Step: 4300  \tTraining accuracy: 0.8330119848251343\n",
      "Step: 4300  \tValid loss: 0.4214782416820526\n",
      "Step: 4400  \tTraining loss: 0.39230436086654663\n",
      "Step: 4400  \tTraining accuracy: 0.8330743312835693\n",
      "Step: 4400  \tValid loss: 0.4214213490486145\n",
      "Step: 4500  \tTraining loss: 0.392107218503952\n",
      "Step: 4500  \tTraining accuracy: 0.8331364989280701\n",
      "Step: 4500  \tValid loss: 0.4214272201061249\n",
      "Step: 4600  \tTraining loss: 0.39191094040870667\n",
      "Step: 4600  \tTraining accuracy: 0.8331884145736694\n",
      "Step: 4600  \tValid loss: 0.42137372493743896\n",
      "Step: 4700  \tTraining loss: 0.39171087741851807\n",
      "Step: 4700  \tTraining accuracy: 0.8332381248474121\n",
      "Step: 4700  \tValid loss: 0.42133909463882446\n",
      "Step: 4800  \tTraining loss: 0.39150118827819824\n",
      "Step: 4800  \tTraining accuracy: 0.8332929015159607\n",
      "Step: 4800  \tValid loss: 0.42134925723075867\n",
      "Step: 4900  \tTraining loss: 0.3912798762321472\n",
      "Step: 4900  \tTraining accuracy: 0.8333407044410706\n",
      "Step: 4900  \tValid loss: 0.4213510751724243\n",
      "Step: 5000  \tTraining loss: 0.39106592535972595\n",
      "Step: 5000  \tTraining accuracy: 0.833377480506897\n",
      "Step: 5000  \tValid loss: 0.42131897807121277\n",
      "Step: 5100  \tTraining loss: 0.390838623046875\n",
      "Step: 5100  \tTraining accuracy: 0.8334082961082458\n",
      "Step: 5100  \tValid loss: 0.4212822914123535\n",
      "Step: 5200  \tTraining loss: 0.39064180850982666\n",
      "Step: 5200  \tTraining accuracy: 0.833437979221344\n",
      "Step: 5200  \tValid loss: 0.42125403881073\n",
      "Step: 5300  \tTraining loss: 0.39045315980911255\n",
      "Step: 5300  \tTraining accuracy: 0.8334664702415466\n",
      "Step: 5300  \tValid loss: 0.4212592542171478\n",
      "Step: 5400  \tTraining loss: 0.3902699947357178\n",
      "Step: 5400  \tTraining accuracy: 0.8334960341453552\n",
      "Step: 5400  \tValid loss: 0.42123767733573914\n",
      "Step: 5500  \tTraining loss: 0.39009466767311096\n",
      "Step: 5500  \tTraining accuracy: 0.8335203528404236\n",
      "Step: 5500  \tValid loss: 0.4212421476840973\n",
      "Step: 5600  \tTraining loss: 0.3899228274822235\n",
      "Step: 5600  \tTraining accuracy: 0.8335376977920532\n",
      "Step: 5600  \tValid loss: 0.42124253511428833\n",
      "Step: 5700  \tTraining loss: 0.38975414633750916\n",
      "Step: 5700  \tTraining accuracy: 0.8335724472999573\n",
      "Step: 5700  \tValid loss: 0.42121052742004395\n",
      "Step: 5800  \tTraining loss: 0.38958966732025146\n",
      "Step: 5800  \tTraining accuracy: 0.8336138129234314\n",
      "Step: 5800  \tValid loss: 0.4212118089199066\n",
      "Step: 5900  \tTraining loss: 0.3894236981868744\n",
      "Step: 5900  \tTraining accuracy: 0.8336557149887085\n",
      "Step: 5900  \tValid loss: 0.4211919605731964\n",
      "Step: 6000  \tTraining loss: 0.3892614543437958\n",
      "Step: 6000  \tTraining accuracy: 0.833696186542511\n",
      "Step: 6000  \tValid loss: 0.42119070887565613\n",
      "Step: 6100  \tTraining loss: 0.3890971541404724\n",
      "Step: 6100  \tTraining accuracy: 0.8337409496307373\n",
      "Step: 6100  \tValid loss: 0.42116907238960266\n",
      "Step: 6200  \tTraining loss: 0.388933002948761\n",
      "Step: 6200  \tTraining accuracy: 0.833782434463501\n",
      "Step: 6200  \tValid loss: 0.42116236686706543\n",
      "Step: 6300  \tTraining loss: 0.38877052068710327\n",
      "Step: 6300  \tTraining accuracy: 0.8338207602500916\n",
      "Step: 6300  \tValid loss: 0.4211414158344269\n",
      "Step: 6400  \tTraining loss: 0.3885806202888489\n",
      "Step: 6400  \tTraining accuracy: 0.8338667750358582\n",
      "Step: 6400  \tValid loss: 0.42100775241851807\n",
      "Step: 6500  \tTraining loss: 0.3883819580078125\n",
      "Step: 6500  \tTraining accuracy: 0.8339289426803589\n",
      "Step: 6500  \tValid loss: 0.42088955640792847\n",
      "Step: 6600  \tTraining loss: 0.38819000124931335\n",
      "Step: 6600  \tTraining accuracy: 0.833999514579773\n",
      "Step: 6600  \tValid loss: 0.4208907186985016\n",
      "Step: 6700  \tTraining loss: 0.3880019783973694\n",
      "Step: 6700  \tTraining accuracy: 0.8340662717819214\n",
      "Step: 6700  \tValid loss: 0.42082276940345764\n",
      "Step: 6800  \tTraining loss: 0.3878195285797119\n",
      "Step: 6800  \tTraining accuracy: 0.8341377377510071\n",
      "Step: 6800  \tValid loss: 0.42075175046920776\n",
      "Step: 6900  \tTraining loss: 0.3876301050186157\n",
      "Step: 6900  \tTraining accuracy: 0.8342071771621704\n",
      "Step: 6900  \tValid loss: 0.4206831455230713\n",
      "Step: 7000  \tTraining loss: 0.38742008805274963\n",
      "Step: 7000  \tTraining accuracy: 0.8342745900154114\n",
      "Step: 7000  \tValid loss: 0.42057061195373535\n",
      "Step: 7100  \tTraining loss: 0.3871963322162628\n",
      "Step: 7100  \tTraining accuracy: 0.8343400359153748\n",
      "Step: 7100  \tValid loss: 0.42047378420829773\n",
      "Step: 7200  \tTraining loss: 0.3869832456111908\n",
      "Step: 7200  \tTraining accuracy: 0.83441162109375\n",
      "Step: 7200  \tValid loss: 0.420384019613266\n",
      "Step: 7300  \tTraining loss: 0.3867703974246979\n",
      "Step: 7300  \tTraining accuracy: 0.8344858884811401\n",
      "Step: 7300  \tValid loss: 0.42035120725631714\n",
      "Step: 7400  \tTraining loss: 0.3865619897842407\n",
      "Step: 7400  \tTraining accuracy: 0.8345611691474915\n",
      "Step: 7400  \tValid loss: 0.420339971780777\n",
      "Step: 7500  \tTraining loss: 0.38634735345840454\n",
      "Step: 7500  \tTraining accuracy: 0.8346390724182129\n",
      "Step: 7500  \tValid loss: 0.4202573299407959\n",
      "Step: 7600  \tTraining loss: 0.3861302435398102\n",
      "Step: 7600  \tTraining accuracy: 0.8347178101539612\n",
      "Step: 7600  \tValid loss: 0.42015716433525085\n",
      "Step: 7700  \tTraining loss: 0.3859352767467499\n",
      "Step: 7700  \tTraining accuracy: 0.8347975015640259\n",
      "Step: 7700  \tValid loss: 0.42010191082954407\n",
      "Step: 7800  \tTraining loss: 0.3857508897781372\n",
      "Step: 7800  \tTraining accuracy: 0.8348795175552368\n",
      "Step: 7800  \tValid loss: 0.42009568214416504\n",
      "Step: 7900  \tTraining loss: 0.38557031750679016\n",
      "Step: 7900  \tTraining accuracy: 0.8349594473838806\n",
      "Step: 7900  \tValid loss: 0.4201129972934723\n",
      "Step: 8000  \tTraining loss: 0.3853774666786194\n",
      "Step: 8000  \tTraining accuracy: 0.835037350654602\n",
      "Step: 8000  \tValid loss: 0.42005324363708496\n",
      "Step: 8100  \tTraining loss: 0.385173499584198\n",
      "Step: 8100  \tTraining accuracy: 0.8351231217384338\n",
      "Step: 8100  \tValid loss: 0.41999176144599915\n",
      "Step: 8200  \tTraining loss: 0.3849816918373108\n",
      "Step: 8200  \tTraining accuracy: 0.8352165222167969\n",
      "Step: 8200  \tValid loss: 0.4198421239852905\n",
      "Step: 8300  \tTraining loss: 0.38480791449546814\n",
      "Step: 8300  \tTraining accuracy: 0.8353186249732971\n",
      "Step: 8300  \tValid loss: 0.41978615522384644\n",
      "Step: 8400  \tTraining loss: 0.3846367299556732\n",
      "Step: 8400  \tTraining accuracy: 0.8354223370552063\n",
      "Step: 8400  \tValid loss: 0.419710636138916\n",
      "Step: 8500  \tTraining loss: 0.3844548463821411\n",
      "Step: 8500  \tTraining accuracy: 0.8355182409286499\n",
      "Step: 8500  \tValid loss: 0.4196684956550598\n",
      "Step: 8600  \tTraining loss: 0.3842749297618866\n",
      "Step: 8600  \tTraining accuracy: 0.8356119394302368\n",
      "Step: 8600  \tValid loss: 0.4196087718009949\n",
      "Step: 8700  \tTraining loss: 0.38409775495529175\n",
      "Step: 8700  \tTraining accuracy: 0.8357086181640625\n",
      "Step: 8700  \tValid loss: 0.41955652832984924\n",
      "Step: 8800  \tTraining loss: 0.38392284512519836\n",
      "Step: 8800  \tTraining accuracy: 0.8358031511306763\n",
      "Step: 8800  \tValid loss: 0.41951197385787964\n",
      "Step: 8900  \tTraining loss: 0.3837457597255707\n",
      "Step: 8900  \tTraining accuracy: 0.8358955383300781\n",
      "Step: 8900  \tValid loss: 0.4194577932357788\n",
      "Step: 9000  \tTraining loss: 0.3835606276988983\n",
      "Step: 9000  \tTraining accuracy: 0.8359884023666382\n",
      "Step: 9000  \tValid loss: 0.4194093942642212\n",
      "Step: 9100  \tTraining loss: 0.38337743282318115\n",
      "Step: 9100  \tTraining accuracy: 0.8360816836357117\n",
      "Step: 9100  \tValid loss: 0.4193415939807892\n",
      "Step: 9200  \tTraining loss: 0.3831944465637207\n",
      "Step: 9200  \tTraining accuracy: 0.8361679911613464\n",
      "Step: 9200  \tValid loss: 0.41926896572113037\n",
      "Step: 9300  \tTraining loss: 0.3830100893974304\n",
      "Step: 9300  \tTraining accuracy: 0.8362524509429932\n",
      "Step: 9300  \tValid loss: 0.419202595949173\n",
      "Step: 9400  \tTraining loss: 0.38282495737075806\n",
      "Step: 9400  \tTraining accuracy: 0.8363362550735474\n",
      "Step: 9400  \tValid loss: 0.41913533210754395\n",
      "Step: 9500  \tTraining loss: 0.38263174891471863\n",
      "Step: 9500  \tTraining accuracy: 0.8364147543907166\n",
      "Step: 9500  \tValid loss: 0.41903939843177795\n",
      "Step: 9600  \tTraining loss: 0.3824216425418854\n",
      "Step: 9600  \tTraining accuracy: 0.836493968963623\n",
      "Step: 9600  \tValid loss: 0.41892436146736145\n",
      "Step: 9700  \tTraining loss: 0.3821811079978943\n",
      "Step: 9700  \tTraining accuracy: 0.8365750312805176\n",
      "Step: 9700  \tValid loss: 0.41883260011672974\n",
      "Step: 9800  \tTraining loss: 0.3819330632686615\n",
      "Step: 9800  \tTraining accuracy: 0.8366509675979614\n",
      "Step: 9800  \tValid loss: 0.41858258843421936\n",
      "Step: 9900  \tTraining loss: 0.3816852569580078\n",
      "Step: 9900  \tTraining accuracy: 0.8367253541946411\n",
      "Step: 9900  \tValid loss: 0.4184240996837616\n",
      "Step: 10000  \tTraining loss: 0.38140225410461426\n",
      "Step: 10000  \tTraining accuracy: 0.8368005156517029\n",
      "Step: 10000  \tValid loss: 0.4179920554161072\n",
      "Step: 10100  \tTraining loss: 0.3811599612236023\n",
      "Step: 10100  \tTraining accuracy: 0.8368775844573975\n",
      "Step: 10100  \tValid loss: 0.41789549589157104\n",
      "Step: 10200  \tTraining loss: 0.38094040751457214\n",
      "Step: 10200  \tTraining accuracy: 0.8369565010070801\n",
      "Step: 10200  \tValid loss: 0.4177931845188141\n",
      "Step: 10300  \tTraining loss: 0.38072532415390015\n",
      "Step: 10300  \tTraining accuracy: 0.8370360136032104\n",
      "Step: 10300  \tValid loss: 0.4176858961582184\n",
      "Step: 10400  \tTraining loss: 0.3805112838745117\n",
      "Step: 10400  \tTraining accuracy: 0.8371140360832214\n",
      "Step: 10400  \tValid loss: 0.4175317585468292\n",
      "Step: 10500  \tTraining loss: 0.38029664754867554\n",
      "Step: 10500  \tTraining accuracy: 0.8371905088424683\n",
      "Step: 10500  \tValid loss: 0.41737157106399536\n",
      "Step: 10600  \tTraining loss: 0.3800836503505707\n",
      "Step: 10600  \tTraining accuracy: 0.8372656106948853\n",
      "Step: 10600  \tValid loss: 0.4172268211841583\n",
      "Step: 10700  \tTraining loss: 0.37984326481819153\n",
      "Step: 10700  \tTraining accuracy: 0.8373424410820007\n",
      "Step: 10700  \tValid loss: 0.41684210300445557\n",
      "Step: 10800  \tTraining loss: 0.3796246349811554\n",
      "Step: 10800  \tTraining accuracy: 0.8374146819114685\n",
      "Step: 10800  \tValid loss: 0.416780561208725\n",
      "Step: 10900  \tTraining loss: 0.379412442445755\n",
      "Step: 10900  \tTraining accuracy: 0.8374856114387512\n",
      "Step: 10900  \tValid loss: 0.4166494309902191\n",
      "Step: 11000  \tTraining loss: 0.3791974186897278\n",
      "Step: 11000  \tTraining accuracy: 0.8375552296638489\n",
      "Step: 11000  \tValid loss: 0.41648030281066895\n",
      "Step: 11100  \tTraining loss: 0.3789881467819214\n",
      "Step: 11100  \tTraining accuracy: 0.8376266956329346\n",
      "Step: 11100  \tValid loss: 0.4163389801979065\n",
      "Step: 11200  \tTraining loss: 0.3787803649902344\n",
      "Step: 11200  \tTraining accuracy: 0.8376988768577576\n",
      "Step: 11200  \tValid loss: 0.416176974773407\n",
      "Step: 11300  \tTraining loss: 0.3785804808139801\n",
      "Step: 11300  \tTraining accuracy: 0.8377697467803955\n",
      "Step: 11300  \tValid loss: 0.4160611033439636\n",
      "Step: 11400  \tTraining loss: 0.37838315963745117\n",
      "Step: 11400  \tTraining accuracy: 0.8378394246101379\n",
      "Step: 11400  \tValid loss: 0.4159449338912964\n",
      "Step: 11500  \tTraining loss: 0.3781718313694\n",
      "Step: 11500  \tTraining accuracy: 0.8379078507423401\n",
      "Step: 11500  \tValid loss: 0.41570407152175903\n",
      "Step: 11600  \tTraining loss: 0.3779687285423279\n",
      "Step: 11600  \tTraining accuracy: 0.8379750847816467\n",
      "Step: 11600  \tValid loss: 0.4154856503009796\n",
      "Step: 11700  \tTraining loss: 0.3777729868888855\n",
      "Step: 11700  \tTraining accuracy: 0.8380411863327026\n",
      "Step: 11700  \tValid loss: 0.4153490960597992\n",
      "Step: 11800  \tTraining loss: 0.37758105993270874\n",
      "Step: 11800  \tTraining accuracy: 0.8381061553955078\n",
      "Step: 11800  \tValid loss: 0.4152361750602722\n",
      "Step: 11900  \tTraining loss: 0.3773922324180603\n",
      "Step: 11900  \tTraining accuracy: 0.8381699919700623\n",
      "Step: 11900  \tValid loss: 0.41512593626976013\n",
      "Step: 12000  \tTraining loss: 0.37720605731010437\n",
      "Step: 12000  \tTraining accuracy: 0.8382328152656555\n",
      "Step: 12000  \tValid loss: 0.4149898886680603\n",
      "Step: 12100  \tTraining loss: 0.3770197331905365\n",
      "Step: 12100  \tTraining accuracy: 0.8382945656776428\n",
      "Step: 12100  \tValid loss: 0.41484320163726807\n",
      "Step: 12200  \tTraining loss: 0.3768359124660492\n",
      "Step: 12200  \tTraining accuracy: 0.8383562564849854\n",
      "Step: 12200  \tValid loss: 0.4147127568721771\n",
      "Step: 12300  \tTraining loss: 0.3766535818576813\n",
      "Step: 12300  \tTraining accuracy: 0.8384206295013428\n",
      "Step: 12300  \tValid loss: 0.41457051038742065\n",
      "Step: 12400  \tTraining loss: 0.37646999955177307\n",
      "Step: 12400  \tTraining accuracy: 0.8384839296340942\n",
      "Step: 12400  \tValid loss: 0.4144718050956726\n",
      "Step: 12500  \tTraining loss: 0.3762874901294708\n",
      "Step: 12500  \tTraining accuracy: 0.8385462164878845\n",
      "Step: 12500  \tValid loss: 0.41436171531677246\n",
      "Step: 12600  \tTraining loss: 0.3761090934276581\n",
      "Step: 12600  \tTraining accuracy: 0.8386075496673584\n",
      "Step: 12600  \tValid loss: 0.4142039120197296\n",
      "Step: 12700  \tTraining loss: 0.3759280741214752\n",
      "Step: 12700  \tTraining accuracy: 0.8386678695678711\n",
      "Step: 12700  \tValid loss: 0.41399627923965454\n",
      "Step: 12800  \tTraining loss: 0.3757466971874237\n",
      "Step: 12800  \tTraining accuracy: 0.8387272953987122\n",
      "Step: 12800  \tValid loss: 0.4138406813144684\n",
      "Step: 12900  \tTraining loss: 0.37556910514831543\n",
      "Step: 12900  \tTraining accuracy: 0.8387830853462219\n",
      "Step: 12900  \tValid loss: 0.4137200713157654\n",
      "Step: 13000  \tTraining loss: 0.3753921389579773\n",
      "Step: 13000  \tTraining accuracy: 0.8388363122940063\n",
      "Step: 13000  \tValid loss: 0.413588285446167\n",
      "Step: 13100  \tTraining loss: 0.37521860003471375\n",
      "Step: 13100  \tTraining accuracy: 0.8388895988464355\n",
      "Step: 13100  \tValid loss: 0.41346579790115356\n",
      "Step: 13200  \tTraining loss: 0.3750503361225128\n",
      "Step: 13200  \tTraining accuracy: 0.8389455080032349\n",
      "Step: 13200  \tValid loss: 0.4133619964122772\n",
      "Step: 13300  \tTraining loss: 0.3748827278614044\n",
      "Step: 13300  \tTraining accuracy: 0.8390005230903625\n",
      "Step: 13300  \tValid loss: 0.41325393319129944\n",
      "Step: 13400  \tTraining loss: 0.3747226297855377\n",
      "Step: 13400  \tTraining accuracy: 0.8390547633171082\n",
      "Step: 13400  \tValid loss: 0.4131835103034973\n",
      "Step: 13500  \tTraining loss: 0.3745700418949127\n",
      "Step: 13500  \tTraining accuracy: 0.8391107320785522\n",
      "Step: 13500  \tValid loss: 0.41310182213783264\n",
      "Step: 13600  \tTraining loss: 0.3744194209575653\n",
      "Step: 13600  \tTraining accuracy: 0.8391667008399963\n",
      "Step: 13600  \tValid loss: 0.41301748156547546\n",
      "Step: 13700  \tTraining loss: 0.37427225708961487\n",
      "Step: 13700  \tTraining accuracy: 0.8392184972763062\n",
      "Step: 13700  \tValid loss: 0.412937730550766\n",
      "Step: 13800  \tTraining loss: 0.3741236627101898\n",
      "Step: 13800  \tTraining accuracy: 0.8392695784568787\n",
      "Step: 13800  \tValid loss: 0.412813663482666\n",
      "Step: 13900  \tTraining loss: 0.3739764392375946\n",
      "Step: 13900  \tTraining accuracy: 0.8393198847770691\n",
      "Step: 13900  \tValid loss: 0.4126664102077484\n",
      "Step: 14000  \tTraining loss: 0.3738346993923187\n",
      "Step: 14000  \tTraining accuracy: 0.8393694758415222\n",
      "Step: 14000  \tValid loss: 0.4125746190547943\n",
      "Step: 14100  \tTraining loss: 0.37370026111602783\n",
      "Step: 14100  \tTraining accuracy: 0.8394184112548828\n",
      "Step: 14100  \tValid loss: 0.4125087857246399\n",
      "Step: 14200  \tTraining loss: 0.3735702931880951\n",
      "Step: 14200  \tTraining accuracy: 0.8394666314125061\n",
      "Step: 14200  \tValid loss: 0.41244634985923767\n",
      "Step: 14300  \tTraining loss: 0.37344107031822205\n",
      "Step: 14300  \tTraining accuracy: 0.8395141363143921\n",
      "Step: 14300  \tValid loss: 0.41235098242759705\n",
      "Step: 14400  \tTraining loss: 0.37331441044807434\n",
      "Step: 14400  \tTraining accuracy: 0.8395610451698303\n",
      "Step: 14400  \tValid loss: 0.41225314140319824\n",
      "Step: 14500  \tTraining loss: 0.3731890022754669\n",
      "Step: 14500  \tTraining accuracy: 0.8396072387695312\n",
      "Step: 14500  \tValid loss: 0.4121570289134979\n",
      "Step: 14600  \tTraining loss: 0.3730658292770386\n",
      "Step: 14600  \tTraining accuracy: 0.8396528363227844\n",
      "Step: 14600  \tValid loss: 0.4120652377605438\n",
      "Step: 14700  \tTraining loss: 0.3729456961154938\n",
      "Step: 14700  \tTraining accuracy: 0.8396978378295898\n",
      "Step: 14700  \tValid loss: 0.41198596358299255\n",
      "Step: 14800  \tTraining loss: 0.3728264570236206\n",
      "Step: 14800  \tTraining accuracy: 0.8397421836853027\n",
      "Step: 14800  \tValid loss: 0.4118776321411133\n",
      "Step: 14900  \tTraining loss: 0.37270843982696533\n",
      "Step: 14900  \tTraining accuracy: 0.8397859334945679\n",
      "Step: 14900  \tValid loss: 0.41176941990852356\n",
      "Step: 15000  \tTraining loss: 0.3725830912590027\n",
      "Step: 15000  \tTraining accuracy: 0.8398290872573853\n",
      "Step: 15000  \tValid loss: 0.41169676184654236\n",
      "Step: 15100  \tTraining loss: 0.3724670708179474\n",
      "Step: 15100  \tTraining accuracy: 0.8398717045783997\n",
      "Step: 15100  \tValid loss: 0.41160330176353455\n",
      "Step: 15200  \tTraining loss: 0.3723539412021637\n",
      "Step: 15200  \tTraining accuracy: 0.8399137258529663\n",
      "Step: 15200  \tValid loss: 0.4115348160266876\n",
      "Step: 15300  \tTraining loss: 0.3722425401210785\n",
      "Step: 15300  \tTraining accuracy: 0.8399537205696106\n",
      "Step: 15300  \tValid loss: 0.4114437401294708\n",
      "Step: 15400  \tTraining loss: 0.37213391065597534\n",
      "Step: 15400  \tTraining accuracy: 0.8399873375892639\n",
      "Step: 15400  \tValid loss: 0.4113920331001282\n",
      "Step: 15500  \tTraining loss: 0.37202343344688416\n",
      "Step: 15500  \tTraining accuracy: 0.8400153517723083\n",
      "Step: 15500  \tValid loss: 0.41131269931793213\n",
      "Step: 15600  \tTraining loss: 0.3719157874584198\n",
      "Step: 15600  \tTraining accuracy: 0.8400401473045349\n",
      "Step: 15600  \tValid loss: 0.41125044226646423\n",
      "Step: 15700  \tTraining loss: 0.37180742621421814\n",
      "Step: 15700  \tTraining accuracy: 0.8400631546974182\n",
      "Step: 15700  \tValid loss: 0.4111556112766266\n",
      "Step: 15800  \tTraining loss: 0.37170112133026123\n",
      "Step: 15800  \tTraining accuracy: 0.8400901556015015\n",
      "Step: 15800  \tValid loss: 0.41109874844551086\n",
      "Step: 15900  \tTraining loss: 0.3715958893299103\n",
      "Step: 15900  \tTraining accuracy: 0.8401182889938354\n",
      "Step: 15900  \tValid loss: 0.41101473569869995\n",
      "Step: 16000  \tTraining loss: 0.37149301171302795\n",
      "Step: 16000  \tTraining accuracy: 0.8401474356651306\n",
      "Step: 16000  \tValid loss: 0.4109744131565094\n",
      "Step: 16100  \tTraining loss: 0.3713870346546173\n",
      "Step: 16100  \tTraining accuracy: 0.840178370475769\n",
      "Step: 16100  \tValid loss: 0.41087260842323303\n",
      "Step: 16200  \tTraining loss: 0.37128525972366333\n",
      "Step: 16200  \tTraining accuracy: 0.840208888053894\n",
      "Step: 16200  \tValid loss: 0.41076651215553284\n",
      "Step: 16300  \tTraining loss: 0.37118539214134216\n",
      "Step: 16300  \tTraining accuracy: 0.8402369618415833\n",
      "Step: 16300  \tValid loss: 0.41068053245544434\n",
      "Step: 16400  \tTraining loss: 0.3710869550704956\n",
      "Step: 16400  \tTraining accuracy: 0.840263307094574\n",
      "Step: 16400  \tValid loss: 0.41061919927597046\n",
      "Step: 16500  \tTraining loss: 0.370987206697464\n",
      "Step: 16500  \tTraining accuracy: 0.8402893543243408\n",
      "Step: 16500  \tValid loss: 0.410535603761673\n",
      "Step: 16600  \tTraining loss: 0.37088340520858765\n",
      "Step: 16600  \tTraining accuracy: 0.840315043926239\n",
      "Step: 16600  \tValid loss: 0.4104394018650055\n",
      "Step: 16700  \tTraining loss: 0.3707786202430725\n",
      "Step: 16700  \tTraining accuracy: 0.8403404355049133\n",
      "Step: 16700  \tValid loss: 0.41035163402557373\n",
      "Step: 16800  \tTraining loss: 0.3706764876842499\n",
      "Step: 16800  \tTraining accuracy: 0.8403668999671936\n",
      "Step: 16800  \tValid loss: 0.4102781414985657\n",
      "Step: 16900  \tTraining loss: 0.37057289481163025\n",
      "Step: 16900  \tTraining accuracy: 0.8403950333595276\n",
      "Step: 16900  \tValid loss: 0.4101782739162445\n",
      "Step: 17000  \tTraining loss: 0.37047451734542847\n",
      "Step: 17000  \tTraining accuracy: 0.8404228687286377\n",
      "Step: 17000  \tValid loss: 0.4101358652114868\n",
      "Step: 17100  \tTraining loss: 0.37037888169288635\n",
      "Step: 17100  \tTraining accuracy: 0.8404503464698792\n",
      "Step: 17100  \tValid loss: 0.4100792407989502\n",
      "Step: 17200  \tTraining loss: 0.37027841806411743\n",
      "Step: 17200  \tTraining accuracy: 0.8404775261878967\n",
      "Step: 17200  \tValid loss: 0.41001641750335693\n",
      "Step: 17300  \tTraining loss: 0.3701774775981903\n",
      "Step: 17300  \tTraining accuracy: 0.8405044078826904\n",
      "Step: 17300  \tValid loss: 0.40994563698768616\n",
      "Step: 17400  \tTraining loss: 0.37007978558540344\n",
      "Step: 17400  \tTraining accuracy: 0.8405269980430603\n",
      "Step: 17400  \tValid loss: 0.4099003076553345\n",
      "Step: 17500  \tTraining loss: 0.36997973918914795\n",
      "Step: 17500  \tTraining accuracy: 0.8405500650405884\n",
      "Step: 17500  \tValid loss: 0.4098382592201233\n",
      "Step: 17600  \tTraining loss: 0.3698795437812805\n",
      "Step: 17600  \tTraining accuracy: 0.8405727744102478\n",
      "Step: 17600  \tValid loss: 0.4097828269004822\n",
      "Step: 17700  \tTraining loss: 0.36977896094322205\n",
      "Step: 17700  \tTraining accuracy: 0.8405953049659729\n",
      "Step: 17700  \tValid loss: 0.4097127914428711\n",
      "Step: 17800  \tTraining loss: 0.3696786165237427\n",
      "Step: 17800  \tTraining accuracy: 0.8406201004981995\n",
      "Step: 17800  \tValid loss: 0.409665048122406\n",
      "Step: 17900  \tTraining loss: 0.36957770586013794\n",
      "Step: 17900  \tTraining accuracy: 0.8406445980072021\n",
      "Step: 17900  \tValid loss: 0.4096132814884186\n",
      "Step: 18000  \tTraining loss: 0.3694815933704376\n",
      "Step: 18000  \tTraining accuracy: 0.8406663537025452\n",
      "Step: 18000  \tValid loss: 0.40957018733024597\n",
      "Step: 18100  \tTraining loss: 0.3693889081478119\n",
      "Step: 18100  \tTraining accuracy: 0.8406878113746643\n",
      "Step: 18100  \tValid loss: 0.4095386862754822\n",
      "Step: 18200  \tTraining loss: 0.36929798126220703\n",
      "Step: 18200  \tTraining accuracy: 0.8407090902328491\n",
      "Step: 18200  \tValid loss: 0.40948060154914856\n",
      "Step: 18300  \tTraining loss: 0.36920905113220215\n",
      "Step: 18300  \tTraining accuracy: 0.8407301306724548\n",
      "Step: 18300  \tValid loss: 0.4094168245792389\n",
      "Step: 18400  \tTraining loss: 0.36912065744400024\n",
      "Step: 18400  \tTraining accuracy: 0.8407508730888367\n",
      "Step: 18400  \tValid loss: 0.4093552529811859\n",
      "Step: 18500  \tTraining loss: 0.36903247237205505\n",
      "Step: 18500  \tTraining accuracy: 0.8407689929008484\n",
      "Step: 18500  \tValid loss: 0.40929198265075684\n",
      "Step: 18600  \tTraining loss: 0.368940144777298\n",
      "Step: 18600  \tTraining accuracy: 0.8407887816429138\n",
      "Step: 18600  \tValid loss: 0.40920138359069824\n",
      "Step: 18700  \tTraining loss: 0.36884281039237976\n",
      "Step: 18700  \tTraining accuracy: 0.8408046364784241\n",
      "Step: 18700  \tValid loss: 0.40909621119499207\n",
      "Step: 18800  \tTraining loss: 0.3687492311000824\n",
      "Step: 18800  \tTraining accuracy: 0.8408185839653015\n",
      "Step: 18800  \tValid loss: 0.4090256094932556\n",
      "Step: 18900  \tTraining loss: 0.3686574697494507\n",
      "Step: 18900  \tTraining accuracy: 0.8408323526382446\n",
      "Step: 18900  \tValid loss: 0.4089473485946655\n",
      "Step: 19000  \tTraining loss: 0.36847689747810364\n",
      "Step: 19000  \tTraining accuracy: 0.8408465385437012\n",
      "Step: 19000  \tValid loss: 0.4084821045398712\n",
      "Step: 19100  \tTraining loss: 0.36831751465797424\n",
      "Step: 19100  \tTraining accuracy: 0.8408623933792114\n",
      "Step: 19100  \tValid loss: 0.4082361161708832\n",
      "Step: 19200  \tTraining loss: 0.3681928515434265\n",
      "Step: 19200  \tTraining accuracy: 0.8408786654472351\n",
      "Step: 19200  \tValid loss: 0.408103883266449\n",
      "Step: 19300  \tTraining loss: 0.3680776357650757\n",
      "Step: 19300  \tTraining accuracy: 0.840893030166626\n",
      "Step: 19300  \tValid loss: 0.40803414583206177\n",
      "Step: 19400  \tTraining loss: 0.367959201335907\n",
      "Step: 19400  \tTraining accuracy: 0.8409060835838318\n",
      "Step: 19400  \tValid loss: 0.4079437255859375\n",
      "Step: 19500  \tTraining loss: 0.36784613132476807\n",
      "Step: 19500  \tTraining accuracy: 0.8409189581871033\n",
      "Step: 19500  \tValid loss: 0.407843679189682\n",
      "Step: 19600  \tTraining loss: 0.3677409291267395\n",
      "Step: 19600  \tTraining accuracy: 0.8409317135810852\n",
      "Step: 19600  \tValid loss: 0.40779951214790344\n",
      "Step: 19700  \tTraining loss: 0.36760619282722473\n",
      "Step: 19700  \tTraining accuracy: 0.8409460783004761\n",
      "Step: 19700  \tValid loss: 0.4076980650424957\n",
      "Step: 19800  \tTraining loss: 0.36747345328330994\n",
      "Step: 19800  \tTraining accuracy: 0.8409614562988281\n",
      "Step: 19800  \tValid loss: 0.4075605273246765\n",
      "Step: 19900  \tTraining loss: 0.36735498905181885\n",
      "Step: 19900  \tTraining accuracy: 0.8409766554832458\n",
      "Step: 19900  \tValid loss: 0.4074881374835968\n",
      "Step: 20000  \tTraining loss: 0.3672410249710083\n",
      "Step: 20000  \tTraining accuracy: 0.8409916758537292\n",
      "Step: 20000  \tValid loss: 0.4074307382106781\n",
      "Step: 20100  \tTraining loss: 0.36713021993637085\n",
      "Step: 20100  \tTraining accuracy: 0.8410077095031738\n",
      "Step: 20100  \tValid loss: 0.4074055552482605\n",
      "Step: 20200  \tTraining loss: 0.3670197129249573\n",
      "Step: 20200  \tTraining accuracy: 0.8410252928733826\n",
      "Step: 20200  \tValid loss: 0.4073373079299927\n",
      "Step: 20300  \tTraining loss: 0.36690038442611694\n",
      "Step: 20300  \tTraining accuracy: 0.84104323387146\n",
      "Step: 20300  \tValid loss: 0.4071499705314636\n",
      "Step: 20400  \tTraining loss: 0.3667854368686676\n",
      "Step: 20400  \tTraining accuracy: 0.8410632014274597\n",
      "Step: 20400  \tValid loss: 0.40707001090049744\n",
      "Step: 20500  \tTraining loss: 0.36667558550834656\n",
      "Step: 20500  \tTraining accuracy: 0.8410824537277222\n",
      "Step: 20500  \tValid loss: 0.40695634484291077\n",
      "Step: 20600  \tTraining loss: 0.36657312512397766\n",
      "Step: 20600  \tTraining accuracy: 0.8410992622375488\n",
      "Step: 20600  \tValid loss: 0.40687841176986694\n",
      "Step: 20700  \tTraining loss: 0.3664703369140625\n",
      "Step: 20700  \tTraining accuracy: 0.8411159515380859\n",
      "Step: 20700  \tValid loss: 0.40678608417510986\n",
      "Step: 20800  \tTraining loss: 0.3663702607154846\n",
      "Step: 20800  \tTraining accuracy: 0.8411325216293335\n",
      "Step: 20800  \tValid loss: 0.4067121744155884\n",
      "Step: 20900  \tTraining loss: 0.36627107858657837\n",
      "Step: 20900  \tTraining accuracy: 0.8411493897438049\n",
      "Step: 20900  \tValid loss: 0.40664127469062805\n",
      "Step: 21000  \tTraining loss: 0.36617088317871094\n",
      "Step: 21000  \tTraining accuracy: 0.8411682844161987\n",
      "Step: 21000  \tValid loss: 0.40655508637428284\n",
      "Step: 21100  \tTraining loss: 0.3660728633403778\n",
      "Step: 21100  \tTraining accuracy: 0.841187059879303\n",
      "Step: 21100  \tValid loss: 0.4064958095550537\n",
      "Step: 21200  \tTraining loss: 0.3659754991531372\n",
      "Step: 21200  \tTraining accuracy: 0.8412055969238281\n",
      "Step: 21200  \tValid loss: 0.4064445495605469\n",
      "Step: 21300  \tTraining loss: 0.3658771216869354\n",
      "Step: 21300  \tTraining accuracy: 0.841222882270813\n",
      "Step: 21300  \tValid loss: 0.4063546359539032\n",
      "Step: 21400  \tTraining loss: 0.36578062176704407\n",
      "Step: 21400  \tTraining accuracy: 0.8412384390830994\n",
      "Step: 21400  \tValid loss: 0.406297504901886\n",
      "Step: 21500  \tTraining loss: 0.3656826317310333\n",
      "Step: 21500  \tTraining accuracy: 0.8412538766860962\n",
      "Step: 21500  \tValid loss: 0.40621256828308105\n",
      "Step: 21600  \tTraining loss: 0.36558446288108826\n",
      "Step: 21600  \tTraining accuracy: 0.8412691354751587\n",
      "Step: 21600  \tValid loss: 0.40613406896591187\n",
      "Step: 21700  \tTraining loss: 0.3654884696006775\n",
      "Step: 21700  \tTraining accuracy: 0.8412858247756958\n",
      "Step: 21700  \tValid loss: 0.40606504678726196\n",
      "Step: 21800  \tTraining loss: 0.36539119482040405\n",
      "Step: 21800  \tTraining accuracy: 0.8413044810295105\n",
      "Step: 21800  \tValid loss: 0.40598264336586\n",
      "Step: 21900  \tTraining loss: 0.3652958571910858\n",
      "Step: 21900  \tTraining accuracy: 0.8413244485855103\n",
      "Step: 21900  \tValid loss: 0.4059176445007324\n",
      "Step: 22000  \tTraining loss: 0.3651995062828064\n",
      "Step: 22000  \tTraining accuracy: 0.8413442969322205\n",
      "Step: 22000  \tValid loss: 0.40585699677467346\n",
      "Step: 22100  \tTraining loss: 0.3651025593280792\n",
      "Step: 22100  \tTraining accuracy: 0.8413639068603516\n",
      "Step: 22100  \tValid loss: 0.4057682156562805\n",
      "Step: 22200  \tTraining loss: 0.3650047481060028\n",
      "Step: 22200  \tTraining accuracy: 0.8413833975791931\n",
      "Step: 22200  \tValid loss: 0.4057082235813141\n",
      "Step: 22300  \tTraining loss: 0.364902138710022\n",
      "Step: 22300  \tTraining accuracy: 0.8414047360420227\n",
      "Step: 22300  \tValid loss: 0.4056282937526703\n",
      "Step: 22400  \tTraining loss: 0.3647933006286621\n",
      "Step: 22400  \tTraining accuracy: 0.8414263725280762\n",
      "Step: 22400  \tValid loss: 0.4055507183074951\n",
      "Step: 22500  \tTraining loss: 0.36468076705932617\n",
      "Step: 22500  \tTraining accuracy: 0.8414503335952759\n",
      "Step: 22500  \tValid loss: 0.40549662709236145\n",
      "Step: 22600  \tTraining loss: 0.364569753408432\n",
      "Step: 22600  \tTraining accuracy: 0.8414715528488159\n",
      "Step: 22600  \tValid loss: 0.4054398238658905\n",
      "Step: 22700  \tTraining loss: 0.36446312069892883\n",
      "Step: 22700  \tTraining accuracy: 0.8414925932884216\n",
      "Step: 22700  \tValid loss: 0.4053914248943329\n",
      "Step: 22800  \tTraining loss: 0.36435189843177795\n",
      "Step: 22800  \tTraining accuracy: 0.841513454914093\n",
      "Step: 22800  \tValid loss: 0.4052714705467224\n",
      "Step: 22900  \tTraining loss: 0.36423787474632263\n",
      "Step: 22900  \tTraining accuracy: 0.8415341973304749\n",
      "Step: 22900  \tValid loss: 0.40520429611206055\n",
      "Step: 23000  \tTraining loss: 0.36412712931632996\n",
      "Step: 23000  \tTraining accuracy: 0.8415551781654358\n",
      "Step: 23000  \tValid loss: 0.4051504135131836\n",
      "Step: 23100  \tTraining loss: 0.3640049397945404\n",
      "Step: 23100  \tTraining accuracy: 0.8415774703025818\n",
      "Step: 23100  \tValid loss: 0.40498101711273193\n",
      "Step: 23200  \tTraining loss: 0.36389267444610596\n",
      "Step: 23200  \tTraining accuracy: 0.8416000604629517\n",
      "Step: 23200  \tValid loss: 0.4049791097640991\n",
      "Step: 23300  \tTraining loss: 0.3637663424015045\n",
      "Step: 23300  \tTraining accuracy: 0.8416219353675842\n",
      "Step: 23300  \tValid loss: 0.40469345450401306\n",
      "Step: 23400  \tTraining loss: 0.3636411428451538\n",
      "Step: 23400  \tTraining accuracy: 0.8416417241096497\n",
      "Step: 23400  \tValid loss: 0.4046659469604492\n",
      "Step: 23500  \tTraining loss: 0.36352822184562683\n",
      "Step: 23500  \tTraining accuracy: 0.8416613340377808\n",
      "Step: 23500  \tValid loss: 0.4046013355255127\n",
      "Step: 23600  \tTraining loss: 0.36340996623039246\n",
      "Step: 23600  \tTraining accuracy: 0.8416793346405029\n",
      "Step: 23600  \tValid loss: 0.4044401943683624\n",
      "Step: 23700  \tTraining loss: 0.363293319940567\n",
      "Step: 23700  \tTraining accuracy: 0.8416976928710938\n",
      "Step: 23700  \tValid loss: 0.4044603109359741\n",
      "Step: 23800  \tTraining loss: 0.3631812036037445\n",
      "Step: 23800  \tTraining accuracy: 0.8417168259620667\n",
      "Step: 23800  \tValid loss: 0.4044083058834076\n",
      "Step: 23900  \tTraining loss: 0.36307206749916077\n",
      "Step: 23900  \tTraining accuracy: 0.8417357802391052\n",
      "Step: 23900  \tValid loss: 0.40438202023506165\n",
      "Step: 24000  \tTraining loss: 0.3629623055458069\n",
      "Step: 24000  \tTraining accuracy: 0.8417545557022095\n",
      "Step: 24000  \tValid loss: 0.4043526351451874\n",
      "Step: 24100  \tTraining loss: 0.36285215616226196\n",
      "Step: 24100  \tTraining accuracy: 0.8417732119560242\n",
      "Step: 24100  \tValid loss: 0.40430328249931335\n",
      "Step: 24200  \tTraining loss: 0.3627430200576782\n",
      "Step: 24200  \tTraining accuracy: 0.8417917490005493\n",
      "Step: 24200  \tValid loss: 0.40428581833839417\n",
      "Step: 24300  \tTraining loss: 0.36263394355773926\n",
      "Step: 24300  \tTraining accuracy: 0.8418101072311401\n",
      "Step: 24300  \tValid loss: 0.40425777435302734\n",
      "Step: 24400  \tTraining loss: 0.36252206563949585\n",
      "Step: 24400  \tTraining accuracy: 0.8418315052986145\n",
      "Step: 24400  \tValid loss: 0.4042167663574219\n",
      "Step: 24500  \tTraining loss: 0.36241012811660767\n",
      "Step: 24500  \tTraining accuracy: 0.8418542146682739\n",
      "Step: 24500  \tValid loss: 0.4041694104671478\n",
      "Step: 24600  \tTraining loss: 0.3622984290122986\n",
      "Step: 24600  \tTraining accuracy: 0.8418780565261841\n",
      "Step: 24600  \tValid loss: 0.4041292667388916\n",
      "Step: 24700  \tTraining loss: 0.3621864318847656\n",
      "Step: 24700  \tTraining accuracy: 0.8419039845466614\n",
      "Step: 24700  \tValid loss: 0.40408802032470703\n",
      "Step: 24800  \tTraining loss: 0.36207494139671326\n",
      "Step: 24800  \tTraining accuracy: 0.841930627822876\n",
      "Step: 24800  \tValid loss: 0.40407422184944153\n",
      "Step: 24900  \tTraining loss: 0.3619634807109833\n",
      "Step: 24900  \tTraining accuracy: 0.8419557213783264\n",
      "Step: 24900  \tValid loss: 0.40405353903770447\n",
      "Step: 25000  \tTraining loss: 0.3618527352809906\n",
      "Step: 25000  \tTraining accuracy: 0.8419796824455261\n",
      "Step: 25000  \tValid loss: 0.4040410816669464\n",
      "Step: 25100  \tTraining loss: 0.3617408871650696\n",
      "Step: 25100  \tTraining accuracy: 0.8420039415359497\n",
      "Step: 25100  \tValid loss: 0.40403035283088684\n",
      "Step: 25200  \tTraining loss: 0.36162737011909485\n",
      "Step: 25200  \tTraining accuracy: 0.8420275449752808\n",
      "Step: 25200  \tValid loss: 0.40400901436805725\n",
      "Step: 25300  \tTraining loss: 0.36151450872421265\n",
      "Step: 25300  \tTraining accuracy: 0.8420509099960327\n",
      "Step: 25300  \tValid loss: 0.4039817750453949\n",
      "Step: 25400  \tTraining loss: 0.361403226852417\n",
      "Step: 25400  \tTraining accuracy: 0.8420736789703369\n",
      "Step: 25400  \tValid loss: 0.4039839208126068\n",
      "Step: 25500  \tTraining loss: 0.36129239201545715\n",
      "Step: 25500  \tTraining accuracy: 0.8420944809913635\n",
      "Step: 25500  \tValid loss: 0.40399491786956787\n",
      "Step: 25600  \tTraining loss: 0.3611829876899719\n",
      "Step: 25600  \tTraining accuracy: 0.8421151638031006\n",
      "Step: 25600  \tValid loss: 0.4040059447288513\n",
      "Step: 25700  \tTraining loss: 0.36107128858566284\n",
      "Step: 25700  \tTraining accuracy: 0.8421334624290466\n",
      "Step: 25700  \tValid loss: 0.4039916694164276\n",
      "Step: 25800  \tTraining loss: 0.3609617054462433\n",
      "Step: 25800  \tTraining accuracy: 0.8421502709388733\n",
      "Step: 25800  \tValid loss: 0.40398040413856506\n",
      "Step: 25900  \tTraining loss: 0.3608526587486267\n",
      "Step: 25900  \tTraining accuracy: 0.8421661257743835\n",
      "Step: 25900  \tValid loss: 0.4039669334888458\n",
      "Step: 26000  \tTraining loss: 0.360745906829834\n",
      "Step: 26000  \tTraining accuracy: 0.8421818017959595\n",
      "Step: 26000  \tValid loss: 0.4039975106716156\n",
      "Step: 26100  \tTraining loss: 0.36063724756240845\n",
      "Step: 26100  \tTraining accuracy: 0.8421982526779175\n",
      "Step: 26100  \tValid loss: 0.40399569272994995\n",
      "Step: 26200  \tTraining loss: 0.36053091287612915\n",
      "Step: 26200  \tTraining accuracy: 0.842215895652771\n",
      "Step: 26200  \tValid loss: 0.4039793014526367\n",
      "Step: 26300  \tTraining loss: 0.3604276478290558\n",
      "Step: 26300  \tTraining accuracy: 0.8422325253486633\n",
      "Step: 26300  \tValid loss: 0.40401455760002136\n",
      "Step: 26400  \tTraining loss: 0.36031967401504517\n",
      "Step: 26400  \tTraining accuracy: 0.842247724533081\n",
      "Step: 26400  \tValid loss: 0.40398091077804565\n",
      "Step: 26500  \tTraining loss: 0.3602190911769867\n",
      "Step: 26500  \tTraining accuracy: 0.8422628045082092\n",
      "Step: 26500  \tValid loss: 0.4040456712245941\n",
      "Step: 26600  \tTraining loss: 0.3601147532463074\n",
      "Step: 26600  \tTraining accuracy: 0.8422786593437195\n",
      "Step: 26600  \tValid loss: 0.40403082966804504\n",
      "Step: 26700  \tTraining loss: 0.36001500487327576\n",
      "Step: 26700  \tTraining accuracy: 0.8422977328300476\n",
      "Step: 26700  \tValid loss: 0.40409067273139954\n",
      "Step: 26800  \tTraining loss: 0.3599112927913666\n",
      "Step: 26800  \tTraining accuracy: 0.8423188328742981\n",
      "Step: 26800  \tValid loss: 0.40405896306037903\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.84233975\n",
      "Precision: 0.8608452\n",
      "Recall: 0.9839872\n",
      "F1 score: 0.89856565\n",
      "AUC: 0.5935979\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0   0.84234   0.860845  0.983987  0.898566  0.593598  0.359838      0.842318   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.403945        0.84231   0.394145      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  26851.0  \n",
      "20\n",
      "(4495, 8)\n",
      "(4495, 1)\n",
      "(2480, 8)\n",
      "(2480, 1)\n",
      "(2015, 8)\n",
      "(2015, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5820035338401794\n",
      "Step: 100  \tTraining accuracy: 0.7032257914543152\n",
      "Step: 100  \tValid loss: 0.5918111801147461\n",
      "Step: 200  \tTraining loss: 0.526917576789856\n",
      "Step: 200  \tTraining accuracy: 0.709084153175354\n",
      "Step: 200  \tValid loss: 0.5368098020553589\n",
      "Step: 300  \tTraining loss: 0.48471885919570923\n",
      "Step: 300  \tTraining accuracy: 0.7240489721298218\n",
      "Step: 300  \tValid loss: 0.48979392647743225\n",
      "Step: 400  \tTraining loss: 0.4607533812522888\n",
      "Step: 400  \tTraining accuracy: 0.736627995967865\n",
      "Step: 400  \tValid loss: 0.462673544883728\n",
      "Step: 500  \tTraining loss: 0.44955044984817505\n",
      "Step: 500  \tTraining accuracy: 0.7461376786231995\n",
      "Step: 500  \tValid loss: 0.4502203166484833\n",
      "Step: 600  \tTraining loss: 0.4439685344696045\n",
      "Step: 600  \tTraining accuracy: 0.7532005310058594\n",
      "Step: 600  \tValid loss: 0.44438472390174866\n",
      "Step: 700  \tTraining loss: 0.44057151675224304\n",
      "Step: 700  \tTraining accuracy: 0.7589287161827087\n",
      "Step: 700  \tValid loss: 0.4411657452583313\n",
      "Step: 800  \tTraining loss: 0.4380156993865967\n",
      "Step: 800  \tTraining accuracy: 0.7631294131278992\n",
      "Step: 800  \tValid loss: 0.43895086646080017\n",
      "Step: 900  \tTraining loss: 0.43564534187316895\n",
      "Step: 900  \tTraining accuracy: 0.766106128692627\n",
      "Step: 900  \tValid loss: 0.43700024485588074\n",
      "Step: 1000  \tTraining loss: 0.43314075469970703\n",
      "Step: 1000  \tTraining accuracy: 0.7684561610221863\n",
      "Step: 1000  \tValid loss: 0.434992253780365\n",
      "Step: 1100  \tTraining loss: 0.4307169020175934\n",
      "Step: 1100  \tTraining accuracy: 0.770453929901123\n",
      "Step: 1100  \tValid loss: 0.4330819845199585\n",
      "Step: 1200  \tTraining loss: 0.4283338785171509\n",
      "Step: 1200  \tTraining accuracy: 0.7723654508590698\n",
      "Step: 1200  \tValid loss: 0.43122968077659607\n",
      "Step: 1300  \tTraining loss: 0.4259355962276459\n",
      "Step: 1300  \tTraining accuracy: 0.7739977836608887\n",
      "Step: 1300  \tValid loss: 0.42929553985595703\n",
      "Step: 1400  \tTraining loss: 0.4235842525959015\n",
      "Step: 1400  \tTraining accuracy: 0.7754130363464355\n",
      "Step: 1400  \tValid loss: 0.4273920953273773\n",
      "Step: 1500  \tTraining loss: 0.4214049279689789\n",
      "Step: 1500  \tTraining accuracy: 0.7767251133918762\n",
      "Step: 1500  \tValid loss: 0.42559972405433655\n",
      "Step: 1600  \tTraining loss: 0.4195249378681183\n",
      "Step: 1600  \tTraining accuracy: 0.7779253125190735\n",
      "Step: 1600  \tValid loss: 0.4240097403526306\n",
      "Step: 1700  \tTraining loss: 0.4178665578365326\n",
      "Step: 1700  \tTraining accuracy: 0.7791013717651367\n",
      "Step: 1700  \tValid loss: 0.42261943221092224\n",
      "Step: 1800  \tTraining loss: 0.4163680076599121\n",
      "Step: 1800  \tTraining accuracy: 0.7802701592445374\n",
      "Step: 1800  \tValid loss: 0.42133837938308716\n",
      "Step: 1900  \tTraining loss: 0.4150892198085785\n",
      "Step: 1900  \tTraining accuracy: 0.7812464237213135\n",
      "Step: 1900  \tValid loss: 0.42023777961730957\n",
      "Step: 2000  \tTraining loss: 0.4140087962150574\n",
      "Step: 2000  \tTraining accuracy: 0.7821682095527649\n",
      "Step: 2000  \tValid loss: 0.4194304943084717\n",
      "Step: 2100  \tTraining loss: 0.41307324171066284\n",
      "Step: 2100  \tTraining accuracy: 0.7830000519752502\n",
      "Step: 2100  \tValid loss: 0.4188159704208374\n",
      "Step: 2200  \tTraining loss: 0.412246435880661\n",
      "Step: 2200  \tTraining accuracy: 0.7838011384010315\n",
      "Step: 2200  \tValid loss: 0.4182794392108917\n",
      "Step: 2300  \tTraining loss: 0.41151559352874756\n",
      "Step: 2300  \tTraining accuracy: 0.7844963669776917\n",
      "Step: 2300  \tValid loss: 0.41783490777015686\n",
      "Step: 2400  \tTraining loss: 0.41086480021476746\n",
      "Step: 2400  \tTraining accuracy: 0.7851229310035706\n",
      "Step: 2400  \tValid loss: 0.41745525598526\n",
      "Step: 2500  \tTraining loss: 0.4102803170681\n",
      "Step: 2500  \tTraining accuracy: 0.7857165336608887\n",
      "Step: 2500  \tValid loss: 0.41712066531181335\n",
      "Step: 2600  \tTraining loss: 0.40975305438041687\n",
      "Step: 2600  \tTraining accuracy: 0.7863159775733948\n",
      "Step: 2600  \tValid loss: 0.41680553555488586\n",
      "Step: 2700  \tTraining loss: 0.40927207469940186\n",
      "Step: 2700  \tTraining accuracy: 0.786949872970581\n",
      "Step: 2700  \tValid loss: 0.4165363013744354\n",
      "Step: 2800  \tTraining loss: 0.40881749987602234\n",
      "Step: 2800  \tTraining accuracy: 0.7875699996948242\n",
      "Step: 2800  \tValid loss: 0.4163062274456024\n",
      "Step: 2900  \tTraining loss: 0.40839478373527527\n",
      "Step: 2900  \tTraining accuracy: 0.7881388664245605\n",
      "Step: 2900  \tValid loss: 0.4160604774951935\n",
      "Step: 3000  \tTraining loss: 0.4079970121383667\n",
      "Step: 3000  \tTraining accuracy: 0.7886955142021179\n",
      "Step: 3000  \tValid loss: 0.4158123731613159\n",
      "Step: 3100  \tTraining loss: 0.40762510895729065\n",
      "Step: 3100  \tTraining accuracy: 0.7892193794250488\n",
      "Step: 3100  \tValid loss: 0.4155605733394623\n",
      "Step: 3200  \tTraining loss: 0.4072714149951935\n",
      "Step: 3200  \tTraining accuracy: 0.7897063493728638\n",
      "Step: 3200  \tValid loss: 0.4153044521808624\n",
      "Step: 3300  \tTraining loss: 0.40692955255508423\n",
      "Step: 3300  \tTraining accuracy: 0.7901566028594971\n",
      "Step: 3300  \tValid loss: 0.41505107283592224\n",
      "Step: 3400  \tTraining loss: 0.4065918028354645\n",
      "Step: 3400  \tTraining accuracy: 0.7905699610710144\n",
      "Step: 3400  \tValid loss: 0.4148065745830536\n",
      "Step: 3500  \tTraining loss: 0.4062618911266327\n",
      "Step: 3500  \tTraining accuracy: 0.790959358215332\n",
      "Step: 3500  \tValid loss: 0.41455453634262085\n",
      "Step: 3600  \tTraining loss: 0.4059392809867859\n",
      "Step: 3600  \tTraining accuracy: 0.7913362383842468\n",
      "Step: 3600  \tValid loss: 0.4142892062664032\n",
      "Step: 3700  \tTraining loss: 0.40562012791633606\n",
      "Step: 3700  \tTraining accuracy: 0.7917168140411377\n",
      "Step: 3700  \tValid loss: 0.4140263795852661\n",
      "Step: 3800  \tTraining loss: 0.40529966354370117\n",
      "Step: 3800  \tTraining accuracy: 0.7920771241188049\n",
      "Step: 3800  \tValid loss: 0.41377055644989014\n",
      "Step: 3900  \tTraining loss: 0.4049740731716156\n",
      "Step: 3900  \tTraining accuracy: 0.7924533486366272\n",
      "Step: 3900  \tValid loss: 0.4135085642337799\n",
      "Step: 4000  \tTraining loss: 0.40464746952056885\n",
      "Step: 4000  \tTraining accuracy: 0.7928105592727661\n",
      "Step: 4000  \tValid loss: 0.4132440388202667\n",
      "Step: 4100  \tTraining loss: 0.40431615710258484\n",
      "Step: 4100  \tTraining accuracy: 0.793161153793335\n",
      "Step: 4100  \tValid loss: 0.41297292709350586\n",
      "Step: 4200  \tTraining loss: 0.4039895534515381\n",
      "Step: 4200  \tTraining accuracy: 0.7935135364532471\n",
      "Step: 4200  \tValid loss: 0.41269853711128235\n",
      "Step: 4300  \tTraining loss: 0.403658926486969\n",
      "Step: 4300  \tTraining accuracy: 0.7938389182090759\n",
      "Step: 4300  \tValid loss: 0.4124208390712738\n",
      "Step: 4400  \tTraining loss: 0.4033131003379822\n",
      "Step: 4400  \tTraining accuracy: 0.7941390872001648\n",
      "Step: 4400  \tValid loss: 0.4121464788913727\n",
      "Step: 4500  \tTraining loss: 0.4029726982116699\n",
      "Step: 4500  \tTraining accuracy: 0.7944332361221313\n",
      "Step: 4500  \tValid loss: 0.41186240315437317\n",
      "Step: 4600  \tTraining loss: 0.402632474899292\n",
      "Step: 4600  \tTraining accuracy: 0.7947169542312622\n",
      "Step: 4600  \tValid loss: 0.41157400608062744\n",
      "Step: 4700  \tTraining loss: 0.4022955596446991\n",
      "Step: 4700  \tTraining accuracy: 0.795009970664978\n",
      "Step: 4700  \tValid loss: 0.4112902283668518\n",
      "Step: 4800  \tTraining loss: 0.40196213126182556\n",
      "Step: 4800  \tTraining accuracy: 0.7952812910079956\n",
      "Step: 4800  \tValid loss: 0.41101330518722534\n",
      "Step: 4900  \tTraining loss: 0.40163007378578186\n",
      "Step: 4900  \tTraining accuracy: 0.7955414652824402\n",
      "Step: 4900  \tValid loss: 0.4107406735420227\n",
      "Step: 5000  \tTraining loss: 0.4012991786003113\n",
      "Step: 5000  \tTraining accuracy: 0.7958067655563354\n",
      "Step: 5000  \tValid loss: 0.41046807169914246\n",
      "Step: 5100  \tTraining loss: 0.4009683132171631\n",
      "Step: 5100  \tTraining accuracy: 0.7960704565048218\n",
      "Step: 5100  \tValid loss: 0.41019973158836365\n",
      "Step: 5200  \tTraining loss: 0.4006374776363373\n",
      "Step: 5200  \tTraining accuracy: 0.7963195443153381\n",
      "Step: 5200  \tValid loss: 0.40993407368659973\n",
      "Step: 5300  \tTraining loss: 0.4003065228462219\n",
      "Step: 5300  \tTraining accuracy: 0.796535849571228\n",
      "Step: 5300  \tValid loss: 0.40966886281967163\n",
      "Step: 5400  \tTraining loss: 0.3999772369861603\n",
      "Step: 5400  \tTraining accuracy: 0.7967398762702942\n",
      "Step: 5400  \tValid loss: 0.4094041883945465\n",
      "Step: 5500  \tTraining loss: 0.39964863657951355\n",
      "Step: 5500  \tTraining accuracy: 0.7969384789466858\n",
      "Step: 5500  \tValid loss: 0.4091418385505676\n",
      "Step: 5600  \tTraining loss: 0.3993138074874878\n",
      "Step: 5600  \tTraining accuracy: 0.7971099019050598\n",
      "Step: 5600  \tValid loss: 0.4088917374610901\n",
      "Step: 5700  \tTraining loss: 0.3989797830581665\n",
      "Step: 5700  \tTraining accuracy: 0.7972614765167236\n",
      "Step: 5700  \tValid loss: 0.40863171219825745\n",
      "Step: 5800  \tTraining loss: 0.3986428678035736\n",
      "Step: 5800  \tTraining accuracy: 0.7974077463150024\n",
      "Step: 5800  \tValid loss: 0.40838372707366943\n",
      "Step: 5900  \tTraining loss: 0.39830833673477173\n",
      "Step: 5900  \tTraining accuracy: 0.7975490093231201\n",
      "Step: 5900  \tValid loss: 0.40814208984375\n",
      "Step: 6000  \tTraining loss: 0.39797598123550415\n",
      "Step: 6000  \tTraining accuracy: 0.7976893186569214\n",
      "Step: 6000  \tValid loss: 0.4078952670097351\n",
      "Step: 6100  \tTraining loss: 0.39764392375946045\n",
      "Step: 6100  \tTraining accuracy: 0.7978286147117615\n",
      "Step: 6100  \tValid loss: 0.4076467454433441\n",
      "Step: 6200  \tTraining loss: 0.39731109142303467\n",
      "Step: 6200  \tTraining accuracy: 0.7979724407196045\n",
      "Step: 6200  \tValid loss: 0.4073996841907501\n",
      "Step: 6300  \tTraining loss: 0.3969801366329193\n",
      "Step: 6300  \tTraining accuracy: 0.7981134653091431\n",
      "Step: 6300  \tValid loss: 0.4071386158466339\n",
      "Step: 6400  \tTraining loss: 0.39664939045906067\n",
      "Step: 6400  \tTraining accuracy: 0.7982500195503235\n",
      "Step: 6400  \tValid loss: 0.4068812429904938\n",
      "Step: 6500  \tTraining loss: 0.3963186740875244\n",
      "Step: 6500  \tTraining accuracy: 0.7983806133270264\n",
      "Step: 6500  \tValid loss: 0.4066219627857208\n",
      "Step: 6600  \tTraining loss: 0.3959883153438568\n",
      "Step: 6600  \tTraining accuracy: 0.7985021471977234\n",
      "Step: 6600  \tValid loss: 0.4063608944416046\n",
      "Step: 6700  \tTraining loss: 0.3956577777862549\n",
      "Step: 6700  \tTraining accuracy: 0.7986083030700684\n",
      "Step: 6700  \tValid loss: 0.40609946846961975\n",
      "Step: 6800  \tTraining loss: 0.3953266739845276\n",
      "Step: 6800  \tTraining accuracy: 0.7987030744552612\n",
      "Step: 6800  \tValid loss: 0.4058377742767334\n",
      "Step: 6900  \tTraining loss: 0.39499571919441223\n",
      "Step: 6900  \tTraining accuracy: 0.7987918257713318\n",
      "Step: 6900  \tValid loss: 0.405575156211853\n",
      "Step: 7000  \tTraining loss: 0.3946649134159088\n",
      "Step: 7000  \tTraining accuracy: 0.7988684177398682\n",
      "Step: 7000  \tValid loss: 0.405309796333313\n",
      "Step: 7100  \tTraining loss: 0.39433491230010986\n",
      "Step: 7100  \tTraining accuracy: 0.7989538908004761\n",
      "Step: 7100  \tValid loss: 0.40504908561706543\n",
      "Step: 7200  \tTraining loss: 0.3939252495765686\n",
      "Step: 7200  \tTraining accuracy: 0.7990478873252869\n",
      "Step: 7200  \tValid loss: 0.40479087829589844\n",
      "Step: 7300  \tTraining loss: 0.393452525138855\n",
      "Step: 7300  \tTraining accuracy: 0.799136221408844\n",
      "Step: 7300  \tValid loss: 0.4045740067958832\n",
      "Step: 7400  \tTraining loss: 0.3929731547832489\n",
      "Step: 7400  \tTraining accuracy: 0.7992175817489624\n",
      "Step: 7400  \tValid loss: 0.4043479263782501\n",
      "Step: 7500  \tTraining loss: 0.39261549711227417\n",
      "Step: 7500  \tTraining accuracy: 0.7992818355560303\n",
      "Step: 7500  \tValid loss: 0.40400683879852295\n",
      "Step: 7600  \tTraining loss: 0.39226233959198\n",
      "Step: 7600  \tTraining accuracy: 0.7993281483650208\n",
      "Step: 7600  \tValid loss: 0.4037565290927887\n",
      "Step: 7700  \tTraining loss: 0.39191582798957825\n",
      "Step: 7700  \tTraining accuracy: 0.7993820309638977\n",
      "Step: 7700  \tValid loss: 0.4034911096096039\n",
      "Step: 7800  \tTraining loss: 0.39157211780548096\n",
      "Step: 7800  \tTraining accuracy: 0.7994344830513\n",
      "Step: 7800  \tValid loss: 0.40324345231056213\n",
      "Step: 7900  \tTraining loss: 0.3912219703197479\n",
      "Step: 7900  \tTraining accuracy: 0.7994884848594666\n",
      "Step: 7900  \tValid loss: 0.40300285816192627\n",
      "Step: 8000  \tTraining loss: 0.39087846875190735\n",
      "Step: 8000  \tTraining accuracy: 0.799538254737854\n",
      "Step: 8000  \tValid loss: 0.4027097821235657\n",
      "Step: 8100  \tTraining loss: 0.3905355930328369\n",
      "Step: 8100  \tTraining accuracy: 0.7995840907096863\n",
      "Step: 8100  \tValid loss: 0.40244805812835693\n",
      "Step: 8200  \tTraining loss: 0.3901962339878082\n",
      "Step: 8200  \tTraining accuracy: 0.7996260523796082\n",
      "Step: 8200  \tValid loss: 0.40219515562057495\n",
      "Step: 8300  \tTraining loss: 0.389859676361084\n",
      "Step: 8300  \tTraining accuracy: 0.7996696829795837\n",
      "Step: 8300  \tValid loss: 0.40193992853164673\n",
      "Step: 8400  \tTraining loss: 0.3895261287689209\n",
      "Step: 8400  \tTraining accuracy: 0.7997322082519531\n",
      "Step: 8400  \tValid loss: 0.4016803503036499\n",
      "Step: 8500  \tTraining loss: 0.38919246196746826\n",
      "Step: 8500  \tTraining accuracy: 0.7998090982437134\n",
      "Step: 8500  \tValid loss: 0.40143340826034546\n",
      "Step: 8600  \tTraining loss: 0.38885870575904846\n",
      "Step: 8600  \tTraining accuracy: 0.799881637096405\n",
      "Step: 8600  \tValid loss: 0.4012087285518646\n",
      "Step: 8700  \tTraining loss: 0.38852816820144653\n",
      "Step: 8700  \tTraining accuracy: 0.7999511361122131\n",
      "Step: 8700  \tValid loss: 0.40099620819091797\n",
      "Step: 8800  \tTraining loss: 0.3882090151309967\n",
      "Step: 8800  \tTraining accuracy: 0.8000394105911255\n",
      "Step: 8800  \tValid loss: 0.40075254440307617\n",
      "Step: 8900  \tTraining loss: 0.38788774609565735\n",
      "Step: 8900  \tTraining accuracy: 0.8001344799995422\n",
      "Step: 8900  \tValid loss: 0.4005242586135864\n",
      "Step: 9000  \tTraining loss: 0.387569785118103\n",
      "Step: 9000  \tTraining accuracy: 0.8002249598503113\n",
      "Step: 9000  \tValid loss: 0.40030041337013245\n",
      "Step: 9100  \tTraining loss: 0.38725998997688293\n",
      "Step: 9100  \tTraining accuracy: 0.800317108631134\n",
      "Step: 9100  \tValid loss: 0.40006157755851746\n",
      "Step: 9200  \tTraining loss: 0.38695627450942993\n",
      "Step: 9200  \tTraining accuracy: 0.8004193902015686\n",
      "Step: 9200  \tValid loss: 0.39982885122299194\n",
      "Step: 9300  \tTraining loss: 0.3866592347621918\n",
      "Step: 9300  \tTraining accuracy: 0.8005291223526001\n",
      "Step: 9300  \tValid loss: 0.39959290623664856\n",
      "Step: 9400  \tTraining loss: 0.38636499643325806\n",
      "Step: 9400  \tTraining accuracy: 0.8006317019462585\n",
      "Step: 9400  \tValid loss: 0.39934903383255005\n",
      "Step: 9500  \tTraining loss: 0.38607433438301086\n",
      "Step: 9500  \tTraining accuracy: 0.8007227182388306\n",
      "Step: 9500  \tValid loss: 0.3991136848926544\n",
      "Step: 9600  \tTraining loss: 0.3857869505882263\n",
      "Step: 9600  \tTraining accuracy: 0.8008130192756653\n",
      "Step: 9600  \tValid loss: 0.39888274669647217\n",
      "Step: 9700  \tTraining loss: 0.38550424575805664\n",
      "Step: 9700  \tTraining accuracy: 0.8008979558944702\n",
      "Step: 9700  \tValid loss: 0.39864352345466614\n",
      "Step: 9800  \tTraining loss: 0.3852252662181854\n",
      "Step: 9800  \tTraining accuracy: 0.8009902834892273\n",
      "Step: 9800  \tValid loss: 0.3984082043170929\n",
      "Step: 9900  \tTraining loss: 0.38494873046875\n",
      "Step: 9900  \tTraining accuracy: 0.801082968711853\n",
      "Step: 9900  \tValid loss: 0.398173987865448\n",
      "Step: 10000  \tTraining loss: 0.38467517495155334\n",
      "Step: 10000  \tTraining accuracy: 0.8011738061904907\n",
      "Step: 10000  \tValid loss: 0.39793872833251953\n",
      "Step: 10100  \tTraining loss: 0.3844044506549835\n",
      "Step: 10100  \tTraining accuracy: 0.8012673258781433\n",
      "Step: 10100  \tValid loss: 0.39771220088005066\n",
      "Step: 10200  \tTraining loss: 0.3841378092765808\n",
      "Step: 10200  \tTraining accuracy: 0.8013632893562317\n",
      "Step: 10200  \tValid loss: 0.39748889207839966\n",
      "Step: 10300  \tTraining loss: 0.38387444615364075\n",
      "Step: 10300  \tTraining accuracy: 0.8014487624168396\n",
      "Step: 10300  \tValid loss: 0.39726653695106506\n",
      "Step: 10400  \tTraining loss: 0.3836064636707306\n",
      "Step: 10400  \tTraining accuracy: 0.8015314936637878\n",
      "Step: 10400  \tValid loss: 0.3970012664794922\n",
      "Step: 10500  \tTraining loss: 0.3833416700363159\n",
      "Step: 10500  \tTraining accuracy: 0.801615834236145\n",
      "Step: 10500  \tValid loss: 0.39678308367729187\n",
      "Step: 10600  \tTraining loss: 0.3830813467502594\n",
      "Step: 10600  \tTraining accuracy: 0.8016996383666992\n",
      "Step: 10600  \tValid loss: 0.39658427238464355\n",
      "Step: 10700  \tTraining loss: 0.38282379508018494\n",
      "Step: 10700  \tTraining accuracy: 0.8017860054969788\n",
      "Step: 10700  \tValid loss: 0.39636826515197754\n",
      "Step: 10800  \tTraining loss: 0.38256391882896423\n",
      "Step: 10800  \tTraining accuracy: 0.8018718361854553\n",
      "Step: 10800  \tValid loss: 0.396144300699234\n",
      "Step: 10900  \tTraining loss: 0.3823052942752838\n",
      "Step: 10900  \tTraining accuracy: 0.801950991153717\n",
      "Step: 10900  \tValid loss: 0.3959282338619232\n",
      "Step: 11000  \tTraining loss: 0.3820498585700989\n",
      "Step: 11000  \tTraining accuracy: 0.8020235300064087\n",
      "Step: 11000  \tValid loss: 0.3957289457321167\n",
      "Step: 11100  \tTraining loss: 0.3818000257015228\n",
      "Step: 11100  \tTraining accuracy: 0.802099883556366\n",
      "Step: 11100  \tValid loss: 0.3955335021018982\n",
      "Step: 11200  \tTraining loss: 0.3815552890300751\n",
      "Step: 11200  \tTraining accuracy: 0.8021748065948486\n",
      "Step: 11200  \tValid loss: 0.3953258991241455\n",
      "Step: 11300  \tTraining loss: 0.3813130557537079\n",
      "Step: 11300  \tTraining accuracy: 0.802251398563385\n",
      "Step: 11300  \tValid loss: 0.3951304852962494\n",
      "Step: 11400  \tTraining loss: 0.38107380270957947\n",
      "Step: 11400  \tTraining accuracy: 0.8023325204849243\n",
      "Step: 11400  \tValid loss: 0.39493823051452637\n",
      "Step: 11500  \tTraining loss: 0.3808371126651764\n",
      "Step: 11500  \tTraining accuracy: 0.8024111986160278\n",
      "Step: 11500  \tValid loss: 0.3947468101978302\n",
      "Step: 11600  \tTraining loss: 0.3806038200855255\n",
      "Step: 11600  \tTraining accuracy: 0.8024933934211731\n",
      "Step: 11600  \tValid loss: 0.3945418894290924\n",
      "Step: 11700  \tTraining loss: 0.3803732693195343\n",
      "Step: 11700  \tTraining accuracy: 0.8025856018066406\n",
      "Step: 11700  \tValid loss: 0.39434200525283813\n",
      "Step: 11800  \tTraining loss: 0.3801315724849701\n",
      "Step: 11800  \tTraining accuracy: 0.8026753067970276\n",
      "Step: 11800  \tValid loss: 0.39410102367401123\n",
      "Step: 11900  \tTraining loss: 0.3798576295375824\n",
      "Step: 11900  \tTraining accuracy: 0.8027700781822205\n",
      "Step: 11900  \tValid loss: 0.3938291668891907\n",
      "Step: 12000  \tTraining loss: 0.37961217761039734\n",
      "Step: 12000  \tTraining accuracy: 0.8028595447540283\n",
      "Step: 12000  \tValid loss: 0.3937090039253235\n",
      "Step: 12100  \tTraining loss: 0.3793746531009674\n",
      "Step: 12100  \tTraining accuracy: 0.8029521107673645\n",
      "Step: 12100  \tValid loss: 0.3935527801513672\n",
      "Step: 12200  \tTraining loss: 0.37914127111434937\n",
      "Step: 12200  \tTraining accuracy: 0.8030367493629456\n",
      "Step: 12200  \tValid loss: 0.39338448643684387\n",
      "Step: 12300  \tTraining loss: 0.37891632318496704\n",
      "Step: 12300  \tTraining accuracy: 0.8031209111213684\n",
      "Step: 12300  \tValid loss: 0.39322730898857117\n",
      "Step: 12400  \tTraining loss: 0.37869951128959656\n",
      "Step: 12400  \tTraining accuracy: 0.8032037615776062\n",
      "Step: 12400  \tValid loss: 0.3930780589580536\n",
      "Step: 12500  \tTraining loss: 0.37848514318466187\n",
      "Step: 12500  \tTraining accuracy: 0.8032816648483276\n",
      "Step: 12500  \tValid loss: 0.3929156959056854\n",
      "Step: 12600  \tTraining loss: 0.37827184796333313\n",
      "Step: 12600  \tTraining accuracy: 0.8033485412597656\n",
      "Step: 12600  \tValid loss: 0.3927552402019501\n",
      "Step: 12700  \tTraining loss: 0.3780611753463745\n",
      "Step: 12700  \tTraining accuracy: 0.8034135699272156\n",
      "Step: 12700  \tValid loss: 0.3925975561141968\n",
      "Step: 12800  \tTraining loss: 0.3778529465198517\n",
      "Step: 12800  \tTraining accuracy: 0.8034740090370178\n",
      "Step: 12800  \tValid loss: 0.3924415707588196\n",
      "Step: 12900  \tTraining loss: 0.37764763832092285\n",
      "Step: 12900  \tTraining accuracy: 0.8035352826118469\n",
      "Step: 12900  \tValid loss: 0.3922954797744751\n",
      "Step: 13000  \tTraining loss: 0.3774341344833374\n",
      "Step: 13000  \tTraining accuracy: 0.8035938739776611\n",
      "Step: 13000  \tValid loss: 0.3921829164028168\n",
      "Step: 13100  \tTraining loss: 0.3772035241127014\n",
      "Step: 13100  \tTraining accuracy: 0.8036566972732544\n",
      "Step: 13100  \tValid loss: 0.3920764923095703\n",
      "Step: 13200  \tTraining loss: 0.37700319290161133\n",
      "Step: 13200  \tTraining accuracy: 0.8037185668945312\n",
      "Step: 13200  \tValid loss: 0.39189985394477844\n",
      "Step: 13300  \tTraining loss: 0.3768082559108734\n",
      "Step: 13300  \tTraining accuracy: 0.8037786483764648\n",
      "Step: 13300  \tValid loss: 0.39172789454460144\n",
      "Step: 13400  \tTraining loss: 0.37661632895469666\n",
      "Step: 13400  \tTraining accuracy: 0.8038411140441895\n",
      "Step: 13400  \tValid loss: 0.3915708065032959\n",
      "Step: 13500  \tTraining loss: 0.3764267563819885\n",
      "Step: 13500  \tTraining accuracy: 0.8039043545722961\n",
      "Step: 13500  \tValid loss: 0.3914247453212738\n",
      "Step: 13600  \tTraining loss: 0.3762398362159729\n",
      "Step: 13600  \tTraining accuracy: 0.8039683103561401\n",
      "Step: 13600  \tValid loss: 0.3912814259529114\n",
      "Step: 13700  \tTraining loss: 0.37605565786361694\n",
      "Step: 13700  \tTraining accuracy: 0.8040321469306946\n",
      "Step: 13700  \tValid loss: 0.3911443054676056\n",
      "Step: 13800  \tTraining loss: 0.37587663531303406\n",
      "Step: 13800  \tTraining accuracy: 0.8040926456451416\n",
      "Step: 13800  \tValid loss: 0.39102333784103394\n",
      "Step: 13900  \tTraining loss: 0.3757002055644989\n",
      "Step: 13900  \tTraining accuracy: 0.8041554689407349\n",
      "Step: 13900  \tValid loss: 0.39090052247047424\n",
      "Step: 14000  \tTraining loss: 0.37552687525749207\n",
      "Step: 14000  \tTraining accuracy: 0.8042197227478027\n",
      "Step: 14000  \tValid loss: 0.3907744586467743\n",
      "Step: 14100  \tTraining loss: 0.3753560185432434\n",
      "Step: 14100  \tTraining accuracy: 0.8042846918106079\n",
      "Step: 14100  \tValid loss: 0.3906557261943817\n",
      "Step: 14200  \tTraining loss: 0.37518754601478577\n",
      "Step: 14200  \tTraining accuracy: 0.8043487668037415\n",
      "Step: 14200  \tValid loss: 0.3905429542064667\n",
      "Step: 14300  \tTraining loss: 0.37502121925354004\n",
      "Step: 14300  \tTraining accuracy: 0.8044134974479675\n",
      "Step: 14300  \tValid loss: 0.39043229818344116\n",
      "Step: 14400  \tTraining loss: 0.37485694885253906\n",
      "Step: 14400  \tTraining accuracy: 0.8044780492782593\n",
      "Step: 14400  \tValid loss: 0.39032360911369324\n",
      "Step: 14500  \tTraining loss: 0.37469470500946045\n",
      "Step: 14500  \tTraining accuracy: 0.80454021692276\n",
      "Step: 14500  \tValid loss: 0.39021578431129456\n",
      "Step: 14600  \tTraining loss: 0.37453433871269226\n",
      "Step: 14600  \tTraining accuracy: 0.8046000003814697\n",
      "Step: 14600  \tValid loss: 0.3901073932647705\n",
      "Step: 14700  \tTraining loss: 0.3743753433227539\n",
      "Step: 14700  \tTraining accuracy: 0.8046544194221497\n",
      "Step: 14700  \tValid loss: 0.3899998962879181\n",
      "Step: 14800  \tTraining loss: 0.3742181062698364\n",
      "Step: 14800  \tTraining accuracy: 0.8047065734863281\n",
      "Step: 14800  \tValid loss: 0.38989925384521484\n",
      "Step: 14900  \tTraining loss: 0.37404462695121765\n",
      "Step: 14900  \tTraining accuracy: 0.8047565221786499\n",
      "Step: 14900  \tValid loss: 0.38982468843460083\n",
      "Step: 15000  \tTraining loss: 0.3738745152950287\n",
      "Step: 15000  \tTraining accuracy: 0.8048072457313538\n",
      "Step: 15000  \tValid loss: 0.389723002910614\n",
      "Step: 15100  \tTraining loss: 0.3737103044986725\n",
      "Step: 15100  \tTraining accuracy: 0.8048573732376099\n",
      "Step: 15100  \tValid loss: 0.3896007239818573\n",
      "Step: 15200  \tTraining loss: 0.37354776263237\n",
      "Step: 15200  \tTraining accuracy: 0.8049089908599854\n",
      "Step: 15200  \tValid loss: 0.3894832134246826\n",
      "Step: 15300  \tTraining loss: 0.3733850419521332\n",
      "Step: 15300  \tTraining accuracy: 0.8049657940864563\n",
      "Step: 15300  \tValid loss: 0.38938072323799133\n",
      "Step: 15400  \tTraining loss: 0.3732261657714844\n",
      "Step: 15400  \tTraining accuracy: 0.8050182461738586\n",
      "Step: 15400  \tValid loss: 0.3892758786678314\n",
      "Step: 15500  \tTraining loss: 0.37306925654411316\n",
      "Step: 15500  \tTraining accuracy: 0.8050699830055237\n",
      "Step: 15500  \tValid loss: 0.3891823887825012\n",
      "Step: 15600  \tTraining loss: 0.3729139566421509\n",
      "Step: 15600  \tTraining accuracy: 0.8051246404647827\n",
      "Step: 15600  \tValid loss: 0.38909071683883667\n",
      "Step: 15700  \tTraining loss: 0.3727574050426483\n",
      "Step: 15700  \tTraining accuracy: 0.8051764965057373\n",
      "Step: 15700  \tValid loss: 0.3890111446380615\n",
      "Step: 15800  \tTraining loss: 0.37260153889656067\n",
      "Step: 15800  \tTraining accuracy: 0.8052347302436829\n",
      "Step: 15800  \tValid loss: 0.3889351487159729\n",
      "Step: 15900  \tTraining loss: 0.37244734168052673\n",
      "Step: 15900  \tTraining accuracy: 0.8052887320518494\n",
      "Step: 15900  \tValid loss: 0.388868123292923\n",
      "Step: 16000  \tTraining loss: 0.3722926080226898\n",
      "Step: 16000  \tTraining accuracy: 0.8053378462791443\n",
      "Step: 16000  \tValid loss: 0.38880831003189087\n",
      "Step: 16100  \tTraining loss: 0.3721427619457245\n",
      "Step: 16100  \tTraining accuracy: 0.8053863644599915\n",
      "Step: 16100  \tValid loss: 0.3887353241443634\n",
      "Step: 16200  \tTraining loss: 0.3719978928565979\n",
      "Step: 16200  \tTraining accuracy: 0.8054322600364685\n",
      "Step: 16200  \tValid loss: 0.38865670561790466\n",
      "Step: 16300  \tTraining loss: 0.3718562126159668\n",
      "Step: 16300  \tTraining accuracy: 0.805472731590271\n",
      "Step: 16300  \tValid loss: 0.3885668218135834\n",
      "Step: 16400  \tTraining loss: 0.37171557545661926\n",
      "Step: 16400  \tTraining accuracy: 0.8055148124694824\n",
      "Step: 16400  \tValid loss: 0.3884841799736023\n",
      "Step: 16500  \tTraining loss: 0.37157657742500305\n",
      "Step: 16500  \tTraining accuracy: 0.8055562973022461\n",
      "Step: 16500  \tValid loss: 0.38840803503990173\n",
      "Step: 16600  \tTraining loss: 0.3714378774166107\n",
      "Step: 16600  \tTraining accuracy: 0.8055980205535889\n",
      "Step: 16600  \tValid loss: 0.3883368968963623\n",
      "Step: 16700  \tTraining loss: 0.3712998628616333\n",
      "Step: 16700  \tTraining accuracy: 0.8056392073631287\n",
      "Step: 16700  \tValid loss: 0.38826245069503784\n",
      "Step: 16800  \tTraining loss: 0.371162474155426\n",
      "Step: 16800  \tTraining accuracy: 0.8056779503822327\n",
      "Step: 16800  \tValid loss: 0.3881930708885193\n",
      "Step: 16900  \tTraining loss: 0.37102600932121277\n",
      "Step: 16900  \tTraining accuracy: 0.8057142496109009\n",
      "Step: 16900  \tValid loss: 0.3881261348724365\n",
      "Step: 17000  \tTraining loss: 0.3708902895450592\n",
      "Step: 17000  \tTraining accuracy: 0.8057500720024109\n",
      "Step: 17000  \tValid loss: 0.38806381821632385\n",
      "Step: 17100  \tTraining loss: 0.37075597047805786\n",
      "Step: 17100  \tTraining accuracy: 0.8057848811149597\n",
      "Step: 17100  \tValid loss: 0.38800036907196045\n",
      "Step: 17200  \tTraining loss: 0.37062203884124756\n",
      "Step: 17200  \tTraining accuracy: 0.805816650390625\n",
      "Step: 17200  \tValid loss: 0.3879398703575134\n",
      "Step: 17300  \tTraining loss: 0.3704889714717865\n",
      "Step: 17300  \tTraining accuracy: 0.8058474063873291\n",
      "Step: 17300  \tValid loss: 0.38788041472435\n",
      "Step: 17400  \tTraining loss: 0.3703559935092926\n",
      "Step: 17400  \tTraining accuracy: 0.8058752417564392\n",
      "Step: 17400  \tValid loss: 0.3878275752067566\n",
      "Step: 17500  \tTraining loss: 0.3702235519886017\n",
      "Step: 17500  \tTraining accuracy: 0.8059027791023254\n",
      "Step: 17500  \tValid loss: 0.3877735733985901\n",
      "Step: 17600  \tTraining loss: 0.3700917363166809\n",
      "Step: 17600  \tTraining accuracy: 0.8059293627738953\n",
      "Step: 17600  \tValid loss: 0.38772302865982056\n",
      "Step: 17700  \tTraining loss: 0.3699601888656616\n",
      "Step: 17700  \tTraining accuracy: 0.8059530854225159\n",
      "Step: 17700  \tValid loss: 0.38767528533935547\n",
      "Step: 17800  \tTraining loss: 0.3698294758796692\n",
      "Step: 17800  \tTraining accuracy: 0.8059784770011902\n",
      "Step: 17800  \tValid loss: 0.38762855529785156\n",
      "Step: 17900  \tTraining loss: 0.3696991503238678\n",
      "Step: 17900  \tTraining accuracy: 0.8060016632080078\n",
      "Step: 17900  \tValid loss: 0.3875838816165924\n",
      "Step: 18000  \tTraining loss: 0.36956971883773804\n",
      "Step: 18000  \tTraining accuracy: 0.8060246706008911\n",
      "Step: 18000  \tValid loss: 0.3875493109226227\n",
      "Step: 18100  \tTraining loss: 0.3694402873516083\n",
      "Step: 18100  \tTraining accuracy: 0.8060473203659058\n",
      "Step: 18100  \tValid loss: 0.3875196874141693\n",
      "Step: 18200  \tTraining loss: 0.36931291222572327\n",
      "Step: 18200  \tTraining accuracy: 0.8060710430145264\n",
      "Step: 18200  \tValid loss: 0.3874768018722534\n",
      "Step: 18300  \tTraining loss: 0.3691861927509308\n",
      "Step: 18300  \tTraining accuracy: 0.8060950636863708\n",
      "Step: 18300  \tValid loss: 0.3874419331550598\n",
      "Step: 18400  \tTraining loss: 0.3690602481365204\n",
      "Step: 18400  \tTraining accuracy: 0.8061188459396362\n",
      "Step: 18400  \tValid loss: 0.3874233067035675\n",
      "Step: 18500  \tTraining loss: 0.3689348101615906\n",
      "Step: 18500  \tTraining accuracy: 0.8061435222625732\n",
      "Step: 18500  \tValid loss: 0.3874043822288513\n",
      "Step: 18600  \tTraining loss: 0.3688103258609772\n",
      "Step: 18600  \tTraining accuracy: 0.8061679601669312\n",
      "Step: 18600  \tValid loss: 0.38738515973091125\n",
      "Step: 18700  \tTraining loss: 0.36868730187416077\n",
      "Step: 18700  \tTraining accuracy: 0.806194543838501\n",
      "Step: 18700  \tValid loss: 0.3873754143714905\n",
      "Step: 18800  \tTraining loss: 0.3685658574104309\n",
      "Step: 18800  \tTraining accuracy: 0.8062208294868469\n",
      "Step: 18800  \tValid loss: 0.38736432790756226\n",
      "Step: 18900  \tTraining loss: 0.3684447705745697\n",
      "Step: 18900  \tTraining accuracy: 0.806246817111969\n",
      "Step: 18900  \tValid loss: 0.38735654950141907\n",
      "Step: 19000  \tTraining loss: 0.3683243691921234\n",
      "Step: 19000  \tTraining accuracy: 0.806272566318512\n",
      "Step: 19000  \tValid loss: 0.38734179735183716\n",
      "Step: 19100  \tTraining loss: 0.36820468306541443\n",
      "Step: 19100  \tTraining accuracy: 0.8062968850135803\n",
      "Step: 19100  \tValid loss: 0.3873302936553955\n",
      "Step: 19200  \tTraining loss: 0.36808496713638306\n",
      "Step: 19200  \tTraining accuracy: 0.8063215017318726\n",
      "Step: 19200  \tValid loss: 0.3873226046562195\n",
      "Step: 19300  \tTraining loss: 0.36796605587005615\n",
      "Step: 19300  \tTraining accuracy: 0.8063482046127319\n",
      "Step: 19300  \tValid loss: 0.3873147666454315\n",
      "Step: 19400  \tTraining loss: 0.36784809827804565\n",
      "Step: 19400  \tTraining accuracy: 0.8063762784004211\n",
      "Step: 19400  \tValid loss: 0.38730618357658386\n",
      "Step: 19500  \tTraining loss: 0.36773034930229187\n",
      "Step: 19500  \tTraining accuracy: 0.8064035773277283\n",
      "Step: 19500  \tValid loss: 0.38729962706565857\n",
      "Step: 19600  \tTraining loss: 0.36761271953582764\n",
      "Step: 19600  \tTraining accuracy: 0.8064317107200623\n",
      "Step: 19600  \tValid loss: 0.3872927129268646\n",
      "Step: 19700  \tTraining loss: 0.36749616265296936\n",
      "Step: 19700  \tTraining accuracy: 0.8064572811126709\n",
      "Step: 19700  \tValid loss: 0.3872849941253662\n",
      "Step: 19800  \tTraining loss: 0.3673805296421051\n",
      "Step: 19800  \tTraining accuracy: 0.8064842820167542\n",
      "Step: 19800  \tValid loss: 0.38727259635925293\n",
      "Step: 19900  \tTraining loss: 0.36726585030555725\n",
      "Step: 19900  \tTraining accuracy: 0.8065109848976135\n",
      "Step: 19900  \tValid loss: 0.3872620761394501\n",
      "Step: 20000  \tTraining loss: 0.3671517074108124\n",
      "Step: 20000  \tTraining accuracy: 0.8065374493598938\n",
      "Step: 20000  \tValid loss: 0.38724958896636963\n",
      "Step: 20100  \tTraining loss: 0.3670354187488556\n",
      "Step: 20100  \tTraining accuracy: 0.8065625429153442\n",
      "Step: 20100  \tValid loss: 0.38720932602882385\n",
      "Step: 20200  \tTraining loss: 0.36692050099372864\n",
      "Step: 20200  \tTraining accuracy: 0.8065868616104126\n",
      "Step: 20200  \tValid loss: 0.38719823956489563\n",
      "Step: 20300  \tTraining loss: 0.36680707335472107\n",
      "Step: 20300  \tTraining accuracy: 0.8066152930259705\n",
      "Step: 20300  \tValid loss: 0.38721203804016113\n",
      "Step: 20400  \tTraining loss: 0.36669445037841797\n",
      "Step: 20400  \tTraining accuracy: 0.8066445589065552\n",
      "Step: 20400  \tValid loss: 0.3872188627719879\n",
      "Step: 20500  \tTraining loss: 0.36658337712287903\n",
      "Step: 20500  \tTraining accuracy: 0.806674599647522\n",
      "Step: 20500  \tValid loss: 0.3872174918651581\n",
      "Step: 20600  \tTraining loss: 0.3664742410182953\n",
      "Step: 20600  \tTraining accuracy: 0.8067044019699097\n",
      "Step: 20600  \tValid loss: 0.3872074484825134\n",
      "Step: 20700  \tTraining loss: 0.3663661777973175\n",
      "Step: 20700  \tTraining accuracy: 0.8067376613616943\n",
      "Step: 20700  \tValid loss: 0.38719820976257324\n",
      "Step: 20800  \tTraining loss: 0.36625880002975464\n",
      "Step: 20800  \tTraining accuracy: 0.8067705631256104\n",
      "Step: 20800  \tValid loss: 0.38718974590301514\n",
      "Step: 20900  \tTraining loss: 0.3661520779132843\n",
      "Step: 20900  \tTraining accuracy: 0.8068063855171204\n",
      "Step: 20900  \tValid loss: 0.387180894613266\n",
      "Step: 21000  \tTraining loss: 0.36604616045951843\n",
      "Step: 21000  \tTraining accuracy: 0.8068434596061707\n",
      "Step: 21000  \tValid loss: 0.38717225193977356\n",
      "Step: 21100  \tTraining loss: 0.36594122648239136\n",
      "Step: 21100  \tTraining accuracy: 0.8068817853927612\n",
      "Step: 21100  \tValid loss: 0.3871636986732483\n",
      "Step: 21200  \tTraining loss: 0.36583656072616577\n",
      "Step: 21200  \tTraining accuracy: 0.8069196939468384\n",
      "Step: 21200  \tValid loss: 0.38715556263923645\n",
      "Step: 21300  \tTraining loss: 0.3657332956790924\n",
      "Step: 21300  \tTraining accuracy: 0.8069572448730469\n",
      "Step: 21300  \tValid loss: 0.3871456980705261\n",
      "Step: 21400  \tTraining loss: 0.36563053727149963\n",
      "Step: 21400  \tTraining accuracy: 0.8069986701011658\n",
      "Step: 21400  \tValid loss: 0.38713517785072327\n",
      "Step: 21500  \tTraining loss: 0.3655185401439667\n",
      "Step: 21500  \tTraining accuracy: 0.8070375919342041\n",
      "Step: 21500  \tValid loss: 0.3871786892414093\n",
      "Step: 21600  \tTraining loss: 0.36539798974990845\n",
      "Step: 21600  \tTraining accuracy: 0.8070751428604126\n",
      "Step: 21600  \tValid loss: 0.38718166947364807\n",
      "Step: 21700  \tTraining loss: 0.36528757214546204\n",
      "Step: 21700  \tTraining accuracy: 0.8071123361587524\n",
      "Step: 21700  \tValid loss: 0.3871193826198578\n",
      "Step: 21800  \tTraining loss: 0.36518386006355286\n",
      "Step: 21800  \tTraining accuracy: 0.8071532845497131\n",
      "Step: 21800  \tValid loss: 0.3871184289455414\n",
      "Step: 21900  \tTraining loss: 0.3650813400745392\n",
      "Step: 21900  \tTraining accuracy: 0.8071963787078857\n",
      "Step: 21900  \tValid loss: 0.38710319995880127\n",
      "Step: 22000  \tTraining loss: 0.3649793565273285\n",
      "Step: 22000  \tTraining accuracy: 0.8072406649589539\n",
      "Step: 22000  \tValid loss: 0.38708561658859253\n",
      "Step: 22100  \tTraining loss: 0.3648780882358551\n",
      "Step: 22100  \tTraining accuracy: 0.8072844743728638\n",
      "Step: 22100  \tValid loss: 0.3870658278465271\n",
      "Step: 22200  \tTraining loss: 0.36477774381637573\n",
      "Step: 22200  \tTraining accuracy: 0.807327926158905\n",
      "Step: 22200  \tValid loss: 0.38704442977905273\n",
      "Step: 22300  \tTraining loss: 0.3646780252456665\n",
      "Step: 22300  \tTraining accuracy: 0.8073709607124329\n",
      "Step: 22300  \tValid loss: 0.3870220184326172\n",
      "Step: 22400  \tTraining loss: 0.36457887291908264\n",
      "Step: 22400  \tTraining accuracy: 0.8074166178703308\n",
      "Step: 22400  \tValid loss: 0.38700249791145325\n",
      "Step: 22500  \tTraining loss: 0.36448073387145996\n",
      "Step: 22500  \tTraining accuracy: 0.8074648380279541\n",
      "Step: 22500  \tValid loss: 0.3869803845882416\n",
      "Step: 22600  \tTraining loss: 0.3643816411495209\n",
      "Step: 22600  \tTraining accuracy: 0.8075146079063416\n",
      "Step: 22600  \tValid loss: 0.386961966753006\n",
      "Step: 22700  \tTraining loss: 0.36428236961364746\n",
      "Step: 22700  \tTraining accuracy: 0.8075639605522156\n",
      "Step: 22700  \tValid loss: 0.3869449198246002\n",
      "Step: 22800  \tTraining loss: 0.3641839921474457\n",
      "Step: 22800  \tTraining accuracy: 0.8076128363609314\n",
      "Step: 22800  \tValid loss: 0.3869246244430542\n",
      "Step: 22900  \tTraining loss: 0.3640865981578827\n",
      "Step: 22900  \tTraining accuracy: 0.8076637387275696\n",
      "Step: 22900  \tValid loss: 0.38690441846847534\n",
      "Step: 23000  \tTraining loss: 0.36398929357528687\n",
      "Step: 23000  \tTraining accuracy: 0.8077142238616943\n",
      "Step: 23000  \tValid loss: 0.38688990473747253\n",
      "Step: 23100  \tTraining loss: 0.3638930916786194\n",
      "Step: 23100  \tTraining accuracy: 0.8077642321586609\n",
      "Step: 23100  \tValid loss: 0.38687965273857117\n",
      "Step: 23200  \tTraining loss: 0.3637957274913788\n",
      "Step: 23200  \tTraining accuracy: 0.807816207408905\n",
      "Step: 23200  \tValid loss: 0.3868711292743683\n",
      "Step: 23300  \tTraining loss: 0.3636987507343292\n",
      "Step: 23300  \tTraining accuracy: 0.8078658580780029\n",
      "Step: 23300  \tValid loss: 0.38684937357902527\n",
      "Step: 23400  \tTraining loss: 0.36360254883766174\n",
      "Step: 23400  \tTraining accuracy: 0.8079155087471008\n",
      "Step: 23400  \tValid loss: 0.38681891560554504\n",
      "Step: 23500  \tTraining loss: 0.363506942987442\n",
      "Step: 23500  \tTraining accuracy: 0.8079657554626465\n",
      "Step: 23500  \tValid loss: 0.3867908716201782\n",
      "Step: 23600  \tTraining loss: 0.36341261863708496\n",
      "Step: 23600  \tTraining accuracy: 0.8080131411552429\n",
      "Step: 23600  \tValid loss: 0.3867616355419159\n",
      "Step: 23700  \tTraining loss: 0.3633190393447876\n",
      "Step: 23700  \tTraining accuracy: 0.8080624938011169\n",
      "Step: 23700  \tValid loss: 0.386728435754776\n",
      "Step: 23800  \tTraining loss: 0.3632267117500305\n",
      "Step: 23800  \tTraining accuracy: 0.8081142902374268\n",
      "Step: 23800  \tValid loss: 0.38669848442077637\n",
      "Step: 23900  \tTraining loss: 0.36313483119010925\n",
      "Step: 23900  \tTraining accuracy: 0.8081669807434082\n",
      "Step: 23900  \tValid loss: 0.3866725564002991\n",
      "Step: 24000  \tTraining loss: 0.36304372549057007\n",
      "Step: 24000  \tTraining accuracy: 0.8082216382026672\n",
      "Step: 24000  \tValid loss: 0.38664814829826355\n",
      "Step: 24100  \tTraining loss: 0.3629530966281891\n",
      "Step: 24100  \tTraining accuracy: 0.8082752823829651\n",
      "Step: 24100  \tValid loss: 0.38662299513816833\n",
      "Step: 24200  \tTraining loss: 0.36286303400993347\n",
      "Step: 24200  \tTraining accuracy: 0.8083267211914062\n",
      "Step: 24200  \tValid loss: 0.3865988552570343\n",
      "Step: 24300  \tTraining loss: 0.3627738356590271\n",
      "Step: 24300  \tTraining accuracy: 0.8083776831626892\n",
      "Step: 24300  \tValid loss: 0.38657301664352417\n",
      "Step: 24400  \tTraining loss: 0.3626853823661804\n",
      "Step: 24400  \tTraining accuracy: 0.8084273338317871\n",
      "Step: 24400  \tValid loss: 0.3865480124950409\n",
      "Step: 24500  \tTraining loss: 0.3625972270965576\n",
      "Step: 24500  \tTraining accuracy: 0.8084765672683716\n",
      "Step: 24500  \tValid loss: 0.38652071356773376\n",
      "Step: 24600  \tTraining loss: 0.3625103235244751\n",
      "Step: 24600  \tTraining accuracy: 0.8085254430770874\n",
      "Step: 24600  \tValid loss: 0.3864923119544983\n",
      "Step: 24700  \tTraining loss: 0.3624236285686493\n",
      "Step: 24700  \tTraining accuracy: 0.8085729479789734\n",
      "Step: 24700  \tValid loss: 0.3864664137363434\n",
      "Step: 24800  \tTraining loss: 0.36233749985694885\n",
      "Step: 24800  \tTraining accuracy: 0.8086201548576355\n",
      "Step: 24800  \tValid loss: 0.38644134998321533\n",
      "Step: 24900  \tTraining loss: 0.36225205659866333\n",
      "Step: 24900  \tTraining accuracy: 0.8086682558059692\n",
      "Step: 24900  \tValid loss: 0.38641834259033203\n",
      "Step: 25000  \tTraining loss: 0.36216747760772705\n",
      "Step: 25000  \tTraining accuracy: 0.8087159991264343\n",
      "Step: 25000  \tValid loss: 0.3863985538482666\n",
      "Step: 25100  \tTraining loss: 0.36208292841911316\n",
      "Step: 25100  \tTraining accuracy: 0.8087620139122009\n",
      "Step: 25100  \tValid loss: 0.3863812983036041\n",
      "Step: 25200  \tTraining loss: 0.36199823021888733\n",
      "Step: 25200  \tTraining accuracy: 0.8088067770004272\n",
      "Step: 25200  \tValid loss: 0.38636425137519836\n",
      "Step: 25300  \tTraining loss: 0.3619142174720764\n",
      "Step: 25300  \tTraining accuracy: 0.8088520765304565\n",
      "Step: 25300  \tValid loss: 0.3863445520401001\n",
      "Step: 25400  \tTraining loss: 0.3618309497833252\n",
      "Step: 25400  \tTraining accuracy: 0.8089001178741455\n",
      "Step: 25400  \tValid loss: 0.3863246738910675\n",
      "Step: 25500  \tTraining loss: 0.36174798011779785\n",
      "Step: 25500  \tTraining accuracy: 0.808946430683136\n",
      "Step: 25500  \tValid loss: 0.3863048851490021\n",
      "Step: 25600  \tTraining loss: 0.36166563630104065\n",
      "Step: 25600  \tTraining accuracy: 0.8089906573295593\n",
      "Step: 25600  \tValid loss: 0.38628828525543213\n",
      "Step: 25700  \tTraining loss: 0.36158403754234314\n",
      "Step: 25700  \tTraining accuracy: 0.8090327978134155\n",
      "Step: 25700  \tValid loss: 0.3862675428390503\n",
      "Step: 25800  \tTraining loss: 0.36150240898132324\n",
      "Step: 25800  \tTraining accuracy: 0.8090745806694031\n",
      "Step: 25800  \tValid loss: 0.3862486779689789\n",
      "Step: 25900  \tTraining loss: 0.36142128705978394\n",
      "Step: 25900  \tTraining accuracy: 0.8091169595718384\n",
      "Step: 25900  \tValid loss: 0.3862288296222687\n",
      "Step: 26000  \tTraining loss: 0.3613409996032715\n",
      "Step: 26000  \tTraining accuracy: 0.8091602325439453\n",
      "Step: 26000  \tValid loss: 0.38620808720588684\n",
      "Step: 26100  \tTraining loss: 0.3612607717514038\n",
      "Step: 26100  \tTraining accuracy: 0.8092032074928284\n",
      "Step: 26100  \tValid loss: 0.386188268661499\n",
      "Step: 26200  \tTraining loss: 0.3611815273761749\n",
      "Step: 26200  \tTraining accuracy: 0.8092458844184875\n",
      "Step: 26200  \tValid loss: 0.3861677348613739\n",
      "Step: 26300  \tTraining loss: 0.3611024022102356\n",
      "Step: 26300  \tTraining accuracy: 0.8092890381813049\n",
      "Step: 26300  \tValid loss: 0.38614845275878906\n",
      "Step: 26400  \tTraining loss: 0.3610236942768097\n",
      "Step: 26400  \tTraining accuracy: 0.8093318939208984\n",
      "Step: 26400  \tValid loss: 0.38612714409828186\n",
      "Step: 26500  \tTraining loss: 0.3609459102153778\n",
      "Step: 26500  \tTraining accuracy: 0.8093743920326233\n",
      "Step: 26500  \tValid loss: 0.3861062526702881\n",
      "Step: 26600  \tTraining loss: 0.3608682155609131\n",
      "Step: 26600  \tTraining accuracy: 0.8094165921211243\n",
      "Step: 26600  \tValid loss: 0.38608601689338684\n",
      "Step: 26700  \tTraining loss: 0.3607906401157379\n",
      "Step: 26700  \tTraining accuracy: 0.8094568252563477\n",
      "Step: 26700  \tValid loss: 0.38606446981430054\n",
      "Step: 26800  \tTraining loss: 0.3607139587402344\n",
      "Step: 26800  \tTraining accuracy: 0.8094967603683472\n",
      "Step: 26800  \tValid loss: 0.38604536652565\n",
      "Step: 26900  \tTraining loss: 0.36063745617866516\n",
      "Step: 26900  \tTraining accuracy: 0.809536337852478\n",
      "Step: 26900  \tValid loss: 0.38602322340011597\n",
      "Step: 27000  \tTraining loss: 0.3605615794658661\n",
      "Step: 27000  \tTraining accuracy: 0.8095756769180298\n",
      "Step: 27000  \tValid loss: 0.38600248098373413\n",
      "Step: 27100  \tTraining loss: 0.36048582196235657\n",
      "Step: 27100  \tTraining accuracy: 0.8096143007278442\n",
      "Step: 27100  \tValid loss: 0.38598161935806274\n",
      "Step: 27200  \tTraining loss: 0.36041125655174255\n",
      "Step: 27200  \tTraining accuracy: 0.8096534609794617\n",
      "Step: 27200  \tValid loss: 0.38595861196517944\n",
      "Step: 27300  \tTraining loss: 0.3603369891643524\n",
      "Step: 27300  \tTraining accuracy: 0.8096939325332642\n",
      "Step: 27300  \tValid loss: 0.3859363794326782\n",
      "Step: 27400  \tTraining loss: 0.360262393951416\n",
      "Step: 27400  \tTraining accuracy: 0.8097333312034607\n",
      "Step: 27400  \tValid loss: 0.38591599464416504\n",
      "Step: 27500  \tTraining loss: 0.36018916964530945\n",
      "Step: 27500  \tTraining accuracy: 0.8097720146179199\n",
      "Step: 27500  \tValid loss: 0.385892778635025\n",
      "Step: 27600  \tTraining loss: 0.3601156175136566\n",
      "Step: 27600  \tTraining accuracy: 0.8098096251487732\n",
      "Step: 27600  \tValid loss: 0.38587233424186707\n",
      "Step: 27700  \tTraining loss: 0.36004310846328735\n",
      "Step: 27700  \tTraining accuracy: 0.8098458051681519\n",
      "Step: 27700  \tValid loss: 0.38584962487220764\n",
      "Step: 27800  \tTraining loss: 0.35997000336647034\n",
      "Step: 27800  \tTraining accuracy: 0.8098800182342529\n",
      "Step: 27800  \tValid loss: 0.3858312666416168\n",
      "Step: 27900  \tTraining loss: 0.3598979413509369\n",
      "Step: 27900  \tTraining accuracy: 0.8099148869514465\n",
      "Step: 27900  \tValid loss: 0.38580939173698425\n",
      "Step: 28000  \tTraining loss: 0.3598260283470154\n",
      "Step: 28000  \tTraining accuracy: 0.8099486231803894\n",
      "Step: 28000  \tValid loss: 0.3857892155647278\n",
      "Step: 28100  \tTraining loss: 0.3597556948661804\n",
      "Step: 28100  \tTraining accuracy: 0.80998295545578\n",
      "Step: 28100  \tValid loss: 0.3857634961605072\n",
      "Step: 28200  \tTraining loss: 0.35968518257141113\n",
      "Step: 28200  \tTraining accuracy: 0.8100170493125916\n",
      "Step: 28200  \tValid loss: 0.3857414126396179\n",
      "Step: 28300  \tTraining loss: 0.3596143126487732\n",
      "Step: 28300  \tTraining accuracy: 0.8100497126579285\n",
      "Step: 28300  \tValid loss: 0.3857230544090271\n",
      "Step: 28400  \tTraining loss: 0.3595446050167084\n",
      "Step: 28400  \tTraining accuracy: 0.8100809454917908\n",
      "Step: 28400  \tValid loss: 0.38570013642311096\n",
      "Step: 28500  \tTraining loss: 0.35947495698928833\n",
      "Step: 28500  \tTraining accuracy: 0.8101119995117188\n",
      "Step: 28500  \tValid loss: 0.3856779634952545\n",
      "Step: 28600  \tTraining loss: 0.3594054877758026\n",
      "Step: 28600  \tTraining accuracy: 0.8101428151130676\n",
      "Step: 28600  \tValid loss: 0.38565680384635925\n",
      "Step: 28700  \tTraining loss: 0.3593360483646393\n",
      "Step: 28700  \tTraining accuracy: 0.8101741671562195\n",
      "Step: 28700  \tValid loss: 0.3856371343135834\n",
      "Step: 28800  \tTraining loss: 0.35926714539527893\n",
      "Step: 28800  \tTraining accuracy: 0.810205340385437\n",
      "Step: 28800  \tValid loss: 0.38561686873435974\n",
      "Step: 28900  \tTraining loss: 0.35919827222824097\n",
      "Step: 28900  \tTraining accuracy: 0.8102362751960754\n",
      "Step: 28900  \tValid loss: 0.3855982720851898\n",
      "Step: 29000  \tTraining loss: 0.359130322933197\n",
      "Step: 29000  \tTraining accuracy: 0.8102678060531616\n",
      "Step: 29000  \tValid loss: 0.3855733275413513\n",
      "Step: 29100  \tTraining loss: 0.3590621054172516\n",
      "Step: 29100  \tTraining accuracy: 0.8102990984916687\n",
      "Step: 29100  \tValid loss: 0.3855544924736023\n",
      "Step: 29200  \tTraining loss: 0.35899388790130615\n",
      "Step: 29200  \tTraining accuracy: 0.8103309273719788\n",
      "Step: 29200  \tValid loss: 0.3855363428592682\n",
      "Step: 29300  \tTraining loss: 0.3589266836643219\n",
      "Step: 29300  \tTraining accuracy: 0.8103632926940918\n",
      "Step: 29300  \tValid loss: 0.3855132758617401\n",
      "Step: 29400  \tTraining loss: 0.35885921120643616\n",
      "Step: 29400  \tTraining accuracy: 0.8103946447372437\n",
      "Step: 29400  \tValid loss: 0.38549211621284485\n",
      "Step: 29500  \tTraining loss: 0.35879194736480713\n",
      "Step: 29500  \tTraining accuracy: 0.8104258179664612\n",
      "Step: 29500  \tValid loss: 0.3854728937149048\n",
      "Step: 29600  \tTraining loss: 0.35872533917427063\n",
      "Step: 29600  \tTraining accuracy: 0.8104568123817444\n",
      "Step: 29600  \tValid loss: 0.38545191287994385\n",
      "Step: 29700  \tTraining loss: 0.35865893959999084\n",
      "Step: 29700  \tTraining accuracy: 0.8104887008666992\n",
      "Step: 29700  \tValid loss: 0.3854310214519501\n",
      "Step: 29800  \tTraining loss: 0.3585927188396454\n",
      "Step: 29800  \tTraining accuracy: 0.810520350933075\n",
      "Step: 29800  \tValid loss: 0.38540762662887573\n",
      "Step: 29900  \tTraining loss: 0.3585263788700104\n",
      "Step: 29900  \tTraining accuracy: 0.8105518221855164\n",
      "Step: 29900  \tValid loss: 0.3853907287120819\n",
      "Step: 30000  \tTraining loss: 0.35846108198165894\n",
      "Step: 30000  \tTraining accuracy: 0.810584545135498\n",
      "Step: 30000  \tValid loss: 0.385366827249527\n",
      "Step: 30100  \tTraining loss: 0.35839614272117615\n",
      "Step: 30100  \tTraining accuracy: 0.8106170892715454\n",
      "Step: 30100  \tValid loss: 0.3853417634963989\n",
      "Step: 30200  \tTraining loss: 0.3583310842514038\n",
      "Step: 30200  \tTraining accuracy: 0.8106512427330017\n",
      "Step: 30200  \tValid loss: 0.38532236218452454\n",
      "Step: 30300  \tTraining loss: 0.3582666218280792\n",
      "Step: 30300  \tTraining accuracy: 0.8106851577758789\n",
      "Step: 30300  \tValid loss: 0.38530346751213074\n",
      "Step: 30400  \tTraining loss: 0.35820192098617554\n",
      "Step: 30400  \tTraining accuracy: 0.8107199668884277\n",
      "Step: 30400  \tValid loss: 0.3852842152118683\n",
      "Step: 30500  \tTraining loss: 0.358139306306839\n",
      "Step: 30500  \tTraining accuracy: 0.8107559680938721\n",
      "Step: 30500  \tValid loss: 0.3852587342262268\n",
      "Step: 30600  \tTraining loss: 0.3580757677555084\n",
      "Step: 30600  \tTraining accuracy: 0.8107917904853821\n",
      "Step: 30600  \tValid loss: 0.38524046540260315\n",
      "Step: 30700  \tTraining loss: 0.3580116927623749\n",
      "Step: 30700  \tTraining accuracy: 0.8108273148536682\n",
      "Step: 30700  \tValid loss: 0.3852231800556183\n",
      "Step: 30800  \tTraining loss: 0.3579487204551697\n",
      "Step: 30800  \tTraining accuracy: 0.8108623027801514\n",
      "Step: 30800  \tValid loss: 0.38520482182502747\n",
      "Step: 30900  \tTraining loss: 0.3578864634037018\n",
      "Step: 30900  \tTraining accuracy: 0.8108959794044495\n",
      "Step: 30900  \tValid loss: 0.3851776421070099\n",
      "Step: 31000  \tTraining loss: 0.35782504081726074\n",
      "Step: 31000  \tTraining accuracy: 0.8109315633773804\n",
      "Step: 31000  \tValid loss: 0.38513538241386414\n",
      "Step: 31100  \tTraining loss: 0.3577640950679779\n",
      "Step: 31100  \tTraining accuracy: 0.8109676241874695\n",
      "Step: 31100  \tValid loss: 0.3851027190685272\n",
      "Step: 31200  \tTraining loss: 0.3577031195163727\n",
      "Step: 31200  \tTraining accuracy: 0.8110035061836243\n",
      "Step: 31200  \tValid loss: 0.3850814402103424\n",
      "Step: 31300  \tTraining loss: 0.3576428294181824\n",
      "Step: 31300  \tTraining accuracy: 0.8110390901565552\n",
      "Step: 31300  \tValid loss: 0.38505828380584717\n",
      "Step: 31400  \tTraining loss: 0.357583224773407\n",
      "Step: 31400  \tTraining accuracy: 0.8110744953155518\n",
      "Step: 31400  \tValid loss: 0.3850334584712982\n",
      "Step: 31500  \tTraining loss: 0.3575230538845062\n",
      "Step: 31500  \tTraining accuracy: 0.8111096620559692\n",
      "Step: 31500  \tValid loss: 0.38501206040382385\n",
      "Step: 31600  \tTraining loss: 0.3574634790420532\n",
      "Step: 31600  \tTraining accuracy: 0.8111446499824524\n",
      "Step: 31600  \tValid loss: 0.38499027490615845\n",
      "Step: 31700  \tTraining loss: 0.3574044406414032\n",
      "Step: 31700  \tTraining accuracy: 0.8111814856529236\n",
      "Step: 31700  \tValid loss: 0.38496533036231995\n",
      "Step: 31800  \tTraining loss: 0.3573452830314636\n",
      "Step: 31800  \tTraining accuracy: 0.8112190961837769\n",
      "Step: 31800  \tValid loss: 0.384943425655365\n",
      "Step: 31900  \tTraining loss: 0.357286661863327\n",
      "Step: 31900  \tTraining accuracy: 0.8112565279006958\n",
      "Step: 31900  \tValid loss: 0.38491934537887573\n",
      "Step: 32000  \tTraining loss: 0.35722804069519043\n",
      "Step: 32000  \tTraining accuracy: 0.8112937211990356\n",
      "Step: 32000  \tValid loss: 0.3848969340324402\n",
      "Step: 32100  \tTraining loss: 0.35716986656188965\n",
      "Step: 32100  \tTraining accuracy: 0.8113306760787964\n",
      "Step: 32100  \tValid loss: 0.3848741948604584\n",
      "Step: 32200  \tTraining loss: 0.35711172223091125\n",
      "Step: 32200  \tTraining accuracy: 0.811367392539978\n",
      "Step: 32200  \tValid loss: 0.38485217094421387\n",
      "Step: 32300  \tTraining loss: 0.3570544123649597\n",
      "Step: 32300  \tTraining accuracy: 0.8114049434661865\n",
      "Step: 32300  \tValid loss: 0.3848295509815216\n",
      "Step: 32400  \tTraining loss: 0.3569965362548828\n",
      "Step: 32400  \tTraining accuracy: 0.8114429116249084\n",
      "Step: 32400  \tValid loss: 0.3848040699958801\n",
      "Step: 32500  \tTraining loss: 0.3569389581680298\n",
      "Step: 32500  \tTraining accuracy: 0.8114806413650513\n",
      "Step: 32500  \tValid loss: 0.38478007912635803\n",
      "Step: 32600  \tTraining loss: 0.3568817377090454\n",
      "Step: 32600  \tTraining accuracy: 0.8115181922912598\n",
      "Step: 32600  \tValid loss: 0.3847538232803345\n",
      "Step: 32700  \tTraining loss: 0.35682493448257446\n",
      "Step: 32700  \tTraining accuracy: 0.8115554451942444\n",
      "Step: 32700  \tValid loss: 0.38472408056259155\n",
      "Step: 32800  \tTraining loss: 0.3567675054073334\n",
      "Step: 32800  \tTraining accuracy: 0.8115931749343872\n",
      "Step: 32800  \tValid loss: 0.3846935033798218\n",
      "Step: 32900  \tTraining loss: 0.35670721530914307\n",
      "Step: 32900  \tTraining accuracy: 0.8116317391395569\n",
      "Step: 32900  \tValid loss: 0.38464003801345825\n",
      "Step: 33000  \tTraining loss: 0.35664746165275574\n",
      "Step: 33000  \tTraining accuracy: 0.8116700053215027\n",
      "Step: 33000  \tValid loss: 0.3846004009246826\n",
      "Step: 33100  \tTraining loss: 0.3565884232521057\n",
      "Step: 33100  \tTraining accuracy: 0.8117074370384216\n",
      "Step: 33100  \tValid loss: 0.3845633566379547\n",
      "Step: 33200  \tTraining loss: 0.35653015971183777\n",
      "Step: 33200  \tTraining accuracy: 0.8117445707321167\n",
      "Step: 33200  \tValid loss: 0.38453036546707153\n",
      "Step: 33300  \tTraining loss: 0.3564729690551758\n",
      "Step: 33300  \tTraining accuracy: 0.8117815256118774\n",
      "Step: 33300  \tValid loss: 0.38449564576148987\n",
      "Step: 33400  \tTraining loss: 0.356414794921875\n",
      "Step: 33400  \tTraining accuracy: 0.8118182420730591\n",
      "Step: 33400  \tValid loss: 0.38447004556655884\n",
      "Step: 33500  \tTraining loss: 0.35635852813720703\n",
      "Step: 33500  \tTraining accuracy: 0.8118557333946228\n",
      "Step: 33500  \tValid loss: 0.38443872332572937\n",
      "Step: 33600  \tTraining loss: 0.356301873922348\n",
      "Step: 33600  \tTraining accuracy: 0.8118936419487\n",
      "Step: 33600  \tValid loss: 0.38441202044487\n",
      "Step: 33700  \tTraining loss: 0.35624614357948303\n",
      "Step: 33700  \tTraining accuracy: 0.8119313716888428\n",
      "Step: 33700  \tValid loss: 0.3843855857849121\n",
      "Step: 33800  \tTraining loss: 0.3561908006668091\n",
      "Step: 33800  \tTraining accuracy: 0.8119688630104065\n",
      "Step: 33800  \tValid loss: 0.38435888290405273\n",
      "Step: 33900  \tTraining loss: 0.3561355471611023\n",
      "Step: 33900  \tTraining accuracy: 0.8120067715644836\n",
      "Step: 33900  \tValid loss: 0.38433459401130676\n",
      "Step: 34000  \tTraining loss: 0.3560800552368164\n",
      "Step: 34000  \tTraining accuracy: 0.8120434880256653\n",
      "Step: 34000  \tValid loss: 0.38431185483932495\n",
      "Step: 34100  \tTraining loss: 0.35602515935897827\n",
      "Step: 34100  \tTraining accuracy: 0.8120799660682678\n",
      "Step: 34100  \tValid loss: 0.38428977131843567\n",
      "Step: 34200  \tTraining loss: 0.3559713661670685\n",
      "Step: 34200  \tTraining accuracy: 0.812116265296936\n",
      "Step: 34200  \tValid loss: 0.3842630684375763\n",
      "Step: 34300  \tTraining loss: 0.3559170067310333\n",
      "Step: 34300  \tTraining accuracy: 0.8121523261070251\n",
      "Step: 34300  \tValid loss: 0.3842426538467407\n",
      "Step: 34400  \tTraining loss: 0.3558632731437683\n",
      "Step: 34400  \tTraining accuracy: 0.8121882081031799\n",
      "Step: 34400  \tValid loss: 0.3842218816280365\n",
      "Step: 34500  \tTraining loss: 0.3558095693588257\n",
      "Step: 34500  \tTraining accuracy: 0.8122238516807556\n",
      "Step: 34500  \tValid loss: 0.3842025399208069\n",
      "Step: 34600  \tTraining loss: 0.3557562530040741\n",
      "Step: 34600  \tTraining accuracy: 0.812259316444397\n",
      "Step: 34600  \tValid loss: 0.3841826617717743\n",
      "Step: 34700  \tTraining loss: 0.3557034134864807\n",
      "Step: 34700  \tTraining accuracy: 0.8122945427894592\n",
      "Step: 34700  \tValid loss: 0.3841615617275238\n",
      "Step: 34800  \tTraining loss: 0.35565096139907837\n",
      "Step: 34800  \tTraining accuracy: 0.8123295903205872\n",
      "Step: 34800  \tValid loss: 0.38414767384529114\n",
      "Step: 34900  \tTraining loss: 0.35559916496276855\n",
      "Step: 34900  \tTraining accuracy: 0.8123647570610046\n",
      "Step: 34900  \tValid loss: 0.3841334581375122\n",
      "Step: 35000  \tTraining loss: 0.35554757714271545\n",
      "Step: 35000  \tTraining accuracy: 0.8123997449874878\n",
      "Step: 35000  \tValid loss: 0.38411805033683777\n",
      "Step: 35100  \tTraining loss: 0.35549384355545044\n",
      "Step: 35100  \tTraining accuracy: 0.8124344944953918\n",
      "Step: 35100  \tValid loss: 0.38406437635421753\n",
      "Step: 35200  \tTraining loss: 0.3554401397705078\n",
      "Step: 35200  \tTraining accuracy: 0.8124690651893616\n",
      "Step: 35200  \tValid loss: 0.38403046131134033\n",
      "Step: 35300  \tTraining loss: 0.35538679361343384\n",
      "Step: 35300  \tTraining accuracy: 0.8125033974647522\n",
      "Step: 35300  \tValid loss: 0.3840036690235138\n",
      "Step: 35400  \tTraining loss: 0.3553348183631897\n",
      "Step: 35400  \tTraining accuracy: 0.8125376105308533\n",
      "Step: 35400  \tValid loss: 0.3839778006076813\n",
      "Step: 35500  \tTraining loss: 0.3552825152873993\n",
      "Step: 35500  \tTraining accuracy: 0.8125715851783752\n",
      "Step: 35500  \tValid loss: 0.38396015763282776\n",
      "Step: 35600  \tTraining loss: 0.355231374502182\n",
      "Step: 35600  \tTraining accuracy: 0.8126053214073181\n",
      "Step: 35600  \tValid loss: 0.3839397132396698\n",
      "Step: 35700  \tTraining loss: 0.3551797866821289\n",
      "Step: 35700  \tTraining accuracy: 0.8126389384269714\n",
      "Step: 35700  \tValid loss: 0.38392534852027893\n",
      "Step: 35800  \tTraining loss: 0.3551287353038788\n",
      "Step: 35800  \tTraining accuracy: 0.8126723766326904\n",
      "Step: 35800  \tValid loss: 0.38391175866127014\n",
      "Step: 35900  \tTraining loss: 0.355078786611557\n",
      "Step: 35900  \tTraining accuracy: 0.8127055764198303\n",
      "Step: 35900  \tValid loss: 0.3839002847671509\n",
      "Step: 36000  \tTraining loss: 0.3550291359424591\n",
      "Step: 36000  \tTraining accuracy: 0.8127389550209045\n",
      "Step: 36000  \tValid loss: 0.3838864862918854\n",
      "Step: 36100  \tTraining loss: 0.3549796938896179\n",
      "Step: 36100  \tTraining accuracy: 0.8127733469009399\n",
      "Step: 36100  \tValid loss: 0.3838612139225006\n",
      "Step: 36200  \tTraining loss: 0.3549313247203827\n",
      "Step: 36200  \tTraining accuracy: 0.8128072023391724\n",
      "Step: 36200  \tValid loss: 0.38383617997169495\n",
      "Step: 36300  \tTraining loss: 0.35488253831863403\n",
      "Step: 36300  \tTraining accuracy: 0.8128393888473511\n",
      "Step: 36300  \tValid loss: 0.3838198781013489\n",
      "Step: 36400  \tTraining loss: 0.3548341989517212\n",
      "Step: 36400  \tTraining accuracy: 0.8128708004951477\n",
      "Step: 36400  \tValid loss: 0.3838004171848297\n",
      "Step: 36500  \tTraining loss: 0.35478681325912476\n",
      "Step: 36500  \tTraining accuracy: 0.8129029273986816\n",
      "Step: 36500  \tValid loss: 0.3837830722332001\n",
      "Step: 36600  \tTraining loss: 0.35473933815956116\n",
      "Step: 36600  \tTraining accuracy: 0.8129357695579529\n",
      "Step: 36600  \tValid loss: 0.38376784324645996\n",
      "Step: 36700  \tTraining loss: 0.35469189286231995\n",
      "Step: 36700  \tTraining accuracy: 0.8129672408103943\n",
      "Step: 36700  \tValid loss: 0.3837531805038452\n",
      "Step: 36800  \tTraining loss: 0.3546451926231384\n",
      "Step: 36800  \tTraining accuracy: 0.8129967451095581\n",
      "Step: 36800  \tValid loss: 0.3837345242500305\n",
      "Step: 36900  \tTraining loss: 0.3545982837677002\n",
      "Step: 36900  \tTraining accuracy: 0.8130260705947876\n",
      "Step: 36900  \tValid loss: 0.3837178349494934\n",
      "Step: 37000  \tTraining loss: 0.35455167293548584\n",
      "Step: 37000  \tTraining accuracy: 0.8130576610565186\n",
      "Step: 37000  \tValid loss: 0.38370025157928467\n",
      "Step: 37100  \tTraining loss: 0.3545047640800476\n",
      "Step: 37100  \tTraining accuracy: 0.8130890727043152\n",
      "Step: 37100  \tValid loss: 0.38368088006973267\n",
      "Step: 37200  \tTraining loss: 0.35445573925971985\n",
      "Step: 37200  \tTraining accuracy: 0.8131217956542969\n",
      "Step: 37200  \tValid loss: 0.3836658000946045\n",
      "Step: 37300  \tTraining loss: 0.35440680384635925\n",
      "Step: 37300  \tTraining accuracy: 0.8131543397903442\n",
      "Step: 37300  \tValid loss: 0.3836486339569092\n",
      "Step: 37400  \tTraining loss: 0.3543579876422882\n",
      "Step: 37400  \tTraining accuracy: 0.813186764717102\n",
      "Step: 37400  \tValid loss: 0.38362929224967957\n",
      "Step: 37500  \tTraining loss: 0.3543088734149933\n",
      "Step: 37500  \tTraining accuracy: 0.8132195472717285\n",
      "Step: 37500  \tValid loss: 0.38361096382141113\n",
      "Step: 37600  \tTraining loss: 0.35426047444343567\n",
      "Step: 37600  \tTraining accuracy: 0.8132522106170654\n",
      "Step: 37600  \tValid loss: 0.38358914852142334\n",
      "Step: 37700  \tTraining loss: 0.35421133041381836\n",
      "Step: 37700  \tTraining accuracy: 0.8132858276367188\n",
      "Step: 37700  \tValid loss: 0.3835673928260803\n",
      "Step: 37800  \tTraining loss: 0.3541628420352936\n",
      "Step: 37800  \tTraining accuracy: 0.81331866979599\n",
      "Step: 37800  \tValid loss: 0.38353756070137024\n",
      "Step: 37900  \tTraining loss: 0.35411301255226135\n",
      "Step: 37900  \tTraining accuracy: 0.8133504986763\n",
      "Step: 37900  \tValid loss: 0.38351622223854065\n",
      "Step: 38000  \tTraining loss: 0.3540637195110321\n",
      "Step: 38000  \tTraining accuracy: 0.8133830428123474\n",
      "Step: 38000  \tValid loss: 0.3834880590438843\n",
      "Step: 38100  \tTraining loss: 0.35400792956352234\n",
      "Step: 38100  \tTraining accuracy: 0.8134154081344604\n",
      "Step: 38100  \tValid loss: 0.3834579288959503\n",
      "Step: 38200  \tTraining loss: 0.35394296050071716\n",
      "Step: 38200  \tTraining accuracy: 0.8134475946426392\n",
      "Step: 38200  \tValid loss: 0.3834352493286133\n",
      "Step: 38300  \tTraining loss: 0.35387730598449707\n",
      "Step: 38300  \tTraining accuracy: 0.8134796023368835\n",
      "Step: 38300  \tValid loss: 0.38340866565704346\n",
      "Step: 38400  \tTraining loss: 0.35381191968917847\n",
      "Step: 38400  \tTraining accuracy: 0.8135120272636414\n",
      "Step: 38400  \tValid loss: 0.3833787143230438\n",
      "Step: 38500  \tTraining loss: 0.35374608635902405\n",
      "Step: 38500  \tTraining accuracy: 0.8135440349578857\n",
      "Step: 38500  \tValid loss: 0.38334983587265015\n",
      "Step: 38600  \tTraining loss: 0.35368111729621887\n",
      "Step: 38600  \tTraining accuracy: 0.8135746717453003\n",
      "Step: 38600  \tValid loss: 0.3833118677139282\n",
      "Step: 38700  \tTraining loss: 0.35361436009407043\n",
      "Step: 38700  \tTraining accuracy: 0.8136051893234253\n",
      "Step: 38700  \tValid loss: 0.38327839970588684\n",
      "Step: 38800  \tTraining loss: 0.3535486161708832\n",
      "Step: 38800  \tTraining accuracy: 0.813635528087616\n",
      "Step: 38800  \tValid loss: 0.383234441280365\n",
      "Step: 38900  \tTraining loss: 0.35344013571739197\n",
      "Step: 38900  \tTraining accuracy: 0.8136665225028992\n",
      "Step: 38900  \tValid loss: 0.383182168006897\n",
      "Step: 39000  \tTraining loss: 0.35325881838798523\n",
      "Step: 39000  \tTraining accuracy: 0.8137000203132629\n",
      "Step: 39000  \tValid loss: 0.3831425905227661\n",
      "Step: 39100  \tTraining loss: 0.3531295657157898\n",
      "Step: 39100  \tTraining accuracy: 0.8137341141700745\n",
      "Step: 39100  \tValid loss: 0.38310086727142334\n",
      "Step: 39200  \tTraining loss: 0.3529938757419586\n",
      "Step: 39200  \tTraining accuracy: 0.8137664198875427\n",
      "Step: 39200  \tValid loss: 0.38300564885139465\n",
      "Step: 39300  \tTraining loss: 0.3528900444507599\n",
      "Step: 39300  \tTraining accuracy: 0.813797652721405\n",
      "Step: 39300  \tValid loss: 0.38290297985076904\n",
      "Step: 39400  \tTraining loss: 0.35277560353279114\n",
      "Step: 39400  \tTraining accuracy: 0.8138275742530823\n",
      "Step: 39400  \tValid loss: 0.382781058549881\n",
      "Step: 39500  \tTraining loss: 0.3526778519153595\n",
      "Step: 39500  \tTraining accuracy: 0.81385737657547\n",
      "Step: 39500  \tValid loss: 0.3826713263988495\n",
      "Step: 39600  \tTraining loss: 0.352591872215271\n",
      "Step: 39600  \tTraining accuracy: 0.8138870596885681\n",
      "Step: 39600  \tValid loss: 0.3826006054878235\n",
      "Step: 39700  \tTraining loss: 0.35252463817596436\n",
      "Step: 39700  \tTraining accuracy: 0.8139165639877319\n",
      "Step: 39700  \tValid loss: 0.3825150430202484\n",
      "Step: 39800  \tTraining loss: 0.3524599075317383\n",
      "Step: 39800  \tTraining accuracy: 0.8139458894729614\n",
      "Step: 39800  \tValid loss: 0.382433146238327\n",
      "Step: 39900  \tTraining loss: 0.35239431262016296\n",
      "Step: 39900  \tTraining accuracy: 0.8139750957489014\n",
      "Step: 39900  \tValid loss: 0.3823661804199219\n",
      "Step: 40000  \tTraining loss: 0.35232990980148315\n",
      "Step: 40000  \tTraining accuracy: 0.8140050172805786\n",
      "Step: 40000  \tValid loss: 0.38230380415916443\n",
      "Step: 40100  \tTraining loss: 0.35226765275001526\n",
      "Step: 40100  \tTraining accuracy: 0.8140347599983215\n",
      "Step: 40100  \tValid loss: 0.3822358250617981\n",
      "Step: 40200  \tTraining loss: 0.3522076904773712\n",
      "Step: 40200  \tTraining accuracy: 0.8140637874603271\n",
      "Step: 40200  \tValid loss: 0.3821619153022766\n",
      "Step: 40300  \tTraining loss: 0.3521481156349182\n",
      "Step: 40300  \tTraining accuracy: 0.8140926957130432\n",
      "Step: 40300  \tValid loss: 0.38209298253059387\n",
      "Step: 40400  \tTraining loss: 0.35208842158317566\n",
      "Step: 40400  \tTraining accuracy: 0.814121425151825\n",
      "Step: 40400  \tValid loss: 0.38202887773513794\n",
      "Step: 40500  \tTraining loss: 0.3520312011241913\n",
      "Step: 40500  \tTraining accuracy: 0.8141500353813171\n",
      "Step: 40500  \tValid loss: 0.381961464881897\n",
      "Step: 40600  \tTraining loss: 0.3519726097583771\n",
      "Step: 40600  \tTraining accuracy: 0.8141785264015198\n",
      "Step: 40600  \tValid loss: 0.38190555572509766\n",
      "Step: 40700  \tTraining loss: 0.3519142270088196\n",
      "Step: 40700  \tTraining accuracy: 0.8142068386077881\n",
      "Step: 40700  \tValid loss: 0.381832480430603\n",
      "Step: 40800  \tTraining loss: 0.35184821486473083\n",
      "Step: 40800  \tTraining accuracy: 0.8142350316047668\n",
      "Step: 40800  \tValid loss: 0.3817923367023468\n",
      "Step: 40900  \tTraining loss: 0.35179033875465393\n",
      "Step: 40900  \tTraining accuracy: 0.814263105392456\n",
      "Step: 40900  \tValid loss: 0.3817271888256073\n",
      "Step: 41000  \tTraining loss: 0.35173311829566956\n",
      "Step: 41000  \tTraining accuracy: 0.814290463924408\n",
      "Step: 41000  \tValid loss: 0.3816629946231842\n",
      "Step: 41100  \tTraining loss: 0.3516770899295807\n",
      "Step: 41100  \tTraining accuracy: 0.8143168687820435\n",
      "Step: 41100  \tValid loss: 0.3815939724445343\n",
      "Step: 41200  \tTraining loss: 0.3516218960285187\n",
      "Step: 41200  \tTraining accuracy: 0.8143432140350342\n",
      "Step: 41200  \tValid loss: 0.3815311789512634\n",
      "Step: 41300  \tTraining loss: 0.35156694054603577\n",
      "Step: 41300  \tTraining accuracy: 0.8143690824508667\n",
      "Step: 41300  \tValid loss: 0.3814754784107208\n",
      "Step: 41400  \tTraining loss: 0.3515135645866394\n",
      "Step: 41400  \tTraining accuracy: 0.8143940567970276\n",
      "Step: 41400  \tValid loss: 0.3814069628715515\n",
      "Step: 41500  \tTraining loss: 0.35146069526672363\n",
      "Step: 41500  \tTraining accuracy: 0.8144199848175049\n",
      "Step: 41500  \tValid loss: 0.38134559988975525\n",
      "Step: 41600  \tTraining loss: 0.3514079749584198\n",
      "Step: 41600  \tTraining accuracy: 0.8144457936286926\n",
      "Step: 41600  \tValid loss: 0.38128527998924255\n",
      "Step: 41700  \tTraining loss: 0.3513558506965637\n",
      "Step: 41700  \tTraining accuracy: 0.814470648765564\n",
      "Step: 41700  \tValid loss: 0.38123059272766113\n",
      "Step: 41800  \tTraining loss: 0.351304292678833\n",
      "Step: 41800  \tTraining accuracy: 0.8144949078559875\n",
      "Step: 41800  \tValid loss: 0.3811701536178589\n",
      "Step: 41900  \tTraining loss: 0.35125356912612915\n",
      "Step: 41900  \tTraining accuracy: 0.8145189881324768\n",
      "Step: 41900  \tValid loss: 0.38111981749534607\n",
      "Step: 42000  \tTraining loss: 0.35120320320129395\n",
      "Step: 42000  \tTraining accuracy: 0.8145429491996765\n",
      "Step: 42000  \tValid loss: 0.3810581862926483\n",
      "Step: 42100  \tTraining loss: 0.35115307569503784\n",
      "Step: 42100  \tTraining accuracy: 0.8145673871040344\n",
      "Step: 42100  \tValid loss: 0.3810071349143982\n",
      "Step: 42200  \tTraining loss: 0.35110342502593994\n",
      "Step: 42200  \tTraining accuracy: 0.814591646194458\n",
      "Step: 42200  \tValid loss: 0.3809564709663391\n",
      "Step: 42300  \tTraining loss: 0.3510558009147644\n",
      "Step: 42300  \tTraining accuracy: 0.8146158456802368\n",
      "Step: 42300  \tValid loss: 0.38089919090270996\n",
      "Step: 42400  \tTraining loss: 0.3510064482688904\n",
      "Step: 42400  \tTraining accuracy: 0.8146399259567261\n",
      "Step: 42400  \tValid loss: 0.3808591663837433\n",
      "Step: 42500  \tTraining loss: 0.3509591817855835\n",
      "Step: 42500  \tTraining accuracy: 0.814663827419281\n",
      "Step: 42500  \tValid loss: 0.3808102309703827\n",
      "Step: 42600  \tTraining loss: 0.35091161727905273\n",
      "Step: 42600  \tTraining accuracy: 0.8146876692771912\n",
      "Step: 42600  \tValid loss: 0.3807653784751892\n",
      "Step: 42700  \tTraining loss: 0.3508641719818115\n",
      "Step: 42700  \tTraining accuracy: 0.8147113919258118\n",
      "Step: 42700  \tValid loss: 0.3807205557823181\n",
      "Step: 42800  \tTraining loss: 0.35081613063812256\n",
      "Step: 42800  \tTraining accuracy: 0.8147349953651428\n",
      "Step: 42800  \tValid loss: 0.3806830048561096\n",
      "Step: 42900  \tTraining loss: 0.3507697880268097\n",
      "Step: 42900  \tTraining accuracy: 0.8147595524787903\n",
      "Step: 42900  \tValid loss: 0.3806343078613281\n",
      "Step: 43000  \tTraining loss: 0.35072270035743713\n",
      "Step: 43000  \tTraining accuracy: 0.8147839903831482\n",
      "Step: 43000  \tValid loss: 0.380594938993454\n",
      "Step: 43100  \tTraining loss: 0.35067713260650635\n",
      "Step: 43100  \tTraining accuracy: 0.8148083090782166\n",
      "Step: 43100  \tValid loss: 0.3805506229400635\n",
      "Step: 43200  \tTraining loss: 0.3506312966346741\n",
      "Step: 43200  \tTraining accuracy: 0.8148325085639954\n",
      "Step: 43200  \tValid loss: 0.38050997257232666\n",
      "Step: 43300  \tTraining loss: 0.3505861759185791\n",
      "Step: 43300  \tTraining accuracy: 0.8148565888404846\n",
      "Step: 43300  \tValid loss: 0.38046368956565857\n",
      "Step: 43400  \tTraining loss: 0.3505411148071289\n",
      "Step: 43400  \tTraining accuracy: 0.8148805499076843\n",
      "Step: 43400  \tValid loss: 0.3804225027561188\n",
      "Step: 43500  \tTraining loss: 0.35049664974212646\n",
      "Step: 43500  \tTraining accuracy: 0.8149044513702393\n",
      "Step: 43500  \tValid loss: 0.3803797960281372\n",
      "Step: 43600  \tTraining loss: 0.3504520356655121\n",
      "Step: 43600  \tTraining accuracy: 0.8149281740188599\n",
      "Step: 43600  \tValid loss: 0.38034147024154663\n",
      "Step: 43700  \tTraining loss: 0.3504085838794708\n",
      "Step: 43700  \tTraining accuracy: 0.8149518370628357\n",
      "Step: 43700  \tValid loss: 0.3802991211414337\n",
      "Step: 43800  \tTraining loss: 0.35036414861679077\n",
      "Step: 43800  \tTraining accuracy: 0.814975380897522\n",
      "Step: 43800  \tValid loss: 0.3802644908428192\n",
      "Step: 43900  \tTraining loss: 0.3503207862377167\n",
      "Step: 43900  \tTraining accuracy: 0.8149988055229187\n",
      "Step: 43900  \tValid loss: 0.38022613525390625\n",
      "Step: 44000  \tTraining loss: 0.3502771556377411\n",
      "Step: 44000  \tTraining accuracy: 0.8150231242179871\n",
      "Step: 44000  \tValid loss: 0.38018861413002014\n",
      "Step: 44100  \tTraining loss: 0.3502337336540222\n",
      "Step: 44100  \tTraining accuracy: 0.8150473833084106\n",
      "Step: 44100  \tValid loss: 0.3801548480987549\n",
      "Step: 44200  \tTraining loss: 0.35019078850746155\n",
      "Step: 44200  \tTraining accuracy: 0.8150714635848999\n",
      "Step: 44200  \tValid loss: 0.38011986017227173\n",
      "Step: 44300  \tTraining loss: 0.35014793276786804\n",
      "Step: 44300  \tTraining accuracy: 0.8150954842567444\n",
      "Step: 44300  \tValid loss: 0.38008448481559753\n",
      "Step: 44400  \tTraining loss: 0.35010606050491333\n",
      "Step: 44400  \tTraining accuracy: 0.8151193857192993\n",
      "Step: 44400  \tValid loss: 0.3800458312034607\n",
      "Step: 44500  \tTraining loss: 0.35006383061408997\n",
      "Step: 44500  \tTraining accuracy: 0.8151431679725647\n",
      "Step: 44500  \tValid loss: 0.3800123333930969\n",
      "Step: 44600  \tTraining loss: 0.3500235080718994\n",
      "Step: 44600  \tTraining accuracy: 0.8151668906211853\n",
      "Step: 44600  \tValid loss: 0.3799775540828705\n",
      "Step: 44700  \tTraining loss: 0.3499825596809387\n",
      "Step: 44700  \tTraining accuracy: 0.8151914477348328\n",
      "Step: 44700  \tValid loss: 0.379950612783432\n",
      "Step: 44800  \tTraining loss: 0.3499426245689392\n",
      "Step: 44800  \tTraining accuracy: 0.8152158856391907\n",
      "Step: 44800  \tValid loss: 0.3799196481704712\n",
      "Step: 44900  \tTraining loss: 0.34990283846855164\n",
      "Step: 44900  \tTraining accuracy: 0.815241277217865\n",
      "Step: 44900  \tValid loss: 0.3798920512199402\n",
      "Step: 45000  \tTraining loss: 0.3498629927635193\n",
      "Step: 45000  \tTraining accuracy: 0.8152672648429871\n",
      "Step: 45000  \tValid loss: 0.37986424565315247\n",
      "Step: 45100  \tTraining loss: 0.34982407093048096\n",
      "Step: 45100  \tTraining accuracy: 0.8152938485145569\n",
      "Step: 45100  \tValid loss: 0.37983438372612\n",
      "Step: 45200  \tTraining loss: 0.3497845232486725\n",
      "Step: 45200  \tTraining accuracy: 0.8153191208839417\n",
      "Step: 45200  \tValid loss: 0.37981078028678894\n",
      "Step: 45300  \tTraining loss: 0.34974607825279236\n",
      "Step: 45300  \tTraining accuracy: 0.8153442144393921\n",
      "Step: 45300  \tValid loss: 0.3797818124294281\n",
      "Step: 45400  \tTraining loss: 0.3497074842453003\n",
      "Step: 45400  \tTraining accuracy: 0.8153682947158813\n",
      "Step: 45400  \tValid loss: 0.3797552287578583\n",
      "Step: 45500  \tTraining loss: 0.34966886043548584\n",
      "Step: 45500  \tTraining accuracy: 0.8153932094573975\n",
      "Step: 45500  \tValid loss: 0.379728764295578\n",
      "Step: 45600  \tTraining loss: 0.3496306240558624\n",
      "Step: 45600  \tTraining accuracy: 0.8154175281524658\n",
      "Step: 45600  \tValid loss: 0.3797053098678589\n",
      "Step: 45700  \tTraining loss: 0.3495929539203644\n",
      "Step: 45700  \tTraining accuracy: 0.8154422640800476\n",
      "Step: 45700  \tValid loss: 0.379676878452301\n",
      "Step: 45800  \tTraining loss: 0.3495550751686096\n",
      "Step: 45800  \tTraining accuracy: 0.8154675960540771\n",
      "Step: 45800  \tValid loss: 0.3796526789665222\n",
      "Step: 45900  \tTraining loss: 0.34951692819595337\n",
      "Step: 45900  \tTraining accuracy: 0.8154928088188171\n",
      "Step: 45900  \tValid loss: 0.3796299397945404\n",
      "Step: 46000  \tTraining loss: 0.34947970509529114\n",
      "Step: 46000  \tTraining accuracy: 0.8155171871185303\n",
      "Step: 46000  \tValid loss: 0.3796042799949646\n",
      "Step: 46100  \tTraining loss: 0.34944188594818115\n",
      "Step: 46100  \tTraining accuracy: 0.8155414462089539\n",
      "Step: 46100  \tValid loss: 0.37958309054374695\n",
      "Step: 46200  \tTraining loss: 0.349404513835907\n",
      "Step: 46200  \tTraining accuracy: 0.81556636095047\n",
      "Step: 46200  \tValid loss: 0.3795616626739502\n",
      "Step: 46300  \tTraining loss: 0.3493673503398895\n",
      "Step: 46300  \tTraining accuracy: 0.8155911564826965\n",
      "Step: 46300  \tValid loss: 0.37953808903694153\n",
      "Step: 46400  \tTraining loss: 0.34933069348335266\n",
      "Step: 46400  \tTraining accuracy: 0.8156163096427917\n",
      "Step: 46400  \tValid loss: 0.379513680934906\n",
      "Step: 46500  \tTraining loss: 0.34929370880126953\n",
      "Step: 46500  \tTraining accuracy: 0.8156413435935974\n",
      "Step: 46500  \tValid loss: 0.37949126958847046\n",
      "Step: 46600  \tTraining loss: 0.3492569029331207\n",
      "Step: 46600  \tTraining accuracy: 0.8156663179397583\n",
      "Step: 46600  \tValid loss: 0.3794712424278259\n",
      "Step: 46700  \tTraining loss: 0.3492204546928406\n",
      "Step: 46700  \tTraining accuracy: 0.8156911134719849\n",
      "Step: 46700  \tValid loss: 0.37944725155830383\n",
      "Step: 46800  \tTraining loss: 0.34918397665023804\n",
      "Step: 46800  \tTraining accuracy: 0.8157168030738831\n",
      "Step: 46800  \tValid loss: 0.379425048828125\n",
      "Step: 46900  \tTraining loss: 0.3491481840610504\n",
      "Step: 46900  \tTraining accuracy: 0.815743088722229\n",
      "Step: 46900  \tValid loss: 0.3794011175632477\n",
      "Step: 47000  \tTraining loss: 0.34911176562309265\n",
      "Step: 47000  \tTraining accuracy: 0.8157692551612854\n",
      "Step: 47000  \tValid loss: 0.3793804943561554\n",
      "Step: 47100  \tTraining loss: 0.34907591342926025\n",
      "Step: 47100  \tTraining accuracy: 0.8157953023910522\n",
      "Step: 47100  \tValid loss: 0.3793588876724243\n",
      "Step: 47200  \tTraining loss: 0.34903955459594727\n",
      "Step: 47200  \tTraining accuracy: 0.8158212900161743\n",
      "Step: 47200  \tValid loss: 0.3793398439884186\n",
      "Step: 47300  \tTraining loss: 0.3490040898323059\n",
      "Step: 47300  \tTraining accuracy: 0.8158470988273621\n",
      "Step: 47300  \tValid loss: 0.37931740283966064\n",
      "Step: 47400  \tTraining loss: 0.34896838665008545\n",
      "Step: 47400  \tTraining accuracy: 0.815872848033905\n",
      "Step: 47400  \tValid loss: 0.37929922342300415\n",
      "Step: 47500  \tTraining loss: 0.3489331007003784\n",
      "Step: 47500  \tTraining accuracy: 0.8158984780311584\n",
      "Step: 47500  \tValid loss: 0.3792743384838104\n",
      "Step: 47600  \tTraining loss: 0.3488976061344147\n",
      "Step: 47600  \tTraining accuracy: 0.8159239888191223\n",
      "Step: 47600  \tValid loss: 0.37925440073013306\n",
      "Step: 47700  \tTraining loss: 0.3488624095916748\n",
      "Step: 47700  \tTraining accuracy: 0.8159493803977966\n",
      "Step: 47700  \tValid loss: 0.3792342245578766\n",
      "Step: 47800  \tTraining loss: 0.3488273024559021\n",
      "Step: 47800  \tTraining accuracy: 0.8159747123718262\n",
      "Step: 47800  \tValid loss: 0.3792126476764679\n",
      "Step: 47900  \tTraining loss: 0.34879204630851746\n",
      "Step: 47900  \tTraining accuracy: 0.8159999251365662\n",
      "Step: 47900  \tValid loss: 0.3791922926902771\n",
      "Step: 48000  \tTraining loss: 0.34875723719596863\n",
      "Step: 48000  \tTraining accuracy: 0.8160250186920166\n",
      "Step: 48000  \tValid loss: 0.3791707158088684\n",
      "Step: 48100  \tTraining loss: 0.3487226963043213\n",
      "Step: 48100  \tTraining accuracy: 0.8160499930381775\n",
      "Step: 48100  \tValid loss: 0.37915098667144775\n",
      "Step: 48200  \tTraining loss: 0.34868741035461426\n",
      "Step: 48200  \tTraining accuracy: 0.8160748481750488\n",
      "Step: 48200  \tValid loss: 0.3791333734989166\n",
      "Step: 48300  \tTraining loss: 0.34865304827690125\n",
      "Step: 48300  \tTraining accuracy: 0.8160996437072754\n",
      "Step: 48300  \tValid loss: 0.37911248207092285\n",
      "Step: 48400  \tTraining loss: 0.34861841797828674\n",
      "Step: 48400  \tTraining accuracy: 0.8161247968673706\n",
      "Step: 48400  \tValid loss: 0.37909427285194397\n",
      "Step: 48500  \tTraining loss: 0.34858402609825134\n",
      "Step: 48500  \tTraining accuracy: 0.8161498308181763\n",
      "Step: 48500  \tValid loss: 0.3790748715400696\n",
      "Step: 48600  \tTraining loss: 0.3485499918460846\n",
      "Step: 48600  \tTraining accuracy: 0.8161747455596924\n",
      "Step: 48600  \tValid loss: 0.3790554106235504\n",
      "Step: 48700  \tTraining loss: 0.34851592779159546\n",
      "Step: 48700  \tTraining accuracy: 0.816199541091919\n",
      "Step: 48700  \tValid loss: 0.3790366053581238\n",
      "Step: 48800  \tTraining loss: 0.3484823703765869\n",
      "Step: 48800  \tTraining accuracy: 0.8162242770195007\n",
      "Step: 48800  \tValid loss: 0.3790167570114136\n",
      "Step: 48900  \tTraining loss: 0.34844809770584106\n",
      "Step: 48900  \tTraining accuracy: 0.816248893737793\n",
      "Step: 48900  \tValid loss: 0.3790016770362854\n",
      "Step: 49000  \tTraining loss: 0.34841403365135193\n",
      "Step: 49000  \tTraining accuracy: 0.8162734508514404\n",
      "Step: 49000  \tValid loss: 0.37898191809654236\n",
      "Step: 49100  \tTraining loss: 0.3483804762363434\n",
      "Step: 49100  \tTraining accuracy: 0.8162978887557983\n",
      "Step: 49100  \tValid loss: 0.37896254658699036\n",
      "Step: 49200  \tTraining loss: 0.34834685921669006\n",
      "Step: 49200  \tTraining accuracy: 0.8163231015205383\n",
      "Step: 49200  \tValid loss: 0.37894555926322937\n",
      "Step: 49300  \tTraining loss: 0.3483133912086487\n",
      "Step: 49300  \tTraining accuracy: 0.8163493275642395\n",
      "Step: 49300  \tValid loss: 0.3789266347885132\n",
      "Step: 49400  \tTraining loss: 0.3482799232006073\n",
      "Step: 49400  \tTraining accuracy: 0.8163754940032959\n",
      "Step: 49400  \tValid loss: 0.3789117932319641\n",
      "Step: 49500  \tTraining loss: 0.3482469618320465\n",
      "Step: 49500  \tTraining accuracy: 0.816402018070221\n",
      "Step: 49500  \tValid loss: 0.3788886070251465\n",
      "Step: 49600  \tTraining loss: 0.34821388125419617\n",
      "Step: 49600  \tTraining accuracy: 0.816429078578949\n",
      "Step: 49600  \tValid loss: 0.3788691461086273\n",
      "Step: 49700  \tTraining loss: 0.3481809198856354\n",
      "Step: 49700  \tTraining accuracy: 0.8164560198783875\n",
      "Step: 49700  \tValid loss: 0.37885159254074097\n",
      "Step: 49800  \tTraining loss: 0.3481481969356537\n",
      "Step: 49800  \tTraining accuracy: 0.816483736038208\n",
      "Step: 49800  \tValid loss: 0.37883225083351135\n",
      "Step: 49900  \tTraining loss: 0.34811532497406006\n",
      "Step: 49900  \tTraining accuracy: 0.8165116310119629\n",
      "Step: 49900  \tValid loss: 0.37881574034690857\n",
      "Step: 50000  \tTraining loss: 0.34808260202407837\n",
      "Step: 50000  \tTraining accuracy: 0.8165393471717834\n",
      "Step: 50000  \tValid loss: 0.3787979483604431\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.81656694\n",
      "Precision: 0.8659591\n",
      "Recall: 0.8585347\n",
      "F1 score: 0.81305146\n",
      "AUC: 0.8408796\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0  0.816567   0.865959  0.858535  0.813051  0.84088  0.348083       0.81654   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.378796       0.816535   0.409215      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  49999.0  \n",
      "21\n",
      "(4785, 8)\n",
      "(4785, 1)\n",
      "(2640, 8)\n",
      "(2640, 1)\n",
      "(2145, 8)\n",
      "(2145, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6146091222763062\n",
      "Step: 100  \tTraining accuracy: 0.6687565445899963\n",
      "Step: 100  \tValid loss: 0.6217399835586548\n",
      "Step: 200  \tTraining loss: 0.46208977699279785\n",
      "Step: 200  \tTraining accuracy: 0.7288053035736084\n",
      "Step: 200  \tValid loss: 0.4702531397342682\n",
      "Step: 300  \tTraining loss: 0.3311331570148468\n",
      "Step: 300  \tTraining accuracy: 0.7824451327323914\n",
      "Step: 300  \tValid loss: 0.3320375978946686\n",
      "Step: 400  \tTraining loss: 0.2803875803947449\n",
      "Step: 400  \tTraining accuracy: 0.8133751153945923\n",
      "Step: 400  \tValid loss: 0.27550333738327026\n",
      "Step: 500  \tTraining loss: 0.2610376179218292\n",
      "Step: 500  \tTraining accuracy: 0.8314640522003174\n",
      "Step: 500  \tValid loss: 0.2533578872680664\n",
      "Step: 600  \tTraining loss: 0.2522015869617462\n",
      "Step: 600  \tTraining accuracy: 0.8435641527175903\n",
      "Step: 600  \tValid loss: 0.24389779567718506\n",
      "Step: 700  \tTraining loss: 0.24752552807331085\n",
      "Step: 700  \tTraining accuracy: 0.8524234294891357\n",
      "Step: 700  \tValid loss: 0.23980754613876343\n",
      "Step: 800  \tTraining loss: 0.24501855671405792\n",
      "Step: 800  \tTraining accuracy: 0.859240710735321\n",
      "Step: 800  \tValid loss: 0.23830173909664154\n",
      "Step: 900  \tTraining loss: 0.2435794621706009\n",
      "Step: 900  \tTraining accuracy: 0.864724338054657\n",
      "Step: 900  \tValid loss: 0.23785300552845\n",
      "Step: 1000  \tTraining loss: 0.24259430170059204\n",
      "Step: 1000  \tTraining accuracy: 0.869207501411438\n",
      "Step: 1000  \tValid loss: 0.23772656917572021\n",
      "Step: 1100  \tTraining loss: 0.2417963594198227\n",
      "Step: 1100  \tTraining accuracy: 0.8728268146514893\n",
      "Step: 1100  \tValid loss: 0.23765937983989716\n",
      "Step: 1200  \tTraining loss: 0.24108067154884338\n",
      "Step: 1200  \tTraining accuracy: 0.875880241394043\n",
      "Step: 1200  \tValid loss: 0.2375725954771042\n",
      "Step: 1300  \tTraining loss: 0.24040693044662476\n",
      "Step: 1300  \tTraining accuracy: 0.878336489200592\n",
      "Step: 1300  \tValid loss: 0.23743785917758942\n",
      "Step: 1400  \tTraining loss: 0.2397710382938385\n",
      "Step: 1400  \tTraining accuracy: 0.8803591728210449\n",
      "Step: 1400  \tValid loss: 0.23723354935646057\n",
      "Step: 1500  \tTraining loss: 0.23919077217578888\n",
      "Step: 1500  \tTraining accuracy: 0.8821028470993042\n",
      "Step: 1500  \tValid loss: 0.23695725202560425\n",
      "Step: 1600  \tTraining loss: 0.2386718988418579\n",
      "Step: 1600  \tTraining accuracy: 0.8836013078689575\n",
      "Step: 1600  \tValid loss: 0.23662936687469482\n",
      "Step: 1700  \tTraining loss: 0.238205224275589\n",
      "Step: 1700  \tTraining accuracy: 0.884861171245575\n",
      "Step: 1700  \tValid loss: 0.23626390099525452\n",
      "Step: 1800  \tTraining loss: 0.23777827620506287\n",
      "Step: 1800  \tTraining accuracy: 0.885977029800415\n",
      "Step: 1800  \tValid loss: 0.23586557805538177\n",
      "Step: 1900  \tTraining loss: 0.2373797446489334\n",
      "Step: 1900  \tTraining accuracy: 0.8869948387145996\n",
      "Step: 1900  \tValid loss: 0.23544107377529144\n",
      "Step: 2000  \tTraining loss: 0.23700039088726044\n",
      "Step: 2000  \tTraining accuracy: 0.8879082798957825\n",
      "Step: 2000  \tValid loss: 0.23500178754329681\n",
      "Step: 2100  \tTraining loss: 0.236637145280838\n",
      "Step: 2100  \tTraining accuracy: 0.888691782951355\n",
      "Step: 2100  \tValid loss: 0.23455962538719177\n",
      "Step: 2200  \tTraining loss: 0.23629312217235565\n",
      "Step: 2200  \tTraining accuracy: 0.8893975615501404\n",
      "Step: 2200  \tValid loss: 0.23412887752056122\n",
      "Step: 2300  \tTraining loss: 0.2359696924686432\n",
      "Step: 2300  \tTraining accuracy: 0.8900406360626221\n",
      "Step: 2300  \tValid loss: 0.23372045159339905\n",
      "Step: 2400  \tTraining loss: 0.2356637418270111\n",
      "Step: 2400  \tTraining accuracy: 0.890646755695343\n",
      "Step: 2400  \tValid loss: 0.23333309590816498\n",
      "Step: 2500  \tTraining loss: 0.23537170886993408\n",
      "Step: 2500  \tTraining accuracy: 0.8912034034729004\n",
      "Step: 2500  \tValid loss: 0.23295858502388\n",
      "Step: 2600  \tTraining loss: 0.23509062826633453\n",
      "Step: 2600  \tTraining accuracy: 0.8917368650436401\n",
      "Step: 2600  \tValid loss: 0.23259109258651733\n",
      "Step: 2700  \tTraining loss: 0.23481601476669312\n",
      "Step: 2700  \tTraining accuracy: 0.8922615647315979\n",
      "Step: 2700  \tValid loss: 0.23222728073596954\n",
      "Step: 2800  \tTraining loss: 0.23454256355762482\n",
      "Step: 2800  \tTraining accuracy: 0.8927405476570129\n",
      "Step: 2800  \tValid loss: 0.2318636178970337\n",
      "Step: 2900  \tTraining loss: 0.23426468670368195\n",
      "Step: 2900  \tTraining accuracy: 0.893174946308136\n",
      "Step: 2900  \tValid loss: 0.23149456083774567\n",
      "Step: 3000  \tTraining loss: 0.23397822678089142\n",
      "Step: 3000  \tTraining accuracy: 0.8936046361923218\n",
      "Step: 3000  \tValid loss: 0.23111246526241302\n",
      "Step: 3100  \tTraining loss: 0.2336816042661667\n",
      "Step: 3100  \tTraining accuracy: 0.8940096497535706\n",
      "Step: 3100  \tValid loss: 0.23071305453777313\n",
      "Step: 3200  \tTraining loss: 0.2333734929561615\n",
      "Step: 3200  \tTraining accuracy: 0.8944054841995239\n",
      "Step: 3200  \tValid loss: 0.23029418289661407\n",
      "Step: 3300  \tTraining loss: 0.2330530732870102\n",
      "Step: 3300  \tTraining accuracy: 0.8947865962982178\n",
      "Step: 3300  \tValid loss: 0.2298516035079956\n",
      "Step: 3400  \tTraining loss: 0.23271790146827698\n",
      "Step: 3400  \tTraining accuracy: 0.895129382610321\n",
      "Step: 3400  \tValid loss: 0.22938065230846405\n",
      "Step: 3500  \tTraining loss: 0.2323649376630783\n",
      "Step: 3500  \tTraining accuracy: 0.8954401612281799\n",
      "Step: 3500  \tValid loss: 0.22887571156024933\n",
      "Step: 3600  \tTraining loss: 0.2319936454296112\n",
      "Step: 3600  \tTraining accuracy: 0.8957069516181946\n",
      "Step: 3600  \tValid loss: 0.2283461093902588\n",
      "Step: 3700  \tTraining loss: 0.23160657286643982\n",
      "Step: 3700  \tTraining accuracy: 0.89593905210495\n",
      "Step: 3700  \tValid loss: 0.22781230509281158\n",
      "Step: 3800  \tTraining loss: 0.2312067300081253\n",
      "Step: 3800  \tTraining accuracy: 0.8961811065673828\n",
      "Step: 3800  \tValid loss: 0.22729575634002686\n",
      "Step: 3900  \tTraining loss: 0.23078575730323792\n",
      "Step: 3900  \tTraining accuracy: 0.8964268565177917\n",
      "Step: 3900  \tValid loss: 0.22679392993450165\n",
      "Step: 4000  \tTraining loss: 0.23031847178936005\n",
      "Step: 4000  \tTraining accuracy: 0.8966442942619324\n",
      "Step: 4000  \tValid loss: 0.22625958919525146\n",
      "Step: 4100  \tTraining loss: 0.2298089861869812\n",
      "Step: 4100  \tTraining accuracy: 0.8968587517738342\n",
      "Step: 4100  \tValid loss: 0.2256712019443512\n",
      "Step: 4200  \tTraining loss: 0.2292819321155548\n",
      "Step: 4200  \tTraining accuracy: 0.8970905542373657\n",
      "Step: 4200  \tValid loss: 0.22508716583251953\n",
      "Step: 4300  \tTraining loss: 0.22875098884105682\n",
      "Step: 4300  \tTraining accuracy: 0.8973090052604675\n",
      "Step: 4300  \tValid loss: 0.22456274926662445\n",
      "Step: 4400  \tTraining loss: 0.22824051976203918\n",
      "Step: 4400  \tTraining accuracy: 0.8975149989128113\n",
      "Step: 4400  \tValid loss: 0.22411397099494934\n",
      "Step: 4500  \tTraining loss: 0.22776463627815247\n",
      "Step: 4500  \tTraining accuracy: 0.8977187871932983\n",
      "Step: 4500  \tValid loss: 0.22374284267425537\n",
      "Step: 4600  \tTraining loss: 0.22730936110019684\n",
      "Step: 4600  \tTraining accuracy: 0.8979135751724243\n",
      "Step: 4600  \tValid loss: 0.22345422208309174\n",
      "Step: 4700  \tTraining loss: 0.22685791552066803\n",
      "Step: 4700  \tTraining accuracy: 0.8981000185012817\n",
      "Step: 4700  \tValid loss: 0.2232353836297989\n",
      "Step: 4800  \tTraining loss: 0.2264021337032318\n",
      "Step: 4800  \tTraining accuracy: 0.8982676267623901\n",
      "Step: 4800  \tValid loss: 0.22305889427661896\n",
      "Step: 4900  \tTraining loss: 0.22594112157821655\n",
      "Step: 4900  \tTraining accuracy: 0.898423969745636\n",
      "Step: 4900  \tValid loss: 0.2228977531194687\n",
      "Step: 5000  \tTraining loss: 0.22547827661037445\n",
      "Step: 5000  \tTraining accuracy: 0.898597240447998\n",
      "Step: 5000  \tValid loss: 0.2227392941713333\n",
      "Step: 5100  \tTraining loss: 0.2250186949968338\n",
      "Step: 5100  \tTraining accuracy: 0.8987864255905151\n",
      "Step: 5100  \tValid loss: 0.2225857377052307\n",
      "Step: 5200  \tTraining loss: 0.22456707060337067\n",
      "Step: 5200  \tTraining accuracy: 0.8989865183830261\n",
      "Step: 5200  \tValid loss: 0.2224453091621399\n",
      "Step: 5300  \tTraining loss: 0.22412797808647156\n",
      "Step: 5300  \tTraining accuracy: 0.8991849422454834\n",
      "Step: 5300  \tValid loss: 0.22232522070407867\n",
      "Step: 5400  \tTraining loss: 0.22370480000972748\n",
      "Step: 5400  \tTraining accuracy: 0.8993759751319885\n",
      "Step: 5400  \tValid loss: 0.22223077714443207\n",
      "Step: 5500  \tTraining loss: 0.22329968214035034\n",
      "Step: 5500  \tTraining accuracy: 0.8995503783226013\n",
      "Step: 5500  \tValid loss: 0.22216495871543884\n",
      "Step: 5600  \tTraining loss: 0.22291389107704163\n",
      "Step: 5600  \tTraining accuracy: 0.8997204303741455\n",
      "Step: 5600  \tValid loss: 0.22212737798690796\n",
      "Step: 5700  \tTraining loss: 0.22254762053489685\n",
      "Step: 5700  \tTraining accuracy: 0.8998899459838867\n",
      "Step: 5700  \tValid loss: 0.22211675345897675\n",
      "Step: 5800  \tTraining loss: 0.22220025956630707\n",
      "Step: 5800  \tTraining accuracy: 0.9000626802444458\n",
      "Step: 5800  \tValid loss: 0.22212907671928406\n",
      "Step: 5900  \tTraining loss: 0.2218707650899887\n",
      "Step: 5900  \tTraining accuracy: 0.9002402424812317\n",
      "Step: 5900  \tValid loss: 0.22216026484966278\n",
      "Step: 6000  \tTraining loss: 0.22155794501304626\n",
      "Step: 6000  \tTraining accuracy: 0.9004083275794983\n",
      "Step: 6000  \tValid loss: 0.2222057282924652\n",
      "Step: 6100  \tTraining loss: 0.2212604433298111\n",
      "Step: 6100  \tTraining accuracy: 0.9005656242370605\n",
      "Step: 6100  \tValid loss: 0.22226114571094513\n",
      "Step: 6200  \tTraining loss: 0.22097675502300262\n",
      "Step: 6200  \tTraining accuracy: 0.9007178544998169\n",
      "Step: 6200  \tValid loss: 0.22232277691364288\n",
      "Step: 6300  \tTraining loss: 0.22070574760437012\n",
      "Step: 6300  \tTraining accuracy: 0.9008752107620239\n",
      "Step: 6300  \tValid loss: 0.22238744795322418\n",
      "Step: 6400  \tTraining loss: 0.22044607996940613\n",
      "Step: 6400  \tTraining accuracy: 0.9010342359542847\n",
      "Step: 6400  \tValid loss: 0.222453311085701\n",
      "Step: 6500  \tTraining loss: 0.2201964259147644\n",
      "Step: 6500  \tTraining accuracy: 0.9011915326118469\n",
      "Step: 6500  \tValid loss: 0.2225174605846405\n",
      "Step: 6600  \tTraining loss: 0.21995562314987183\n",
      "Step: 6600  \tTraining accuracy: 0.9013504385948181\n",
      "Step: 6600  \tValid loss: 0.2225787490606308\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.9015093\n",
      "Precision: 0.9165154\n",
      "Recall: 0.9021885\n",
      "F1 score: 0.89440763\n",
      "AUC: 0.9149592\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.901509   0.916515  0.902188  0.894408  0.914959  0.219741      0.901401   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.222117       0.901353   0.254282      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  6691.0  \n",
      "22\n",
      "(5075, 8)\n",
      "(5075, 1)\n",
      "(2800, 8)\n",
      "(2800, 1)\n",
      "(2275, 8)\n",
      "(2275, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6128777265548706\n",
      "Step: 100  \tTraining accuracy: 0.7095566391944885\n",
      "Step: 100  \tValid loss: 0.6436007618904114\n",
      "Step: 200  \tTraining loss: 0.5681918859481812\n",
      "Step: 200  \tTraining accuracy: 0.7121182084083557\n",
      "Step: 200  \tValid loss: 0.6075941324234009\n",
      "Step: 300  \tTraining loss: 0.5318895578384399\n",
      "Step: 300  \tTraining accuracy: 0.7181477546691895\n",
      "Step: 300  \tValid loss: 0.5664876699447632\n",
      "Step: 400  \tTraining loss: 0.5086007714271545\n",
      "Step: 400  \tTraining accuracy: 0.724700927734375\n",
      "Step: 400  \tValid loss: 0.5404271483421326\n",
      "Step: 500  \tTraining loss: 0.4933057725429535\n",
      "Step: 500  \tTraining accuracy: 0.7299178838729858\n",
      "Step: 500  \tValid loss: 0.5239054560661316\n",
      "Step: 600  \tTraining loss: 0.4822402596473694\n",
      "Step: 600  \tTraining accuracy: 0.7337035536766052\n",
      "Step: 600  \tValid loss: 0.5122309923171997\n",
      "Step: 700  \tTraining loss: 0.47480496764183044\n",
      "Step: 700  \tTraining accuracy: 0.7367942333221436\n",
      "Step: 700  \tValid loss: 0.5043293237686157\n",
      "Step: 800  \tTraining loss: 0.4702083468437195\n",
      "Step: 800  \tTraining accuracy: 0.7390081882476807\n",
      "Step: 800  \tValid loss: 0.4992867708206177\n",
      "Step: 900  \tTraining loss: 0.4671018719673157\n",
      "Step: 900  \tTraining accuracy: 0.7416749000549316\n",
      "Step: 900  \tValid loss: 0.49572357535362244\n",
      "Step: 1000  \tTraining loss: 0.4648677110671997\n",
      "Step: 1000  \tTraining accuracy: 0.7439357042312622\n",
      "Step: 1000  \tValid loss: 0.4932480752468109\n",
      "Step: 1100  \tTraining loss: 0.46324363350868225\n",
      "Step: 1100  \tTraining accuracy: 0.7457095980644226\n",
      "Step: 1100  \tValid loss: 0.4913387596607208\n",
      "Step: 1200  \tTraining loss: 0.46198299527168274\n",
      "Step: 1200  \tTraining accuracy: 0.7472092509269714\n",
      "Step: 1200  \tValid loss: 0.4898874759674072\n",
      "Step: 1300  \tTraining loss: 0.46100687980651855\n",
      "Step: 1300  \tTraining accuracy: 0.7486029267311096\n",
      "Step: 1300  \tValid loss: 0.4886290729045868\n",
      "Step: 1400  \tTraining loss: 0.4602296054363251\n",
      "Step: 1400  \tTraining accuracy: 0.7498485445976257\n",
      "Step: 1400  \tValid loss: 0.48762181401252747\n",
      "Step: 1500  \tTraining loss: 0.4595686197280884\n",
      "Step: 1500  \tTraining accuracy: 0.7510582804679871\n",
      "Step: 1500  \tValid loss: 0.48678258061408997\n",
      "Step: 1600  \tTraining loss: 0.45898282527923584\n",
      "Step: 1600  \tTraining accuracy: 0.752130925655365\n",
      "Step: 1600  \tValid loss: 0.48605093359947205\n",
      "Step: 1700  \tTraining loss: 0.45840156078338623\n",
      "Step: 1700  \tTraining accuracy: 0.7530676126480103\n",
      "Step: 1700  \tValid loss: 0.4852999448776245\n",
      "Step: 1800  \tTraining loss: 0.4579022526741028\n",
      "Step: 1800  \tTraining accuracy: 0.7538691163063049\n",
      "Step: 1800  \tValid loss: 0.48469215631484985\n",
      "Step: 1900  \tTraining loss: 0.4574325680732727\n",
      "Step: 1900  \tTraining accuracy: 0.7545573115348816\n",
      "Step: 1900  \tValid loss: 0.48417794704437256\n",
      "Step: 2000  \tTraining loss: 0.4569840431213379\n",
      "Step: 2000  \tTraining accuracy: 0.7552052736282349\n",
      "Step: 2000  \tValid loss: 0.4836513102054596\n",
      "Step: 2100  \tTraining loss: 0.45655182003974915\n",
      "Step: 2100  \tTraining accuracy: 0.7558236122131348\n",
      "Step: 2100  \tValid loss: 0.4831732511520386\n",
      "Step: 2200  \tTraining loss: 0.45613741874694824\n",
      "Step: 2200  \tTraining accuracy: 0.7563661336898804\n",
      "Step: 2200  \tValid loss: 0.4827052056789398\n",
      "Step: 2300  \tTraining loss: 0.4557417035102844\n",
      "Step: 2300  \tTraining accuracy: 0.7569479942321777\n",
      "Step: 2300  \tValid loss: 0.48226606845855713\n",
      "Step: 2400  \tTraining loss: 0.4553634524345398\n",
      "Step: 2400  \tTraining accuracy: 0.7575390338897705\n",
      "Step: 2400  \tValid loss: 0.48185476660728455\n",
      "Step: 2500  \tTraining loss: 0.45499807596206665\n",
      "Step: 2500  \tTraining accuracy: 0.7581220269203186\n",
      "Step: 2500  \tValid loss: 0.48144933581352234\n",
      "Step: 2600  \tTraining loss: 0.4545770585536957\n",
      "Step: 2600  \tTraining accuracy: 0.7586399912834167\n",
      "Step: 2600  \tValid loss: 0.4809736907482147\n",
      "Step: 2700  \tTraining loss: 0.45399653911590576\n",
      "Step: 2700  \tTraining accuracy: 0.7590779662132263\n",
      "Step: 2700  \tValid loss: 0.4803942143917084\n",
      "Step: 2800  \tTraining loss: 0.4535483419895172\n",
      "Step: 2800  \tTraining accuracy: 0.7594447135925293\n",
      "Step: 2800  \tValid loss: 0.47985219955444336\n",
      "Step: 2900  \tTraining loss: 0.45314115285873413\n",
      "Step: 2900  \tTraining accuracy: 0.7597441673278809\n",
      "Step: 2900  \tValid loss: 0.47929656505584717\n",
      "Step: 3000  \tTraining loss: 0.45276862382888794\n",
      "Step: 3000  \tTraining accuracy: 0.7600200176239014\n",
      "Step: 3000  \tValid loss: 0.478759765625\n",
      "Step: 3100  \tTraining loss: 0.45241865515708923\n",
      "Step: 3100  \tTraining accuracy: 0.7602810263633728\n",
      "Step: 3100  \tValid loss: 0.4782971739768982\n",
      "Step: 3200  \tTraining loss: 0.4520852863788605\n",
      "Step: 3200  \tTraining accuracy: 0.7605066895484924\n",
      "Step: 3200  \tValid loss: 0.4779009222984314\n",
      "Step: 3300  \tTraining loss: 0.4517709016799927\n",
      "Step: 3300  \tTraining accuracy: 0.760763943195343\n",
      "Step: 3300  \tValid loss: 0.47751420736312866\n",
      "Step: 3400  \tTraining loss: 0.4514712989330292\n",
      "Step: 3400  \tTraining accuracy: 0.7610263824462891\n",
      "Step: 3400  \tValid loss: 0.47714024782180786\n",
      "Step: 3500  \tTraining loss: 0.45118841528892517\n",
      "Step: 3500  \tTraining accuracy: 0.7612708210945129\n",
      "Step: 3500  \tValid loss: 0.4767884314060211\n",
      "Step: 3600  \tTraining loss: 0.45091989636421204\n",
      "Step: 3600  \tTraining accuracy: 0.7614791989326477\n",
      "Step: 3600  \tValid loss: 0.47639352083206177\n",
      "Step: 3700  \tTraining loss: 0.45062515139579773\n",
      "Step: 3700  \tTraining accuracy: 0.7616762518882751\n",
      "Step: 3700  \tValid loss: 0.4760763943195343\n",
      "Step: 3800  \tTraining loss: 0.45035484433174133\n",
      "Step: 3800  \tTraining accuracy: 0.7618837356567383\n",
      "Step: 3800  \tValid loss: 0.47574928402900696\n",
      "Step: 3900  \tTraining loss: 0.45010486245155334\n",
      "Step: 3900  \tTraining accuracy: 0.7621034979820251\n",
      "Step: 3900  \tValid loss: 0.4754408001899719\n",
      "Step: 4000  \tTraining loss: 0.44986024498939514\n",
      "Step: 4000  \tTraining accuracy: 0.7623021602630615\n",
      "Step: 4000  \tValid loss: 0.47513577342033386\n",
      "Step: 4100  \tTraining loss: 0.44962289929389954\n",
      "Step: 4100  \tTraining accuracy: 0.7624983191490173\n",
      "Step: 4100  \tValid loss: 0.4748329818248749\n",
      "Step: 4200  \tTraining loss: 0.4493829011917114\n",
      "Step: 4200  \tTraining accuracy: 0.7626897692680359\n",
      "Step: 4200  \tValid loss: 0.47452881932258606\n",
      "Step: 4300  \tTraining loss: 0.44915640354156494\n",
      "Step: 4300  \tTraining accuracy: 0.7629046440124512\n",
      "Step: 4300  \tValid loss: 0.4742416739463806\n",
      "Step: 4400  \tTraining loss: 0.44893911480903625\n",
      "Step: 4400  \tTraining accuracy: 0.7631436586380005\n",
      "Step: 4400  \tValid loss: 0.47397616505622864\n",
      "Step: 4500  \tTraining loss: 0.4487302899360657\n",
      "Step: 4500  \tTraining accuracy: 0.7633785605430603\n",
      "Step: 4500  \tValid loss: 0.4737567901611328\n",
      "Step: 4600  \tTraining loss: 0.4485253393650055\n",
      "Step: 4600  \tTraining accuracy: 0.7635965943336487\n",
      "Step: 4600  \tValid loss: 0.47349464893341064\n",
      "Step: 4700  \tTraining loss: 0.44832858443260193\n",
      "Step: 4700  \tTraining accuracy: 0.7637989521026611\n",
      "Step: 4700  \tValid loss: 0.473318487405777\n",
      "Step: 4800  \tTraining loss: 0.44813403487205505\n",
      "Step: 4800  \tTraining accuracy: 0.7640010118484497\n",
      "Step: 4800  \tValid loss: 0.4731154143810272\n",
      "Step: 4900  \tTraining loss: 0.44794681668281555\n",
      "Step: 4900  \tTraining accuracy: 0.7642049789428711\n",
      "Step: 4900  \tValid loss: 0.4729148745536804\n",
      "Step: 5000  \tTraining loss: 0.4477478563785553\n",
      "Step: 5000  \tTraining accuracy: 0.7644106149673462\n",
      "Step: 5000  \tValid loss: 0.47264373302459717\n",
      "Step: 5100  \tTraining loss: 0.44756466150283813\n",
      "Step: 5100  \tTraining accuracy: 0.7646080851554871\n",
      "Step: 5100  \tValid loss: 0.47249624133110046\n",
      "Step: 5200  \tTraining loss: 0.44738924503326416\n",
      "Step: 5200  \tTraining accuracy: 0.7647979259490967\n",
      "Step: 5200  \tValid loss: 0.4723135232925415\n",
      "Step: 5300  \tTraining loss: 0.4472103416919708\n",
      "Step: 5300  \tTraining accuracy: 0.7649692893028259\n",
      "Step: 5300  \tValid loss: 0.4721701443195343\n",
      "Step: 5400  \tTraining loss: 0.4470385015010834\n",
      "Step: 5400  \tTraining accuracy: 0.765152633190155\n",
      "Step: 5400  \tValid loss: 0.47200605273246765\n",
      "Step: 5500  \tTraining loss: 0.4468729794025421\n",
      "Step: 5500  \tTraining accuracy: 0.7653364539146423\n",
      "Step: 5500  \tValid loss: 0.47187915444374084\n",
      "Step: 5600  \tTraining loss: 0.4467155635356903\n",
      "Step: 5600  \tTraining accuracy: 0.7655083537101746\n",
      "Step: 5600  \tValid loss: 0.4717436730861664\n",
      "Step: 5700  \tTraining loss: 0.44656091928482056\n",
      "Step: 5700  \tTraining accuracy: 0.7656776905059814\n",
      "Step: 5700  \tValid loss: 0.4716392159461975\n",
      "Step: 5800  \tTraining loss: 0.44640952348709106\n",
      "Step: 5800  \tTraining accuracy: 0.7658547759056091\n",
      "Step: 5800  \tValid loss: 0.4714648425579071\n",
      "Step: 5900  \tTraining loss: 0.4462580680847168\n",
      "Step: 5900  \tTraining accuracy: 0.766014039516449\n",
      "Step: 5900  \tValid loss: 0.47132015228271484\n",
      "Step: 6000  \tTraining loss: 0.44611015915870667\n",
      "Step: 6000  \tTraining accuracy: 0.76615971326828\n",
      "Step: 6000  \tValid loss: 0.4712001383304596\n",
      "Step: 6100  \tTraining loss: 0.44596245884895325\n",
      "Step: 6100  \tTraining accuracy: 0.7662875056266785\n",
      "Step: 6100  \tValid loss: 0.4710645377635956\n",
      "Step: 6200  \tTraining loss: 0.44581887125968933\n",
      "Step: 6200  \tTraining accuracy: 0.7664191722869873\n",
      "Step: 6200  \tValid loss: 0.4709198772907257\n",
      "Step: 6300  \tTraining loss: 0.4456751346588135\n",
      "Step: 6300  \tTraining accuracy: 0.7665544748306274\n",
      "Step: 6300  \tValid loss: 0.47081294655799866\n",
      "Step: 6400  \tTraining loss: 0.4455316662788391\n",
      "Step: 6400  \tTraining accuracy: 0.7666824460029602\n",
      "Step: 6400  \tValid loss: 0.4707023501396179\n",
      "Step: 6500  \tTraining loss: 0.44538840651512146\n",
      "Step: 6500  \tTraining accuracy: 0.7668125629425049\n",
      "Step: 6500  \tValid loss: 0.4705689251422882\n",
      "Step: 6600  \tTraining loss: 0.4452056586742401\n",
      "Step: 6600  \tTraining accuracy: 0.766940176486969\n",
      "Step: 6600  \tValid loss: 0.47060269117355347\n",
      "Step: 6700  \tTraining loss: 0.44491586089134216\n",
      "Step: 6700  \tTraining accuracy: 0.767044723033905\n",
      "Step: 6700  \tValid loss: 0.4708973169326782\n",
      "Step: 6800  \tTraining loss: 0.4446187913417816\n",
      "Step: 6800  \tTraining accuracy: 0.7671417593955994\n",
      "Step: 6800  \tValid loss: 0.47089600563049316\n",
      "Step: 6900  \tTraining loss: 0.4442964792251587\n",
      "Step: 6900  \tTraining accuracy: 0.767257571220398\n",
      "Step: 6900  \tValid loss: 0.47072833776474\n",
      "Step: 7000  \tTraining loss: 0.4439696669578552\n",
      "Step: 7000  \tTraining accuracy: 0.7673657536506653\n",
      "Step: 7000  \tValid loss: 0.47051531076431274\n",
      "Step: 7100  \tTraining loss: 0.4436473548412323\n",
      "Step: 7100  \tTraining accuracy: 0.7674736976623535\n",
      "Step: 7100  \tValid loss: 0.4701709747314453\n",
      "Step: 7200  \tTraining loss: 0.44335177540779114\n",
      "Step: 7200  \tTraining accuracy: 0.7675910592079163\n",
      "Step: 7200  \tValid loss: 0.46976345777511597\n",
      "Step: 7300  \tTraining loss: 0.4430792033672333\n",
      "Step: 7300  \tTraining accuracy: 0.7677241563796997\n",
      "Step: 7300  \tValid loss: 0.4694623649120331\n",
      "Step: 7400  \tTraining loss: 0.44283467531204224\n",
      "Step: 7400  \tTraining accuracy: 0.7678710222244263\n",
      "Step: 7400  \tValid loss: 0.4691452085971832\n",
      "Step: 7500  \tTraining loss: 0.44261762499809265\n",
      "Step: 7500  \tTraining accuracy: 0.7680113911628723\n",
      "Step: 7500  \tValid loss: 0.4688631296157837\n",
      "Step: 7600  \tTraining loss: 0.4424233138561249\n",
      "Step: 7600  \tTraining accuracy: 0.7681480050086975\n",
      "Step: 7600  \tValid loss: 0.46853405237197876\n",
      "Step: 7700  \tTraining loss: 0.44224581122398376\n",
      "Step: 7700  \tTraining accuracy: 0.7682874798774719\n",
      "Step: 7700  \tValid loss: 0.4682903289794922\n",
      "Step: 7800  \tTraining loss: 0.44208383560180664\n",
      "Step: 7800  \tTraining accuracy: 0.7684321999549866\n",
      "Step: 7800  \tValid loss: 0.46811437606811523\n",
      "Step: 7900  \tTraining loss: 0.44193416833877563\n",
      "Step: 7900  \tTraining accuracy: 0.7685707807540894\n",
      "Step: 7900  \tValid loss: 0.4679602086544037\n",
      "Step: 8000  \tTraining loss: 0.4417934715747833\n",
      "Step: 8000  \tTraining accuracy: 0.7687046527862549\n",
      "Step: 8000  \tValid loss: 0.46774402260780334\n",
      "Step: 8100  \tTraining loss: 0.4416590929031372\n",
      "Step: 8100  \tTraining accuracy: 0.7688363790512085\n",
      "Step: 8100  \tValid loss: 0.4676283597946167\n",
      "Step: 8200  \tTraining loss: 0.4415302574634552\n",
      "Step: 8200  \tTraining accuracy: 0.7689685225486755\n",
      "Step: 8200  \tValid loss: 0.46748408675193787\n",
      "Step: 8300  \tTraining loss: 0.44140270352363586\n",
      "Step: 8300  \tTraining accuracy: 0.7690998911857605\n",
      "Step: 8300  \tValid loss: 0.4673178493976593\n",
      "Step: 8400  \tTraining loss: 0.4412759840488434\n",
      "Step: 8400  \tTraining accuracy: 0.7692268490791321\n",
      "Step: 8400  \tValid loss: 0.4672083258628845\n",
      "Step: 8500  \tTraining loss: 0.44115176796913147\n",
      "Step: 8500  \tTraining accuracy: 0.7693543434143066\n",
      "Step: 8500  \tValid loss: 0.46712157130241394\n",
      "Step: 8600  \tTraining loss: 0.4410301446914673\n",
      "Step: 8600  \tTraining accuracy: 0.7694846391677856\n",
      "Step: 8600  \tValid loss: 0.4669428765773773\n",
      "Step: 8700  \tTraining loss: 0.4409075379371643\n",
      "Step: 8700  \tTraining accuracy: 0.7696187496185303\n",
      "Step: 8700  \tValid loss: 0.4668746590614319\n",
      "Step: 8800  \tTraining loss: 0.44078612327575684\n",
      "Step: 8800  \tTraining accuracy: 0.769752025604248\n",
      "Step: 8800  \tValid loss: 0.4667517840862274\n",
      "Step: 8900  \tTraining loss: 0.44066426157951355\n",
      "Step: 8900  \tTraining accuracy: 0.7698822617530823\n",
      "Step: 8900  \tValid loss: 0.466653972864151\n",
      "Step: 9000  \tTraining loss: 0.44054168462753296\n",
      "Step: 9000  \tTraining accuracy: 0.7700140476226807\n",
      "Step: 9000  \tValid loss: 0.4665502905845642\n",
      "Step: 9100  \tTraining loss: 0.4404183328151703\n",
      "Step: 9100  \tTraining accuracy: 0.7701494097709656\n",
      "Step: 9100  \tValid loss: 0.4664754867553711\n",
      "Step: 9200  \tTraining loss: 0.4402928650379181\n",
      "Step: 9200  \tTraining accuracy: 0.7702818512916565\n",
      "Step: 9200  \tValid loss: 0.46642255783081055\n",
      "Step: 9300  \tTraining loss: 0.4401680827140808\n",
      "Step: 9300  \tTraining accuracy: 0.7704145908355713\n",
      "Step: 9300  \tValid loss: 0.46633023023605347\n",
      "Step: 9400  \tTraining loss: 0.4400440752506256\n",
      "Step: 9400  \tTraining accuracy: 0.7705466151237488\n",
      "Step: 9400  \tValid loss: 0.4662555754184723\n",
      "Step: 9500  \tTraining loss: 0.439919650554657\n",
      "Step: 9500  \tTraining accuracy: 0.7706748247146606\n",
      "Step: 9500  \tValid loss: 0.46620282530784607\n",
      "Step: 9600  \tTraining loss: 0.43979620933532715\n",
      "Step: 9600  \tTraining accuracy: 0.7707961797714233\n",
      "Step: 9600  \tValid loss: 0.4661965072154999\n",
      "Step: 9700  \tTraining loss: 0.43967384099960327\n",
      "Step: 9700  \tTraining accuracy: 0.7709201574325562\n",
      "Step: 9700  \tValid loss: 0.4661194682121277\n",
      "Step: 9800  \tTraining loss: 0.4395541548728943\n",
      "Step: 9800  \tTraining accuracy: 0.771043598651886\n",
      "Step: 9800  \tValid loss: 0.4660686254501343\n",
      "Step: 9900  \tTraining loss: 0.4394363760948181\n",
      "Step: 9900  \tTraining accuracy: 0.7711665034294128\n",
      "Step: 9900  \tValid loss: 0.46600767970085144\n",
      "Step: 10000  \tTraining loss: 0.4393209218978882\n",
      "Step: 10000  \tTraining accuracy: 0.7712879776954651\n",
      "Step: 10000  \tValid loss: 0.4660203456878662\n",
      "Step: 10100  \tTraining loss: 0.43920889496803284\n",
      "Step: 10100  \tTraining accuracy: 0.7714070081710815\n",
      "Step: 10100  \tValid loss: 0.46588894724845886\n",
      "Step: 10200  \tTraining loss: 0.4390975832939148\n",
      "Step: 10200  \tTraining accuracy: 0.7715246677398682\n",
      "Step: 10200  \tValid loss: 0.46589013934135437\n",
      "Step: 10300  \tTraining loss: 0.4389885663986206\n",
      "Step: 10300  \tTraining accuracy: 0.7716467380523682\n",
      "Step: 10300  \tValid loss: 0.4658415913581848\n",
      "Step: 10400  \tTraining loss: 0.4388832449913025\n",
      "Step: 10400  \tTraining accuracy: 0.7717731595039368\n",
      "Step: 10400  \tValid loss: 0.4657800793647766\n",
      "Step: 10500  \tTraining loss: 0.43877866864204407\n",
      "Step: 10500  \tTraining accuracy: 0.7718990445137024\n",
      "Step: 10500  \tValid loss: 0.4657458961009979\n",
      "Step: 10600  \tTraining loss: 0.43867725133895874\n",
      "Step: 10600  \tTraining accuracy: 0.7720252871513367\n",
      "Step: 10600  \tValid loss: 0.465691477060318\n",
      "Step: 10700  \tTraining loss: 0.43857651948928833\n",
      "Step: 10700  \tTraining accuracy: 0.7721520066261292\n",
      "Step: 10700  \tValid loss: 0.46568936109542847\n",
      "Step: 10800  \tTraining loss: 0.43847811222076416\n",
      "Step: 10800  \tTraining accuracy: 0.7722772359848022\n",
      "Step: 10800  \tValid loss: 0.4656549096107483\n",
      "Step: 10900  \tTraining loss: 0.43838170170783997\n",
      "Step: 10900  \tTraining accuracy: 0.7723919749259949\n",
      "Step: 10900  \tValid loss: 0.4656096398830414\n",
      "Step: 11000  \tTraining loss: 0.4382869303226471\n",
      "Step: 11000  \tTraining accuracy: 0.7725001573562622\n",
      "Step: 11000  \tValid loss: 0.46559926867485046\n",
      "Step: 11100  \tTraining loss: 0.4381946623325348\n",
      "Step: 11100  \tTraining accuracy: 0.7726099491119385\n",
      "Step: 11100  \tValid loss: 0.46556758880615234\n",
      "Step: 11200  \tTraining loss: 0.4381035566329956\n",
      "Step: 11200  \tTraining accuracy: 0.772718608379364\n",
      "Step: 11200  \tValid loss: 0.4655109643936157\n",
      "Step: 11300  \tTraining loss: 0.4380136728286743\n",
      "Step: 11300  \tTraining accuracy: 0.7728254199028015\n",
      "Step: 11300  \tValid loss: 0.4654816687107086\n",
      "Step: 11400  \tTraining loss: 0.4379257559776306\n",
      "Step: 11400  \tTraining accuracy: 0.7729302644729614\n",
      "Step: 11400  \tValid loss: 0.4654577076435089\n",
      "Step: 11500  \tTraining loss: 0.4378390908241272\n",
      "Step: 11500  \tTraining accuracy: 0.7730333209037781\n",
      "Step: 11500  \tValid loss: 0.4654192626476288\n",
      "Step: 11600  \tTraining loss: 0.4377533197402954\n",
      "Step: 11600  \tTraining accuracy: 0.77313631772995\n",
      "Step: 11600  \tValid loss: 0.46542027592658997\n",
      "Step: 11700  \tTraining loss: 0.4376694858074188\n",
      "Step: 11700  \tTraining accuracy: 0.7732400298118591\n",
      "Step: 11700  \tValid loss: 0.4653971791267395\n",
      "Step: 11800  \tTraining loss: 0.43758755922317505\n",
      "Step: 11800  \tTraining accuracy: 0.7733453512191772\n",
      "Step: 11800  \tValid loss: 0.46533825993537903\n",
      "Step: 11900  \tTraining loss: 0.43750596046447754\n",
      "Step: 11900  \tTraining accuracy: 0.7734563946723938\n",
      "Step: 11900  \tValid loss: 0.4653182923793793\n",
      "Step: 12000  \tTraining loss: 0.437425434589386\n",
      "Step: 12000  \tTraining accuracy: 0.7735630869865417\n",
      "Step: 12000  \tValid loss: 0.4653099775314331\n",
      "Step: 12100  \tTraining loss: 0.43734654784202576\n",
      "Step: 12100  \tTraining accuracy: 0.7736655473709106\n",
      "Step: 12100  \tValid loss: 0.46526584029197693\n",
      "Step: 12200  \tTraining loss: 0.4372679889202118\n",
      "Step: 12200  \tTraining accuracy: 0.7737647294998169\n",
      "Step: 12200  \tValid loss: 0.46524858474731445\n",
      "Step: 12300  \tTraining loss: 0.4371904730796814\n",
      "Step: 12300  \tTraining accuracy: 0.7738598585128784\n",
      "Step: 12300  \tValid loss: 0.4652315080165863\n",
      "Step: 12400  \tTraining loss: 0.4371146559715271\n",
      "Step: 12400  \tTraining accuracy: 0.7739518284797668\n",
      "Step: 12400  \tValid loss: 0.4651908278465271\n",
      "Step: 12500  \tTraining loss: 0.4370396137237549\n",
      "Step: 12500  \tTraining accuracy: 0.7740423679351807\n",
      "Step: 12500  \tValid loss: 0.46517041325569153\n",
      "Step: 12600  \tTraining loss: 0.4369652569293976\n",
      "Step: 12600  \tTraining accuracy: 0.7741314768791199\n",
      "Step: 12600  \tValid loss: 0.46514374017715454\n",
      "Step: 12700  \tTraining loss: 0.436892032623291\n",
      "Step: 12700  \tTraining accuracy: 0.7742214798927307\n",
      "Step: 12700  \tValid loss: 0.4650987386703491\n",
      "Step: 12800  \tTraining loss: 0.43681880831718445\n",
      "Step: 12800  \tTraining accuracy: 0.7743154764175415\n",
      "Step: 12800  \tValid loss: 0.4651028513908386\n",
      "Step: 12900  \tTraining loss: 0.4367474317550659\n",
      "Step: 12900  \tTraining accuracy: 0.7744079828262329\n",
      "Step: 12900  \tValid loss: 0.46508222818374634\n",
      "Step: 13000  \tTraining loss: 0.4366760849952698\n",
      "Step: 13000  \tTraining accuracy: 0.7745029330253601\n",
      "Step: 13000  \tValid loss: 0.4650746285915375\n",
      "Step: 13100  \tTraining loss: 0.4366064667701721\n",
      "Step: 13100  \tTraining accuracy: 0.7745933532714844\n",
      "Step: 13100  \tValid loss: 0.4650260806083679\n",
      "Step: 13200  \tTraining loss: 0.43653661012649536\n",
      "Step: 13200  \tTraining accuracy: 0.7746816873550415\n",
      "Step: 13200  \tValid loss: 0.46503809094429016\n",
      "Step: 13300  \tTraining loss: 0.4364686906337738\n",
      "Step: 13300  \tTraining accuracy: 0.7747686505317688\n",
      "Step: 13300  \tValid loss: 0.46498575806617737\n",
      "Step: 13400  \tTraining loss: 0.4364003837108612\n",
      "Step: 13400  \tTraining accuracy: 0.7748543620109558\n",
      "Step: 13400  \tValid loss: 0.4649767577648163\n",
      "Step: 13500  \tTraining loss: 0.43633273243904114\n",
      "Step: 13500  \tTraining accuracy: 0.7749350666999817\n",
      "Step: 13500  \tValid loss: 0.4649730622768402\n",
      "Step: 13600  \tTraining loss: 0.436266154050827\n",
      "Step: 13600  \tTraining accuracy: 0.7750146389007568\n",
      "Step: 13600  \tValid loss: 0.4649379253387451\n",
      "Step: 13700  \tTraining loss: 0.43619978427886963\n",
      "Step: 13700  \tTraining accuracy: 0.7750930190086365\n",
      "Step: 13700  \tValid loss: 0.4649074375629425\n",
      "Step: 13800  \tTraining loss: 0.43613383173942566\n",
      "Step: 13800  \tTraining accuracy: 0.7751724123954773\n",
      "Step: 13800  \tValid loss: 0.4649142920970917\n",
      "Step: 13900  \tTraining loss: 0.4360690712928772\n",
      "Step: 13900  \tTraining accuracy: 0.7752492427825928\n",
      "Step: 13900  \tValid loss: 0.46487295627593994\n",
      "Step: 14000  \tTraining loss: 0.43600454926490784\n",
      "Step: 14000  \tTraining accuracy: 0.7753278017044067\n",
      "Step: 14000  \tValid loss: 0.46484705805778503\n",
      "Step: 14100  \tTraining loss: 0.4359396994113922\n",
      "Step: 14100  \tTraining accuracy: 0.7754003405570984\n",
      "Step: 14100  \tValid loss: 0.4648386836051941\n",
      "Step: 14200  \tTraining loss: 0.43587616086006165\n",
      "Step: 14200  \tTraining accuracy: 0.7754669189453125\n",
      "Step: 14200  \tValid loss: 0.4647902250289917\n",
      "Step: 14300  \tTraining loss: 0.435811847448349\n",
      "Step: 14300  \tTraining accuracy: 0.775536060333252\n",
      "Step: 14300  \tValid loss: 0.4648013412952423\n",
      "Step: 14400  \tTraining loss: 0.4357492923736572\n",
      "Step: 14400  \tTraining accuracy: 0.7756063342094421\n",
      "Step: 14400  \tValid loss: 0.46472498774528503\n",
      "Step: 14500  \tTraining loss: 0.43568551540374756\n",
      "Step: 14500  \tTraining accuracy: 0.7756797075271606\n",
      "Step: 14500  \tValid loss: 0.4647204279899597\n",
      "Step: 14600  \tTraining loss: 0.43562307953834534\n",
      "Step: 14600  \tTraining accuracy: 0.7757506966590881\n",
      "Step: 14600  \tValid loss: 0.4646744132041931\n",
      "Step: 14700  \tTraining loss: 0.43555909395217896\n",
      "Step: 14700  \tTraining accuracy: 0.7758213877677917\n",
      "Step: 14700  \tValid loss: 0.46469926834106445\n",
      "Step: 14800  \tTraining loss: 0.43549689650535583\n",
      "Step: 14800  \tTraining accuracy: 0.7758890986442566\n",
      "Step: 14800  \tValid loss: 0.4646286368370056\n",
      "Step: 14900  \tTraining loss: 0.43543314933776855\n",
      "Step: 14900  \tTraining accuracy: 0.7759579420089722\n",
      "Step: 14900  \tValid loss: 0.4646545946598053\n",
      "Step: 15000  \tTraining loss: 0.4353600740432739\n",
      "Step: 15000  \tTraining accuracy: 0.7760232090950012\n",
      "Step: 15000  \tValid loss: 0.4645855724811554\n",
      "Step: 15100  \tTraining loss: 0.4352785348892212\n",
      "Step: 15100  \tTraining accuracy: 0.7760934829711914\n",
      "Step: 15100  \tValid loss: 0.46459364891052246\n",
      "Step: 15200  \tTraining loss: 0.43520116806030273\n",
      "Step: 15200  \tTraining accuracy: 0.776170015335083\n",
      "Step: 15200  \tValid loss: 0.46465572714805603\n",
      "Step: 15300  \tTraining loss: 0.4351237416267395\n",
      "Step: 15300  \tTraining accuracy: 0.7762454748153687\n",
      "Step: 15300  \tValid loss: 0.4645358622074127\n",
      "Step: 15400  \tTraining loss: 0.43505051732063293\n",
      "Step: 15400  \tTraining accuracy: 0.7763200402259827\n",
      "Step: 15400  \tValid loss: 0.4645286202430725\n",
      "Step: 15500  \tTraining loss: 0.43497806787490845\n",
      "Step: 15500  \tTraining accuracy: 0.7763910293579102\n",
      "Step: 15500  \tValid loss: 0.46453577280044556\n",
      "Step: 15600  \tTraining loss: 0.4349079728126526\n",
      "Step: 15600  \tTraining accuracy: 0.7764604687690735\n",
      "Step: 15600  \tValid loss: 0.46449655294418335\n",
      "Step: 15700  \tTraining loss: 0.4348238408565521\n",
      "Step: 15700  \tTraining accuracy: 0.7765315771102905\n",
      "Step: 15700  \tValid loss: 0.4645911157131195\n",
      "Step: 15800  \tTraining loss: 0.4347470700740814\n",
      "Step: 15800  \tTraining accuracy: 0.7765998840332031\n",
      "Step: 15800  \tValid loss: 0.46456795930862427\n",
      "Step: 15900  \tTraining loss: 0.43467238545417786\n",
      "Step: 15900  \tTraining accuracy: 0.7766667008399963\n",
      "Step: 15900  \tValid loss: 0.4645621180534363\n",
      "Step: 16000  \tTraining loss: 0.4345990717411041\n",
      "Step: 16000  \tTraining accuracy: 0.7767326831817627\n",
      "Step: 16000  \tValid loss: 0.46453186869621277\n",
      "Step: 16100  \tTraining loss: 0.4345267713069916\n",
      "Step: 16100  \tTraining accuracy: 0.7767954468727112\n",
      "Step: 16100  \tValid loss: 0.4645287096500397\n",
      "Step: 16200  \tTraining loss: 0.43445563316345215\n",
      "Step: 16200  \tTraining accuracy: 0.7768567204475403\n",
      "Step: 16200  \tValid loss: 0.46448808908462524\n",
      "Step: 16300  \tTraining loss: 0.43437620997428894\n",
      "Step: 16300  \tTraining accuracy: 0.7769173383712769\n",
      "Step: 16300  \tValid loss: 0.46453630924224854\n",
      "Step: 16400  \tTraining loss: 0.4343000054359436\n",
      "Step: 16400  \tTraining accuracy: 0.7769771218299866\n",
      "Step: 16400  \tValid loss: 0.46455371379852295\n",
      "Step: 16500  \tTraining loss: 0.43422654271125793\n",
      "Step: 16500  \tTraining accuracy: 0.7770362496376038\n",
      "Step: 16500  \tValid loss: 0.4644535183906555\n",
      "Step: 16600  \tTraining loss: 0.4341544806957245\n",
      "Step: 16600  \tTraining accuracy: 0.7770946621894836\n",
      "Step: 16600  \tValid loss: 0.46443167328834534\n",
      "Step: 16700  \tTraining loss: 0.4340825080871582\n",
      "Step: 16700  \tTraining accuracy: 0.7771522998809814\n",
      "Step: 16700  \tValid loss: 0.46442845463752747\n",
      "Step: 16800  \tTraining loss: 0.4340120851993561\n",
      "Step: 16800  \tTraining accuracy: 0.77721107006073\n",
      "Step: 16800  \tValid loss: 0.4643467366695404\n",
      "Step: 16900  \tTraining loss: 0.4339403510093689\n",
      "Step: 16900  \tTraining accuracy: 0.7772685885429382\n",
      "Step: 16900  \tValid loss: 0.46434029936790466\n",
      "Step: 17000  \tTraining loss: 0.433870404958725\n",
      "Step: 17000  \tTraining accuracy: 0.7773241996765137\n",
      "Step: 17000  \tValid loss: 0.4642614722251892\n",
      "Step: 17100  \tTraining loss: 0.4337991774082184\n",
      "Step: 17100  \tTraining accuracy: 0.7773792147636414\n",
      "Step: 17100  \tValid loss: 0.46423959732055664\n",
      "Step: 17200  \tTraining loss: 0.4337291121482849\n",
      "Step: 17200  \tTraining accuracy: 0.7774335145950317\n",
      "Step: 17200  \tValid loss: 0.46418535709381104\n",
      "Step: 17300  \tTraining loss: 0.43365898728370667\n",
      "Step: 17300  \tTraining accuracy: 0.7774872779846191\n",
      "Step: 17300  \tValid loss: 0.46417030692100525\n",
      "Step: 17400  \tTraining loss: 0.43359047174453735\n",
      "Step: 17400  \tTraining accuracy: 0.7775403261184692\n",
      "Step: 17400  \tValid loss: 0.4640944004058838\n",
      "Step: 17500  \tTraining loss: 0.4335213899612427\n",
      "Step: 17500  \tTraining accuracy: 0.7775956392288208\n",
      "Step: 17500  \tValid loss: 0.46406233310699463\n",
      "Step: 17600  \tTraining loss: 0.43345266580581665\n",
      "Step: 17600  \tTraining accuracy: 0.7776503562927246\n",
      "Step: 17600  \tValid loss: 0.46404820680618286\n",
      "Step: 17700  \tTraining loss: 0.4333854615688324\n",
      "Step: 17700  \tTraining accuracy: 0.7777032852172852\n",
      "Step: 17700  \tValid loss: 0.4639875590801239\n",
      "Step: 17800  \tTraining loss: 0.4333178997039795\n",
      "Step: 17800  \tTraining accuracy: 0.7777539491653442\n",
      "Step: 17800  \tValid loss: 0.46396082639694214\n",
      "Step: 17900  \tTraining loss: 0.43325257301330566\n",
      "Step: 17900  \tTraining accuracy: 0.777806282043457\n",
      "Step: 17900  \tValid loss: 0.46385475993156433\n",
      "Step: 18000  \tTraining loss: 0.4331854283809662\n",
      "Step: 18000  \tTraining accuracy: 0.777858555316925\n",
      "Step: 18000  \tValid loss: 0.4638502597808838\n",
      "Step: 18100  \tTraining loss: 0.43312036991119385\n",
      "Step: 18100  \tTraining accuracy: 0.7779102921485901\n",
      "Step: 18100  \tValid loss: 0.46379968523979187\n",
      "Step: 18200  \tTraining loss: 0.4330553412437439\n",
      "Step: 18200  \tTraining accuracy: 0.7779614329338074\n",
      "Step: 18200  \tValid loss: 0.46377697587013245\n",
      "Step: 18300  \tTraining loss: 0.4329909682273865\n",
      "Step: 18300  \tTraining accuracy: 0.778010904788971\n",
      "Step: 18300  \tValid loss: 0.4637443423271179\n",
      "Step: 18400  \tTraining loss: 0.43292680382728577\n",
      "Step: 18400  \tTraining accuracy: 0.7780582904815674\n",
      "Step: 18400  \tValid loss: 0.46375253796577454\n",
      "Step: 18500  \tTraining loss: 0.43286454677581787\n",
      "Step: 18500  \tTraining accuracy: 0.7781051397323608\n",
      "Step: 18500  \tValid loss: 0.46367719769477844\n",
      "Step: 18600  \tTraining loss: 0.4328018128871918\n",
      "Step: 18600  \tTraining accuracy: 0.7781509160995483\n",
      "Step: 18600  \tValid loss: 0.4636891484260559\n",
      "Step: 18700  \tTraining loss: 0.4327409267425537\n",
      "Step: 18700  \tTraining accuracy: 0.7781946659088135\n",
      "Step: 18700  \tValid loss: 0.4636366367340088\n",
      "Step: 18800  \tTraining loss: 0.4326792061328888\n",
      "Step: 18800  \tTraining accuracy: 0.7782394886016846\n",
      "Step: 18800  \tValid loss: 0.46367737650871277\n",
      "Step: 18900  \tTraining loss: 0.4326198697090149\n",
      "Step: 18900  \tTraining accuracy: 0.7782817482948303\n",
      "Step: 18900  \tValid loss: 0.46358346939086914\n",
      "Step: 19000  \tTraining loss: 0.43256065249443054\n",
      "Step: 19000  \tTraining accuracy: 0.7783204317092896\n",
      "Step: 19000  \tValid loss: 0.46356362104415894\n",
      "Step: 19100  \tTraining loss: 0.43250229954719543\n",
      "Step: 19100  \tTraining accuracy: 0.7783592343330383\n",
      "Step: 19100  \tValid loss: 0.46349409222602844\n",
      "Step: 19200  \tTraining loss: 0.43244409561157227\n",
      "Step: 19200  \tTraining accuracy: 0.7783976793289185\n",
      "Step: 19200  \tValid loss: 0.46347716450691223\n",
      "Step: 19300  \tTraining loss: 0.4323863685131073\n",
      "Step: 19300  \tTraining accuracy: 0.7784356474876404\n",
      "Step: 19300  \tValid loss: 0.4634474515914917\n",
      "Step: 19400  \tTraining loss: 0.43232932686805725\n",
      "Step: 19400  \tTraining accuracy: 0.7784733176231384\n",
      "Step: 19400  \tValid loss: 0.4634569585323334\n",
      "Step: 19500  \tTraining loss: 0.43227460980415344\n",
      "Step: 19500  \tTraining accuracy: 0.7785100340843201\n",
      "Step: 19500  \tValid loss: 0.4633769392967224\n",
      "Step: 19600  \tTraining loss: 0.4322175085544586\n",
      "Step: 19600  \tTraining accuracy: 0.7785443663597107\n",
      "Step: 19600  \tValid loss: 0.46338561177253723\n",
      "Step: 19700  \tTraining loss: 0.4321622848510742\n",
      "Step: 19700  \tTraining accuracy: 0.7785767912864685\n",
      "Step: 19700  \tValid loss: 0.46338191628456116\n",
      "Step: 19800  \tTraining loss: 0.43210792541503906\n",
      "Step: 19800  \tTraining accuracy: 0.7786054611206055\n",
      "Step: 19800  \tValid loss: 0.4633701741695404\n",
      "Step: 19900  \tTraining loss: 0.4320547878742218\n",
      "Step: 19900  \tTraining accuracy: 0.7786343097686768\n",
      "Step: 19900  \tValid loss: 0.4632861018180847\n",
      "Step: 20000  \tTraining loss: 0.43200352787971497\n",
      "Step: 20000  \tTraining accuracy: 0.7786633968353271\n",
      "Step: 20000  \tValid loss: 0.4632241427898407\n",
      "Step: 20100  \tTraining loss: 0.4319501519203186\n",
      "Step: 20100  \tTraining accuracy: 0.7786931395530701\n",
      "Step: 20100  \tValid loss: 0.46321210265159607\n",
      "Step: 20200  \tTraining loss: 0.4318990111351013\n",
      "Step: 20200  \tTraining accuracy: 0.7787231206893921\n",
      "Step: 20200  \tValid loss: 0.4631633758544922\n",
      "Step: 20300  \tTraining loss: 0.43184694647789\n",
      "Step: 20300  \tTraining accuracy: 0.7787522673606873\n",
      "Step: 20300  \tValid loss: 0.4631890058517456\n",
      "Step: 20400  \tTraining loss: 0.43179595470428467\n",
      "Step: 20400  \tTraining accuracy: 0.7787792682647705\n",
      "Step: 20400  \tValid loss: 0.46315446496009827\n",
      "Step: 20500  \tTraining loss: 0.4317466616630554\n",
      "Step: 20500  \tTraining accuracy: 0.7788063883781433\n",
      "Step: 20500  \tValid loss: 0.4632153809070587\n",
      "Step: 20600  \tTraining loss: 0.4316965341567993\n",
      "Step: 20600  \tTraining accuracy: 0.7788318991661072\n",
      "Step: 20600  \tValid loss: 0.4630942940711975\n",
      "Step: 20700  \tTraining loss: 0.43164917826652527\n",
      "Step: 20700  \tTraining accuracy: 0.7788537740707397\n",
      "Step: 20700  \tValid loss: 0.463043212890625\n",
      "Step: 20800  \tTraining loss: 0.43159404397010803\n",
      "Step: 20800  \tTraining accuracy: 0.778877317905426\n",
      "Step: 20800  \tValid loss: 0.4629628658294678\n",
      "Step: 20900  \tTraining loss: 0.43153291940689087\n",
      "Step: 20900  \tTraining accuracy: 0.7789016366004944\n",
      "Step: 20900  \tValid loss: 0.46305209398269653\n",
      "Step: 21000  \tTraining loss: 0.43148043751716614\n",
      "Step: 21000  \tTraining accuracy: 0.7789247035980225\n",
      "Step: 21000  \tValid loss: 0.46304479241371155\n",
      "Step: 21100  \tTraining loss: 0.43142953515052795\n",
      "Step: 21100  \tTraining accuracy: 0.7789475917816162\n",
      "Step: 21100  \tValid loss: 0.46304604411125183\n",
      "Step: 21200  \tTraining loss: 0.4313793182373047\n",
      "Step: 21200  \tTraining accuracy: 0.7789703011512756\n",
      "Step: 21200  \tValid loss: 0.46304142475128174\n",
      "Step: 21300  \tTraining loss: 0.4313298463821411\n",
      "Step: 21300  \tTraining accuracy: 0.7789922952651978\n",
      "Step: 21300  \tValid loss: 0.4629698395729065\n",
      "Step: 21400  \tTraining loss: 0.4312807321548462\n",
      "Step: 21400  \tTraining accuracy: 0.7790113091468811\n",
      "Step: 21400  \tValid loss: 0.4629131257534027\n",
      "Step: 21500  \tTraining loss: 0.43123170733451843\n",
      "Step: 21500  \tTraining accuracy: 0.7790287733078003\n",
      "Step: 21500  \tValid loss: 0.4629342257976532\n",
      "Step: 21600  \tTraining loss: 0.4311850965023041\n",
      "Step: 21600  \tTraining accuracy: 0.7790461182594299\n",
      "Step: 21600  \tValid loss: 0.46291622519493103\n",
      "Step: 21700  \tTraining loss: 0.4311387836933136\n",
      "Step: 21700  \tTraining accuracy: 0.7790632247924805\n",
      "Step: 21700  \tValid loss: 0.46289822459220886\n",
      "Step: 21800  \tTraining loss: 0.4310929775238037\n",
      "Step: 21800  \tTraining accuracy: 0.7790802121162415\n",
      "Step: 21800  \tValid loss: 0.46284058690071106\n",
      "Step: 21900  \tTraining loss: 0.43104737997055054\n",
      "Step: 21900  \tTraining accuracy: 0.7790970802307129\n",
      "Step: 21900  \tValid loss: 0.46281373500823975\n",
      "Step: 22000  \tTraining loss: 0.4310016632080078\n",
      "Step: 22000  \tTraining accuracy: 0.77911376953125\n",
      "Step: 22000  \tValid loss: 0.46280792355537415\n",
      "Step: 22100  \tTraining loss: 0.43095606565475464\n",
      "Step: 22100  \tTraining accuracy: 0.7791302800178528\n",
      "Step: 22100  \tValid loss: 0.46280351281166077\n",
      "Step: 22200  \tTraining loss: 0.43091151118278503\n",
      "Step: 22200  \tTraining accuracy: 0.779146671295166\n",
      "Step: 22200  \tValid loss: 0.46278703212738037\n",
      "Step: 22300  \tTraining loss: 0.43086686730384827\n",
      "Step: 22300  \tTraining accuracy: 0.7791628837585449\n",
      "Step: 22300  \tValid loss: 0.4627665579319\n",
      "Step: 22400  \tTraining loss: 0.4308225214481354\n",
      "Step: 22400  \tTraining accuracy: 0.7791789770126343\n",
      "Step: 22400  \tValid loss: 0.46273672580718994\n",
      "Step: 22500  \tTraining loss: 0.4307786524295807\n",
      "Step: 22500  \tTraining accuracy: 0.7791949510574341\n",
      "Step: 22500  \tValid loss: 0.462708443403244\n",
      "Step: 22600  \tTraining loss: 0.4307348132133484\n",
      "Step: 22600  \tTraining accuracy: 0.7792107462882996\n",
      "Step: 22600  \tValid loss: 0.46267881989479065\n",
      "Step: 22700  \tTraining loss: 0.4306904375553131\n",
      "Step: 22700  \tTraining accuracy: 0.7792276740074158\n",
      "Step: 22700  \tValid loss: 0.4626599848270416\n",
      "Step: 22800  \tTraining loss: 0.4306471347808838\n",
      "Step: 22800  \tTraining accuracy: 0.7792453765869141\n",
      "Step: 22800  \tValid loss: 0.4626327157020569\n",
      "Step: 22900  \tTraining loss: 0.43060368299484253\n",
      "Step: 22900  \tTraining accuracy: 0.7792620658874512\n",
      "Step: 22900  \tValid loss: 0.462580144405365\n",
      "Step: 23000  \tTraining loss: 0.4305603504180908\n",
      "Step: 23000  \tTraining accuracy: 0.7792772650718689\n",
      "Step: 23000  \tValid loss: 0.4625461995601654\n",
      "Step: 23100  \tTraining loss: 0.43051549792289734\n",
      "Step: 23100  \tTraining accuracy: 0.7792906761169434\n",
      "Step: 23100  \tValid loss: 0.4625939130783081\n",
      "Step: 23200  \tTraining loss: 0.4304724931716919\n",
      "Step: 23200  \tTraining accuracy: 0.7793035507202148\n",
      "Step: 23200  \tValid loss: 0.46254220604896545\n",
      "Step: 23300  \tTraining loss: 0.430428147315979\n",
      "Step: 23300  \tTraining accuracy: 0.7793163061141968\n",
      "Step: 23300  \tValid loss: 0.4625721573829651\n",
      "Step: 23400  \tTraining loss: 0.4303850829601288\n",
      "Step: 23400  \tTraining accuracy: 0.7793288826942444\n",
      "Step: 23400  \tValid loss: 0.46248701214790344\n",
      "Step: 23500  \tTraining loss: 0.43034127354621887\n",
      "Step: 23500  \tTraining accuracy: 0.779341459274292\n",
      "Step: 23500  \tValid loss: 0.4625071585178375\n",
      "Step: 23600  \tTraining loss: 0.4302977919578552\n",
      "Step: 23600  \tTraining accuracy: 0.7793538570404053\n",
      "Step: 23600  \tValid loss: 0.46247297525405884\n",
      "Step: 23700  \tTraining loss: 0.43025410175323486\n",
      "Step: 23700  \tTraining accuracy: 0.7793678045272827\n",
      "Step: 23700  \tValid loss: 0.46244150400161743\n",
      "Step: 23800  \tTraining loss: 0.4302110970020294\n",
      "Step: 23800  \tTraining accuracy: 0.779381275177002\n",
      "Step: 23800  \tValid loss: 0.46239668130874634\n",
      "Step: 23900  \tTraining loss: 0.430167019367218\n",
      "Step: 23900  \tTraining accuracy: 0.7793933749198914\n",
      "Step: 23900  \tValid loss: 0.4624122977256775\n",
      "Step: 24000  \tTraining loss: 0.43012353777885437\n",
      "Step: 24000  \tTraining accuracy: 0.7794053554534912\n",
      "Step: 24000  \tValid loss: 0.4623452425003052\n",
      "Step: 24100  \tTraining loss: 0.43008023500442505\n",
      "Step: 24100  \tTraining accuracy: 0.7794172763824463\n",
      "Step: 24100  \tValid loss: 0.4623200297355652\n",
      "Step: 24200  \tTraining loss: 0.43003618717193604\n",
      "Step: 24200  \tTraining accuracy: 0.7794290781021118\n",
      "Step: 24200  \tValid loss: 0.4622859060764313\n",
      "Step: 24300  \tTraining loss: 0.42999130487442017\n",
      "Step: 24300  \tTraining accuracy: 0.7794407606124878\n",
      "Step: 24300  \tValid loss: 0.46229055523872375\n",
      "Step: 24400  \tTraining loss: 0.4299466013908386\n",
      "Step: 24400  \tTraining accuracy: 0.779452383518219\n",
      "Step: 24400  \tValid loss: 0.46224766969680786\n",
      "Step: 24500  \tTraining loss: 0.4299010932445526\n",
      "Step: 24500  \tTraining accuracy: 0.7794638872146606\n",
      "Step: 24500  \tValid loss: 0.462247371673584\n",
      "Step: 24600  \tTraining loss: 0.42985644936561584\n",
      "Step: 24600  \tTraining accuracy: 0.7794752717018127\n",
      "Step: 24600  \tValid loss: 0.46222659945487976\n",
      "Step: 24700  \tTraining loss: 0.42981061339378357\n",
      "Step: 24700  \tTraining accuracy: 0.779488205909729\n",
      "Step: 24700  \tValid loss: 0.4621841013431549\n",
      "Step: 24800  \tTraining loss: 0.42976418137550354\n",
      "Step: 24800  \tTraining accuracy: 0.7795026302337646\n",
      "Step: 24800  \tValid loss: 0.4622150659561157\n",
      "Step: 24900  \tTraining loss: 0.4297172427177429\n",
      "Step: 24900  \tTraining accuracy: 0.7795157432556152\n",
      "Step: 24900  \tValid loss: 0.4622315466403961\n",
      "Step: 25000  \tTraining loss: 0.4296704828739166\n",
      "Step: 25000  \tTraining accuracy: 0.7795267105102539\n",
      "Step: 25000  \tValid loss: 0.4621678292751312\n",
      "Step: 25100  \tTraining loss: 0.4296228587627411\n",
      "Step: 25100  \tTraining accuracy: 0.7795376777648926\n",
      "Step: 25100  \tValid loss: 0.46212467551231384\n",
      "Step: 25200  \tTraining loss: 0.4295746386051178\n",
      "Step: 25200  \tTraining accuracy: 0.7795493006706238\n",
      "Step: 25200  \tValid loss: 0.46212175488471985\n",
      "Step: 25300  \tTraining loss: 0.429525762796402\n",
      "Step: 25300  \tTraining accuracy: 0.7795619964599609\n",
      "Step: 25300  \tValid loss: 0.4621106684207916\n",
      "Step: 25400  \tTraining loss: 0.4294763505458832\n",
      "Step: 25400  \tTraining accuracy: 0.7795746326446533\n",
      "Step: 25400  \tValid loss: 0.462128221988678\n",
      "Step: 25500  \tTraining loss: 0.4294264316558838\n",
      "Step: 25500  \tTraining accuracy: 0.7795871496200562\n",
      "Step: 25500  \tValid loss: 0.46210378408432007\n",
      "Step: 25600  \tTraining loss: 0.42937591671943665\n",
      "Step: 25600  \tTraining accuracy: 0.7795987725257874\n",
      "Step: 25600  \tValid loss: 0.4620938003063202\n",
      "Step: 25700  \tTraining loss: 0.42932525277137756\n",
      "Step: 25700  \tTraining accuracy: 0.779609203338623\n",
      "Step: 25700  \tValid loss: 0.4620698392391205\n",
      "Step: 25800  \tTraining loss: 0.429273784160614\n",
      "Step: 25800  \tTraining accuracy: 0.7796195149421692\n",
      "Step: 25800  \tValid loss: 0.46208706498146057\n",
      "Step: 25900  \tTraining loss: 0.4292222261428833\n",
      "Step: 25900  \tTraining accuracy: 0.7796297073364258\n",
      "Step: 25900  \tValid loss: 0.46204817295074463\n",
      "Step: 26000  \tTraining loss: 0.4291696846485138\n",
      "Step: 26000  \tTraining accuracy: 0.7796398997306824\n",
      "Step: 26000  \tValid loss: 0.46201372146606445\n",
      "Step: 26100  \tTraining loss: 0.4291165769100189\n",
      "Step: 26100  \tTraining accuracy: 0.7796499729156494\n",
      "Step: 26100  \tValid loss: 0.4620233178138733\n",
      "Step: 26200  \tTraining loss: 0.4290638566017151\n",
      "Step: 26200  \tTraining accuracy: 0.7796599864959717\n",
      "Step: 26200  \tValid loss: 0.4619896113872528\n",
      "Step: 26300  \tTraining loss: 0.4290095865726471\n",
      "Step: 26300  \tTraining accuracy: 0.7796698808670044\n",
      "Step: 26300  \tValid loss: 0.4620060920715332\n",
      "Step: 26400  \tTraining loss: 0.4289548993110657\n",
      "Step: 26400  \tTraining accuracy: 0.7796797752380371\n",
      "Step: 26400  \tValid loss: 0.4620039463043213\n",
      "Step: 26500  \tTraining loss: 0.42890027165412903\n",
      "Step: 26500  \tTraining accuracy: 0.7796895503997803\n",
      "Step: 26500  \tValid loss: 0.4619981050491333\n",
      "Step: 26600  \tTraining loss: 0.4288439154624939\n",
      "Step: 26600  \tTraining accuracy: 0.7796992659568787\n",
      "Step: 26600  \tValid loss: 0.46202173829078674\n",
      "Step: 26700  \tTraining loss: 0.4287881851196289\n",
      "Step: 26700  \tTraining accuracy: 0.7797088623046875\n",
      "Step: 26700  \tValid loss: 0.46202388405799866\n",
      "Step: 26800  \tTraining loss: 0.4287315309047699\n",
      "Step: 26800  \tTraining accuracy: 0.7797176837921143\n",
      "Step: 26800  \tValid loss: 0.46206673979759216\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7797254\n",
      "Precision: 0.79109764\n",
      "Recall: 0.69356984\n",
      "F1 score: 0.7285052\n",
      "AUC: 0.77355796\n",
      "   accuracy  precision   recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.779725   0.791098  0.69357  0.728505  0.773558  0.428685       0.77972   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0   0.46194       0.779746    0.40299      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  26882.0  \n",
      "23\n",
      "(4205, 8)\n",
      "(4205, 1)\n",
      "(2320, 8)\n",
      "(2320, 1)\n",
      "(1885, 8)\n",
      "(1885, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.584475040435791\n",
      "Step: 100  \tTraining accuracy: 0.7074910998344421\n",
      "Step: 100  \tValid loss: 0.585616409778595\n",
      "Step: 200  \tTraining loss: 0.5623981952667236\n",
      "Step: 200  \tTraining accuracy: 0.7206500172615051\n",
      "Step: 200  \tValid loss: 0.5677506923675537\n",
      "Step: 300  \tTraining loss: 0.5475091934204102\n",
      "Step: 300  \tTraining accuracy: 0.7280856370925903\n",
      "Step: 300  \tValid loss: 0.5476097464561462\n",
      "Step: 400  \tTraining loss: 0.531868577003479\n",
      "Step: 400  \tTraining accuracy: 0.7351791858673096\n",
      "Step: 400  \tValid loss: 0.5262094140052795\n",
      "Step: 500  \tTraining loss: 0.5181801915168762\n",
      "Step: 500  \tTraining accuracy: 0.7418681383132935\n",
      "Step: 500  \tValid loss: 0.5086103677749634\n",
      "Step: 600  \tTraining loss: 0.5054556131362915\n",
      "Step: 600  \tTraining accuracy: 0.747292160987854\n",
      "Step: 600  \tValid loss: 0.4932432472705841\n",
      "Step: 700  \tTraining loss: 0.4933233857154846\n",
      "Step: 700  \tTraining accuracy: 0.7514131665229797\n",
      "Step: 700  \tValid loss: 0.47797662019729614\n",
      "Step: 800  \tTraining loss: 0.48322373628616333\n",
      "Step: 800  \tTraining accuracy: 0.7551645040512085\n",
      "Step: 800  \tValid loss: 0.4644409120082855\n",
      "Step: 900  \tTraining loss: 0.47583499550819397\n",
      "Step: 900  \tTraining accuracy: 0.7589144706726074\n",
      "Step: 900  \tValid loss: 0.4540203809738159\n",
      "Step: 1000  \tTraining loss: 0.4705711007118225\n",
      "Step: 1000  \tTraining accuracy: 0.7630640268325806\n",
      "Step: 1000  \tValid loss: 0.4465804398059845\n",
      "Step: 1100  \tTraining loss: 0.46672141551971436\n",
      "Step: 1100  \tTraining accuracy: 0.7665590643882751\n",
      "Step: 1100  \tValid loss: 0.4414300322532654\n",
      "Step: 1200  \tTraining loss: 0.46365949511528015\n",
      "Step: 1200  \tTraining accuracy: 0.769694447517395\n",
      "Step: 1200  \tValid loss: 0.43783020973205566\n",
      "Step: 1300  \tTraining loss: 0.4610775113105774\n",
      "Step: 1300  \tTraining accuracy: 0.7723566889762878\n",
      "Step: 1300  \tValid loss: 0.43553540110588074\n",
      "Step: 1400  \tTraining loss: 0.45874327421188354\n",
      "Step: 1400  \tTraining accuracy: 0.7747654914855957\n",
      "Step: 1400  \tValid loss: 0.43376868963241577\n",
      "Step: 1500  \tTraining loss: 0.45674076676368713\n",
      "Step: 1500  \tTraining accuracy: 0.7769978046417236\n",
      "Step: 1500  \tValid loss: 0.4327121078968048\n",
      "Step: 1600  \tTraining loss: 0.45513811707496643\n",
      "Step: 1600  \tTraining accuracy: 0.7789651155471802\n",
      "Step: 1600  \tValid loss: 0.43196767568588257\n",
      "Step: 1700  \tTraining loss: 0.4536466598510742\n",
      "Step: 1700  \tTraining accuracy: 0.7805498242378235\n",
      "Step: 1700  \tValid loss: 0.43164414167404175\n",
      "Step: 1800  \tTraining loss: 0.4523616433143616\n",
      "Step: 1800  \tTraining accuracy: 0.7820214033126831\n",
      "Step: 1800  \tValid loss: 0.431934118270874\n",
      "Step: 1900  \tTraining loss: 0.45121845602989197\n",
      "Step: 1900  \tTraining accuracy: 0.7834110260009766\n",
      "Step: 1900  \tValid loss: 0.43210798501968384\n",
      "Step: 2000  \tTraining loss: 0.450157105922699\n",
      "Step: 2000  \tTraining accuracy: 0.7845727205276489\n",
      "Step: 2000  \tValid loss: 0.43187347054481506\n",
      "Step: 2100  \tTraining loss: 0.4492306113243103\n",
      "Step: 2100  \tTraining accuracy: 0.7856094837188721\n",
      "Step: 2100  \tValid loss: 0.431483656167984\n",
      "Step: 2200  \tTraining loss: 0.44840067625045776\n",
      "Step: 2200  \tTraining accuracy: 0.7866106033325195\n",
      "Step: 2200  \tValid loss: 0.43126246333122253\n",
      "Step: 2300  \tTraining loss: 0.44765016436576843\n",
      "Step: 2300  \tTraining accuracy: 0.7875809073448181\n",
      "Step: 2300  \tValid loss: 0.43107905983924866\n",
      "Step: 2400  \tTraining loss: 0.4469532072544098\n",
      "Step: 2400  \tTraining accuracy: 0.7884787321090698\n",
      "Step: 2400  \tValid loss: 0.43079912662506104\n",
      "Step: 2500  \tTraining loss: 0.4462336599826813\n",
      "Step: 2500  \tTraining accuracy: 0.7893469929695129\n",
      "Step: 2500  \tValid loss: 0.4305257797241211\n",
      "Step: 2600  \tTraining loss: 0.44561997056007385\n",
      "Step: 2600  \tTraining accuracy: 0.7901331186294556\n",
      "Step: 2600  \tValid loss: 0.43036431074142456\n",
      "Step: 2700  \tTraining loss: 0.44509580731391907\n",
      "Step: 2700  \tTraining accuracy: 0.7908509373664856\n",
      "Step: 2700  \tValid loss: 0.43018850684165955\n",
      "Step: 2800  \tTraining loss: 0.4445783793926239\n",
      "Step: 2800  \tTraining accuracy: 0.7915166020393372\n",
      "Step: 2800  \tValid loss: 0.42988473176956177\n",
      "Step: 2900  \tTraining loss: 0.44406986236572266\n",
      "Step: 2900  \tTraining accuracy: 0.7920896410942078\n",
      "Step: 2900  \tValid loss: 0.429519921541214\n",
      "Step: 3000  \tTraining loss: 0.4435344934463501\n",
      "Step: 3000  \tTraining accuracy: 0.7926358580589294\n",
      "Step: 3000  \tValid loss: 0.4292736351490021\n",
      "Step: 3100  \tTraining loss: 0.4429713487625122\n",
      "Step: 3100  \tTraining accuracy: 0.7931229472160339\n",
      "Step: 3100  \tValid loss: 0.4288700520992279\n",
      "Step: 3200  \tTraining loss: 0.44245415925979614\n",
      "Step: 3200  \tTraining accuracy: 0.7936055064201355\n",
      "Step: 3200  \tValid loss: 0.4285406768321991\n",
      "Step: 3300  \tTraining loss: 0.4419991970062256\n",
      "Step: 3300  \tTraining accuracy: 0.7940363883972168\n",
      "Step: 3300  \tValid loss: 0.4283338785171509\n",
      "Step: 3400  \tTraining loss: 0.44157421588897705\n",
      "Step: 3400  \tTraining accuracy: 0.7944451570510864\n",
      "Step: 3400  \tValid loss: 0.4280255138874054\n",
      "Step: 3500  \tTraining loss: 0.44115620851516724\n",
      "Step: 3500  \tTraining accuracy: 0.7948439717292786\n",
      "Step: 3500  \tValid loss: 0.42767760157585144\n",
      "Step: 3600  \tTraining loss: 0.44077083468437195\n",
      "Step: 3600  \tTraining accuracy: 0.7951801419258118\n",
      "Step: 3600  \tValid loss: 0.4274168014526367\n",
      "Step: 3700  \tTraining loss: 0.440409392118454\n",
      "Step: 3700  \tTraining accuracy: 0.7955206632614136\n",
      "Step: 3700  \tValid loss: 0.4272300899028778\n",
      "Step: 3800  \tTraining loss: 0.43983370065689087\n",
      "Step: 3800  \tTraining accuracy: 0.7957732677459717\n",
      "Step: 3800  \tValid loss: 0.4265480935573578\n",
      "Step: 3900  \tTraining loss: 0.4390982687473297\n",
      "Step: 3900  \tTraining accuracy: 0.796003520488739\n",
      "Step: 3900  \tValid loss: 0.4265247583389282\n",
      "Step: 4000  \tTraining loss: 0.43861445784568787\n",
      "Step: 4000  \tTraining accuracy: 0.79618901014328\n",
      "Step: 4000  \tValid loss: 0.4265839457511902\n",
      "Step: 4100  \tTraining loss: 0.4382043480873108\n",
      "Step: 4100  \tTraining accuracy: 0.7963594198226929\n",
      "Step: 4100  \tValid loss: 0.4263918995857239\n",
      "Step: 4200  \tTraining loss: 0.43782123923301697\n",
      "Step: 4200  \tTraining accuracy: 0.7965359687805176\n",
      "Step: 4200  \tValid loss: 0.4261623024940491\n",
      "Step: 4300  \tTraining loss: 0.4374658763408661\n",
      "Step: 4300  \tTraining accuracy: 0.7967237830162048\n",
      "Step: 4300  \tValid loss: 0.4260355532169342\n",
      "Step: 4400  \tTraining loss: 0.43712350726127625\n",
      "Step: 4400  \tTraining accuracy: 0.7968920469284058\n",
      "Step: 4400  \tValid loss: 0.4258962571620941\n",
      "Step: 4500  \tTraining loss: 0.4367973804473877\n",
      "Step: 4500  \tTraining accuracy: 0.7970446944236755\n",
      "Step: 4500  \tValid loss: 0.4257093369960785\n",
      "Step: 4600  \tTraining loss: 0.4364844858646393\n",
      "Step: 4600  \tTraining accuracy: 0.7971985340118408\n",
      "Step: 4600  \tValid loss: 0.4255872070789337\n",
      "Step: 4700  \tTraining loss: 0.4361538589000702\n",
      "Step: 4700  \tTraining accuracy: 0.7973405718803406\n",
      "Step: 4700  \tValid loss: 0.4253695011138916\n",
      "Step: 4800  \tTraining loss: 0.43585070967674255\n",
      "Step: 4800  \tTraining accuracy: 0.7974842190742493\n",
      "Step: 4800  \tValid loss: 0.42520037293434143\n",
      "Step: 4900  \tTraining loss: 0.43555742502212524\n",
      "Step: 4900  \tTraining accuracy: 0.7976096272468567\n",
      "Step: 4900  \tValid loss: 0.425022691488266\n",
      "Step: 5000  \tTraining loss: 0.4352690577507019\n",
      "Step: 5000  \tTraining accuracy: 0.797737181186676\n",
      "Step: 5000  \tValid loss: 0.4248059093952179\n",
      "Step: 5100  \tTraining loss: 0.4349847733974457\n",
      "Step: 5100  \tTraining accuracy: 0.7978573441505432\n",
      "Step: 5100  \tValid loss: 0.4245942234992981\n",
      "Step: 5200  \tTraining loss: 0.4347061812877655\n",
      "Step: 5200  \tTraining accuracy: 0.7979843616485596\n",
      "Step: 5200  \tValid loss: 0.42448145151138306\n",
      "Step: 5300  \tTraining loss: 0.4344314932823181\n",
      "Step: 5300  \tTraining accuracy: 0.7981065511703491\n",
      "Step: 5300  \tValid loss: 0.4243130385875702\n",
      "Step: 5400  \tTraining loss: 0.4341617524623871\n",
      "Step: 5400  \tTraining accuracy: 0.7982175350189209\n",
      "Step: 5400  \tValid loss: 0.42411041259765625\n",
      "Step: 5500  \tTraining loss: 0.4338953197002411\n",
      "Step: 5500  \tTraining accuracy: 0.7983287572860718\n",
      "Step: 5500  \tValid loss: 0.4239114820957184\n",
      "Step: 5600  \tTraining loss: 0.4336344599723816\n",
      "Step: 5600  \tTraining accuracy: 0.798453152179718\n",
      "Step: 5600  \tValid loss: 0.42365172505378723\n",
      "Step: 5700  \tTraining loss: 0.4333781599998474\n",
      "Step: 5700  \tTraining accuracy: 0.798575222492218\n",
      "Step: 5700  \tValid loss: 0.42343416810035706\n",
      "Step: 5800  \tTraining loss: 0.4331224858760834\n",
      "Step: 5800  \tTraining accuracy: 0.7986992597579956\n",
      "Step: 5800  \tValid loss: 0.42323175072669983\n",
      "Step: 5900  \tTraining loss: 0.43286022543907166\n",
      "Step: 5900  \tTraining accuracy: 0.7988231182098389\n",
      "Step: 5900  \tValid loss: 0.4230577051639557\n",
      "Step: 6000  \tTraining loss: 0.43259677290916443\n",
      "Step: 6000  \tTraining accuracy: 0.7989428639411926\n",
      "Step: 6000  \tValid loss: 0.4228464961051941\n",
      "Step: 6100  \tTraining loss: 0.4323267340660095\n",
      "Step: 6100  \tTraining accuracy: 0.7990624904632568\n",
      "Step: 6100  \tValid loss: 0.4228988587856293\n",
      "Step: 6200  \tTraining loss: 0.4319665729999542\n",
      "Step: 6200  \tTraining accuracy: 0.799160897731781\n",
      "Step: 6200  \tValid loss: 0.4233388304710388\n",
      "Step: 6300  \tTraining loss: 0.43168044090270996\n",
      "Step: 6300  \tTraining accuracy: 0.7992447018623352\n",
      "Step: 6300  \tValid loss: 0.42345792055130005\n",
      "Step: 6400  \tTraining loss: 0.4314122796058655\n",
      "Step: 6400  \tTraining accuracy: 0.799333393573761\n",
      "Step: 6400  \tValid loss: 0.4235537648200989\n",
      "Step: 6500  \tTraining loss: 0.4311507046222687\n",
      "Step: 6500  \tTraining accuracy: 0.7994118928909302\n",
      "Step: 6500  \tValid loss: 0.42368194460868835\n",
      "Step: 6600  \tTraining loss: 0.4309042990207672\n",
      "Step: 6600  \tTraining accuracy: 0.7995007634162903\n",
      "Step: 6600  \tValid loss: 0.42365798354148865\n",
      "Step: 6700  \tTraining loss: 0.4306693971157074\n",
      "Step: 6700  \tTraining accuracy: 0.7995851635932922\n",
      "Step: 6700  \tValid loss: 0.42361560463905334\n",
      "Step: 6800  \tTraining loss: 0.43044087290763855\n",
      "Step: 6800  \tTraining accuracy: 0.7996740937232971\n",
      "Step: 6800  \tValid loss: 0.42356494069099426\n",
      "Step: 6900  \tTraining loss: 0.4302176237106323\n",
      "Step: 6900  \tTraining accuracy: 0.7997708916664124\n",
      "Step: 6900  \tValid loss: 0.42354047298431396\n",
      "Step: 7000  \tTraining loss: 0.4299994707107544\n",
      "Step: 7000  \tTraining accuracy: 0.7998579740524292\n",
      "Step: 7000  \tValid loss: 0.4235152006149292\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7999393\n",
      "Precision: 0.8163993\n",
      "Recall: 0.9236975\n",
      "F1 score: 0.8492653\n",
      "AUC: 0.7106292\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.799939   0.816399  0.923697  0.849265  0.710629  0.429837      0.799946   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.422728       0.799879   0.424674      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  7075.0  \n",
      "24\n",
      "(5945, 8)\n",
      "(5945, 1)\n",
      "(3200, 8)\n",
      "(3200, 1)\n",
      "(2600, 8)\n",
      "(2600, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6273791790008545\n",
      "Step: 100  \tTraining accuracy: 0.6487804651260376\n",
      "Step: 100  \tValid loss: 0.654646098613739\n",
      "Step: 200  \tTraining loss: 0.5733171105384827\n",
      "Step: 200  \tTraining accuracy: 0.665912926197052\n",
      "Step: 200  \tValid loss: 0.6032599806785583\n",
      "Step: 300  \tTraining loss: 0.5566603541374207\n",
      "Step: 300  \tTraining accuracy: 0.68581622838974\n",
      "Step: 300  \tValid loss: 0.5778540968894958\n",
      "Step: 400  \tTraining loss: 0.5469697713851929\n",
      "Step: 400  \tTraining accuracy: 0.6969402432441711\n",
      "Step: 400  \tValid loss: 0.5630251169204712\n",
      "Step: 500  \tTraining loss: 0.5418309569358826\n",
      "Step: 500  \tTraining accuracy: 0.7047520279884338\n",
      "Step: 500  \tValid loss: 0.5554345846176147\n",
      "Step: 600  \tTraining loss: 0.5393330454826355\n",
      "Step: 600  \tTraining accuracy: 0.7103139162063599\n",
      "Step: 600  \tValid loss: 0.5519466400146484\n",
      "Step: 700  \tTraining loss: 0.5377421975135803\n",
      "Step: 700  \tTraining accuracy: 0.7142184376716614\n",
      "Step: 700  \tValid loss: 0.5502082109451294\n",
      "Step: 800  \tTraining loss: 0.5360873341560364\n",
      "Step: 800  \tTraining accuracy: 0.717025876045227\n",
      "Step: 800  \tValid loss: 0.5492882132530212\n",
      "Step: 900  \tTraining loss: 0.5349046587944031\n",
      "Step: 900  \tTraining accuracy: 0.71930330991745\n",
      "Step: 900  \tValid loss: 0.5474547147750854\n",
      "Step: 1000  \tTraining loss: 0.5337609052658081\n",
      "Step: 1000  \tTraining accuracy: 0.7209941744804382\n",
      "Step: 1000  \tValid loss: 0.5464908480644226\n",
      "Step: 1100  \tTraining loss: 0.5325058698654175\n",
      "Step: 1100  \tTraining accuracy: 0.7227602601051331\n",
      "Step: 1100  \tValid loss: 0.5455230474472046\n",
      "Step: 1200  \tTraining loss: 0.5310702323913574\n",
      "Step: 1200  \tTraining accuracy: 0.7242193222045898\n",
      "Step: 1200  \tValid loss: 0.5449342131614685\n",
      "Step: 1300  \tTraining loss: 0.5287495255470276\n",
      "Step: 1300  \tTraining accuracy: 0.725547194480896\n",
      "Step: 1300  \tValid loss: 0.5422298908233643\n",
      "Step: 1400  \tTraining loss: 0.5260106325149536\n",
      "Step: 1400  \tTraining accuracy: 0.7267225384712219\n",
      "Step: 1400  \tValid loss: 0.5390008687973022\n",
      "Step: 1500  \tTraining loss: 0.5229148268699646\n",
      "Step: 1500  \tTraining accuracy: 0.7277828454971313\n",
      "Step: 1500  \tValid loss: 0.5354779958724976\n",
      "Step: 1600  \tTraining loss: 0.5205881595611572\n",
      "Step: 1600  \tTraining accuracy: 0.7286569476127625\n",
      "Step: 1600  \tValid loss: 0.5327404737472534\n",
      "Step: 1700  \tTraining loss: 0.5192593336105347\n",
      "Step: 1700  \tTraining accuracy: 0.7295901775360107\n",
      "Step: 1700  \tValid loss: 0.5315983295440674\n",
      "Step: 1800  \tTraining loss: 0.5180706977844238\n",
      "Step: 1800  \tTraining accuracy: 0.7304313778877258\n",
      "Step: 1800  \tValid loss: 0.530772864818573\n",
      "Step: 1900  \tTraining loss: 0.5170454382896423\n",
      "Step: 1900  \tTraining accuracy: 0.7311494946479797\n",
      "Step: 1900  \tValid loss: 0.5299774408340454\n",
      "Step: 2000  \tTraining loss: 0.5161803960800171\n",
      "Step: 2000  \tTraining accuracy: 0.73187255859375\n",
      "Step: 2000  \tValid loss: 0.5292014479637146\n",
      "Step: 2100  \tTraining loss: 0.5152825713157654\n",
      "Step: 2100  \tTraining accuracy: 0.7325043082237244\n",
      "Step: 2100  \tValid loss: 0.5284397006034851\n",
      "Step: 2200  \tTraining loss: 0.5145373940467834\n",
      "Step: 2200  \tTraining accuracy: 0.7330971360206604\n",
      "Step: 2200  \tValid loss: 0.5275901556015015\n",
      "Step: 2300  \tTraining loss: 0.5138183832168579\n",
      "Step: 2300  \tTraining accuracy: 0.7337772250175476\n",
      "Step: 2300  \tValid loss: 0.5266319513320923\n",
      "Step: 2400  \tTraining loss: 0.5131590962409973\n",
      "Step: 2400  \tTraining accuracy: 0.7344248294830322\n",
      "Step: 2400  \tValid loss: 0.5256917476654053\n",
      "Step: 2500  \tTraining loss: 0.5125184655189514\n",
      "Step: 2500  \tTraining accuracy: 0.7350994348526001\n",
      "Step: 2500  \tValid loss: 0.524741530418396\n",
      "Step: 2600  \tTraining loss: 0.5118548274040222\n",
      "Step: 2600  \tTraining accuracy: 0.7358013391494751\n",
      "Step: 2600  \tValid loss: 0.5237886309623718\n",
      "Step: 2700  \tTraining loss: 0.5111402869224548\n",
      "Step: 2700  \tTraining accuracy: 0.7365337610244751\n",
      "Step: 2700  \tValid loss: 0.5228235721588135\n",
      "Step: 2800  \tTraining loss: 0.5103874802589417\n",
      "Step: 2800  \tTraining accuracy: 0.7371633648872375\n",
      "Step: 2800  \tValid loss: 0.5219062566757202\n",
      "Step: 2900  \tTraining loss: 0.5095766186714172\n",
      "Step: 2900  \tTraining accuracy: 0.7377667427062988\n",
      "Step: 2900  \tValid loss: 0.520904541015625\n",
      "Step: 3000  \tTraining loss: 0.5087976455688477\n",
      "Step: 3000  \tTraining accuracy: 0.7383407950401306\n",
      "Step: 3000  \tValid loss: 0.5198505520820618\n",
      "Step: 3100  \tTraining loss: 0.507938027381897\n",
      "Step: 3100  \tTraining accuracy: 0.7388743758201599\n",
      "Step: 3100  \tValid loss: 0.5186951160430908\n",
      "Step: 3200  \tTraining loss: 0.5070443153381348\n",
      "Step: 3200  \tTraining accuracy: 0.7393903136253357\n",
      "Step: 3200  \tValid loss: 0.5177931785583496\n",
      "Step: 3300  \tTraining loss: 0.5061199069023132\n",
      "Step: 3300  \tTraining accuracy: 0.7398797869682312\n",
      "Step: 3300  \tValid loss: 0.5168876051902771\n",
      "Step: 3400  \tTraining loss: 0.5051060914993286\n",
      "Step: 3400  \tTraining accuracy: 0.7403222322463989\n",
      "Step: 3400  \tValid loss: 0.5161677598953247\n",
      "Step: 3500  \tTraining loss: 0.5040881037712097\n",
      "Step: 3500  \tTraining accuracy: 0.740776002407074\n",
      "Step: 3500  \tValid loss: 0.5154138803482056\n",
      "Step: 3600  \tTraining loss: 0.5031284689903259\n",
      "Step: 3600  \tTraining accuracy: 0.7412306070327759\n",
      "Step: 3600  \tValid loss: 0.5144494771957397\n",
      "Step: 3700  \tTraining loss: 0.5022618770599365\n",
      "Step: 3700  \tTraining accuracy: 0.741632342338562\n",
      "Step: 3700  \tValid loss: 0.5135618448257446\n",
      "Step: 3800  \tTraining loss: 0.5013232231140137\n",
      "Step: 3800  \tTraining accuracy: 0.7420262694358826\n",
      "Step: 3800  \tValid loss: 0.5131483674049377\n",
      "Step: 3900  \tTraining loss: 0.5004765391349792\n",
      "Step: 3900  \tTraining accuracy: 0.7424306869506836\n",
      "Step: 3900  \tValid loss: 0.5124865174293518\n",
      "Step: 4000  \tTraining loss: 0.49971359968185425\n",
      "Step: 4000  \tTraining accuracy: 0.7428706884384155\n",
      "Step: 4000  \tValid loss: 0.5120430588722229\n",
      "Step: 4100  \tTraining loss: 0.4990066885948181\n",
      "Step: 4100  \tTraining accuracy: 0.7432910203933716\n",
      "Step: 4100  \tValid loss: 0.5116350054740906\n",
      "Step: 4200  \tTraining loss: 0.4983620345592499\n",
      "Step: 4200  \tTraining accuracy: 0.7436767816543579\n",
      "Step: 4200  \tValid loss: 0.5113022327423096\n",
      "Step: 4300  \tTraining loss: 0.49781134724617004\n",
      "Step: 4300  \tTraining accuracy: 0.7440443634986877\n",
      "Step: 4300  \tValid loss: 0.5110163688659668\n",
      "Step: 4400  \tTraining loss: 0.4972822964191437\n",
      "Step: 4400  \tTraining accuracy: 0.7443715929985046\n",
      "Step: 4400  \tValid loss: 0.5106230974197388\n",
      "Step: 4500  \tTraining loss: 0.4967127740383148\n",
      "Step: 4500  \tTraining accuracy: 0.744684100151062\n",
      "Step: 4500  \tValid loss: 0.5100229382514954\n",
      "Step: 4600  \tTraining loss: 0.4961327314376831\n",
      "Step: 4600  \tTraining accuracy: 0.7449941039085388\n",
      "Step: 4600  \tValid loss: 0.5092639327049255\n",
      "Step: 4700  \tTraining loss: 0.49561408162117004\n",
      "Step: 4700  \tTraining accuracy: 0.7453017830848694\n",
      "Step: 4700  \tValid loss: 0.5084725022315979\n",
      "Step: 4800  \tTraining loss: 0.4951852858066559\n",
      "Step: 4800  \tTraining accuracy: 0.7455928921699524\n",
      "Step: 4800  \tValid loss: 0.5079850554466248\n",
      "Step: 4900  \tTraining loss: 0.49479860067367554\n",
      "Step: 4900  \tTraining accuracy: 0.745873749256134\n",
      "Step: 4900  \tValid loss: 0.5076940059661865\n",
      "Step: 5000  \tTraining loss: 0.49444329738616943\n",
      "Step: 5000  \tTraining accuracy: 0.7461570501327515\n",
      "Step: 5000  \tValid loss: 0.5074913501739502\n",
      "Step: 5100  \tTraining loss: 0.4940665364265442\n",
      "Step: 5100  \tTraining accuracy: 0.7464290857315063\n",
      "Step: 5100  \tValid loss: 0.5073558688163757\n",
      "Step: 5200  \tTraining loss: 0.4937170743942261\n",
      "Step: 5200  \tTraining accuracy: 0.7466988563537598\n",
      "Step: 5200  \tValid loss: 0.5071952939033508\n",
      "Step: 5300  \tTraining loss: 0.49335822463035583\n",
      "Step: 5300  \tTraining accuracy: 0.7469696998596191\n",
      "Step: 5300  \tValid loss: 0.5069370865821838\n",
      "Step: 5400  \tTraining loss: 0.49300578236579895\n",
      "Step: 5400  \tTraining accuracy: 0.7472192645072937\n",
      "Step: 5400  \tValid loss: 0.5067260265350342\n",
      "Step: 5500  \tTraining loss: 0.49268731474876404\n",
      "Step: 5500  \tTraining accuracy: 0.7474471926689148\n",
      "Step: 5500  \tValid loss: 0.5065737366676331\n",
      "Step: 5600  \tTraining loss: 0.49238330125808716\n",
      "Step: 5600  \tTraining accuracy: 0.747688353061676\n",
      "Step: 5600  \tValid loss: 0.5064070820808411\n",
      "Step: 5700  \tTraining loss: 0.49202799797058105\n",
      "Step: 5700  \tTraining accuracy: 0.7479209899902344\n",
      "Step: 5700  \tValid loss: 0.5062851905822754\n",
      "Step: 5800  \tTraining loss: 0.4916680157184601\n",
      "Step: 5800  \tTraining accuracy: 0.7481485605239868\n",
      "Step: 5800  \tValid loss: 0.5062358975410461\n",
      "Step: 5900  \tTraining loss: 0.4911811947822571\n",
      "Step: 5900  \tTraining accuracy: 0.7483711838722229\n",
      "Step: 5900  \tValid loss: 0.5063530802726746\n",
      "Step: 6000  \tTraining loss: 0.4906822144985199\n",
      "Step: 6000  \tTraining accuracy: 0.7486221194267273\n",
      "Step: 6000  \tValid loss: 0.5064041018486023\n",
      "Step: 6100  \tTraining loss: 0.4903634786605835\n",
      "Step: 6100  \tTraining accuracy: 0.7488788366317749\n",
      "Step: 6100  \tValid loss: 0.5064364075660706\n",
      "Step: 6200  \tTraining loss: 0.4900633692741394\n",
      "Step: 6200  \tTraining accuracy: 0.7491354942321777\n",
      "Step: 6200  \tValid loss: 0.5065222978591919\n",
      "Step: 6300  \tTraining loss: 0.48978370428085327\n",
      "Step: 6300  \tTraining accuracy: 0.7493894100189209\n",
      "Step: 6300  \tValid loss: 0.5065884590148926\n",
      "Step: 6400  \tTraining loss: 0.4895213842391968\n",
      "Step: 6400  \tTraining accuracy: 0.749642014503479\n",
      "Step: 6400  \tValid loss: 0.506640613079071\n",
      "Step: 6500  \tTraining loss: 0.4892749786376953\n",
      "Step: 6500  \tTraining accuracy: 0.7499000430107117\n",
      "Step: 6500  \tValid loss: 0.506655216217041\n",
      "Step: 6600  \tTraining loss: 0.48903873562812805\n",
      "Step: 6600  \tTraining accuracy: 0.75015789270401\n",
      "Step: 6600  \tValid loss: 0.5067043900489807\n",
      "Step: 6700  \tTraining loss: 0.4888143837451935\n",
      "Step: 6700  \tTraining accuracy: 0.7504119277000427\n",
      "Step: 6700  \tValid loss: 0.5067009925842285\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7506634\n",
      "Precision: 0.7952775\n",
      "Recall: 0.8732175\n",
      "F1 score: 0.785729\n",
      "AUC: 0.72899383\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.750663   0.795277  0.873218  0.785729  0.728994  0.488646      0.750503   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.506221       0.750448   0.513939      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  6775.0  \n",
      "25\n",
      "(7105, 8)\n",
      "(7105, 1)\n",
      "(3840, 8)\n",
      "(3840, 1)\n",
      "(3120, 8)\n",
      "(3120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6702252626419067\n",
      "Step: 100  \tTraining accuracy: 0.6068965792655945\n",
      "Step: 100  \tValid loss: 0.671026885509491\n",
      "Step: 200  \tTraining loss: 0.6097018718719482\n",
      "Step: 200  \tTraining accuracy: 0.6264997720718384\n",
      "Step: 200  \tValid loss: 0.6104182004928589\n",
      "Step: 300  \tTraining loss: 0.5618662238121033\n",
      "Step: 300  \tTraining accuracy: 0.6528735756874084\n",
      "Step: 300  \tValid loss: 0.5589483976364136\n",
      "Step: 400  \tTraining loss: 0.5501832962036133\n",
      "Step: 400  \tTraining accuracy: 0.6702028512954712\n",
      "Step: 400  \tValid loss: 0.5449330806732178\n",
      "Step: 500  \tTraining loss: 0.5470070242881775\n",
      "Step: 500  \tTraining accuracy: 0.6802335381507874\n",
      "Step: 500  \tValid loss: 0.5408331751823425\n",
      "Step: 600  \tTraining loss: 0.5440289378166199\n",
      "Step: 600  \tTraining accuracy: 0.6864910125732422\n",
      "Step: 600  \tValid loss: 0.5381710529327393\n",
      "Step: 700  \tTraining loss: 0.5417031645774841\n",
      "Step: 700  \tTraining accuracy: 0.6907153129577637\n",
      "Step: 700  \tValid loss: 0.5365200042724609\n",
      "Step: 800  \tTraining loss: 0.5401555895805359\n",
      "Step: 800  \tTraining accuracy: 0.6941170692443848\n",
      "Step: 800  \tValid loss: 0.5356442928314209\n",
      "Step: 900  \tTraining loss: 0.538531482219696\n",
      "Step: 900  \tTraining accuracy: 0.6968777179718018\n",
      "Step: 900  \tValid loss: 0.5351305603981018\n",
      "Step: 1000  \tTraining loss: 0.5369042754173279\n",
      "Step: 1000  \tTraining accuracy: 0.6990874409675598\n",
      "Step: 1000  \tValid loss: 0.5346008539199829\n",
      "Step: 1100  \tTraining loss: 0.5358066558837891\n",
      "Step: 1100  \tTraining accuracy: 0.7011336088180542\n",
      "Step: 1100  \tValid loss: 0.5344187021255493\n",
      "Step: 1200  \tTraining loss: 0.5351090431213379\n",
      "Step: 1200  \tTraining accuracy: 0.7028982639312744\n",
      "Step: 1200  \tValid loss: 0.534210205078125\n",
      "Step: 1300  \tTraining loss: 0.5345119833946228\n",
      "Step: 1300  \tTraining accuracy: 0.7045797109603882\n",
      "Step: 1300  \tValid loss: 0.5338999032974243\n",
      "Step: 1400  \tTraining loss: 0.533934473991394\n",
      "Step: 1400  \tTraining accuracy: 0.706122636795044\n",
      "Step: 1400  \tValid loss: 0.5337225794792175\n",
      "Step: 1500  \tTraining loss: 0.5333324074745178\n",
      "Step: 1500  \tTraining accuracy: 0.7074480056762695\n",
      "Step: 1500  \tValid loss: 0.5335391759872437\n",
      "Step: 1600  \tTraining loss: 0.5326641201972961\n",
      "Step: 1600  \tTraining accuracy: 0.7085748314857483\n",
      "Step: 1600  \tValid loss: 0.5334154963493347\n",
      "Step: 1700  \tTraining loss: 0.5318706631660461\n",
      "Step: 1700  \tTraining accuracy: 0.7095479369163513\n",
      "Step: 1700  \tValid loss: 0.5331385731697083\n",
      "Step: 1800  \tTraining loss: 0.5307571887969971\n",
      "Step: 1800  \tTraining accuracy: 0.7104341983795166\n",
      "Step: 1800  \tValid loss: 0.5326643586158752\n",
      "Step: 1900  \tTraining loss: 0.529420793056488\n",
      "Step: 1900  \tTraining accuracy: 0.7112208008766174\n",
      "Step: 1900  \tValid loss: 0.5325413346290588\n",
      "Step: 2000  \tTraining loss: 0.5280408263206482\n",
      "Step: 2000  \tTraining accuracy: 0.7119413614273071\n",
      "Step: 2000  \tValid loss: 0.5324999690055847\n",
      "Step: 2100  \tTraining loss: 0.5266693234443665\n",
      "Step: 2100  \tTraining accuracy: 0.7124841809272766\n",
      "Step: 2100  \tValid loss: 0.5322385430335999\n",
      "Step: 2200  \tTraining loss: 0.5254610180854797\n",
      "Step: 2200  \tTraining accuracy: 0.7130029201507568\n",
      "Step: 2200  \tValid loss: 0.5320907235145569\n",
      "Step: 2300  \tTraining loss: 0.5244103074073792\n",
      "Step: 2300  \tTraining accuracy: 0.7135671973228455\n",
      "Step: 2300  \tValid loss: 0.5320786237716675\n",
      "Step: 2400  \tTraining loss: 0.5234739780426025\n",
      "Step: 2400  \tTraining accuracy: 0.7141984105110168\n",
      "Step: 2400  \tValid loss: 0.5318552255630493\n",
      "Step: 2500  \tTraining loss: 0.522589921951294\n",
      "Step: 2500  \tTraining accuracy: 0.7147694230079651\n",
      "Step: 2500  \tValid loss: 0.5317174196243286\n",
      "Step: 2600  \tTraining loss: 0.5217879414558411\n",
      "Step: 2600  \tTraining accuracy: 0.7152872681617737\n",
      "Step: 2600  \tValid loss: 0.5315224528312683\n",
      "Step: 2700  \tTraining loss: 0.5211309790611267\n",
      "Step: 2700  \tTraining accuracy: 0.7158358097076416\n",
      "Step: 2700  \tValid loss: 0.5312649607658386\n",
      "Step: 2800  \tTraining loss: 0.520533561706543\n",
      "Step: 2800  \tTraining accuracy: 0.7163728475570679\n",
      "Step: 2800  \tValid loss: 0.5310063362121582\n",
      "Step: 2900  \tTraining loss: 0.5199525356292725\n",
      "Step: 2900  \tTraining accuracy: 0.7168523073196411\n",
      "Step: 2900  \tValid loss: 0.530252993106842\n",
      "Step: 3000  \tTraining loss: 0.5193108320236206\n",
      "Step: 3000  \tTraining accuracy: 0.717299222946167\n",
      "Step: 3000  \tValid loss: 0.5295512676239014\n",
      "Step: 3100  \tTraining loss: 0.518730103969574\n",
      "Step: 3100  \tTraining accuracy: 0.7176958918571472\n",
      "Step: 3100  \tValid loss: 0.5289254784584045\n",
      "Step: 3200  \tTraining loss: 0.518183171749115\n",
      "Step: 3200  \tTraining accuracy: 0.7180763483047485\n",
      "Step: 3200  \tValid loss: 0.5283105969429016\n",
      "Step: 3300  \tTraining loss: 0.5175951719284058\n",
      "Step: 3300  \tTraining accuracy: 0.7184706330299377\n",
      "Step: 3300  \tValid loss: 0.5277446508407593\n",
      "Step: 3400  \tTraining loss: 0.5169729590415955\n",
      "Step: 3400  \tTraining accuracy: 0.7188541293144226\n",
      "Step: 3400  \tValid loss: 0.5271210074424744\n",
      "Step: 3500  \tTraining loss: 0.5164394974708557\n",
      "Step: 3500  \tTraining accuracy: 0.7192318439483643\n",
      "Step: 3500  \tValid loss: 0.526671826839447\n",
      "Step: 3600  \tTraining loss: 0.5159451365470886\n",
      "Step: 3600  \tTraining accuracy: 0.7195842862129211\n",
      "Step: 3600  \tValid loss: 0.5263367295265198\n",
      "Step: 3700  \tTraining loss: 0.5154771208763123\n",
      "Step: 3700  \tTraining accuracy: 0.7199232578277588\n",
      "Step: 3700  \tValid loss: 0.5259348154067993\n",
      "Step: 3800  \tTraining loss: 0.5149906277656555\n",
      "Step: 3800  \tTraining accuracy: 0.7202460765838623\n",
      "Step: 3800  \tValid loss: 0.5253252387046814\n",
      "Step: 3900  \tTraining loss: 0.5144326686859131\n",
      "Step: 3900  \tTraining accuracy: 0.7205520868301392\n",
      "Step: 3900  \tValid loss: 0.5245412588119507\n",
      "Step: 4000  \tTraining loss: 0.5137952566146851\n",
      "Step: 4000  \tTraining accuracy: 0.7208444476127625\n",
      "Step: 4000  \tValid loss: 0.5237823128700256\n",
      "Step: 4100  \tTraining loss: 0.513288140296936\n",
      "Step: 4100  \tTraining accuracy: 0.7211135625839233\n",
      "Step: 4100  \tValid loss: 0.5233123898506165\n",
      "Step: 4200  \tTraining loss: 0.5128697752952576\n",
      "Step: 4200  \tTraining accuracy: 0.7213868498802185\n",
      "Step: 4200  \tValid loss: 0.522868812084198\n",
      "Step: 4300  \tTraining loss: 0.5125049352645874\n",
      "Step: 4300  \tTraining accuracy: 0.7216573357582092\n",
      "Step: 4300  \tValid loss: 0.5225459933280945\n",
      "Step: 4400  \tTraining loss: 0.5121676921844482\n",
      "Step: 4400  \tTraining accuracy: 0.7219153642654419\n",
      "Step: 4400  \tValid loss: 0.522240936756134\n",
      "Step: 4500  \tTraining loss: 0.511841893196106\n",
      "Step: 4500  \tTraining accuracy: 0.7221506237983704\n",
      "Step: 4500  \tValid loss: 0.5218818187713623\n",
      "Step: 4600  \tTraining loss: 0.5115492343902588\n",
      "Step: 4600  \tTraining accuracy: 0.7223739624023438\n",
      "Step: 4600  \tValid loss: 0.5216305255889893\n",
      "Step: 4700  \tTraining loss: 0.5112678408622742\n",
      "Step: 4700  \tTraining accuracy: 0.7225953340530396\n",
      "Step: 4700  \tValid loss: 0.52138751745224\n",
      "Step: 4800  \tTraining loss: 0.5109938979148865\n",
      "Step: 4800  \tTraining accuracy: 0.7228268384933472\n",
      "Step: 4800  \tValid loss: 0.5211142897605896\n",
      "Step: 4900  \tTraining loss: 0.5107372403144836\n",
      "Step: 4900  \tTraining accuracy: 0.7230459451675415\n",
      "Step: 4900  \tValid loss: 0.5209039449691772\n",
      "Step: 5000  \tTraining loss: 0.5104960203170776\n",
      "Step: 5000  \tTraining accuracy: 0.7232733368873596\n",
      "Step: 5000  \tValid loss: 0.5207116007804871\n",
      "Step: 5100  \tTraining loss: 0.5102691054344177\n",
      "Step: 5100  \tTraining accuracy: 0.7235044240951538\n",
      "Step: 5100  \tValid loss: 0.5205360054969788\n",
      "Step: 5200  \tTraining loss: 0.5100522041320801\n",
      "Step: 5200  \tTraining accuracy: 0.7237210273742676\n",
      "Step: 5200  \tValid loss: 0.5203655958175659\n",
      "Step: 5300  \tTraining loss: 0.5098448991775513\n",
      "Step: 5300  \tTraining accuracy: 0.723922610282898\n",
      "Step: 5300  \tValid loss: 0.5202030539512634\n",
      "Step: 5400  \tTraining loss: 0.5096458792686462\n",
      "Step: 5400  \tTraining accuracy: 0.7241193056106567\n",
      "Step: 5400  \tValid loss: 0.5200532078742981\n",
      "Step: 5500  \tTraining loss: 0.509455144405365\n",
      "Step: 5500  \tTraining accuracy: 0.7243114113807678\n",
      "Step: 5500  \tValid loss: 0.5198901891708374\n",
      "Step: 5600  \tTraining loss: 0.5092702507972717\n",
      "Step: 5600  \tTraining accuracy: 0.724496603012085\n",
      "Step: 5600  \tValid loss: 0.5197353959083557\n",
      "Step: 5700  \tTraining loss: 0.5090843439102173\n",
      "Step: 5700  \tTraining accuracy: 0.7246676683425903\n",
      "Step: 5700  \tValid loss: 0.5196106433868408\n",
      "Step: 5800  \tTraining loss: 0.5088775157928467\n",
      "Step: 5800  \tTraining accuracy: 0.7248389720916748\n",
      "Step: 5800  \tValid loss: 0.5194694399833679\n",
      "Step: 5900  \tTraining loss: 0.5086814165115356\n",
      "Step: 5900  \tTraining accuracy: 0.7250056266784668\n",
      "Step: 5900  \tValid loss: 0.5193434357643127\n",
      "Step: 6000  \tTraining loss: 0.5084895491600037\n",
      "Step: 6000  \tTraining accuracy: 0.7251583337783813\n",
      "Step: 6000  \tValid loss: 0.5191829800605774\n",
      "Step: 6100  \tTraining loss: 0.5083053112030029\n",
      "Step: 6100  \tTraining accuracy: 0.72530597448349\n",
      "Step: 6100  \tValid loss: 0.5190144777297974\n",
      "Step: 6200  \tTraining loss: 0.5081303715705872\n",
      "Step: 6200  \tTraining accuracy: 0.7254441976547241\n",
      "Step: 6200  \tValid loss: 0.5188804864883423\n",
      "Step: 6300  \tTraining loss: 0.507958710193634\n",
      "Step: 6300  \tTraining accuracy: 0.7255688905715942\n",
      "Step: 6300  \tValid loss: 0.5187584757804871\n",
      "Step: 6400  \tTraining loss: 0.5077903270721436\n",
      "Step: 6400  \tTraining accuracy: 0.7256919145584106\n",
      "Step: 6400  \tValid loss: 0.5186251997947693\n",
      "Step: 6500  \tTraining loss: 0.5076255202293396\n",
      "Step: 6500  \tTraining accuracy: 0.7258055806159973\n",
      "Step: 6500  \tValid loss: 0.5185326337814331\n",
      "Step: 6600  \tTraining loss: 0.5074630379676819\n",
      "Step: 6600  \tTraining accuracy: 0.725914716720581\n",
      "Step: 6600  \tValid loss: 0.5184347033500671\n",
      "Step: 6700  \tTraining loss: 0.5073024034500122\n",
      "Step: 6700  \tTraining accuracy: 0.7260333895683289\n",
      "Step: 6700  \tValid loss: 0.5183429718017578\n",
      "Step: 6800  \tTraining loss: 0.5071001052856445\n",
      "Step: 6800  \tTraining accuracy: 0.7261422276496887\n",
      "Step: 6800  \tValid loss: 0.5180597901344299\n",
      "Step: 6900  \tTraining loss: 0.5069128274917603\n",
      "Step: 6900  \tTraining accuracy: 0.726252019405365\n",
      "Step: 6900  \tValid loss: 0.5178200006484985\n",
      "Step: 7000  \tTraining loss: 0.5067358016967773\n",
      "Step: 7000  \tTraining accuracy: 0.726365864276886\n",
      "Step: 7000  \tValid loss: 0.5177696943283081\n",
      "Step: 7100  \tTraining loss: 0.5065622329711914\n",
      "Step: 7100  \tTraining accuracy: 0.7264683842658997\n",
      "Step: 7100  \tValid loss: 0.5176761746406555\n",
      "Step: 7200  \tTraining loss: 0.5063918232917786\n",
      "Step: 7200  \tTraining accuracy: 0.7265720367431641\n",
      "Step: 7200  \tValid loss: 0.5175979137420654\n",
      "Step: 7300  \tTraining loss: 0.5062218904495239\n",
      "Step: 7300  \tTraining accuracy: 0.7266786694526672\n",
      "Step: 7300  \tValid loss: 0.5175144672393799\n",
      "Step: 7400  \tTraining loss: 0.5060510635375977\n",
      "Step: 7400  \tTraining accuracy: 0.7267833948135376\n",
      "Step: 7400  \tValid loss: 0.5174097418785095\n",
      "Step: 7500  \tTraining loss: 0.5058817863464355\n",
      "Step: 7500  \tTraining accuracy: 0.7268939018249512\n",
      "Step: 7500  \tValid loss: 0.5172910690307617\n",
      "Step: 7600  \tTraining loss: 0.5057146549224854\n",
      "Step: 7600  \tTraining accuracy: 0.7270052433013916\n",
      "Step: 7600  \tValid loss: 0.5172002911567688\n",
      "Step: 7700  \tTraining loss: 0.505547285079956\n",
      "Step: 7700  \tTraining accuracy: 0.7271164059638977\n",
      "Step: 7700  \tValid loss: 0.5171100497245789\n",
      "Step: 7800  \tTraining loss: 0.5053784251213074\n",
      "Step: 7800  \tTraining accuracy: 0.7272229194641113\n",
      "Step: 7800  \tValid loss: 0.5170125365257263\n",
      "Step: 7900  \tTraining loss: 0.5052092671394348\n",
      "Step: 7900  \tTraining accuracy: 0.7273222208023071\n",
      "Step: 7900  \tValid loss: 0.5169037580490112\n",
      "Step: 8000  \tTraining loss: 0.505040168762207\n",
      "Step: 8000  \tTraining accuracy: 0.7274278998374939\n",
      "Step: 8000  \tValid loss: 0.5168120265007019\n",
      "Step: 8100  \tTraining loss: 0.5048706531524658\n",
      "Step: 8100  \tTraining accuracy: 0.7275380492210388\n",
      "Step: 8100  \tValid loss: 0.5167083740234375\n",
      "Step: 8200  \tTraining loss: 0.504700243473053\n",
      "Step: 8200  \tTraining accuracy: 0.7276411652565002\n",
      "Step: 8200  \tValid loss: 0.5166069865226746\n",
      "Step: 8300  \tTraining loss: 0.5045247673988342\n",
      "Step: 8300  \tTraining accuracy: 0.7277374267578125\n",
      "Step: 8300  \tValid loss: 0.5164415240287781\n",
      "Step: 8400  \tTraining loss: 0.5043414235115051\n",
      "Step: 8400  \tTraining accuracy: 0.7278237342834473\n",
      "Step: 8400  \tValid loss: 0.5163963437080383\n",
      "Step: 8500  \tTraining loss: 0.5041708946228027\n",
      "Step: 8500  \tTraining accuracy: 0.7279004454612732\n",
      "Step: 8500  \tValid loss: 0.5162995457649231\n",
      "Step: 8600  \tTraining loss: 0.5040013790130615\n",
      "Step: 8600  \tTraining accuracy: 0.7279728651046753\n",
      "Step: 8600  \tValid loss: 0.5162065029144287\n",
      "Step: 8700  \tTraining loss: 0.5038352608680725\n",
      "Step: 8700  \tTraining accuracy: 0.7280411124229431\n",
      "Step: 8700  \tValid loss: 0.516091525554657\n",
      "Step: 8800  \tTraining loss: 0.5036717057228088\n",
      "Step: 8800  \tTraining accuracy: 0.7281143069267273\n",
      "Step: 8800  \tValid loss: 0.515990674495697\n",
      "Step: 8900  \tTraining loss: 0.5035101771354675\n",
      "Step: 8900  \tTraining accuracy: 0.7281963229179382\n",
      "Step: 8900  \tValid loss: 0.5158966779708862\n",
      "Step: 9000  \tTraining loss: 0.5033295750617981\n",
      "Step: 9000  \tTraining accuracy: 0.7282788753509521\n",
      "Step: 9000  \tValid loss: 0.5156999230384827\n",
      "Step: 9100  \tTraining loss: 0.5031445622444153\n",
      "Step: 9100  \tTraining accuracy: 0.728359580039978\n",
      "Step: 9100  \tValid loss: 0.5155699253082275\n",
      "Step: 9200  \tTraining loss: 0.5029724836349487\n",
      "Step: 9200  \tTraining accuracy: 0.7284471392631531\n",
      "Step: 9200  \tValid loss: 0.515459418296814\n",
      "Step: 9300  \tTraining loss: 0.5028091073036194\n",
      "Step: 9300  \tTraining accuracy: 0.7285380959510803\n",
      "Step: 9300  \tValid loss: 0.5153638124465942\n",
      "Step: 9400  \tTraining loss: 0.5026535987854004\n",
      "Step: 9400  \tTraining accuracy: 0.7286248803138733\n",
      "Step: 9400  \tValid loss: 0.5152871012687683\n",
      "Step: 9500  \tTraining loss: 0.5025041699409485\n",
      "Step: 9500  \tTraining accuracy: 0.7287098169326782\n",
      "Step: 9500  \tValid loss: 0.5152288675308228\n",
      "Step: 9600  \tTraining loss: 0.5023584365844727\n",
      "Step: 9600  \tTraining accuracy: 0.7287959456443787\n",
      "Step: 9600  \tValid loss: 0.5151631236076355\n",
      "Step: 9700  \tTraining loss: 0.5022123456001282\n",
      "Step: 9700  \tTraining accuracy: 0.7288795113563538\n",
      "Step: 9700  \tValid loss: 0.5150372982025146\n",
      "Step: 9800  \tTraining loss: 0.5020753145217896\n",
      "Step: 9800  \tTraining accuracy: 0.7289672493934631\n",
      "Step: 9800  \tValid loss: 0.5149917006492615\n",
      "Step: 9900  \tTraining loss: 0.501943051815033\n",
      "Step: 9900  \tTraining accuracy: 0.7290582656860352\n",
      "Step: 9900  \tValid loss: 0.5149397253990173\n",
      "Step: 10000  \tTraining loss: 0.5018128752708435\n",
      "Step: 10000  \tTraining accuracy: 0.7291545867919922\n",
      "Step: 10000  \tValid loss: 0.5148828029632568\n",
      "Step: 10100  \tTraining loss: 0.5016661286354065\n",
      "Step: 10100  \tTraining accuracy: 0.7292433381080627\n",
      "Step: 10100  \tValid loss: 0.5145822763442993\n",
      "Step: 10200  \tTraining loss: 0.5015320777893066\n",
      "Step: 10200  \tTraining accuracy: 0.7293218970298767\n",
      "Step: 10200  \tValid loss: 0.5144657492637634\n",
      "Step: 10300  \tTraining loss: 0.5014063119888306\n",
      "Step: 10300  \tTraining accuracy: 0.7293996214866638\n",
      "Step: 10300  \tValid loss: 0.5143895149230957\n",
      "Step: 10400  \tTraining loss: 0.5012831091880798\n",
      "Step: 10400  \tTraining accuracy: 0.729478657245636\n",
      "Step: 10400  \tValid loss: 0.5142848491668701\n",
      "Step: 10500  \tTraining loss: 0.5011658668518066\n",
      "Step: 10500  \tTraining accuracy: 0.7295622229576111\n",
      "Step: 10500  \tValid loss: 0.5142065286636353\n",
      "Step: 10600  \tTraining loss: 0.5010542869567871\n",
      "Step: 10600  \tTraining accuracy: 0.7296482920646667\n",
      "Step: 10600  \tValid loss: 0.5141229033470154\n",
      "Step: 10700  \tTraining loss: 0.5009433031082153\n",
      "Step: 10700  \tTraining accuracy: 0.7297273874282837\n",
      "Step: 10700  \tValid loss: 0.514105498790741\n",
      "Step: 10800  \tTraining loss: 0.5008386373519897\n",
      "Step: 10800  \tTraining accuracy: 0.7297977805137634\n",
      "Step: 10800  \tValid loss: 0.5140608549118042\n",
      "Step: 10900  \tTraining loss: 0.5007365345954895\n",
      "Step: 10900  \tTraining accuracy: 0.729870080947876\n",
      "Step: 10900  \tValid loss: 0.5139995813369751\n",
      "Step: 11000  \tTraining loss: 0.5006357431411743\n",
      "Step: 11000  \tTraining accuracy: 0.7299391627311707\n",
      "Step: 11000  \tValid loss: 0.5139493346214294\n",
      "Step: 11100  \tTraining loss: 0.5005386471748352\n",
      "Step: 11100  \tTraining accuracy: 0.73000568151474\n",
      "Step: 11100  \tValid loss: 0.5139089226722717\n",
      "Step: 11200  \tTraining loss: 0.5004454255104065\n",
      "Step: 11200  \tTraining accuracy: 0.7300748825073242\n",
      "Step: 11200  \tValid loss: 0.5138620138168335\n",
      "Step: 11300  \tTraining loss: 0.5003523826599121\n",
      "Step: 11300  \tTraining accuracy: 0.7301484942436218\n",
      "Step: 11300  \tValid loss: 0.5137965679168701\n",
      "Step: 11400  \tTraining loss: 0.5002628564834595\n",
      "Step: 11400  \tTraining accuracy: 0.7302220463752747\n",
      "Step: 11400  \tValid loss: 0.5137738585472107\n",
      "Step: 11500  \tTraining loss: 0.5001752376556396\n",
      "Step: 11500  \tTraining accuracy: 0.7302937507629395\n",
      "Step: 11500  \tValid loss: 0.5137376189231873\n",
      "Step: 11600  \tTraining loss: 0.5000912547111511\n",
      "Step: 11600  \tTraining accuracy: 0.7303635478019714\n",
      "Step: 11600  \tValid loss: 0.5137081742286682\n",
      "Step: 11700  \tTraining loss: 0.5000041723251343\n",
      "Step: 11700  \tTraining accuracy: 0.7304321527481079\n",
      "Step: 11700  \tValid loss: 0.513744592666626\n",
      "Step: 11800  \tTraining loss: 0.4999198615550995\n",
      "Step: 11800  \tTraining accuracy: 0.7304965853691101\n",
      "Step: 11800  \tValid loss: 0.5136593580245972\n",
      "Step: 11900  \tTraining loss: 0.49984192848205566\n",
      "Step: 11900  \tTraining accuracy: 0.7305598855018616\n",
      "Step: 11900  \tValid loss: 0.513616144657135\n",
      "Step: 12000  \tTraining loss: 0.4997655749320984\n",
      "Step: 12000  \tTraining accuracy: 0.7306209802627563\n",
      "Step: 12000  \tValid loss: 0.5135730504989624\n",
      "Step: 12100  \tTraining loss: 0.49968773126602173\n",
      "Step: 12100  \tTraining accuracy: 0.7306798696517944\n",
      "Step: 12100  \tValid loss: 0.5135522484779358\n",
      "Step: 12200  \tTraining loss: 0.49961143732070923\n",
      "Step: 12200  \tTraining accuracy: 0.7307436466217041\n",
      "Step: 12200  \tValid loss: 0.513489842414856\n",
      "Step: 12300  \tTraining loss: 0.4995386600494385\n",
      "Step: 12300  \tTraining accuracy: 0.7308092713356018\n",
      "Step: 12300  \tValid loss: 0.5134427547454834\n",
      "Step: 12400  \tTraining loss: 0.49945470690727234\n",
      "Step: 12400  \tTraining accuracy: 0.7308704257011414\n",
      "Step: 12400  \tValid loss: 0.5136102437973022\n",
      "Step: 12500  \tTraining loss: 0.49937352538108826\n",
      "Step: 12500  \tTraining accuracy: 0.7309282422065735\n",
      "Step: 12500  \tValid loss: 0.5135782957077026\n",
      "Step: 12600  \tTraining loss: 0.4993024170398712\n",
      "Step: 12600  \tTraining accuracy: 0.730984628200531\n",
      "Step: 12600  \tValid loss: 0.5135451555252075\n",
      "Step: 12700  \tTraining loss: 0.49923497438430786\n",
      "Step: 12700  \tTraining accuracy: 0.7310423254966736\n",
      "Step: 12700  \tValid loss: 0.5134938955307007\n",
      "Step: 12800  \tTraining loss: 0.49916747212409973\n",
      "Step: 12800  \tTraining accuracy: 0.7311069965362549\n",
      "Step: 12800  \tValid loss: 0.5134578347206116\n",
      "Step: 12900  \tTraining loss: 0.49910324811935425\n",
      "Step: 12900  \tTraining accuracy: 0.7311750054359436\n",
      "Step: 12900  \tValid loss: 0.5134241580963135\n",
      "Step: 13000  \tTraining loss: 0.49904054403305054\n",
      "Step: 13000  \tTraining accuracy: 0.7312425374984741\n",
      "Step: 13000  \tValid loss: 0.513401985168457\n",
      "Step: 13100  \tTraining loss: 0.4989793300628662\n",
      "Step: 13100  \tTraining accuracy: 0.7313079833984375\n",
      "Step: 13100  \tValid loss: 0.5133578181266785\n",
      "Step: 13200  \tTraining loss: 0.4989205300807953\n",
      "Step: 13200  \tTraining accuracy: 0.7313724160194397\n",
      "Step: 13200  \tValid loss: 0.5133432149887085\n",
      "Step: 13300  \tTraining loss: 0.49886175990104675\n",
      "Step: 13300  \tTraining accuracy: 0.7314374446868896\n",
      "Step: 13300  \tValid loss: 0.5133160352706909\n",
      "Step: 13400  \tTraining loss: 0.4988037943840027\n",
      "Step: 13400  \tTraining accuracy: 0.7315036654472351\n",
      "Step: 13400  \tValid loss: 0.5132750868797302\n",
      "Step: 13500  \tTraining loss: 0.49874743819236755\n",
      "Step: 13500  \tTraining accuracy: 0.7315704822540283\n",
      "Step: 13500  \tValid loss: 0.5132561326026917\n",
      "Step: 13600  \tTraining loss: 0.49869170784950256\n",
      "Step: 13600  \tTraining accuracy: 0.7316389083862305\n",
      "Step: 13600  \tValid loss: 0.5132321715354919\n",
      "Step: 13700  \tTraining loss: 0.4986378848552704\n",
      "Step: 13700  \tTraining accuracy: 0.7317053079605103\n",
      "Step: 13700  \tValid loss: 0.5132121443748474\n",
      "Step: 13800  \tTraining loss: 0.498584121465683\n",
      "Step: 13800  \tTraining accuracy: 0.7317728400230408\n",
      "Step: 13800  \tValid loss: 0.5131792426109314\n",
      "Step: 13900  \tTraining loss: 0.4985312521457672\n",
      "Step: 13900  \tTraining accuracy: 0.7318444848060608\n",
      "Step: 13900  \tValid loss: 0.5131526589393616\n",
      "Step: 14000  \tTraining loss: 0.4984784722328186\n",
      "Step: 14000  \tTraining accuracy: 0.7319166660308838\n",
      "Step: 14000  \tValid loss: 0.5131304860115051\n",
      "Step: 14100  \tTraining loss: 0.49842652678489685\n",
      "Step: 14100  \tTraining accuracy: 0.7319878339767456\n",
      "Step: 14100  \tValid loss: 0.513096272945404\n",
      "Step: 14200  \tTraining loss: 0.49837470054626465\n",
      "Step: 14200  \tTraining accuracy: 0.7320579886436462\n",
      "Step: 14200  \tValid loss: 0.5130709409713745\n",
      "Step: 14300  \tTraining loss: 0.4983237385749817\n",
      "Step: 14300  \tTraining accuracy: 0.7321271300315857\n",
      "Step: 14300  \tValid loss: 0.5130449533462524\n",
      "Step: 14400  \tTraining loss: 0.498273104429245\n",
      "Step: 14400  \tTraining accuracy: 0.7321953177452087\n",
      "Step: 14400  \tValid loss: 0.513020932674408\n",
      "Step: 14500  \tTraining loss: 0.498223215341568\n",
      "Step: 14500  \tTraining accuracy: 0.7322625517845154\n",
      "Step: 14500  \tValid loss: 0.5130010843276978\n",
      "Step: 14600  \tTraining loss: 0.4981730282306671\n",
      "Step: 14600  \tTraining accuracy: 0.732330322265625\n",
      "Step: 14600  \tValid loss: 0.5129640698432922\n",
      "Step: 14700  \tTraining loss: 0.49811771512031555\n",
      "Step: 14700  \tTraining accuracy: 0.7323967218399048\n",
      "Step: 14700  \tValid loss: 0.5129503607749939\n",
      "Step: 14800  \tTraining loss: 0.49806055426597595\n",
      "Step: 14800  \tTraining accuracy: 0.7324621677398682\n",
      "Step: 14800  \tValid loss: 0.5128711462020874\n",
      "Step: 14900  \tTraining loss: 0.49800384044647217\n",
      "Step: 14900  \tTraining accuracy: 0.7325282096862793\n",
      "Step: 14900  \tValid loss: 0.5128042697906494\n",
      "Step: 15000  \tTraining loss: 0.49794846773147583\n",
      "Step: 15000  \tTraining accuracy: 0.7325933575630188\n",
      "Step: 15000  \tValid loss: 0.5127384066581726\n",
      "Step: 15100  \tTraining loss: 0.49789509177207947\n",
      "Step: 15100  \tTraining accuracy: 0.7326595187187195\n",
      "Step: 15100  \tValid loss: 0.512694776058197\n",
      "Step: 15200  \tTraining loss: 0.49783775210380554\n",
      "Step: 15200  \tTraining accuracy: 0.7327224612236023\n",
      "Step: 15200  \tValid loss: 0.5126196146011353\n",
      "Step: 15300  \tTraining loss: 0.4977840781211853\n",
      "Step: 15300  \tTraining accuracy: 0.7327813506126404\n",
      "Step: 15300  \tValid loss: 0.5125657916069031\n",
      "Step: 15400  \tTraining loss: 0.49772825837135315\n",
      "Step: 15400  \tTraining accuracy: 0.7328376173973083\n",
      "Step: 15400  \tValid loss: 0.5125030279159546\n",
      "Step: 15500  \tTraining loss: 0.49767330288887024\n",
      "Step: 15500  \tTraining accuracy: 0.7328917384147644\n",
      "Step: 15500  \tValid loss: 0.5124422311782837\n",
      "Step: 15600  \tTraining loss: 0.49761638045310974\n",
      "Step: 15600  \tTraining accuracy: 0.7329442501068115\n",
      "Step: 15600  \tValid loss: 0.5123708248138428\n",
      "Step: 15700  \tTraining loss: 0.4975607991218567\n",
      "Step: 15700  \tTraining accuracy: 0.7329942584037781\n",
      "Step: 15700  \tValid loss: 0.5123232007026672\n",
      "Step: 15800  \tTraining loss: 0.4974925220012665\n",
      "Step: 15800  \tTraining accuracy: 0.73304682970047\n",
      "Step: 15800  \tValid loss: 0.512235701084137\n",
      "Step: 15900  \tTraining loss: 0.49743151664733887\n",
      "Step: 15900  \tTraining accuracy: 0.7331010103225708\n",
      "Step: 15900  \tValid loss: 0.5122107863426208\n",
      "Step: 16000  \tTraining loss: 0.49737420678138733\n",
      "Step: 16000  \tTraining accuracy: 0.7331526875495911\n",
      "Step: 16000  \tValid loss: 0.5121483206748962\n",
      "Step: 16100  \tTraining loss: 0.4973159730434418\n",
      "Step: 16100  \tTraining accuracy: 0.7332046031951904\n",
      "Step: 16100  \tValid loss: 0.5120842456817627\n",
      "Step: 16200  \tTraining loss: 0.4972587823867798\n",
      "Step: 16200  \tTraining accuracy: 0.7332562804222107\n",
      "Step: 16200  \tValid loss: 0.5120408535003662\n",
      "Step: 16300  \tTraining loss: 0.49720171093940735\n",
      "Step: 16300  \tTraining accuracy: 0.7333082556724548\n",
      "Step: 16300  \tValid loss: 0.5119622349739075\n",
      "Step: 16400  \tTraining loss: 0.4971414804458618\n",
      "Step: 16400  \tTraining accuracy: 0.7333608865737915\n",
      "Step: 16400  \tValid loss: 0.5119033455848694\n",
      "Step: 16500  \tTraining loss: 0.4970819652080536\n",
      "Step: 16500  \tTraining accuracy: 0.7334115505218506\n",
      "Step: 16500  \tValid loss: 0.5118351578712463\n",
      "Step: 16600  \tTraining loss: 0.4970225393772125\n",
      "Step: 16600  \tTraining accuracy: 0.733462929725647\n",
      "Step: 16600  \tValid loss: 0.5117724537849426\n",
      "Step: 16700  \tTraining loss: 0.4969618618488312\n",
      "Step: 16700  \tTraining accuracy: 0.7335175275802612\n",
      "Step: 16700  \tValid loss: 0.5117033123970032\n",
      "Step: 16800  \tTraining loss: 0.49690112471580505\n",
      "Step: 16800  \tTraining accuracy: 0.7335705757141113\n",
      "Step: 16800  \tValid loss: 0.5116323232650757\n",
      "Step: 16900  \tTraining loss: 0.49684029817581177\n",
      "Step: 16900  \tTraining accuracy: 0.7336221933364868\n",
      "Step: 16900  \tValid loss: 0.5115540623664856\n",
      "Step: 17000  \tTraining loss: 0.49677926301956177\n",
      "Step: 17000  \tTraining accuracy: 0.733673632144928\n",
      "Step: 17000  \tValid loss: 0.5114880204200745\n",
      "Step: 17100  \tTraining loss: 0.4967191815376282\n",
      "Step: 17100  \tTraining accuracy: 0.7337282299995422\n",
      "Step: 17100  \tValid loss: 0.5114180445671082\n",
      "Step: 17200  \tTraining loss: 0.49665775895118713\n",
      "Step: 17200  \tTraining accuracy: 0.733782172203064\n",
      "Step: 17200  \tValid loss: 0.5113608241081238\n",
      "Step: 17300  \tTraining loss: 0.496597558259964\n",
      "Step: 17300  \tTraining accuracy: 0.7338350415229797\n",
      "Step: 17300  \tValid loss: 0.5113046169281006\n",
      "Step: 17400  \tTraining loss: 0.4965384602546692\n",
      "Step: 17400  \tTraining accuracy: 0.7338857054710388\n",
      "Step: 17400  \tValid loss: 0.5112372040748596\n",
      "Step: 17500  \tTraining loss: 0.49647778272628784\n",
      "Step: 17500  \tTraining accuracy: 0.7339377999305725\n",
      "Step: 17500  \tValid loss: 0.5111684203147888\n",
      "Step: 17600  \tTraining loss: 0.4964185655117035\n",
      "Step: 17600  \tTraining accuracy: 0.7339913845062256\n",
      "Step: 17600  \tValid loss: 0.5110971927642822\n",
      "Step: 17700  \tTraining loss: 0.49635517597198486\n",
      "Step: 17700  \tTraining accuracy: 0.7340410947799683\n",
      "Step: 17700  \tValid loss: 0.511018693447113\n",
      "Step: 17800  \tTraining loss: 0.4962579905986786\n",
      "Step: 17800  \tTraining accuracy: 0.7340870499610901\n",
      "Step: 17800  \tValid loss: 0.5108673572540283\n",
      "Step: 17900  \tTraining loss: 0.49617600440979004\n",
      "Step: 17900  \tTraining accuracy: 0.7341352701187134\n",
      "Step: 17900  \tValid loss: 0.5107802748680115\n",
      "Step: 18000  \tTraining loss: 0.49610549211502075\n",
      "Step: 18000  \tTraining accuracy: 0.7341817617416382\n",
      "Step: 18000  \tValid loss: 0.5107318758964539\n",
      "Step: 18100  \tTraining loss: 0.4960426986217499\n",
      "Step: 18100  \tTraining accuracy: 0.7342269420623779\n",
      "Step: 18100  \tValid loss: 0.5106803774833679\n",
      "Step: 18200  \tTraining loss: 0.49598050117492676\n",
      "Step: 18200  \tTraining accuracy: 0.7342684864997864\n",
      "Step: 18200  \tValid loss: 0.5106328725814819\n",
      "Step: 18300  \tTraining loss: 0.4959205090999603\n",
      "Step: 18300  \tTraining accuracy: 0.7343064546585083\n",
      "Step: 18300  \tValid loss: 0.5105854868888855\n",
      "Step: 18400  \tTraining loss: 0.4958599805831909\n",
      "Step: 18400  \tTraining accuracy: 0.7343444228172302\n",
      "Step: 18400  \tValid loss: 0.5105329751968384\n",
      "Step: 18500  \tTraining loss: 0.49579864740371704\n",
      "Step: 18500  \tTraining accuracy: 0.7343831658363342\n",
      "Step: 18500  \tValid loss: 0.5104585289955139\n",
      "Step: 18600  \tTraining loss: 0.49573829770088196\n",
      "Step: 18600  \tTraining accuracy: 0.7344210743904114\n",
      "Step: 18600  \tValid loss: 0.510415256023407\n",
      "Step: 18700  \tTraining loss: 0.49567845463752747\n",
      "Step: 18700  \tTraining accuracy: 0.7344574332237244\n",
      "Step: 18700  \tValid loss: 0.5103455185890198\n",
      "Step: 18800  \tTraining loss: 0.4956209063529968\n",
      "Step: 18800  \tTraining accuracy: 0.7344937324523926\n",
      "Step: 18800  \tValid loss: 0.5103059411048889\n",
      "Step: 18900  \tTraining loss: 0.49556827545166016\n",
      "Step: 18900  \tTraining accuracy: 0.7345286011695862\n",
      "Step: 18900  \tValid loss: 0.5102521181106567\n",
      "Step: 19000  \tTraining loss: 0.4955165982246399\n",
      "Step: 19000  \tTraining accuracy: 0.7345626950263977\n",
      "Step: 19000  \tValid loss: 0.5101917386054993\n",
      "Step: 19100  \tTraining loss: 0.4954676926136017\n",
      "Step: 19100  \tTraining accuracy: 0.7346001267433167\n",
      "Step: 19100  \tValid loss: 0.5101507306098938\n",
      "Step: 19200  \tTraining loss: 0.49541792273521423\n",
      "Step: 19200  \tTraining accuracy: 0.7346383333206177\n",
      "Step: 19200  \tValid loss: 0.5100774765014648\n",
      "Step: 19300  \tTraining loss: 0.49537181854248047\n",
      "Step: 19300  \tTraining accuracy: 0.7346761226654053\n",
      "Step: 19300  \tValid loss: 0.5100411772727966\n",
      "Step: 19400  \tTraining loss: 0.4953199326992035\n",
      "Step: 19400  \tTraining accuracy: 0.7347112894058228\n",
      "Step: 19400  \tValid loss: 0.5099628567695618\n",
      "Step: 19500  \tTraining loss: 0.49526888132095337\n",
      "Step: 19500  \tTraining accuracy: 0.7347439527511597\n",
      "Step: 19500  \tValid loss: 0.5099021792411804\n",
      "Step: 19600  \tTraining loss: 0.49521809816360474\n",
      "Step: 19600  \tTraining accuracy: 0.7347740530967712\n",
      "Step: 19600  \tValid loss: 0.5098182559013367\n",
      "Step: 19700  \tTraining loss: 0.4951700270175934\n",
      "Step: 19700  \tTraining accuracy: 0.7348034977912903\n",
      "Step: 19700  \tValid loss: 0.5097744464874268\n",
      "Step: 19800  \tTraining loss: 0.4951232969760895\n",
      "Step: 19800  \tTraining accuracy: 0.7348330020904541\n",
      "Step: 19800  \tValid loss: 0.5097107291221619\n",
      "Step: 19900  \tTraining loss: 0.49507737159729004\n",
      "Step: 19900  \tTraining accuracy: 0.7348597049713135\n",
      "Step: 19900  \tValid loss: 0.5096741914749146\n",
      "Step: 20000  \tTraining loss: 0.49502474069595337\n",
      "Step: 20000  \tTraining accuracy: 0.7348840236663818\n",
      "Step: 20000  \tValid loss: 0.5095993280410767\n",
      "Step: 20100  \tTraining loss: 0.49497178196907043\n",
      "Step: 20100  \tTraining accuracy: 0.7349066734313965\n",
      "Step: 20100  \tValid loss: 0.509540319442749\n",
      "Step: 20200  \tTraining loss: 0.49492502212524414\n",
      "Step: 20200  \tTraining accuracy: 0.7349305152893066\n",
      "Step: 20200  \tValid loss: 0.5094957947731018\n",
      "Step: 20300  \tTraining loss: 0.4948809742927551\n",
      "Step: 20300  \tTraining accuracy: 0.7349554896354675\n",
      "Step: 20300  \tValid loss: 0.50946444272995\n",
      "Step: 20400  \tTraining loss: 0.4948417842388153\n",
      "Step: 20400  \tTraining accuracy: 0.7349784970283508\n",
      "Step: 20400  \tValid loss: 0.5094309449195862\n",
      "Step: 20500  \tTraining loss: 0.4948037266731262\n",
      "Step: 20500  \tTraining accuracy: 0.735002338886261\n",
      "Step: 20500  \tValid loss: 0.509419322013855\n",
      "Step: 20600  \tTraining loss: 0.49476662278175354\n",
      "Step: 20600  \tTraining accuracy: 0.7350273132324219\n",
      "Step: 20600  \tValid loss: 0.5093871355056763\n",
      "Step: 20700  \tTraining loss: 0.49473047256469727\n",
      "Step: 20700  \tTraining accuracy: 0.7350537776947021\n",
      "Step: 20700  \tValid loss: 0.5093444585800171\n",
      "Step: 20800  \tTraining loss: 0.4946961998939514\n",
      "Step: 20800  \tTraining accuracy: 0.7350799441337585\n",
      "Step: 20800  \tValid loss: 0.5093103647232056\n",
      "Step: 20900  \tTraining loss: 0.49466365575790405\n",
      "Step: 20900  \tTraining accuracy: 0.735105574131012\n",
      "Step: 20900  \tValid loss: 0.5092834830284119\n",
      "Step: 21000  \tTraining loss: 0.4946288466453552\n",
      "Step: 21000  \tTraining accuracy: 0.7351312637329102\n",
      "Step: 21000  \tValid loss: 0.5092398524284363\n",
      "Step: 21100  \tTraining loss: 0.49459078907966614\n",
      "Step: 21100  \tTraining accuracy: 0.7351560592651367\n",
      "Step: 21100  \tValid loss: 0.5091959238052368\n",
      "Step: 21200  \tTraining loss: 0.49455514550209045\n",
      "Step: 21200  \tTraining accuracy: 0.7351792454719543\n",
      "Step: 21200  \tValid loss: 0.509141206741333\n",
      "Step: 21300  \tTraining loss: 0.4945220351219177\n",
      "Step: 21300  \tTraining accuracy: 0.7352026104927063\n",
      "Step: 21300  \tValid loss: 0.5090983510017395\n",
      "Step: 21400  \tTraining loss: 0.4944807291030884\n",
      "Step: 21400  \tTraining accuracy: 0.7352273464202881\n",
      "Step: 21400  \tValid loss: 0.5090978741645813\n",
      "Step: 21500  \tTraining loss: 0.4944417178630829\n",
      "Step: 21500  \tTraining accuracy: 0.7352529168128967\n",
      "Step: 21500  \tValid loss: 0.5090774297714233\n",
      "Step: 21600  \tTraining loss: 0.4944087862968445\n",
      "Step: 21600  \tTraining accuracy: 0.7352798581123352\n",
      "Step: 21600  \tValid loss: 0.5089671611785889\n",
      "Step: 21700  \tTraining loss: 0.4943769872188568\n",
      "Step: 21700  \tTraining accuracy: 0.735307514667511\n",
      "Step: 21700  \tValid loss: 0.5088937878608704\n",
      "Step: 21800  \tTraining loss: 0.49434685707092285\n",
      "Step: 21800  \tTraining accuracy: 0.7353346347808838\n",
      "Step: 21800  \tValid loss: 0.5088456273078918\n",
      "Step: 21900  \tTraining loss: 0.4943173825740814\n",
      "Step: 21900  \tTraining accuracy: 0.7353614568710327\n",
      "Step: 21900  \tValid loss: 0.5087740421295166\n",
      "Step: 22000  \tTraining loss: 0.49428674578666687\n",
      "Step: 22000  \tTraining accuracy: 0.7353877425193787\n",
      "Step: 22000  \tValid loss: 0.5087184309959412\n",
      "Step: 22100  \tTraining loss: 0.49425676465034485\n",
      "Step: 22100  \tTraining accuracy: 0.7354147434234619\n",
      "Step: 22100  \tValid loss: 0.5086515545845032\n",
      "Step: 22200  \tTraining loss: 0.4942280352115631\n",
      "Step: 22200  \tTraining accuracy: 0.7354424595832825\n",
      "Step: 22200  \tValid loss: 0.5085968971252441\n",
      "Step: 22300  \tTraining loss: 0.4941970705986023\n",
      "Step: 22300  \tTraining accuracy: 0.7354699373245239\n",
      "Step: 22300  \tValid loss: 0.5085340738296509\n",
      "Step: 22400  \tTraining loss: 0.49416908621788025\n",
      "Step: 22400  \tTraining accuracy: 0.7354971766471863\n",
      "Step: 22400  \tValid loss: 0.5084946751594543\n",
      "Step: 22500  \tTraining loss: 0.49414098262786865\n",
      "Step: 22500  \tTraining accuracy: 0.7355241775512695\n",
      "Step: 22500  \tValid loss: 0.5084441900253296\n",
      "Step: 22600  \tTraining loss: 0.49411436915397644\n",
      "Step: 22600  \tTraining accuracy: 0.7355515956878662\n",
      "Step: 22600  \tValid loss: 0.508398711681366\n",
      "Step: 22700  \tTraining loss: 0.4940871596336365\n",
      "Step: 22700  \tTraining accuracy: 0.7355796694755554\n",
      "Step: 22700  \tValid loss: 0.5083635449409485\n",
      "Step: 22800  \tTraining loss: 0.49405980110168457\n",
      "Step: 22800  \tTraining accuracy: 0.7356075048446655\n",
      "Step: 22800  \tValid loss: 0.5083008408546448\n",
      "Step: 22900  \tTraining loss: 0.4940337836742401\n",
      "Step: 22900  \tTraining accuracy: 0.7356341481208801\n",
      "Step: 22900  \tValid loss: 0.5082728862762451\n",
      "Step: 23000  \tTraining loss: 0.49400895833969116\n",
      "Step: 23000  \tTraining accuracy: 0.7356624603271484\n",
      "Step: 23000  \tValid loss: 0.5082417130470276\n",
      "Step: 23100  \tTraining loss: 0.4939820170402527\n",
      "Step: 23100  \tTraining accuracy: 0.7356910705566406\n",
      "Step: 23100  \tValid loss: 0.5081712007522583\n",
      "Step: 23200  \tTraining loss: 0.4939574897289276\n",
      "Step: 23200  \tTraining accuracy: 0.7357192039489746\n",
      "Step: 23200  \tValid loss: 0.5081393718719482\n",
      "Step: 23300  \tTraining loss: 0.4939320385456085\n",
      "Step: 23300  \tTraining accuracy: 0.7357458472251892\n",
      "Step: 23300  \tValid loss: 0.5081061720848083\n",
      "Step: 23400  \tTraining loss: 0.4939078986644745\n",
      "Step: 23400  \tTraining accuracy: 0.7357725501060486\n",
      "Step: 23400  \tValid loss: 0.5080749988555908\n",
      "Step: 23500  \tTraining loss: 0.4938831925392151\n",
      "Step: 23500  \tTraining accuracy: 0.7357999682426453\n",
      "Step: 23500  \tValid loss: 0.508023202419281\n",
      "Step: 23600  \tTraining loss: 0.49385836720466614\n",
      "Step: 23600  \tTraining accuracy: 0.7358258962631226\n",
      "Step: 23600  \tValid loss: 0.5079835057258606\n",
      "Step: 23700  \tTraining loss: 0.4938346743583679\n",
      "Step: 23700  \tTraining accuracy: 0.7358516454696655\n",
      "Step: 23700  \tValid loss: 0.5079577565193176\n",
      "Step: 23800  \tTraining loss: 0.4938110411167145\n",
      "Step: 23800  \tTraining accuracy: 0.7358771562576294\n",
      "Step: 23800  \tValid loss: 0.5079173445701599\n",
      "Step: 23900  \tTraining loss: 0.4937870502471924\n",
      "Step: 23900  \tTraining accuracy: 0.7359024882316589\n",
      "Step: 23900  \tValid loss: 0.5078792572021484\n",
      "Step: 24000  \tTraining loss: 0.4937645494937897\n",
      "Step: 24000  \tTraining accuracy: 0.7359275817871094\n",
      "Step: 24000  \tValid loss: 0.5078192353248596\n",
      "Step: 24100  \tTraining loss: 0.493739515542984\n",
      "Step: 24100  \tTraining accuracy: 0.7359524369239807\n",
      "Step: 24100  \tValid loss: 0.507787823677063\n",
      "Step: 24200  \tTraining loss: 0.4937163293361664\n",
      "Step: 24200  \tTraining accuracy: 0.7359771132469177\n",
      "Step: 24200  \tValid loss: 0.5077484250068665\n",
      "Step: 24300  \tTraining loss: 0.4936942756175995\n",
      "Step: 24300  \tTraining accuracy: 0.7360022068023682\n",
      "Step: 24300  \tValid loss: 0.5077200531959534\n",
      "Step: 24400  \tTraining loss: 0.4936704933643341\n",
      "Step: 24400  \tTraining accuracy: 0.7360284924507141\n",
      "Step: 24400  \tValid loss: 0.507676362991333\n",
      "Step: 24500  \tTraining loss: 0.49364763498306274\n",
      "Step: 24500  \tTraining accuracy: 0.7360554933547974\n",
      "Step: 24500  \tValid loss: 0.5076345801353455\n",
      "Step: 24600  \tTraining loss: 0.4936249852180481\n",
      "Step: 24600  \tTraining accuracy: 0.7360821962356567\n",
      "Step: 24600  \tValid loss: 0.5076006650924683\n",
      "Step: 24700  \tTraining loss: 0.4936036169528961\n",
      "Step: 24700  \tTraining accuracy: 0.7361087799072266\n",
      "Step: 24700  \tValid loss: 0.5075470805168152\n",
      "Step: 24800  \tTraining loss: 0.49358072876930237\n",
      "Step: 24800  \tTraining accuracy: 0.7361350655555725\n",
      "Step: 24800  \tValid loss: 0.5075260400772095\n",
      "Step: 24900  \tTraining loss: 0.4935578405857086\n",
      "Step: 24900  \tTraining accuracy: 0.736160933971405\n",
      "Step: 24900  \tValid loss: 0.507481575012207\n",
      "Step: 25000  \tTraining loss: 0.4935392737388611\n",
      "Step: 25000  \tTraining accuracy: 0.7361853718757629\n",
      "Step: 25000  \tValid loss: 0.5074576139450073\n",
      "Step: 25100  \tTraining loss: 0.49351420998573303\n",
      "Step: 25100  \tTraining accuracy: 0.7362096905708313\n",
      "Step: 25100  \tValid loss: 0.5073959827423096\n",
      "Step: 25200  \tTraining loss: 0.49349305033683777\n",
      "Step: 25200  \tTraining accuracy: 0.7362331748008728\n",
      "Step: 25200  \tValid loss: 0.507379949092865\n",
      "Step: 25300  \tTraining loss: 0.49346980452537537\n",
      "Step: 25300  \tTraining accuracy: 0.7362557053565979\n",
      "Step: 25300  \tValid loss: 0.5073296427726746\n",
      "Step: 25400  \tTraining loss: 0.49344831705093384\n",
      "Step: 25400  \tTraining accuracy: 0.7362779974937439\n",
      "Step: 25400  \tValid loss: 0.5072688460350037\n",
      "Step: 25500  \tTraining loss: 0.49342599511146545\n",
      "Step: 25500  \tTraining accuracy: 0.7362995743751526\n",
      "Step: 25500  \tValid loss: 0.5072575807571411\n",
      "Step: 25600  \tTraining loss: 0.493404746055603\n",
      "Step: 25600  \tTraining accuracy: 0.7363207340240479\n",
      "Step: 25600  \tValid loss: 0.5072163343429565\n",
      "Step: 25700  \tTraining loss: 0.49338212609291077\n",
      "Step: 25700  \tTraining accuracy: 0.7363436222076416\n",
      "Step: 25700  \tValid loss: 0.5071646571159363\n",
      "Step: 25800  \tTraining loss: 0.49335622787475586\n",
      "Step: 25800  \tTraining accuracy: 0.7363666296005249\n",
      "Step: 25800  \tValid loss: 0.5071278214454651\n",
      "Step: 25900  \tTraining loss: 0.493333101272583\n",
      "Step: 25900  \tTraining accuracy: 0.736388623714447\n",
      "Step: 25900  \tValid loss: 0.5070911049842834\n",
      "Step: 26000  \tTraining loss: 0.4933083951473236\n",
      "Step: 26000  \tTraining accuracy: 0.7364112734794617\n",
      "Step: 26000  \tValid loss: 0.5070594549179077\n",
      "Step: 26100  \tTraining loss: 0.493285596370697\n",
      "Step: 26100  \tTraining accuracy: 0.7364351153373718\n",
      "Step: 26100  \tValid loss: 0.5070240497589111\n",
      "Step: 26200  \tTraining loss: 0.49326103925704956\n",
      "Step: 26200  \tTraining accuracy: 0.7364580035209656\n",
      "Step: 26200  \tValid loss: 0.5069646835327148\n",
      "Step: 26300  \tTraining loss: 0.4932381510734558\n",
      "Step: 26300  \tTraining accuracy: 0.7364811897277832\n",
      "Step: 26300  \tValid loss: 0.5069409012794495\n",
      "Step: 26400  \tTraining loss: 0.49321430921554565\n",
      "Step: 26400  \tTraining accuracy: 0.7365034222602844\n",
      "Step: 26400  \tValid loss: 0.5068960785865784\n",
      "Step: 26500  \tTraining loss: 0.4931897521018982\n",
      "Step: 26500  \tTraining accuracy: 0.7365263104438782\n",
      "Step: 26500  \tValid loss: 0.5068528652191162\n",
      "Step: 26600  \tTraining loss: 0.4931657910346985\n",
      "Step: 26600  \tTraining accuracy: 0.7365500926971436\n",
      "Step: 26600  \tValid loss: 0.5068241357803345\n",
      "Step: 26700  \tTraining loss: 0.4931431710720062\n",
      "Step: 26700  \tTraining accuracy: 0.7365755438804626\n",
      "Step: 26700  \tValid loss: 0.5067864656448364\n",
      "Step: 26800  \tTraining loss: 0.493120014667511\n",
      "Step: 26800  \tTraining accuracy: 0.7366008162498474\n",
      "Step: 26800  \tValid loss: 0.5067294239997864\n",
      "Step: 26900  \tTraining loss: 0.49309664964675903\n",
      "Step: 26900  \tTraining accuracy: 0.7366258502006531\n",
      "Step: 26900  \tValid loss: 0.5066853165626526\n",
      "Step: 27000  \tTraining loss: 0.49307408928871155\n",
      "Step: 27000  \tTraining accuracy: 0.7366507649421692\n",
      "Step: 27000  \tValid loss: 0.5066515803337097\n",
      "Step: 27100  \tTraining loss: 0.49305054545402527\n",
      "Step: 27100  \tTraining accuracy: 0.7366746664047241\n",
      "Step: 27100  \tValid loss: 0.5065997838973999\n",
      "Step: 27200  \tTraining loss: 0.4930277466773987\n",
      "Step: 27200  \tTraining accuracy: 0.7366986870765686\n",
      "Step: 27200  \tValid loss: 0.5065479278564453\n",
      "Step: 27300  \tTraining loss: 0.49299776554107666\n",
      "Step: 27300  \tTraining accuracy: 0.7367240786552429\n",
      "Step: 27300  \tValid loss: 0.5065077543258667\n",
      "Step: 27400  \tTraining loss: 0.4929681718349457\n",
      "Step: 27400  \tTraining accuracy: 0.7367529273033142\n",
      "Step: 27400  \tValid loss: 0.506419837474823\n",
      "Step: 27500  \tTraining loss: 0.4929421544075012\n",
      "Step: 27500  \tTraining accuracy: 0.7367805242538452\n",
      "Step: 27500  \tValid loss: 0.5063711404800415\n",
      "Step: 27600  \tTraining loss: 0.49291470646858215\n",
      "Step: 27600  \tTraining accuracy: 0.7368061542510986\n",
      "Step: 27600  \tValid loss: 0.5063307285308838\n",
      "Step: 27700  \tTraining loss: 0.4928854703903198\n",
      "Step: 27700  \tTraining accuracy: 0.736831545829773\n",
      "Step: 27700  \tValid loss: 0.5063069462776184\n",
      "Step: 27800  \tTraining loss: 0.492855966091156\n",
      "Step: 27800  \tTraining accuracy: 0.73685622215271\n",
      "Step: 27800  \tValid loss: 0.5062735080718994\n",
      "Step: 27900  \tTraining loss: 0.4928278625011444\n",
      "Step: 27900  \tTraining accuracy: 0.7368807792663574\n",
      "Step: 27900  \tValid loss: 0.5062304735183716\n",
      "Step: 28000  \tTraining loss: 0.49280285835266113\n",
      "Step: 28000  \tTraining accuracy: 0.7369046211242676\n",
      "Step: 28000  \tValid loss: 0.5062002539634705\n",
      "Step: 28100  \tTraining loss: 0.492777556180954\n",
      "Step: 28100  \tTraining accuracy: 0.7369275689125061\n",
      "Step: 28100  \tValid loss: 0.5061666369438171\n",
      "Step: 28200  \tTraining loss: 0.4927521049976349\n",
      "Step: 28200  \tTraining accuracy: 0.7369508147239685\n",
      "Step: 28200  \tValid loss: 0.5061284303665161\n",
      "Step: 28300  \tTraining loss: 0.49273014068603516\n",
      "Step: 28300  \tTraining accuracy: 0.7369751930236816\n",
      "Step: 28300  \tValid loss: 0.5060945749282837\n",
      "Step: 28400  \tTraining loss: 0.4927043616771698\n",
      "Step: 28400  \tTraining accuracy: 0.7369998693466187\n",
      "Step: 28400  \tValid loss: 0.5060537457466125\n",
      "Step: 28500  \tTraining loss: 0.4926789402961731\n",
      "Step: 28500  \tTraining accuracy: 0.7370251417160034\n",
      "Step: 28500  \tValid loss: 0.5060136914253235\n",
      "Step: 28600  \tTraining loss: 0.492654412984848\n",
      "Step: 28600  \tTraining accuracy: 0.7370494604110718\n",
      "Step: 28600  \tValid loss: 0.5059828162193298\n",
      "Step: 28700  \tTraining loss: 0.4926294684410095\n",
      "Step: 28700  \tTraining accuracy: 0.7370719313621521\n",
      "Step: 28700  \tValid loss: 0.5059115886688232\n",
      "Step: 28800  \tTraining loss: 0.49260595440864563\n",
      "Step: 28800  \tTraining accuracy: 0.7370944619178772\n",
      "Step: 28800  \tValid loss: 0.5058833360671997\n",
      "Step: 28900  \tTraining loss: 0.49258115887641907\n",
      "Step: 28900  \tTraining accuracy: 0.7371160387992859\n",
      "Step: 28900  \tValid loss: 0.5058321356773376\n",
      "Step: 29000  \tTraining loss: 0.4925573468208313\n",
      "Step: 29000  \tTraining accuracy: 0.7371370792388916\n",
      "Step: 29000  \tValid loss: 0.5057792663574219\n",
      "Step: 29100  \tTraining loss: 0.492533415555954\n",
      "Step: 29100  \tTraining accuracy: 0.7371576428413391\n",
      "Step: 29100  \tValid loss: 0.5057192444801331\n",
      "Step: 29200  \tTraining loss: 0.4925105571746826\n",
      "Step: 29200  \tTraining accuracy: 0.7371788620948792\n",
      "Step: 29200  \tValid loss: 0.5056816339492798\n",
      "Step: 29300  \tTraining loss: 0.4924866557121277\n",
      "Step: 29300  \tTraining accuracy: 0.7371994256973267\n",
      "Step: 29300  \tValid loss: 0.5056129097938538\n",
      "Step: 29400  \tTraining loss: 0.4924638271331787\n",
      "Step: 29400  \tTraining accuracy: 0.7372190952301025\n",
      "Step: 29400  \tValid loss: 0.5055662989616394\n",
      "Step: 29500  \tTraining loss: 0.4924379885196686\n",
      "Step: 29500  \tTraining accuracy: 0.7372391223907471\n",
      "Step: 29500  \tValid loss: 0.5055285692214966\n",
      "Step: 29600  \tTraining loss: 0.4924163222312927\n",
      "Step: 29600  \tTraining accuracy: 0.7372597455978394\n",
      "Step: 29600  \tValid loss: 0.5054994821548462\n",
      "Step: 29700  \tTraining loss: 0.49239182472229004\n",
      "Step: 29700  \tTraining accuracy: 0.7372797727584839\n",
      "Step: 29700  \tValid loss: 0.5054366588592529\n",
      "Step: 29800  \tTraining loss: 0.49236899614334106\n",
      "Step: 29800  \tTraining accuracy: 0.7372989058494568\n",
      "Step: 29800  \tValid loss: 0.5054072737693787\n",
      "Step: 29900  \tTraining loss: 0.49234604835510254\n",
      "Step: 29900  \tTraining accuracy: 0.7373191714286804\n",
      "Step: 29900  \tValid loss: 0.5053678750991821\n",
      "Step: 30000  \tTraining loss: 0.4923221170902252\n",
      "Step: 30000  \tTraining accuracy: 0.7373392581939697\n",
      "Step: 30000  \tValid loss: 0.5052903890609741\n",
      "Step: 30100  \tTraining loss: 0.49229952692985535\n",
      "Step: 30100  \tTraining accuracy: 0.7373586893081665\n",
      "Step: 30100  \tValid loss: 0.5052754878997803\n",
      "Step: 30200  \tTraining loss: 0.4922764301300049\n",
      "Step: 30200  \tTraining accuracy: 0.7373785376548767\n",
      "Step: 30200  \tValid loss: 0.5052211284637451\n",
      "Step: 30300  \tTraining loss: 0.4922550618648529\n",
      "Step: 30300  \tTraining accuracy: 0.7373982071876526\n",
      "Step: 30300  \tValid loss: 0.5051974654197693\n",
      "Step: 30400  \tTraining loss: 0.4922298192977905\n",
      "Step: 30400  \tTraining accuracy: 0.7374177575111389\n",
      "Step: 30400  \tValid loss: 0.5051313638687134\n",
      "Step: 30500  \tTraining loss: 0.49220672249794006\n",
      "Step: 30500  \tTraining accuracy: 0.7374371886253357\n",
      "Step: 30500  \tValid loss: 0.5050857663154602\n",
      "Step: 30600  \tTraining loss: 0.49218451976776123\n",
      "Step: 30600  \tTraining accuracy: 0.7374565005302429\n",
      "Step: 30600  \tValid loss: 0.5050505995750427\n",
      "Step: 30700  \tTraining loss: 0.4921620488166809\n",
      "Step: 30700  \tTraining accuracy: 0.7374756932258606\n",
      "Step: 30700  \tValid loss: 0.5049839615821838\n",
      "Step: 30800  \tTraining loss: 0.49214065074920654\n",
      "Step: 30800  \tTraining accuracy: 0.7374947667121887\n",
      "Step: 30800  \tValid loss: 0.5049359202384949\n",
      "Step: 30900  \tTraining loss: 0.49211740493774414\n",
      "Step: 30900  \tTraining accuracy: 0.7375136613845825\n",
      "Step: 30900  \tValid loss: 0.5049079060554504\n",
      "Step: 31000  \tTraining loss: 0.4920938313007355\n",
      "Step: 31000  \tTraining accuracy: 0.737531304359436\n",
      "Step: 31000  \tValid loss: 0.5048776865005493\n",
      "Step: 31100  \tTraining loss: 0.49207085371017456\n",
      "Step: 31100  \tTraining accuracy: 0.737547755241394\n",
      "Step: 31100  \tValid loss: 0.5048385262489319\n",
      "Step: 31200  \tTraining loss: 0.4920486807823181\n",
      "Step: 31200  \tTraining accuracy: 0.7375640273094177\n",
      "Step: 31200  \tValid loss: 0.5048003196716309\n",
      "Step: 31300  \tTraining loss: 0.4920249581336975\n",
      "Step: 31300  \tTraining accuracy: 0.7375802397727966\n",
      "Step: 31300  \tValid loss: 0.5047658681869507\n",
      "Step: 31400  \tTraining loss: 0.4920029640197754\n",
      "Step: 31400  \tTraining accuracy: 0.7375972270965576\n",
      "Step: 31400  \tValid loss: 0.5047239065170288\n",
      "Step: 31500  \tTraining loss: 0.4919792711734772\n",
      "Step: 31500  \tTraining accuracy: 0.7376143336296082\n",
      "Step: 31500  \tValid loss: 0.5046769380569458\n",
      "Step: 31600  \tTraining loss: 0.49195849895477295\n",
      "Step: 31600  \tTraining accuracy: 0.7376313209533691\n",
      "Step: 31600  \tValid loss: 0.5046390891075134\n",
      "Step: 31700  \tTraining loss: 0.4919354319572449\n",
      "Step: 31700  \tTraining accuracy: 0.7376486659049988\n",
      "Step: 31700  \tValid loss: 0.5046084523200989\n",
      "Step: 31800  \tTraining loss: 0.4919123351573944\n",
      "Step: 31800  \tTraining accuracy: 0.7376665472984314\n",
      "Step: 31800  \tValid loss: 0.5045679211616516\n",
      "Step: 31900  \tTraining loss: 0.49188926815986633\n",
      "Step: 31900  \tTraining accuracy: 0.7376832365989685\n",
      "Step: 31900  \tValid loss: 0.5045253038406372\n",
      "Step: 32000  \tTraining loss: 0.4918653964996338\n",
      "Step: 32000  \tTraining accuracy: 0.7377007007598877\n",
      "Step: 32000  \tValid loss: 0.5044977068901062\n",
      "Step: 32100  \tTraining loss: 0.4918416738510132\n",
      "Step: 32100  \tTraining accuracy: 0.7377194166183472\n",
      "Step: 32100  \tValid loss: 0.5044466257095337\n",
      "Step: 32200  \tTraining loss: 0.49181845784187317\n",
      "Step: 32200  \tTraining accuracy: 0.7377379536628723\n",
      "Step: 32200  \tValid loss: 0.5044063329696655\n",
      "Step: 32300  \tTraining loss: 0.4917963147163391\n",
      "Step: 32300  \tTraining accuracy: 0.7377564311027527\n",
      "Step: 32300  \tValid loss: 0.5043696761131287\n",
      "Step: 32400  \tTraining loss: 0.4917728900909424\n",
      "Step: 32400  \tTraining accuracy: 0.7377752065658569\n",
      "Step: 32400  \tValid loss: 0.5043491721153259\n",
      "Step: 32500  \tTraining loss: 0.4917490482330322\n",
      "Step: 32500  \tTraining accuracy: 0.7377945184707642\n",
      "Step: 32500  \tValid loss: 0.5043075084686279\n",
      "Step: 32600  \tTraining loss: 0.49172550439834595\n",
      "Step: 32600  \tTraining accuracy: 0.7378137111663818\n",
      "Step: 32600  \tValid loss: 0.5042889714241028\n",
      "Step: 32700  \tTraining loss: 0.49170321226119995\n",
      "Step: 32700  \tTraining accuracy: 0.7378328442573547\n",
      "Step: 32700  \tValid loss: 0.5042641162872314\n",
      "Step: 32800  \tTraining loss: 0.4916799068450928\n",
      "Step: 32800  \tTraining accuracy: 0.7378517985343933\n",
      "Step: 32800  \tValid loss: 0.5042325854301453\n",
      "Step: 32900  \tTraining loss: 0.4916566014289856\n",
      "Step: 32900  \tTraining accuracy: 0.7378706336021423\n",
      "Step: 32900  \tValid loss: 0.504189133644104\n",
      "Step: 33000  \tTraining loss: 0.49163299798965454\n",
      "Step: 33000  \tTraining accuracy: 0.7378889322280884\n",
      "Step: 33000  \tValid loss: 0.5041661858558655\n",
      "Step: 33100  \tTraining loss: 0.49160313606262207\n",
      "Step: 33100  \tTraining accuracy: 0.7379058599472046\n",
      "Step: 33100  \tValid loss: 0.5041322112083435\n",
      "Step: 33200  \tTraining loss: 0.49156448245048523\n",
      "Step: 33200  \tTraining accuracy: 0.7379237413406372\n",
      "Step: 33200  \tValid loss: 0.504083514213562\n",
      "Step: 33300  \tTraining loss: 0.49153944849967957\n",
      "Step: 33300  \tTraining accuracy: 0.7379423379898071\n",
      "Step: 33300  \tValid loss: 0.5040276646614075\n",
      "Step: 33400  \tTraining loss: 0.4915148615837097\n",
      "Step: 33400  \tTraining accuracy: 0.7379602193832397\n",
      "Step: 33400  \tValid loss: 0.5040088295936584\n",
      "Step: 33500  \tTraining loss: 0.4914855360984802\n",
      "Step: 33500  \tTraining accuracy: 0.7379773855209351\n",
      "Step: 33500  \tValid loss: 0.5039432048797607\n",
      "Step: 33600  \tTraining loss: 0.4914189577102661\n",
      "Step: 33600  \tTraining accuracy: 0.7379950284957886\n",
      "Step: 33600  \tValid loss: 0.5038549900054932\n",
      "Step: 33700  \tTraining loss: 0.49136847257614136\n",
      "Step: 33700  \tTraining accuracy: 0.7380123734474182\n",
      "Step: 33700  \tValid loss: 0.503818154335022\n",
      "Step: 33800  \tTraining loss: 0.49119850993156433\n",
      "Step: 33800  \tTraining accuracy: 0.7380309104919434\n",
      "Step: 33800  \tValid loss: 0.5039731860160828\n",
      "Step: 33900  \tTraining loss: 0.49110665917396545\n",
      "Step: 33900  \tTraining accuracy: 0.738048255443573\n",
      "Step: 33900  \tValid loss: 0.5039710402488708\n",
      "Step: 34000  \tTraining loss: 0.49104127287864685\n",
      "Step: 34000  \tTraining accuracy: 0.7380657196044922\n",
      "Step: 34000  \tValid loss: 0.5039312243461609\n",
      "Step: 34100  \tTraining loss: 0.49098125100135803\n",
      "Step: 34100  \tTraining accuracy: 0.738088071346283\n",
      "Step: 34100  \tValid loss: 0.5038776397705078\n",
      "Step: 34200  \tTraining loss: 0.49092987179756165\n",
      "Step: 34200  \tTraining accuracy: 0.7381119728088379\n",
      "Step: 34200  \tValid loss: 0.5038489103317261\n",
      "Step: 34300  \tTraining loss: 0.49088460206985474\n",
      "Step: 34300  \tTraining accuracy: 0.7381342649459839\n",
      "Step: 34300  \tValid loss: 0.5038257837295532\n",
      "Step: 34400  \tTraining loss: 0.49084731936454773\n",
      "Step: 34400  \tTraining accuracy: 0.7381566166877747\n",
      "Step: 34400  \tValid loss: 0.5037931799888611\n",
      "Step: 34500  \tTraining loss: 0.49081358313560486\n",
      "Step: 34500  \tTraining accuracy: 0.738179087638855\n",
      "Step: 34500  \tValid loss: 0.5037845373153687\n",
      "Step: 34600  \tTraining loss: 0.49078044295310974\n",
      "Step: 34600  \tTraining accuracy: 0.7382028698921204\n",
      "Step: 34600  \tValid loss: 0.5037482380867004\n",
      "Step: 34700  \tTraining loss: 0.4907482862472534\n",
      "Step: 34700  \tTraining accuracy: 0.7382277250289917\n",
      "Step: 34700  \tValid loss: 0.5037335157394409\n",
      "Step: 34800  \tTraining loss: 0.49071571230888367\n",
      "Step: 34800  \tTraining accuracy: 0.7382524609565735\n",
      "Step: 34800  \tValid loss: 0.5037007927894592\n",
      "Step: 34900  \tTraining loss: 0.49068450927734375\n",
      "Step: 34900  \tTraining accuracy: 0.7382776737213135\n",
      "Step: 34900  \tValid loss: 0.5036906003952026\n",
      "Step: 35000  \tTraining loss: 0.49065443873405457\n",
      "Step: 35000  \tTraining accuracy: 0.7383027076721191\n",
      "Step: 35000  \tValid loss: 0.5036444067955017\n",
      "Step: 35100  \tTraining loss: 0.49062657356262207\n",
      "Step: 35100  \tTraining accuracy: 0.7383272051811218\n",
      "Step: 35100  \tValid loss: 0.5036318302154541\n",
      "Step: 35200  \tTraining loss: 0.4905990660190582\n",
      "Step: 35200  \tTraining accuracy: 0.7383515238761902\n",
      "Step: 35200  \tValid loss: 0.5036171078681946\n",
      "Step: 35300  \tTraining loss: 0.4905676543712616\n",
      "Step: 35300  \tTraining accuracy: 0.7383759617805481\n",
      "Step: 35300  \tValid loss: 0.5035704970359802\n",
      "Step: 35400  \tTraining loss: 0.49053892493247986\n",
      "Step: 35400  \tTraining accuracy: 0.7384010553359985\n",
      "Step: 35400  \tValid loss: 0.5035458207130432\n",
      "Step: 35500  \tTraining loss: 0.49051058292388916\n",
      "Step: 35500  \tTraining accuracy: 0.7384269833564758\n",
      "Step: 35500  \tValid loss: 0.5035527944564819\n",
      "Step: 35600  \tTraining loss: 0.49048349261283875\n",
      "Step: 35600  \tTraining accuracy: 0.7384533882141113\n",
      "Step: 35600  \tValid loss: 0.5035108327865601\n",
      "Step: 35700  \tTraining loss: 0.49045711755752563\n",
      "Step: 35700  \tTraining accuracy: 0.7384800910949707\n",
      "Step: 35700  \tValid loss: 0.5035037398338318\n",
      "Step: 35800  \tTraining loss: 0.49042990803718567\n",
      "Step: 35800  \tTraining accuracy: 0.738506555557251\n",
      "Step: 35800  \tValid loss: 0.5034903287887573\n",
      "Step: 35900  \tTraining loss: 0.4904039204120636\n",
      "Step: 35900  \tTraining accuracy: 0.738532543182373\n",
      "Step: 35900  \tValid loss: 0.5034928917884827\n",
      "Step: 36000  \tTraining loss: 0.4903780519962311\n",
      "Step: 36000  \tTraining accuracy: 0.738557755947113\n",
      "Step: 36000  \tValid loss: 0.5034650564193726\n",
      "Step: 36100  \tTraining loss: 0.4903532564640045\n",
      "Step: 36100  \tTraining accuracy: 0.7385826706886292\n",
      "Step: 36100  \tValid loss: 0.5034466981887817\n",
      "Step: 36200  \tTraining loss: 0.4903278350830078\n",
      "Step: 36200  \tTraining accuracy: 0.7386075854301453\n",
      "Step: 36200  \tValid loss: 0.5034398436546326\n",
      "Step: 36300  \tTraining loss: 0.4903026819229126\n",
      "Step: 36300  \tTraining accuracy: 0.7386323809623718\n",
      "Step: 36300  \tValid loss: 0.5034030675888062\n",
      "Step: 36400  \tTraining loss: 0.4902781546115875\n",
      "Step: 36400  \tTraining accuracy: 0.7386570572853088\n",
      "Step: 36400  \tValid loss: 0.5033963918685913\n",
      "Step: 36500  \tTraining loss: 0.4902545213699341\n",
      "Step: 36500  \tTraining accuracy: 0.7386811971664429\n",
      "Step: 36500  \tValid loss: 0.5033720135688782\n",
      "Step: 36600  \tTraining loss: 0.49023061990737915\n",
      "Step: 36600  \tTraining accuracy: 0.7387046217918396\n",
      "Step: 36600  \tValid loss: 0.5033617615699768\n",
      "Step: 36700  \tTraining loss: 0.4902062714099884\n",
      "Step: 36700  \tTraining accuracy: 0.7387279272079468\n",
      "Step: 36700  \tValid loss: 0.5033378005027771\n",
      "Step: 36800  \tTraining loss: 0.49018344283103943\n",
      "Step: 36800  \tTraining accuracy: 0.7387511134147644\n",
      "Step: 36800  \tValid loss: 0.5033190846443176\n",
      "Step: 36900  \tTraining loss: 0.49015989899635315\n",
      "Step: 36900  \tTraining accuracy: 0.7387741804122925\n",
      "Step: 36900  \tValid loss: 0.5033181309700012\n",
      "Step: 37000  \tTraining loss: 0.49013760685920715\n",
      "Step: 37000  \tTraining accuracy: 0.7387967109680176\n",
      "Step: 37000  \tValid loss: 0.5033150315284729\n",
      "Step: 37100  \tTraining loss: 0.490115761756897\n",
      "Step: 37100  \tTraining accuracy: 0.7388189435005188\n",
      "Step: 37100  \tValid loss: 0.5032793879508972\n",
      "Step: 37200  \tTraining loss: 0.490093857049942\n",
      "Step: 37200  \tTraining accuracy: 0.7388412356376648\n",
      "Step: 37200  \tValid loss: 0.5032821893692017\n",
      "Step: 37300  \tTraining loss: 0.49007225036621094\n",
      "Step: 37300  \tTraining accuracy: 0.7388620972633362\n",
      "Step: 37300  \tValid loss: 0.5032594799995422\n",
      "Step: 37400  \tTraining loss: 0.4900502860546112\n",
      "Step: 37400  \tTraining accuracy: 0.7388822436332703\n",
      "Step: 37400  \tValid loss: 0.5032581090927124\n",
      "Step: 37500  \tTraining loss: 0.49002885818481445\n",
      "Step: 37500  \tTraining accuracy: 0.7389009594917297\n",
      "Step: 37500  \tValid loss: 0.5032484531402588\n",
      "Step: 37600  \tTraining loss: 0.4900088310241699\n",
      "Step: 37600  \tTraining accuracy: 0.738917887210846\n",
      "Step: 37600  \tValid loss: 0.5032346248626709\n",
      "Step: 37700  \tTraining loss: 0.4899865686893463\n",
      "Step: 37700  \tTraining accuracy: 0.7389332056045532\n",
      "Step: 37700  \tValid loss: 0.5032193660736084\n",
      "Step: 37800  \tTraining loss: 0.4899658262729645\n",
      "Step: 37800  \tTraining accuracy: 0.7389482259750366\n",
      "Step: 37800  \tValid loss: 0.5032088756561279\n",
      "Step: 37900  \tTraining loss: 0.48994553089141846\n",
      "Step: 37900  \tTraining accuracy: 0.7389631867408752\n",
      "Step: 37900  \tValid loss: 0.5031949281692505\n",
      "Step: 38000  \tTraining loss: 0.48992469906806946\n",
      "Step: 38000  \tTraining accuracy: 0.7389788627624512\n",
      "Step: 38000  \tValid loss: 0.5031906366348267\n",
      "Step: 38100  \tTraining loss: 0.48990458250045776\n",
      "Step: 38100  \tTraining accuracy: 0.7389949560165405\n",
      "Step: 38100  \tValid loss: 0.503166913986206\n",
      "Step: 38200  \tTraining loss: 0.4898844361305237\n",
      "Step: 38200  \tTraining accuracy: 0.7390109896659851\n",
      "Step: 38200  \tValid loss: 0.5031720399856567\n",
      "Step: 38300  \tTraining loss: 0.48986443877220154\n",
      "Step: 38300  \tTraining accuracy: 0.7390269637107849\n",
      "Step: 38300  \tValid loss: 0.5031599402427673\n",
      "Step: 38400  \tTraining loss: 0.48984387516975403\n",
      "Step: 38400  \tTraining accuracy: 0.7390433549880981\n",
      "Step: 38400  \tValid loss: 0.5031731128692627\n",
      "Step: 38500  \tTraining loss: 0.48982366919517517\n",
      "Step: 38500  \tTraining accuracy: 0.73906010389328\n",
      "Step: 38500  \tValid loss: 0.5031379461288452\n",
      "Step: 38600  \tTraining loss: 0.4898037612438202\n",
      "Step: 38600  \tTraining accuracy: 0.7390774488449097\n",
      "Step: 38600  \tValid loss: 0.5031439065933228\n",
      "Step: 38700  \tTraining loss: 0.4897836744785309\n",
      "Step: 38700  \tTraining accuracy: 0.7390936017036438\n",
      "Step: 38700  \tValid loss: 0.503122091293335\n",
      "Step: 38800  \tTraining loss: 0.48976290225982666\n",
      "Step: 38800  \tTraining accuracy: 0.7391091585159302\n",
      "Step: 38800  \tValid loss: 0.5031133890151978\n",
      "Step: 38900  \tTraining loss: 0.48974308371543884\n",
      "Step: 38900  \tTraining accuracy: 0.739124596118927\n",
      "Step: 38900  \tValid loss: 0.5031139850616455\n",
      "Step: 39000  \tTraining loss: 0.4897237718105316\n",
      "Step: 39000  \tTraining accuracy: 0.7391397953033447\n",
      "Step: 39000  \tValid loss: 0.50310218334198\n",
      "Step: 39100  \tTraining loss: 0.4897051751613617\n",
      "Step: 39100  \tTraining accuracy: 0.7391550540924072\n",
      "Step: 39100  \tValid loss: 0.5030970573425293\n",
      "Step: 39200  \tTraining loss: 0.4896872639656067\n",
      "Step: 39200  \tTraining accuracy: 0.7391708493232727\n",
      "Step: 39200  \tValid loss: 0.5030949711799622\n",
      "Step: 39300  \tTraining loss: 0.4896485209465027\n",
      "Step: 39300  \tTraining accuracy: 0.7391875982284546\n",
      "Step: 39300  \tValid loss: 0.5032103657722473\n",
      "Step: 39400  \tTraining loss: 0.4896254539489746\n",
      "Step: 39400  \tTraining accuracy: 0.7392041087150574\n",
      "Step: 39400  \tValid loss: 0.5031864047050476\n",
      "Step: 39500  \tTraining loss: 0.489606648683548\n",
      "Step: 39500  \tTraining accuracy: 0.739219069480896\n",
      "Step: 39500  \tValid loss: 0.5032034516334534\n",
      "Step: 39600  \tTraining loss: 0.4895799458026886\n",
      "Step: 39600  \tTraining accuracy: 0.7392330765724182\n",
      "Step: 39600  \tValid loss: 0.5032435655593872\n",
      "Step: 39700  \tTraining loss: 0.48955750465393066\n",
      "Step: 39700  \tTraining accuracy: 0.739246129989624\n",
      "Step: 39700  \tValid loss: 0.5032436847686768\n",
      "Step: 39800  \tTraining loss: 0.48953497409820557\n",
      "Step: 39800  \tTraining accuracy: 0.7392585277557373\n",
      "Step: 39800  \tValid loss: 0.5032758712768555\n",
      "Step: 39900  \tTraining loss: 0.48951560258865356\n",
      "Step: 39900  \tTraining accuracy: 0.7392714619636536\n",
      "Step: 39900  \tValid loss: 0.503264307975769\n",
      "Step: 40000  \tTraining loss: 0.48949572443962097\n",
      "Step: 40000  \tTraining accuracy: 0.7392828464508057\n",
      "Step: 40000  \tValid loss: 0.5032771825790405\n",
      "Step: 40100  \tTraining loss: 0.48947715759277344\n",
      "Step: 40100  \tTraining accuracy: 0.7392945885658264\n",
      "Step: 40100  \tValid loss: 0.5032987594604492\n",
      "Step: 40200  \tTraining loss: 0.4894563555717468\n",
      "Step: 40200  \tTraining accuracy: 0.7393067479133606\n",
      "Step: 40200  \tValid loss: 0.5033010244369507\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.73931855\n",
      "Precision: 0.7513277\n",
      "Recall: 0.72157216\n",
      "F1 score: 0.7243828\n",
      "AUC: 0.75527173\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.739319   0.751328  0.721572  0.724383  0.755272  0.489456      0.739296   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.503061       0.739294   0.545693      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  40207.0  \n",
      "26\n",
      "(4350, 8)\n",
      "(4350, 1)\n",
      "(2320, 8)\n",
      "(2320, 1)\n",
      "(1885, 8)\n",
      "(1885, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6280474662780762\n",
      "Step: 100  \tTraining accuracy: 0.6200000047683716\n",
      "Step: 100  \tValid loss: 0.6246702671051025\n",
      "Step: 200  \tTraining loss: 0.5700405240058899\n",
      "Step: 200  \tTraining accuracy: 0.6528477072715759\n",
      "Step: 200  \tValid loss: 0.5691547393798828\n",
      "Step: 300  \tTraining loss: 0.5391561388969421\n",
      "Step: 300  \tTraining accuracy: 0.6762348413467407\n",
      "Step: 300  \tValid loss: 0.5409895777702332\n",
      "Step: 400  \tTraining loss: 0.5179102420806885\n",
      "Step: 400  \tTraining accuracy: 0.6912543773651123\n",
      "Step: 400  \tValid loss: 0.52158123254776\n",
      "Step: 500  \tTraining loss: 0.5021141767501831\n",
      "Step: 500  \tTraining accuracy: 0.7018148899078369\n",
      "Step: 500  \tValid loss: 0.5040830969810486\n",
      "Step: 600  \tTraining loss: 0.4907523989677429\n",
      "Step: 600  \tTraining accuracy: 0.7106419205665588\n",
      "Step: 600  \tValid loss: 0.4936513900756836\n",
      "Step: 700  \tTraining loss: 0.48221129179000854\n",
      "Step: 700  \tTraining accuracy: 0.718803882598877\n",
      "Step: 700  \tValid loss: 0.48744839429855347\n",
      "Step: 800  \tTraining loss: 0.47603553533554077\n",
      "Step: 800  \tTraining accuracy: 0.725414514541626\n",
      "Step: 800  \tValid loss: 0.4845646321773529\n",
      "Step: 900  \tTraining loss: 0.47216683626174927\n",
      "Step: 900  \tTraining accuracy: 0.7307459712028503\n",
      "Step: 900  \tValid loss: 0.4838668704032898\n",
      "Step: 1000  \tTraining loss: 0.46954813599586487\n",
      "Step: 1000  \tTraining accuracy: 0.7351281642913818\n",
      "Step: 1000  \tValid loss: 0.4841269552707672\n",
      "Step: 1100  \tTraining loss: 0.46745172142982483\n",
      "Step: 1100  \tTraining accuracy: 0.738921046257019\n",
      "Step: 1100  \tValid loss: 0.48474979400634766\n",
      "Step: 1200  \tTraining loss: 0.4656431972980499\n",
      "Step: 1200  \tTraining accuracy: 0.7421766519546509\n",
      "Step: 1200  \tValid loss: 0.4853121042251587\n",
      "Step: 1300  \tTraining loss: 0.4640503227710724\n",
      "Step: 1300  \tTraining accuracy: 0.7449116706848145\n",
      "Step: 1300  \tValid loss: 0.4859049916267395\n",
      "Step: 1400  \tTraining loss: 0.4626530408859253\n",
      "Step: 1400  \tTraining accuracy: 0.7471898794174194\n",
      "Step: 1400  \tValid loss: 0.4862523078918457\n",
      "Step: 1500  \tTraining loss: 0.4613112807273865\n",
      "Step: 1500  \tTraining accuracy: 0.7492587566375732\n",
      "Step: 1500  \tValid loss: 0.48654428124427795\n",
      "Step: 1600  \tTraining loss: 0.4599723219871521\n",
      "Step: 1600  \tTraining accuracy: 0.7510985732078552\n",
      "Step: 1600  \tValid loss: 0.486473023891449\n",
      "Step: 1700  \tTraining loss: 0.45868808031082153\n",
      "Step: 1700  \tTraining accuracy: 0.7527579069137573\n",
      "Step: 1700  \tValid loss: 0.48638826608657837\n",
      "Step: 1800  \tTraining loss: 0.45743343234062195\n",
      "Step: 1800  \tTraining accuracy: 0.7541943192481995\n",
      "Step: 1800  \tValid loss: 0.4861734211444855\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.75543135\n",
      "Precision: 0.768559\n",
      "Recall: 0.7318087\n",
      "F1 score: 0.7369277\n",
      "AUC: 0.7785177\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.755431   0.768559  0.731809  0.736928  0.778518  0.456228      0.754627   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.483842       0.754381   0.468556      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  1895.0  \n",
      "27\n",
      "(4350, 8)\n",
      "(4350, 1)\n",
      "(2400, 8)\n",
      "(2400, 1)\n",
      "(1950, 8)\n",
      "(1950, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6112598776817322\n",
      "Step: 100  \tTraining accuracy: 0.6657471060752869\n",
      "Step: 100  \tValid loss: 0.6311395168304443\n",
      "Step: 200  \tTraining loss: 0.5610777139663696\n",
      "Step: 200  \tTraining accuracy: 0.6747126579284668\n",
      "Step: 200  \tValid loss: 0.5783953070640564\n",
      "Step: 300  \tTraining loss: 0.5085490942001343\n",
      "Step: 300  \tTraining accuracy: 0.6984827518463135\n",
      "Step: 300  \tValid loss: 0.5197572708129883\n",
      "Step: 400  \tTraining loss: 0.49591919779777527\n",
      "Step: 400  \tTraining accuracy: 0.7163875102996826\n",
      "Step: 400  \tValid loss: 0.5043551921844482\n",
      "Step: 500  \tTraining loss: 0.49160823225975037\n",
      "Step: 500  \tTraining accuracy: 0.7276117205619812\n",
      "Step: 500  \tValid loss: 0.49899232387542725\n",
      "Step: 600  \tTraining loss: 0.488103449344635\n",
      "Step: 600  \tTraining accuracy: 0.734900712966919\n",
      "Step: 600  \tValid loss: 0.49483948945999146\n",
      "Step: 700  \tTraining loss: 0.48506632447242737\n",
      "Step: 700  \tTraining accuracy: 0.7406542897224426\n",
      "Step: 700  \tValid loss: 0.49115294218063354\n",
      "Step: 800  \tTraining loss: 0.4823673665523529\n",
      "Step: 800  \tTraining accuracy: 0.7450881004333496\n",
      "Step: 800  \tValid loss: 0.48772114515304565\n",
      "Step: 900  \tTraining loss: 0.4797406494617462\n",
      "Step: 900  \tTraining accuracy: 0.748586893081665\n",
      "Step: 900  \tValid loss: 0.4843255579471588\n",
      "Step: 1000  \tTraining loss: 0.4772215783596039\n",
      "Step: 1000  \tTraining accuracy: 0.7514458298683167\n",
      "Step: 1000  \tValid loss: 0.4810152053833008\n",
      "Step: 1100  \tTraining loss: 0.47505983710289\n",
      "Step: 1100  \tTraining accuracy: 0.7537493109703064\n",
      "Step: 1100  \tValid loss: 0.4781436026096344\n",
      "Step: 1200  \tTraining loss: 0.47331503033638\n",
      "Step: 1200  \tTraining accuracy: 0.7558020949363708\n",
      "Step: 1200  \tValid loss: 0.4759334921836853\n",
      "Step: 1300  \tTraining loss: 0.4719216525554657\n",
      "Step: 1300  \tTraining accuracy: 0.7576276063919067\n",
      "Step: 1300  \tValid loss: 0.4742402136325836\n",
      "Step: 1400  \tTraining loss: 0.47072526812553406\n",
      "Step: 1400  \tTraining accuracy: 0.759276270866394\n",
      "Step: 1400  \tValid loss: 0.47292038798332214\n",
      "Step: 1500  \tTraining loss: 0.4696338474750519\n",
      "Step: 1500  \tTraining accuracy: 0.7606579661369324\n",
      "Step: 1500  \tValid loss: 0.47179433703422546\n",
      "Step: 1600  \tTraining loss: 0.4685917794704437\n",
      "Step: 1600  \tTraining accuracy: 0.7618613243103027\n",
      "Step: 1600  \tValid loss: 0.4708171486854553\n",
      "Step: 1700  \tTraining loss: 0.4675635099411011\n",
      "Step: 1700  \tTraining accuracy: 0.7629258036613464\n",
      "Step: 1700  \tValid loss: 0.4699324667453766\n",
      "Step: 1800  \tTraining loss: 0.4665274918079376\n",
      "Step: 1800  \tTraining accuracy: 0.7639540433883667\n",
      "Step: 1800  \tValid loss: 0.46907928586006165\n",
      "Step: 1900  \tTraining loss: 0.46546968817710876\n",
      "Step: 1900  \tTraining accuracy: 0.7648835182189941\n",
      "Step: 1900  \tValid loss: 0.4682639539241791\n",
      "Step: 2000  \tTraining loss: 0.4643750488758087\n",
      "Step: 2000  \tTraining accuracy: 0.7657824754714966\n",
      "Step: 2000  \tValid loss: 0.4674510061740875\n",
      "Step: 2100  \tTraining loss: 0.46323108673095703\n",
      "Step: 2100  \tTraining accuracy: 0.766655445098877\n",
      "Step: 2100  \tValid loss: 0.4666275978088379\n",
      "Step: 2200  \tTraining loss: 0.46203163266181946\n",
      "Step: 2200  \tTraining accuracy: 0.7674793004989624\n",
      "Step: 2200  \tValid loss: 0.46576762199401855\n",
      "Step: 2300  \tTraining loss: 0.4607775807380676\n",
      "Step: 2300  \tTraining accuracy: 0.76827073097229\n",
      "Step: 2300  \tValid loss: 0.46484655141830444\n",
      "Step: 2400  \tTraining loss: 0.45947927236557007\n",
      "Step: 2400  \tTraining accuracy: 0.7690144181251526\n",
      "Step: 2400  \tValid loss: 0.4638954699039459\n",
      "Step: 2500  \tTraining loss: 0.458162784576416\n",
      "Step: 2500  \tTraining accuracy: 0.7697161436080933\n",
      "Step: 2500  \tValid loss: 0.46289730072021484\n",
      "Step: 2600  \tTraining loss: 0.4568484127521515\n",
      "Step: 2600  \tTraining accuracy: 0.7704079151153564\n",
      "Step: 2600  \tValid loss: 0.46190205216407776\n",
      "Step: 2700  \tTraining loss: 0.455560564994812\n",
      "Step: 2700  \tTraining accuracy: 0.7711212038993835\n",
      "Step: 2700  \tValid loss: 0.46087586879730225\n",
      "Step: 2800  \tTraining loss: 0.45431074500083923\n",
      "Step: 2800  \tTraining accuracy: 0.7718328237533569\n",
      "Step: 2800  \tValid loss: 0.45985057950019836\n",
      "Step: 2900  \tTraining loss: 0.45310670137405396\n",
      "Step: 2900  \tTraining accuracy: 0.7725105881690979\n",
      "Step: 2900  \tValid loss: 0.458820641040802\n",
      "Step: 3000  \tTraining loss: 0.4519502818584442\n",
      "Step: 3000  \tTraining accuracy: 0.7731891870498657\n",
      "Step: 3000  \tValid loss: 0.4578059911727905\n",
      "Step: 3100  \tTraining loss: 0.4508429765701294\n",
      "Step: 3100  \tTraining accuracy: 0.7738534212112427\n",
      "Step: 3100  \tValid loss: 0.45682087540626526\n",
      "Step: 3200  \tTraining loss: 0.44978436827659607\n",
      "Step: 3200  \tTraining accuracy: 0.7745338678359985\n",
      "Step: 3200  \tValid loss: 0.4558412730693817\n",
      "Step: 3300  \tTraining loss: 0.44877374172210693\n",
      "Step: 3300  \tTraining accuracy: 0.7752148509025574\n",
      "Step: 3300  \tValid loss: 0.4548904299736023\n",
      "Step: 3400  \tTraining loss: 0.44780486822128296\n",
      "Step: 3400  \tTraining accuracy: 0.775886058807373\n",
      "Step: 3400  \tValid loss: 0.45395946502685547\n",
      "Step: 3500  \tTraining loss: 0.44687342643737793\n",
      "Step: 3500  \tTraining accuracy: 0.7765184044837952\n",
      "Step: 3500  \tValid loss: 0.4530502259731293\n",
      "Step: 3600  \tTraining loss: 0.4459778964519501\n",
      "Step: 3600  \tTraining accuracy: 0.7771151065826416\n",
      "Step: 3600  \tValid loss: 0.45216289162635803\n",
      "Step: 3700  \tTraining loss: 0.4451090395450592\n",
      "Step: 3700  \tTraining accuracy: 0.7776822447776794\n",
      "Step: 3700  \tValid loss: 0.45127472281455994\n",
      "Step: 3800  \tTraining loss: 0.44426342844963074\n",
      "Step: 3800  \tTraining accuracy: 0.7782467603683472\n",
      "Step: 3800  \tValid loss: 0.45038992166519165\n",
      "Step: 3900  \tTraining loss: 0.44343817234039307\n",
      "Step: 3900  \tTraining accuracy: 0.7787848711013794\n",
      "Step: 3900  \tValid loss: 0.4494986832141876\n",
      "Step: 4000  \tTraining loss: 0.44261789321899414\n",
      "Step: 4000  \tTraining accuracy: 0.7792812585830688\n",
      "Step: 4000  \tValid loss: 0.44858795404434204\n",
      "Step: 4100  \tTraining loss: 0.4418034553527832\n",
      "Step: 4100  \tTraining accuracy: 0.7797389030456543\n",
      "Step: 4100  \tValid loss: 0.4476180076599121\n",
      "Step: 4200  \tTraining loss: 0.44096487760543823\n",
      "Step: 4200  \tTraining accuracy: 0.7801578640937805\n",
      "Step: 4200  \tValid loss: 0.44648662209510803\n",
      "Step: 4300  \tTraining loss: 0.44006240367889404\n",
      "Step: 4300  \tTraining accuracy: 0.7805382013320923\n",
      "Step: 4300  \tValid loss: 0.4452870786190033\n",
      "Step: 4400  \tTraining loss: 0.4391820430755615\n",
      "Step: 4400  \tTraining accuracy: 0.7809089422225952\n",
      "Step: 4400  \tValid loss: 0.44439005851745605\n",
      "Step: 4500  \tTraining loss: 0.43837374448776245\n",
      "Step: 4500  \tTraining accuracy: 0.7812837362289429\n",
      "Step: 4500  \tValid loss: 0.44381827116012573\n",
      "Step: 4600  \tTraining loss: 0.4376089572906494\n",
      "Step: 4600  \tTraining accuracy: 0.7816395163536072\n",
      "Step: 4600  \tValid loss: 0.4433797299861908\n",
      "Step: 4700  \tTraining loss: 0.4368716776371002\n",
      "Step: 4700  \tTraining accuracy: 0.7819725871086121\n",
      "Step: 4700  \tValid loss: 0.4429668188095093\n",
      "Step: 4800  \tTraining loss: 0.43614739179611206\n",
      "Step: 4800  \tTraining accuracy: 0.7822794914245605\n",
      "Step: 4800  \tValid loss: 0.442576140165329\n",
      "Step: 4900  \tTraining loss: 0.43543654680252075\n",
      "Step: 4900  \tTraining accuracy: 0.7825737595558167\n",
      "Step: 4900  \tValid loss: 0.4422077536582947\n",
      "Step: 5000  \tTraining loss: 0.43473437428474426\n",
      "Step: 5000  \tTraining accuracy: 0.78285151720047\n",
      "Step: 5000  \tValid loss: 0.4418695569038391\n",
      "Step: 5100  \tTraining loss: 0.43400800228118896\n",
      "Step: 5100  \tTraining accuracy: 0.7830932140350342\n",
      "Step: 5100  \tValid loss: 0.4414498507976532\n",
      "Step: 5200  \tTraining loss: 0.4332754909992218\n",
      "Step: 5200  \tTraining accuracy: 0.783329963684082\n",
      "Step: 5200  \tValid loss: 0.4410853981971741\n",
      "Step: 5300  \tTraining loss: 0.4325754940509796\n",
      "Step: 5300  \tTraining accuracy: 0.7835643291473389\n",
      "Step: 5300  \tValid loss: 0.4407815635204315\n",
      "Step: 5400  \tTraining loss: 0.4319029748439789\n",
      "Step: 5400  \tTraining accuracy: 0.7837855815887451\n",
      "Step: 5400  \tValid loss: 0.44053101539611816\n",
      "Step: 5500  \tTraining loss: 0.4312453866004944\n",
      "Step: 5500  \tTraining accuracy: 0.7840050458908081\n",
      "Step: 5500  \tValid loss: 0.44032591581344604\n",
      "Step: 5600  \tTraining loss: 0.43060916662216187\n",
      "Step: 5600  \tTraining accuracy: 0.7842124700546265\n",
      "Step: 5600  \tValid loss: 0.4401599168777466\n",
      "Step: 5700  \tTraining loss: 0.429988831281662\n",
      "Step: 5700  \tTraining accuracy: 0.7844187021255493\n",
      "Step: 5700  \tValid loss: 0.44002941250801086\n",
      "Step: 5800  \tTraining loss: 0.42938166856765747\n",
      "Step: 5800  \tTraining accuracy: 0.7846236824989319\n",
      "Step: 5800  \tValid loss: 0.43992674350738525\n",
      "Step: 5900  \tTraining loss: 0.42878979444503784\n",
      "Step: 5900  \tTraining accuracy: 0.7848177552223206\n",
      "Step: 5900  \tValid loss: 0.4398542642593384\n",
      "Step: 6000  \tTraining loss: 0.42816007137298584\n",
      "Step: 6000  \tTraining accuracy: 0.7850053310394287\n",
      "Step: 6000  \tValid loss: 0.4397642910480499\n",
      "Step: 6100  \tTraining loss: 0.4275486171245575\n",
      "Step: 6100  \tTraining accuracy: 0.7851999402046204\n",
      "Step: 6100  \tValid loss: 0.4397056996822357\n",
      "Step: 6200  \tTraining loss: 0.4269692897796631\n",
      "Step: 6200  \tTraining accuracy: 0.7853901386260986\n",
      "Step: 6200  \tValid loss: 0.439674973487854\n",
      "Step: 6300  \tTraining loss: 0.42639726400375366\n",
      "Step: 6300  \tTraining accuracy: 0.7855724096298218\n",
      "Step: 6300  \tValid loss: 0.4396529495716095\n",
      "Step: 6400  \tTraining loss: 0.42582955956459045\n",
      "Step: 6400  \tTraining accuracy: 0.7857579588890076\n",
      "Step: 6400  \tValid loss: 0.4396304190158844\n",
      "Step: 6500  \tTraining loss: 0.42526546120643616\n",
      "Step: 6500  \tTraining accuracy: 0.7859467267990112\n",
      "Step: 6500  \tValid loss: 0.4396052062511444\n",
      "Step: 6600  \tTraining loss: 0.42469772696495056\n",
      "Step: 6600  \tTraining accuracy: 0.7861402034759521\n",
      "Step: 6600  \tValid loss: 0.43958714604377747\n",
      "Step: 6700  \tTraining loss: 0.42413511872291565\n",
      "Step: 6700  \tTraining accuracy: 0.7863192558288574\n",
      "Step: 6700  \tValid loss: 0.4395725429058075\n",
      "Step: 6800  \tTraining loss: 0.42357414960861206\n",
      "Step: 6800  \tTraining accuracy: 0.7864793539047241\n",
      "Step: 6800  \tValid loss: 0.4395550489425659\n",
      "Step: 6900  \tTraining loss: 0.42301496863365173\n",
      "Step: 6900  \tTraining accuracy: 0.786626398563385\n",
      "Step: 6900  \tValid loss: 0.43953511118888855\n",
      "Step: 7000  \tTraining loss: 0.42246171832084656\n",
      "Step: 7000  \tTraining accuracy: 0.7867791056632996\n",
      "Step: 7000  \tValid loss: 0.439510703086853\n",
      "Step: 7100  \tTraining loss: 0.42191019654273987\n",
      "Step: 7100  \tTraining accuracy: 0.7869405746459961\n",
      "Step: 7100  \tValid loss: 0.439485102891922\n",
      "Step: 7200  \tTraining loss: 0.4213646352291107\n",
      "Step: 7200  \tTraining accuracy: 0.7870959043502808\n",
      "Step: 7200  \tValid loss: 0.43945983052253723\n",
      "Step: 7300  \tTraining loss: 0.4208228290081024\n",
      "Step: 7300  \tTraining accuracy: 0.7872421741485596\n",
      "Step: 7300  \tValid loss: 0.43943771719932556\n",
      "Step: 7400  \tTraining loss: 0.4202894866466522\n",
      "Step: 7400  \tTraining accuracy: 0.7873813509941101\n",
      "Step: 7400  \tValid loss: 0.4394144117832184\n",
      "Step: 7500  \tTraining loss: 0.4195399582386017\n",
      "Step: 7500  \tTraining accuracy: 0.7875306606292725\n",
      "Step: 7500  \tValid loss: 0.439254492521286\n",
      "Step: 7600  \tTraining loss: 0.41898706555366516\n",
      "Step: 7600  \tTraining accuracy: 0.7877004146575928\n",
      "Step: 7600  \tValid loss: 0.43918514251708984\n",
      "Step: 7700  \tTraining loss: 0.41846248507499695\n",
      "Step: 7700  \tTraining accuracy: 0.7878626585006714\n",
      "Step: 7700  \tValid loss: 0.43912526965141296\n",
      "Step: 7800  \tTraining loss: 0.4179515540599823\n",
      "Step: 7800  \tTraining accuracy: 0.7880266904830933\n",
      "Step: 7800  \tValid loss: 0.4390726685523987\n",
      "Step: 7900  \tTraining loss: 0.41745027899742126\n",
      "Step: 7900  \tTraining accuracy: 0.7881850600242615\n",
      "Step: 7900  \tValid loss: 0.43903452157974243\n",
      "Step: 8000  \tTraining loss: 0.4166300296783447\n",
      "Step: 8000  \tTraining accuracy: 0.7883452773094177\n",
      "Step: 8000  \tValid loss: 0.4383641481399536\n",
      "Step: 8100  \tTraining loss: 0.416101336479187\n",
      "Step: 8100  \tTraining accuracy: 0.7885057330131531\n",
      "Step: 8100  \tValid loss: 0.4382353723049164\n",
      "Step: 8200  \tTraining loss: 0.4156203269958496\n",
      "Step: 8200  \tTraining accuracy: 0.7886608839035034\n",
      "Step: 8200  \tValid loss: 0.4381485879421234\n",
      "Step: 8300  \tTraining loss: 0.41514214873313904\n",
      "Step: 8300  \tTraining accuracy: 0.7888122797012329\n",
      "Step: 8300  \tValid loss: 0.4381023049354553\n",
      "Step: 8400  \tTraining loss: 0.41468313336372375\n",
      "Step: 8400  \tTraining accuracy: 0.7889600396156311\n",
      "Step: 8400  \tValid loss: 0.4380698502063751\n",
      "Step: 8500  \tTraining loss: 0.4142291843891144\n",
      "Step: 8500  \tTraining accuracy: 0.7891151309013367\n",
      "Step: 8500  \tValid loss: 0.4380483031272888\n",
      "Step: 8600  \tTraining loss: 0.41379159688949585\n",
      "Step: 8600  \tTraining accuracy: 0.7892612814903259\n",
      "Step: 8600  \tValid loss: 0.4380391538143158\n",
      "Step: 8700  \tTraining loss: 0.41335609555244446\n",
      "Step: 8700  \tTraining accuracy: 0.789393424987793\n",
      "Step: 8700  \tValid loss: 0.43803587555885315\n",
      "Step: 8800  \tTraining loss: 0.41293051838874817\n",
      "Step: 8800  \tTraining accuracy: 0.7895054221153259\n",
      "Step: 8800  \tValid loss: 0.43804341554641724\n",
      "Step: 8900  \tTraining loss: 0.4125187397003174\n",
      "Step: 8900  \tTraining accuracy: 0.7896110415458679\n",
      "Step: 8900  \tValid loss: 0.4380726218223572\n",
      "Step: 9000  \tTraining loss: 0.4121086001396179\n",
      "Step: 9000  \tTraining accuracy: 0.7897090911865234\n",
      "Step: 9000  \tValid loss: 0.4380882680416107\n",
      "Step: 9100  \tTraining loss: 0.4117027819156647\n",
      "Step: 9100  \tTraining accuracy: 0.7897999882698059\n",
      "Step: 9100  \tValid loss: 0.4381180703639984\n",
      "Step: 9200  \tTraining loss: 0.4113087058067322\n",
      "Step: 9200  \tTraining accuracy: 0.7898862957954407\n",
      "Step: 9200  \tValid loss: 0.43816491961479187\n",
      "Step: 9300  \tTraining loss: 0.4109231233596802\n",
      "Step: 9300  \tTraining accuracy: 0.7899795174598694\n",
      "Step: 9300  \tValid loss: 0.4382271468639374\n",
      "Step: 9400  \tTraining loss: 0.4105423390865326\n",
      "Step: 9400  \tTraining accuracy: 0.7900805473327637\n",
      "Step: 9400  \tValid loss: 0.43828389048576355\n",
      "Step: 9500  \tTraining loss: 0.4101654291152954\n",
      "Step: 9500  \tTraining accuracy: 0.7901806235313416\n",
      "Step: 9500  \tValid loss: 0.4383562207221985\n",
      "Step: 9600  \tTraining loss: 0.4097961187362671\n",
      "Step: 9600  \tTraining accuracy: 0.790270209312439\n",
      "Step: 9600  \tValid loss: 0.4384315013885498\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.79035914\n",
      "Precision: 0.82884544\n",
      "Recall: 0.8602603\n",
      "F1 score: 0.80505866\n",
      "AUC: 0.7966434\n",
      "   accuracy  precision   recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.790359   0.828845  0.86026  0.805059  0.796643  0.409602       0.79026   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.438032       0.790279   0.432802      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  9652.0  \n",
      "28\n",
      "(4350, 8)\n",
      "(4350, 1)\n",
      "(2320, 8)\n",
      "(2320, 1)\n",
      "(1885, 8)\n",
      "(1885, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6418691873550415\n",
      "Step: 100  \tTraining accuracy: 0.6485057473182678\n",
      "Step: 100  \tValid loss: 0.6319034695625305\n",
      "Step: 200  \tTraining loss: 0.6205521821975708\n",
      "Step: 200  \tTraining accuracy: 0.6598992347717285\n",
      "Step: 200  \tValid loss: 0.6066833734512329\n",
      "Step: 300  \tTraining loss: 0.6013503670692444\n",
      "Step: 300  \tTraining accuracy: 0.665517270565033\n",
      "Step: 300  \tValid loss: 0.5892870426177979\n",
      "Step: 400  \tTraining loss: 0.5837562084197998\n",
      "Step: 400  \tTraining accuracy: 0.672063946723938\n",
      "Step: 400  \tValid loss: 0.5741129517555237\n",
      "Step: 500  \tTraining loss: 0.5717273950576782\n",
      "Step: 500  \tTraining accuracy: 0.6791806817054749\n",
      "Step: 500  \tValid loss: 0.5649223327636719\n",
      "Step: 600  \tTraining loss: 0.5642333626747131\n",
      "Step: 600  \tTraining accuracy: 0.6861538290977478\n",
      "Step: 600  \tValid loss: 0.560408890247345\n",
      "Step: 700  \tTraining loss: 0.5589892268180847\n",
      "Step: 700  \tTraining accuracy: 0.692295253276825\n",
      "Step: 700  \tValid loss: 0.5573326349258423\n",
      "Step: 800  \tTraining loss: 0.5536174178123474\n",
      "Step: 800  \tTraining accuracy: 0.6967074275016785\n",
      "Step: 800  \tValid loss: 0.5528772473335266\n",
      "Step: 900  \tTraining loss: 0.5488833785057068\n",
      "Step: 900  \tTraining accuracy: 0.7002198100090027\n",
      "Step: 900  \tValid loss: 0.5491927862167358\n",
      "Step: 1000  \tTraining loss: 0.5449066758155823\n",
      "Step: 1000  \tTraining accuracy: 0.702944278717041\n",
      "Step: 1000  \tValid loss: 0.5464377999305725\n",
      "Step: 1100  \tTraining loss: 0.5407063961029053\n",
      "Step: 1100  \tTraining accuracy: 0.705072283744812\n",
      "Step: 1100  \tValid loss: 0.5429210662841797\n",
      "Step: 1200  \tTraining loss: 0.5368194580078125\n",
      "Step: 1200  \tTraining accuracy: 0.707064151763916\n",
      "Step: 1200  \tValid loss: 0.5395088195800781\n",
      "Step: 1300  \tTraining loss: 0.5332363247871399\n",
      "Step: 1300  \tTraining accuracy: 0.7088029384613037\n",
      "Step: 1300  \tValid loss: 0.5361020565032959\n",
      "Step: 1400  \tTraining loss: 0.5301038026809692\n",
      "Step: 1400  \tTraining accuracy: 0.7103015780448914\n",
      "Step: 1400  \tValid loss: 0.5330880284309387\n",
      "Step: 1500  \tTraining loss: 0.5275042653083801\n",
      "Step: 1500  \tTraining accuracy: 0.7116822600364685\n",
      "Step: 1500  \tValid loss: 0.5305953621864319\n",
      "Step: 1600  \tTraining loss: 0.5254448056221008\n",
      "Step: 1600  \tTraining accuracy: 0.7130959033966064\n",
      "Step: 1600  \tValid loss: 0.5284786224365234\n",
      "Step: 1700  \tTraining loss: 0.5237448811531067\n",
      "Step: 1700  \tTraining accuracy: 0.7144657373428345\n",
      "Step: 1700  \tValid loss: 0.526729941368103\n",
      "Step: 1800  \tTraining loss: 0.5222694277763367\n",
      "Step: 1800  \tTraining accuracy: 0.7158126831054688\n",
      "Step: 1800  \tValid loss: 0.5253296494483948\n",
      "Step: 1900  \tTraining loss: 0.5209454894065857\n",
      "Step: 1900  \tTraining accuracy: 0.7170329689979553\n",
      "Step: 1900  \tValid loss: 0.5241139531135559\n",
      "Step: 2000  \tTraining loss: 0.5197150111198425\n",
      "Step: 2000  \tTraining accuracy: 0.7182180285453796\n",
      "Step: 2000  \tValid loss: 0.5228992700576782\n",
      "Step: 2100  \tTraining loss: 0.518479585647583\n",
      "Step: 2100  \tTraining accuracy: 0.7193331718444824\n",
      "Step: 2100  \tValid loss: 0.5214868783950806\n",
      "Step: 2200  \tTraining loss: 0.5171651840209961\n",
      "Step: 2200  \tTraining accuracy: 0.720382571220398\n",
      "Step: 2200  \tValid loss: 0.5196905136108398\n",
      "Step: 2300  \tTraining loss: 0.515799880027771\n",
      "Step: 2300  \tTraining accuracy: 0.7213595509529114\n",
      "Step: 2300  \tValid loss: 0.5178179740905762\n",
      "Step: 2400  \tTraining loss: 0.5144607424736023\n",
      "Step: 2400  \tTraining accuracy: 0.7223230600357056\n",
      "Step: 2400  \tValid loss: 0.5159844160079956\n",
      "Step: 2500  \tTraining loss: 0.5131312608718872\n",
      "Step: 2500  \tTraining accuracy: 0.7232937216758728\n",
      "Step: 2500  \tValid loss: 0.5141527652740479\n",
      "Step: 2600  \tTraining loss: 0.5118290185928345\n",
      "Step: 2600  \tTraining accuracy: 0.7242570519447327\n",
      "Step: 2600  \tValid loss: 0.5122808218002319\n",
      "Step: 2700  \tTraining loss: 0.510404109954834\n",
      "Step: 2700  \tTraining accuracy: 0.7251565456390381\n",
      "Step: 2700  \tValid loss: 0.5101190209388733\n",
      "Step: 2800  \tTraining loss: 0.5090044736862183\n",
      "Step: 2800  \tTraining accuracy: 0.7259905934333801\n",
      "Step: 2800  \tValid loss: 0.5080423355102539\n",
      "Step: 2900  \tTraining loss: 0.5077587962150574\n",
      "Step: 2900  \tTraining accuracy: 0.7267743945121765\n",
      "Step: 2900  \tValid loss: 0.5062379837036133\n",
      "Step: 3000  \tTraining loss: 0.5065586566925049\n",
      "Step: 3000  \tTraining accuracy: 0.7275208234786987\n",
      "Step: 3000  \tValid loss: 0.5044822096824646\n",
      "Step: 3100  \tTraining loss: 0.5054352879524231\n",
      "Step: 3100  \tTraining accuracy: 0.7282068729400635\n",
      "Step: 3100  \tValid loss: 0.5027982592582703\n",
      "Step: 3200  \tTraining loss: 0.50443035364151\n",
      "Step: 3200  \tTraining accuracy: 0.7288679480552673\n",
      "Step: 3200  \tValid loss: 0.5015621185302734\n",
      "Step: 3300  \tTraining loss: 0.5035435557365417\n",
      "Step: 3300  \tTraining accuracy: 0.7295206785202026\n",
      "Step: 3300  \tValid loss: 0.5005199313163757\n",
      "Step: 3400  \tTraining loss: 0.5027778744697571\n",
      "Step: 3400  \tTraining accuracy: 0.7301449179649353\n",
      "Step: 3400  \tValid loss: 0.49959421157836914\n",
      "Step: 3500  \tTraining loss: 0.5020933151245117\n",
      "Step: 3500  \tTraining accuracy: 0.7307059168815613\n",
      "Step: 3500  \tValid loss: 0.4987620413303375\n",
      "Step: 3600  \tTraining loss: 0.5014634728431702\n",
      "Step: 3600  \tTraining accuracy: 0.7312517762184143\n",
      "Step: 3600  \tValid loss: 0.49809202551841736\n",
      "Step: 3700  \tTraining loss: 0.5008758902549744\n",
      "Step: 3700  \tTraining accuracy: 0.7317676544189453\n",
      "Step: 3700  \tValid loss: 0.49761220812797546\n",
      "Step: 3800  \tTraining loss: 0.5003515481948853\n",
      "Step: 3800  \tTraining accuracy: 0.7322342991828918\n",
      "Step: 3800  \tValid loss: 0.4970809519290924\n",
      "Step: 3900  \tTraining loss: 0.49986928701400757\n",
      "Step: 3900  \tTraining accuracy: 0.7327009439468384\n",
      "Step: 3900  \tValid loss: 0.49657461047172546\n",
      "Step: 4000  \tTraining loss: 0.49942219257354736\n",
      "Step: 4000  \tTraining accuracy: 0.7331262230873108\n",
      "Step: 4000  \tValid loss: 0.4960976541042328\n",
      "Step: 4100  \tTraining loss: 0.4990018904209137\n",
      "Step: 4100  \tTraining accuracy: 0.7335305213928223\n",
      "Step: 4100  \tValid loss: 0.4956863522529602\n",
      "Step: 4200  \tTraining loss: 0.4986167550086975\n",
      "Step: 4200  \tTraining accuracy: 0.7339406609535217\n",
      "Step: 4200  \tValid loss: 0.4953598082065582\n",
      "Step: 4300  \tTraining loss: 0.4982588589191437\n",
      "Step: 4300  \tTraining accuracy: 0.7343479990959167\n",
      "Step: 4300  \tValid loss: 0.49507445096969604\n",
      "Step: 4400  \tTraining loss: 0.4979158341884613\n",
      "Step: 4400  \tTraining accuracy: 0.7347312569618225\n",
      "Step: 4400  \tValid loss: 0.49480849504470825\n",
      "Step: 4500  \tTraining loss: 0.4975866675376892\n",
      "Step: 4500  \tTraining accuracy: 0.7351077795028687\n",
      "Step: 4500  \tValid loss: 0.4945739805698395\n",
      "Step: 4600  \tTraining loss: 0.4972798824310303\n",
      "Step: 4600  \tTraining accuracy: 0.7355140447616577\n",
      "Step: 4600  \tValid loss: 0.4943832457065582\n",
      "Step: 4700  \tTraining loss: 0.4969880282878876\n",
      "Step: 4700  \tTraining accuracy: 0.7359153628349304\n",
      "Step: 4700  \tValid loss: 0.49420806765556335\n",
      "Step: 4800  \tTraining loss: 0.4967062771320343\n",
      "Step: 4800  \tTraining accuracy: 0.7362973093986511\n",
      "Step: 4800  \tValid loss: 0.4940405488014221\n",
      "Step: 4900  \tTraining loss: 0.4964362680912018\n",
      "Step: 4900  \tTraining accuracy: 0.7366563081741333\n",
      "Step: 4900  \tValid loss: 0.49390342831611633\n",
      "Step: 5000  \tTraining loss: 0.49617186188697815\n",
      "Step: 5000  \tTraining accuracy: 0.7370007634162903\n",
      "Step: 5000  \tValid loss: 0.4937720000743866\n",
      "Step: 5100  \tTraining loss: 0.4958960711956024\n",
      "Step: 5100  \tTraining accuracy: 0.7373386025428772\n",
      "Step: 5100  \tValid loss: 0.49367213249206543\n",
      "Step: 5200  \tTraining loss: 0.4955822229385376\n",
      "Step: 5200  \tTraining accuracy: 0.7376587390899658\n",
      "Step: 5200  \tValid loss: 0.4936314523220062\n",
      "Step: 5300  \tTraining loss: 0.49528634548187256\n",
      "Step: 5300  \tTraining accuracy: 0.7379376888275146\n",
      "Step: 5300  \tValid loss: 0.49351081252098083\n",
      "Step: 5400  \tTraining loss: 0.495002806186676\n",
      "Step: 5400  \tTraining accuracy: 0.7382128238677979\n",
      "Step: 5400  \tValid loss: 0.493338018655777\n",
      "Step: 5500  \tTraining loss: 0.4947202801704407\n",
      "Step: 5500  \tTraining accuracy: 0.7384693026542664\n",
      "Step: 5500  \tValid loss: 0.4931804835796356\n",
      "Step: 5600  \tTraining loss: 0.49442631006240845\n",
      "Step: 5600  \tTraining accuracy: 0.7387164831161499\n",
      "Step: 5600  \tValid loss: 0.49303311109542847\n",
      "Step: 5700  \tTraining loss: 0.49413713812828064\n",
      "Step: 5700  \tTraining accuracy: 0.73896324634552\n",
      "Step: 5700  \tValid loss: 0.49285051226615906\n",
      "Step: 5800  \tTraining loss: 0.49385926127433777\n",
      "Step: 5800  \tTraining accuracy: 0.7392135858535767\n",
      "Step: 5800  \tValid loss: 0.4926762580871582\n",
      "Step: 5900  \tTraining loss: 0.4935912787914276\n",
      "Step: 5900  \tTraining accuracy: 0.7394653558731079\n",
      "Step: 5900  \tValid loss: 0.4925152063369751\n",
      "Step: 6000  \tTraining loss: 0.49332737922668457\n",
      "Step: 6000  \tTraining accuracy: 0.7397243976593018\n",
      "Step: 6000  \tValid loss: 0.49236956238746643\n",
      "Step: 6100  \tTraining loss: 0.493069052696228\n",
      "Step: 6100  \tTraining accuracy: 0.7399942278862\n",
      "Step: 6100  \tValid loss: 0.4922102391719818\n",
      "Step: 6200  \tTraining loss: 0.4928188920021057\n",
      "Step: 6200  \tTraining accuracy: 0.7402685284614563\n",
      "Step: 6200  \tValid loss: 0.49204733967781067\n",
      "Step: 6300  \tTraining loss: 0.4925757050514221\n",
      "Step: 6300  \tTraining accuracy: 0.7405340671539307\n",
      "Step: 6300  \tValid loss: 0.49187546968460083\n",
      "Step: 6400  \tTraining loss: 0.4923401474952698\n",
      "Step: 6400  \tTraining accuracy: 0.740807831287384\n",
      "Step: 6400  \tValid loss: 0.4917084872722626\n",
      "Step: 6500  \tTraining loss: 0.49210959672927856\n",
      "Step: 6500  \tTraining accuracy: 0.7410803437232971\n",
      "Step: 6500  \tValid loss: 0.4915241003036499\n",
      "Step: 6600  \tTraining loss: 0.4918818473815918\n",
      "Step: 6600  \tTraining accuracy: 0.7413462996482849\n",
      "Step: 6600  \tValid loss: 0.49135005474090576\n",
      "Step: 6700  \tTraining loss: 0.4916607141494751\n",
      "Step: 6700  \tTraining accuracy: 0.741614818572998\n",
      "Step: 6700  \tValid loss: 0.49117404222488403\n",
      "Step: 6800  \tTraining loss: 0.49144262075424194\n",
      "Step: 6800  \tTraining accuracy: 0.7419047951698303\n",
      "Step: 6800  \tValid loss: 0.4910393953323364\n",
      "Step: 6900  \tTraining loss: 0.4912196397781372\n",
      "Step: 6900  \tTraining accuracy: 0.7421914935112\n",
      "Step: 6900  \tValid loss: 0.4909648001194\n",
      "Step: 7000  \tTraining loss: 0.49100708961486816\n",
      "Step: 7000  \tTraining accuracy: 0.742469847202301\n",
      "Step: 7000  \tValid loss: 0.4908819794654846\n",
      "Step: 7100  \tTraining loss: 0.4907969534397125\n",
      "Step: 7100  \tTraining accuracy: 0.7427486777305603\n",
      "Step: 7100  \tValid loss: 0.490784227848053\n",
      "Step: 7200  \tTraining loss: 0.4905933439731598\n",
      "Step: 7200  \tTraining accuracy: 0.7430278658866882\n",
      "Step: 7200  \tValid loss: 0.4906734526157379\n",
      "Step: 7300  \tTraining loss: 0.49039578437805176\n",
      "Step: 7300  \tTraining accuracy: 0.7432944774627686\n",
      "Step: 7300  \tValid loss: 0.4905579686164856\n",
      "Step: 7400  \tTraining loss: 0.490202933549881\n",
      "Step: 7400  \tTraining accuracy: 0.7435522675514221\n",
      "Step: 7400  \tValid loss: 0.49044835567474365\n",
      "Step: 7500  \tTraining loss: 0.4900124669075012\n",
      "Step: 7500  \tTraining accuracy: 0.7438125610351562\n",
      "Step: 7500  \tValid loss: 0.4903627634048462\n",
      "Step: 7600  \tTraining loss: 0.48982787132263184\n",
      "Step: 7600  \tTraining accuracy: 0.7440705895423889\n",
      "Step: 7600  \tValid loss: 0.4902494251728058\n",
      "Step: 7700  \tTraining loss: 0.4896489381790161\n",
      "Step: 7700  \tTraining accuracy: 0.7443142533302307\n",
      "Step: 7700  \tValid loss: 0.4901229441165924\n",
      "Step: 7800  \tTraining loss: 0.4894748032093048\n",
      "Step: 7800  \tTraining accuracy: 0.7445440888404846\n",
      "Step: 7800  \tValid loss: 0.4899950325489044\n",
      "Step: 7900  \tTraining loss: 0.48930609226226807\n",
      "Step: 7900  \tTraining accuracy: 0.7447680234909058\n",
      "Step: 7900  \tValid loss: 0.48987051844596863\n",
      "Step: 8000  \tTraining loss: 0.48914051055908203\n",
      "Step: 8000  \tTraining accuracy: 0.744986355304718\n",
      "Step: 8000  \tValid loss: 0.4897543489933014\n",
      "Step: 8100  \tTraining loss: 0.4889814555644989\n",
      "Step: 8100  \tTraining accuracy: 0.7451992630958557\n",
      "Step: 8100  \tValid loss: 0.4896377623081207\n",
      "Step: 8200  \tTraining loss: 0.48881974816322327\n",
      "Step: 8200  \tTraining accuracy: 0.7454127073287964\n",
      "Step: 8200  \tValid loss: 0.4895358979701996\n",
      "Step: 8300  \tTraining loss: 0.4886534810066223\n",
      "Step: 8300  \tTraining accuracy: 0.7456279993057251\n",
      "Step: 8300  \tValid loss: 0.48941949009895325\n",
      "Step: 8400  \tTraining loss: 0.488496869802475\n",
      "Step: 8400  \tTraining accuracy: 0.745832622051239\n",
      "Step: 8400  \tValid loss: 0.48931658267974854\n",
      "Step: 8500  \tTraining loss: 0.488341361284256\n",
      "Step: 8500  \tTraining accuracy: 0.7460364699363708\n",
      "Step: 8500  \tValid loss: 0.4892149865627289\n",
      "Step: 8600  \tTraining loss: 0.4881902039051056\n",
      "Step: 8600  \tTraining accuracy: 0.7462383508682251\n",
      "Step: 8600  \tValid loss: 0.4891142249107361\n",
      "Step: 8700  \tTraining loss: 0.4880421459674835\n",
      "Step: 8700  \tTraining accuracy: 0.7464395761489868\n",
      "Step: 8700  \tValid loss: 0.48902544379234314\n",
      "Step: 8800  \tTraining loss: 0.4878937005996704\n",
      "Step: 8800  \tTraining accuracy: 0.7466441988945007\n",
      "Step: 8800  \tValid loss: 0.4889321029186249\n",
      "Step: 8900  \tTraining loss: 0.48775339126586914\n",
      "Step: 8900  \tTraining accuracy: 0.7468456029891968\n",
      "Step: 8900  \tValid loss: 0.4888441860675812\n",
      "Step: 9000  \tTraining loss: 0.4876142740249634\n",
      "Step: 9000  \tTraining accuracy: 0.7470437288284302\n",
      "Step: 9000  \tValid loss: 0.4887828826904297\n",
      "Step: 9100  \tTraining loss: 0.487478107213974\n",
      "Step: 9100  \tTraining accuracy: 0.7472465634346008\n",
      "Step: 9100  \tValid loss: 0.48872101306915283\n",
      "Step: 9200  \tTraining loss: 0.4873487949371338\n",
      "Step: 9200  \tTraining accuracy: 0.7474372386932373\n",
      "Step: 9200  \tValid loss: 0.4886702001094818\n",
      "Step: 9300  \tTraining loss: 0.4872245490550995\n",
      "Step: 9300  \tTraining accuracy: 0.747623860836029\n",
      "Step: 9300  \tValid loss: 0.48864325881004333\n",
      "Step: 9400  \tTraining loss: 0.48709815740585327\n",
      "Step: 9400  \tTraining accuracy: 0.7478102445602417\n",
      "Step: 9400  \tValid loss: 0.48858842253685\n",
      "Step: 9500  \tTraining loss: 0.4869743287563324\n",
      "Step: 9500  \tTraining accuracy: 0.7479827404022217\n",
      "Step: 9500  \tValid loss: 0.48853328824043274\n",
      "Step: 9600  \tTraining loss: 0.48685458302497864\n",
      "Step: 9600  \tTraining accuracy: 0.7481626272201538\n",
      "Step: 9600  \tValid loss: 0.48849767446517944\n",
      "Step: 9700  \tTraining loss: 0.4867318868637085\n",
      "Step: 9700  \tTraining accuracy: 0.7483412623405457\n",
      "Step: 9700  \tValid loss: 0.48848825693130493\n",
      "Step: 9800  \tTraining loss: 0.48661527037620544\n",
      "Step: 9800  \tTraining accuracy: 0.7485150098800659\n",
      "Step: 9800  \tValid loss: 0.48843351006507874\n",
      "Step: 9900  \tTraining loss: 0.4865085780620575\n",
      "Step: 9900  \tTraining accuracy: 0.7486828565597534\n",
      "Step: 9900  \tValid loss: 0.48839402198791504\n",
      "Step: 10000  \tTraining loss: 0.48639795184135437\n",
      "Step: 10000  \tTraining accuracy: 0.7488402724266052\n",
      "Step: 10000  \tValid loss: 0.48833659291267395\n",
      "Step: 10100  \tTraining loss: 0.4862983822822571\n",
      "Step: 10100  \tTraining accuracy: 0.7489981055259705\n",
      "Step: 10100  \tValid loss: 0.4882969260215759\n",
      "Step: 10200  \tTraining loss: 0.48619475960731506\n",
      "Step: 10200  \tTraining accuracy: 0.7491515874862671\n",
      "Step: 10200  \tValid loss: 0.48824793100357056\n",
      "Step: 10300  \tTraining loss: 0.48609668016433716\n",
      "Step: 10300  \tTraining accuracy: 0.7492952942848206\n",
      "Step: 10300  \tValid loss: 0.48820433020591736\n",
      "Step: 10400  \tTraining loss: 0.48599138855934143\n",
      "Step: 10400  \tTraining accuracy: 0.749438464641571\n",
      "Step: 10400  \tValid loss: 0.4881426990032196\n",
      "Step: 10500  \tTraining loss: 0.4858977496623993\n",
      "Step: 10500  \tTraining accuracy: 0.7495856285095215\n",
      "Step: 10500  \tValid loss: 0.48810651898384094\n",
      "Step: 10600  \tTraining loss: 0.48580437898635864\n",
      "Step: 10600  \tTraining accuracy: 0.7497321963310242\n",
      "Step: 10600  \tValid loss: 0.4880595803260803\n",
      "Step: 10700  \tTraining loss: 0.48571187257766724\n",
      "Step: 10700  \tTraining accuracy: 0.7498781681060791\n",
      "Step: 10700  \tValid loss: 0.4880179464817047\n",
      "Step: 10800  \tTraining loss: 0.4856223165988922\n",
      "Step: 10800  \tTraining accuracy: 0.7500258088111877\n",
      "Step: 10800  \tValid loss: 0.4879779815673828\n",
      "Step: 10900  \tTraining loss: 0.4855365455150604\n",
      "Step: 10900  \tTraining accuracy: 0.7501750588417053\n",
      "Step: 10900  \tValid loss: 0.4879465103149414\n",
      "Step: 11000  \tTraining loss: 0.48545271158218384\n",
      "Step: 11000  \tTraining accuracy: 0.7503215670585632\n",
      "Step: 11000  \tValid loss: 0.48792025446891785\n",
      "Step: 11100  \tTraining loss: 0.48537304997444153\n",
      "Step: 11100  \tTraining accuracy: 0.7504653930664062\n",
      "Step: 11100  \tValid loss: 0.48789241909980774\n",
      "Step: 11200  \tTraining loss: 0.48529836535453796\n",
      "Step: 11200  \tTraining accuracy: 0.7505993247032166\n",
      "Step: 11200  \tValid loss: 0.4878862500190735\n",
      "Step: 11300  \tTraining loss: 0.4852142930030823\n",
      "Step: 11300  \tTraining accuracy: 0.7507308721542358\n",
      "Step: 11300  \tValid loss: 0.4878528416156769\n",
      "Step: 11400  \tTraining loss: 0.4851420521736145\n",
      "Step: 11400  \tTraining accuracy: 0.7508529424667358\n",
      "Step: 11400  \tValid loss: 0.4878441095352173\n",
      "Step: 11500  \tTraining loss: 0.4850679636001587\n",
      "Step: 11500  \tTraining accuracy: 0.750978946685791\n",
      "Step: 11500  \tValid loss: 0.48782986402511597\n",
      "Step: 11600  \tTraining loss: 0.4849908947944641\n",
      "Step: 11600  \tTraining accuracy: 0.7511058449745178\n",
      "Step: 11600  \tValid loss: 0.48780402541160583\n",
      "Step: 11700  \tTraining loss: 0.48492270708084106\n",
      "Step: 11700  \tTraining accuracy: 0.7512305378913879\n",
      "Step: 11700  \tValid loss: 0.4877929091453552\n",
      "Step: 11800  \tTraining loss: 0.4848534166812897\n",
      "Step: 11800  \tTraining accuracy: 0.7513511180877686\n",
      "Step: 11800  \tValid loss: 0.4877851903438568\n",
      "Step: 11900  \tTraining loss: 0.48478612303733826\n",
      "Step: 11900  \tTraining accuracy: 0.751466691493988\n",
      "Step: 11900  \tValid loss: 0.487772673368454\n",
      "Step: 12000  \tTraining loss: 0.4847162961959839\n",
      "Step: 12000  \tTraining accuracy: 0.7515852451324463\n",
      "Step: 12000  \tValid loss: 0.48776113986968994\n",
      "Step: 12100  \tTraining loss: 0.48465707898139954\n",
      "Step: 12100  \tTraining accuracy: 0.7516998648643494\n",
      "Step: 12100  \tValid loss: 0.48773887753486633\n",
      "Step: 12200  \tTraining loss: 0.4845832884311676\n",
      "Step: 12200  \tTraining accuracy: 0.7518097758293152\n",
      "Step: 12200  \tValid loss: 0.48774802684783936\n",
      "Step: 12300  \tTraining loss: 0.4845184087753296\n",
      "Step: 12300  \tTraining accuracy: 0.751917839050293\n",
      "Step: 12300  \tValid loss: 0.48774370551109314\n",
      "Step: 12400  \tTraining loss: 0.48446109890937805\n",
      "Step: 12400  \tTraining accuracy: 0.7520241737365723\n",
      "Step: 12400  \tValid loss: 0.4877503514289856\n",
      "Step: 12500  \tTraining loss: 0.4843999147415161\n",
      "Step: 12500  \tTraining accuracy: 0.7521297335624695\n",
      "Step: 12500  \tValid loss: 0.48775291442871094\n",
      "Step: 12600  \tTraining loss: 0.4843338131904602\n",
      "Step: 12600  \tTraining accuracy: 0.7522345185279846\n",
      "Step: 12600  \tValid loss: 0.48773762583732605\n",
      "Step: 12700  \tTraining loss: 0.4842749238014221\n",
      "Step: 12700  \tTraining accuracy: 0.7523404359817505\n",
      "Step: 12700  \tValid loss: 0.4877406060695648\n",
      "Step: 12800  \tTraining loss: 0.4842163324356079\n",
      "Step: 12800  \tTraining accuracy: 0.7524446845054626\n",
      "Step: 12800  \tValid loss: 0.4877426028251648\n",
      "Step: 12900  \tTraining loss: 0.4841635227203369\n",
      "Step: 12900  \tTraining accuracy: 0.7525509595870972\n",
      "Step: 12900  \tValid loss: 0.4877221882343292\n",
      "Step: 13000  \tTraining loss: 0.48409509658813477\n",
      "Step: 13000  \tTraining accuracy: 0.7526565194129944\n",
      "Step: 13000  \tValid loss: 0.48772987723350525\n",
      "Step: 13100  \tTraining loss: 0.48403701186180115\n",
      "Step: 13100  \tTraining accuracy: 0.7527604103088379\n",
      "Step: 13100  \tValid loss: 0.48772552609443665\n",
      "Step: 13200  \tTraining loss: 0.4839787483215332\n",
      "Step: 13200  \tTraining accuracy: 0.7528636455535889\n",
      "Step: 13200  \tValid loss: 0.48772552609443665\n",
      "Step: 13300  \tTraining loss: 0.48392924666404724\n",
      "Step: 13300  \tTraining accuracy: 0.7529670596122742\n",
      "Step: 13300  \tValid loss: 0.48772305250167847\n",
      "Step: 13400  \tTraining loss: 0.4838690459728241\n",
      "Step: 13400  \tTraining accuracy: 0.7530689239501953\n",
      "Step: 13400  \tValid loss: 0.4877270460128784\n",
      "Step: 13500  \tTraining loss: 0.4838164448738098\n",
      "Step: 13500  \tTraining accuracy: 0.7531684637069702\n",
      "Step: 13500  \tValid loss: 0.4877322018146515\n",
      "Step: 13600  \tTraining loss: 0.48375752568244934\n",
      "Step: 13600  \tTraining accuracy: 0.753266453742981\n",
      "Step: 13600  \tValid loss: 0.48772308230400085\n",
      "Step: 13700  \tTraining loss: 0.48370644450187683\n",
      "Step: 13700  \tTraining accuracy: 0.753362238407135\n",
      "Step: 13700  \tValid loss: 0.48772215843200684\n",
      "Step: 13800  \tTraining loss: 0.4836503267288208\n",
      "Step: 13800  \tTraining accuracy: 0.7534480690956116\n",
      "Step: 13800  \tValid loss: 0.4877232015132904\n",
      "Step: 13900  \tTraining loss: 0.4836066663265228\n",
      "Step: 13900  \tTraining accuracy: 0.7535275816917419\n",
      "Step: 13900  \tValid loss: 0.48773834109306335\n",
      "Step: 14000  \tTraining loss: 0.48354771733283997\n",
      "Step: 14000  \tTraining accuracy: 0.7536060214042664\n",
      "Step: 14000  \tValid loss: 0.48772725462913513\n",
      "Step: 14100  \tTraining loss: 0.48349329829216003\n",
      "Step: 14100  \tTraining accuracy: 0.7536832690238953\n",
      "Step: 14100  \tValid loss: 0.487721711397171\n",
      "Step: 14200  \tTraining loss: 0.4834429919719696\n",
      "Step: 14200  \tTraining accuracy: 0.7537603378295898\n",
      "Step: 14200  \tValid loss: 0.4877181053161621\n",
      "Step: 14300  \tTraining loss: 0.4833923876285553\n",
      "Step: 14300  \tTraining accuracy: 0.7538395524024963\n",
      "Step: 14300  \tValid loss: 0.48771193623542786\n",
      "Step: 14400  \tTraining loss: 0.4833439290523529\n",
      "Step: 14400  \tTraining accuracy: 0.7539176344871521\n",
      "Step: 14400  \tValid loss: 0.4877239465713501\n",
      "Step: 14500  \tTraining loss: 0.48329293727874756\n",
      "Step: 14500  \tTraining accuracy: 0.7539947032928467\n",
      "Step: 14500  \tValid loss: 0.4877156615257263\n",
      "Step: 14600  \tTraining loss: 0.4832434356212616\n",
      "Step: 14600  \tTraining accuracy: 0.7540714740753174\n",
      "Step: 14600  \tValid loss: 0.48771703243255615\n",
      "Step: 14700  \tTraining loss: 0.48320043087005615\n",
      "Step: 14700  \tTraining accuracy: 0.7541463971138\n",
      "Step: 14700  \tValid loss: 0.48772507905960083\n",
      "Step: 14800  \tTraining loss: 0.4831458330154419\n",
      "Step: 14800  \tTraining accuracy: 0.7542203068733215\n",
      "Step: 14800  \tValid loss: 0.48769721388816833\n",
      "Step: 14900  \tTraining loss: 0.4830802083015442\n",
      "Step: 14900  \tTraining accuracy: 0.7542924284934998\n",
      "Step: 14900  \tValid loss: 0.4876203238964081\n",
      "Step: 15000  \tTraining loss: 0.483020156621933\n",
      "Step: 15000  \tTraining accuracy: 0.7543620467185974\n",
      "Step: 15000  \tValid loss: 0.48753952980041504\n",
      "Step: 15100  \tTraining loss: 0.4829627573490143\n",
      "Step: 15100  \tTraining accuracy: 0.7544353604316711\n",
      "Step: 15100  \tValid loss: 0.48747652769088745\n",
      "Step: 15200  \tTraining loss: 0.4829084873199463\n",
      "Step: 15200  \tTraining accuracy: 0.7545092701911926\n",
      "Step: 15200  \tValid loss: 0.4874436855316162\n",
      "Step: 15300  \tTraining loss: 0.48286473751068115\n",
      "Step: 15300  \tTraining accuracy: 0.75457763671875\n",
      "Step: 15300  \tValid loss: 0.4874323904514313\n",
      "Step: 15400  \tTraining loss: 0.48280543088912964\n",
      "Step: 15400  \tTraining accuracy: 0.7546420693397522\n",
      "Step: 15400  \tValid loss: 0.48740530014038086\n",
      "Step: 15500  \tTraining loss: 0.48276659846305847\n",
      "Step: 15500  \tTraining accuracy: 0.754707932472229\n",
      "Step: 15500  \tValid loss: 0.4874178171157837\n",
      "Step: 15600  \tTraining loss: 0.48271170258522034\n",
      "Step: 15600  \tTraining accuracy: 0.7547751665115356\n",
      "Step: 15600  \tValid loss: 0.4874109625816345\n",
      "Step: 15700  \tTraining loss: 0.4826638400554657\n",
      "Step: 15700  \tTraining accuracy: 0.7548445463180542\n",
      "Step: 15700  \tValid loss: 0.4874247610569\n",
      "Step: 15800  \tTraining loss: 0.48261624574661255\n",
      "Step: 15800  \tTraining accuracy: 0.7549130320549011\n",
      "Step: 15800  \tValid loss: 0.4874310791492462\n",
      "Step: 15900  \tTraining loss: 0.4825683534145355\n",
      "Step: 15900  \tTraining accuracy: 0.7549806833267212\n",
      "Step: 15900  \tValid loss: 0.48743611574172974\n",
      "Step: 16000  \tTraining loss: 0.4825153648853302\n",
      "Step: 16000  \tTraining accuracy: 0.7550474405288696\n",
      "Step: 16000  \tValid loss: 0.4874460697174072\n",
      "Step: 16100  \tTraining loss: 0.48247015476226807\n",
      "Step: 16100  \tTraining accuracy: 0.755113422870636\n",
      "Step: 16100  \tValid loss: 0.48745816946029663\n",
      "Step: 16200  \tTraining loss: 0.48242446780204773\n",
      "Step: 16200  \tTraining accuracy: 0.7551785707473755\n",
      "Step: 16200  \tValid loss: 0.48746800422668457\n",
      "Step: 16300  \tTraining loss: 0.48237738013267517\n",
      "Step: 16300  \tTraining accuracy: 0.7552428841590881\n",
      "Step: 16300  \tValid loss: 0.48746457695961\n",
      "Step: 16400  \tTraining loss: 0.4823417365550995\n",
      "Step: 16400  \tTraining accuracy: 0.7553079128265381\n",
      "Step: 16400  \tValid loss: 0.48748621344566345\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7553742\n",
      "Precision: 0.7779204\n",
      "Recall: 0.7945804\n",
      "F1 score: 0.7633771\n",
      "AUC: 0.7714415\n",
      "   accuracy  precision   recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.755374    0.77792  0.79458  0.763377  0.771442  0.482302      0.755321   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.487389       0.755319   0.496311      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  16491.0  \n",
      "29\n",
      "(8990, 8)\n",
      "(8990, 1)\n",
      "(4960, 8)\n",
      "(4960, 1)\n",
      "(4030, 8)\n",
      "(4030, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5724784135818481\n",
      "Step: 100  \tTraining accuracy: 0.7466073632240295\n",
      "Step: 100  \tValid loss: 0.5630401372909546\n",
      "Step: 200  \tTraining loss: 0.5021324753761292\n",
      "Step: 200  \tTraining accuracy: 0.7662959098815918\n",
      "Step: 200  \tValid loss: 0.48346006870269775\n",
      "Step: 300  \tTraining loss: 0.45539793372154236\n",
      "Step: 300  \tTraining accuracy: 0.7794438004493713\n",
      "Step: 300  \tValid loss: 0.44086530804634094\n",
      "Step: 400  \tTraining loss: 0.4071914851665497\n",
      "Step: 400  \tTraining accuracy: 0.7901477813720703\n",
      "Step: 400  \tValid loss: 0.39697250723838806\n",
      "Step: 500  \tTraining loss: 0.38576894998550415\n",
      "Step: 500  \tTraining accuracy: 0.8007786273956299\n",
      "Step: 500  \tValid loss: 0.37751197814941406\n",
      "Step: 600  \tTraining loss: 0.3771160840988159\n",
      "Step: 600  \tTraining accuracy: 0.8086864352226257\n",
      "Step: 600  \tValid loss: 0.36996084451675415\n",
      "Step: 700  \tTraining loss: 0.373460054397583\n",
      "Step: 700  \tTraining accuracy: 0.8142209053039551\n",
      "Step: 700  \tValid loss: 0.36688098311424255\n",
      "Step: 800  \tTraining loss: 0.3714163303375244\n",
      "Step: 800  \tTraining accuracy: 0.8183907866477966\n",
      "Step: 800  \tValid loss: 0.36499154567718506\n",
      "Step: 900  \tTraining loss: 0.36983630061149597\n",
      "Step: 900  \tTraining accuracy: 0.8215271830558777\n",
      "Step: 900  \tValid loss: 0.36335617303848267\n",
      "Step: 1000  \tTraining loss: 0.36830049753189087\n",
      "Step: 1000  \tTraining accuracy: 0.8238568902015686\n",
      "Step: 1000  \tValid loss: 0.3616466522216797\n",
      "Step: 1100  \tTraining loss: 0.36678680777549744\n",
      "Step: 1100  \tTraining accuracy: 0.8256846070289612\n",
      "Step: 1100  \tValid loss: 0.3599275052547455\n",
      "Step: 1200  \tTraining loss: 0.36558517813682556\n",
      "Step: 1200  \tTraining accuracy: 0.8270880579948425\n",
      "Step: 1200  \tValid loss: 0.3588387072086334\n",
      "Step: 1300  \tTraining loss: 0.3646291494369507\n",
      "Step: 1300  \tTraining accuracy: 0.8282091021537781\n",
      "Step: 1300  \tValid loss: 0.35809004306793213\n",
      "Step: 1400  \tTraining loss: 0.36381807923316956\n",
      "Step: 1400  \tTraining accuracy: 0.8292093873023987\n",
      "Step: 1400  \tValid loss: 0.3574804365634918\n",
      "Step: 1500  \tTraining loss: 0.36301299929618835\n",
      "Step: 1500  \tTraining accuracy: 0.8299413323402405\n",
      "Step: 1500  \tValid loss: 0.3568921387195587\n",
      "Step: 1600  \tTraining loss: 0.3622165620326996\n",
      "Step: 1600  \tTraining accuracy: 0.8306003212928772\n",
      "Step: 1600  \tValid loss: 0.3563367426395416\n",
      "Step: 1700  \tTraining loss: 0.3614859879016876\n",
      "Step: 1700  \tTraining accuracy: 0.8311760425567627\n",
      "Step: 1700  \tValid loss: 0.35583969950675964\n",
      "Step: 1800  \tTraining loss: 0.3608216345310211\n",
      "Step: 1800  \tTraining accuracy: 0.8316192626953125\n",
      "Step: 1800  \tValid loss: 0.3553794324398041\n",
      "Step: 1900  \tTraining loss: 0.3602098524570465\n",
      "Step: 1900  \tTraining accuracy: 0.8320055603981018\n",
      "Step: 1900  \tValid loss: 0.3549402356147766\n",
      "Step: 2000  \tTraining loss: 0.3596385717391968\n",
      "Step: 2000  \tTraining accuracy: 0.8323864340782166\n",
      "Step: 2000  \tValid loss: 0.354554146528244\n",
      "Step: 2100  \tTraining loss: 0.35909220576286316\n",
      "Step: 2100  \tTraining accuracy: 0.8327599763870239\n",
      "Step: 2100  \tValid loss: 0.3541862666606903\n",
      "Step: 2200  \tTraining loss: 0.358568400144577\n",
      "Step: 2200  \tTraining accuracy: 0.8330729007720947\n",
      "Step: 2200  \tValid loss: 0.3538376986980438\n",
      "Step: 2300  \tTraining loss: 0.358059287071228\n",
      "Step: 2300  \tTraining accuracy: 0.8333457112312317\n",
      "Step: 2300  \tValid loss: 0.3534940779209137\n",
      "Step: 2400  \tTraining loss: 0.357568621635437\n",
      "Step: 2400  \tTraining accuracy: 0.8336165547370911\n",
      "Step: 2400  \tValid loss: 0.3531477153301239\n",
      "Step: 2500  \tTraining loss: 0.3570975363254547\n",
      "Step: 2500  \tTraining accuracy: 0.8338698148727417\n",
      "Step: 2500  \tValid loss: 0.3527839183807373\n",
      "Step: 2600  \tTraining loss: 0.35655677318573\n",
      "Step: 2600  \tTraining accuracy: 0.8341076374053955\n",
      "Step: 2600  \tValid loss: 0.35220056772232056\n",
      "Step: 2700  \tTraining loss: 0.35602203011512756\n",
      "Step: 2700  \tTraining accuracy: 0.8343547582626343\n",
      "Step: 2700  \tValid loss: 0.3517531752586365\n",
      "Step: 2800  \tTraining loss: 0.35558822751045227\n",
      "Step: 2800  \tTraining accuracy: 0.8345697522163391\n",
      "Step: 2800  \tValid loss: 0.3513927459716797\n",
      "Step: 2900  \tTraining loss: 0.3551977872848511\n",
      "Step: 2900  \tTraining accuracy: 0.8347774147987366\n",
      "Step: 2900  \tValid loss: 0.3509967625141144\n",
      "Step: 3000  \tTraining loss: 0.35482022166252136\n",
      "Step: 3000  \tTraining accuracy: 0.8349804878234863\n",
      "Step: 3000  \tValid loss: 0.35061538219451904\n",
      "Step: 3100  \tTraining loss: 0.35444802045822144\n",
      "Step: 3100  \tTraining accuracy: 0.8351720571517944\n",
      "Step: 3100  \tValid loss: 0.3502667546272278\n",
      "Step: 3200  \tTraining loss: 0.35406580567359924\n",
      "Step: 3200  \tTraining accuracy: 0.8353708982467651\n",
      "Step: 3200  \tValid loss: 0.34994980692863464\n",
      "Step: 3300  \tTraining loss: 0.35364511609077454\n",
      "Step: 3300  \tTraining accuracy: 0.8355266451835632\n",
      "Step: 3300  \tValid loss: 0.3496289551258087\n",
      "Step: 3400  \tTraining loss: 0.35325688123703003\n",
      "Step: 3400  \tTraining accuracy: 0.8356814384460449\n",
      "Step: 3400  \tValid loss: 0.34937915205955505\n",
      "Step: 3500  \tTraining loss: 0.3528650999069214\n",
      "Step: 3500  \tTraining accuracy: 0.8358369469642639\n",
      "Step: 3500  \tValid loss: 0.34912678599357605\n",
      "Step: 3600  \tTraining loss: 0.3524661958217621\n",
      "Step: 3600  \tTraining accuracy: 0.8359946012496948\n",
      "Step: 3600  \tValid loss: 0.34890466928482056\n",
      "Step: 3700  \tTraining loss: 0.3520602881908417\n",
      "Step: 3700  \tTraining accuracy: 0.8361513018608093\n",
      "Step: 3700  \tValid loss: 0.3487071990966797\n",
      "Step: 3800  \tTraining loss: 0.3516473174095154\n",
      "Step: 3800  \tTraining accuracy: 0.8363233208656311\n",
      "Step: 3800  \tValid loss: 0.34851980209350586\n",
      "Step: 3900  \tTraining loss: 0.3512328565120697\n",
      "Step: 3900  \tTraining accuracy: 0.8365080952644348\n",
      "Step: 3900  \tValid loss: 0.34833887219429016\n",
      "Step: 4000  \tTraining loss: 0.3508222997188568\n",
      "Step: 4000  \tTraining accuracy: 0.836718738079071\n",
      "Step: 4000  \tValid loss: 0.34816429018974304\n",
      "Step: 4100  \tTraining loss: 0.35041800141334534\n",
      "Step: 4100  \tTraining accuracy: 0.8369381427764893\n",
      "Step: 4100  \tValid loss: 0.34799638390541077\n",
      "Step: 4200  \tTraining loss: 0.3500218093395233\n",
      "Step: 4200  \tTraining accuracy: 0.8371376395225525\n",
      "Step: 4200  \tValid loss: 0.34783175587654114\n",
      "Step: 4300  \tTraining loss: 0.3496284484863281\n",
      "Step: 4300  \tTraining accuracy: 0.8373185992240906\n",
      "Step: 4300  \tValid loss: 0.34762659668922424\n",
      "Step: 4400  \tTraining loss: 0.34925323724746704\n",
      "Step: 4400  \tTraining accuracy: 0.8374835252761841\n",
      "Step: 4400  \tValid loss: 0.3474941551685333\n",
      "Step: 4500  \tTraining loss: 0.34889668226242065\n",
      "Step: 4500  \tTraining accuracy: 0.8376360535621643\n",
      "Step: 4500  \tValid loss: 0.3473608195781708\n",
      "Step: 4600  \tTraining loss: 0.34856030344963074\n",
      "Step: 4600  \tTraining accuracy: 0.8377647995948792\n",
      "Step: 4600  \tValid loss: 0.3472457826137543\n",
      "Step: 4700  \tTraining loss: 0.3482431173324585\n",
      "Step: 4700  \tTraining accuracy: 0.8378676176071167\n",
      "Step: 4700  \tValid loss: 0.34714123606681824\n",
      "Step: 4800  \tTraining loss: 0.3479360342025757\n",
      "Step: 4800  \tTraining accuracy: 0.8379696607589722\n",
      "Step: 4800  \tValid loss: 0.3470507562160492\n",
      "Step: 4900  \tTraining loss: 0.3476438820362091\n",
      "Step: 4900  \tTraining accuracy: 0.838080108165741\n",
      "Step: 4900  \tValid loss: 0.34687262773513794\n",
      "Step: 5000  \tTraining loss: 0.34736546874046326\n",
      "Step: 5000  \tTraining accuracy: 0.8382006883621216\n",
      "Step: 5000  \tValid loss: 0.34678417444229126\n",
      "Step: 5100  \tTraining loss: 0.3470942974090576\n",
      "Step: 5100  \tTraining accuracy: 0.8383352160453796\n",
      "Step: 5100  \tValid loss: 0.3467220664024353\n",
      "Step: 5200  \tTraining loss: 0.3468236029148102\n",
      "Step: 5200  \tTraining accuracy: 0.8384450674057007\n",
      "Step: 5200  \tValid loss: 0.346686452627182\n",
      "Step: 5300  \tTraining loss: 0.34654155373573303\n",
      "Step: 5300  \tTraining accuracy: 0.8385444283485413\n",
      "Step: 5300  \tValid loss: 0.3466792404651642\n",
      "Step: 5400  \tTraining loss: 0.346241295337677\n",
      "Step: 5400  \tTraining accuracy: 0.838652491569519\n",
      "Step: 5400  \tValid loss: 0.34666168689727783\n",
      "Step: 5500  \tTraining loss: 0.34593069553375244\n",
      "Step: 5500  \tTraining accuracy: 0.8387525677680969\n",
      "Step: 5500  \tValid loss: 0.346587210893631\n",
      "Step: 5600  \tTraining loss: 0.34561753273010254\n",
      "Step: 5600  \tTraining accuracy: 0.8388530015945435\n",
      "Step: 5600  \tValid loss: 0.34643569588661194\n",
      "Step: 5700  \tTraining loss: 0.3453069031238556\n",
      "Step: 5700  \tTraining accuracy: 0.8389557600021362\n",
      "Step: 5700  \tValid loss: 0.34621790051460266\n",
      "Step: 5800  \tTraining loss: 0.34500041604042053\n",
      "Step: 5800  \tTraining accuracy: 0.8390646576881409\n",
      "Step: 5800  \tValid loss: 0.3460005819797516\n",
      "Step: 5900  \tTraining loss: 0.34469521045684814\n",
      "Step: 5900  \tTraining accuracy: 0.8391812443733215\n",
      "Step: 5900  \tValid loss: 0.34576287865638733\n",
      "Step: 6000  \tTraining loss: 0.34438014030456543\n",
      "Step: 6000  \tTraining accuracy: 0.839293897151947\n",
      "Step: 6000  \tValid loss: 0.34548547863960266\n",
      "Step: 6100  \tTraining loss: 0.3440593183040619\n",
      "Step: 6100  \tTraining accuracy: 0.8394046425819397\n",
      "Step: 6100  \tValid loss: 0.3452266454696655\n",
      "Step: 6200  \tTraining loss: 0.34374427795410156\n",
      "Step: 6200  \tTraining accuracy: 0.8395154476165771\n",
      "Step: 6200  \tValid loss: 0.34496769309043884\n",
      "Step: 6300  \tTraining loss: 0.34342890977859497\n",
      "Step: 6300  \tTraining accuracy: 0.8396298289299011\n",
      "Step: 6300  \tValid loss: 0.3447076976299286\n",
      "Step: 6400  \tTraining loss: 0.34310922026634216\n",
      "Step: 6400  \tTraining accuracy: 0.8397467136383057\n",
      "Step: 6400  \tValid loss: 0.3444356620311737\n",
      "Step: 6500  \tTraining loss: 0.34278613328933716\n",
      "Step: 6500  \tTraining accuracy: 0.8398599624633789\n",
      "Step: 6500  \tValid loss: 0.34415721893310547\n",
      "Step: 6600  \tTraining loss: 0.34238943457603455\n",
      "Step: 6600  \tTraining accuracy: 0.8399697542190552\n",
      "Step: 6600  \tValid loss: 0.3438865840435028\n",
      "Step: 6700  \tTraining loss: 0.34199804067611694\n",
      "Step: 6700  \tTraining accuracy: 0.8400787711143494\n",
      "Step: 6700  \tValid loss: 0.34357166290283203\n",
      "Step: 6800  \tTraining loss: 0.3416401743888855\n",
      "Step: 6800  \tTraining accuracy: 0.8401820659637451\n",
      "Step: 6800  \tValid loss: 0.34333187341690063\n",
      "Step: 6900  \tTraining loss: 0.3413088023662567\n",
      "Step: 6900  \tTraining accuracy: 0.8402774930000305\n",
      "Step: 6900  \tValid loss: 0.3431164622306824\n",
      "Step: 7000  \tTraining loss: 0.34098827838897705\n",
      "Step: 7000  \tTraining accuracy: 0.8403677940368652\n",
      "Step: 7000  \tValid loss: 0.3428906798362732\n",
      "Step: 7100  \tTraining loss: 0.3406892716884613\n",
      "Step: 7100  \tTraining accuracy: 0.8404523730278015\n",
      "Step: 7100  \tValid loss: 0.3427433371543884\n",
      "Step: 7200  \tTraining loss: 0.34039899706840515\n",
      "Step: 7200  \tTraining accuracy: 0.8405337929725647\n",
      "Step: 7200  \tValid loss: 0.3426109552383423\n",
      "Step: 7300  \tTraining loss: 0.3401181101799011\n",
      "Step: 7300  \tTraining accuracy: 0.8406190872192383\n",
      "Step: 7300  \tValid loss: 0.3424946069717407\n",
      "Step: 7400  \tTraining loss: 0.33984628319740295\n",
      "Step: 7400  \tTraining accuracy: 0.8407043218612671\n",
      "Step: 7400  \tValid loss: 0.3423941731452942\n",
      "Step: 7500  \tTraining loss: 0.3395850956439972\n",
      "Step: 7500  \tTraining accuracy: 0.8407925367355347\n",
      "Step: 7500  \tValid loss: 0.3423182964324951\n",
      "Step: 7600  \tTraining loss: 0.3393361568450928\n",
      "Step: 7600  \tTraining accuracy: 0.8408724665641785\n",
      "Step: 7600  \tValid loss: 0.342245876789093\n",
      "Step: 7700  \tTraining loss: 0.3390980064868927\n",
      "Step: 7700  \tTraining accuracy: 0.8409481644630432\n",
      "Step: 7700  \tValid loss: 0.34219130873680115\n",
      "Step: 7800  \tTraining loss: 0.338867723941803\n",
      "Step: 7800  \tTraining accuracy: 0.8410133123397827\n",
      "Step: 7800  \tValid loss: 0.3421476185321808\n",
      "Step: 7900  \tTraining loss: 0.3386446237564087\n",
      "Step: 7900  \tTraining accuracy: 0.8410753607749939\n",
      "Step: 7900  \tValid loss: 0.3421185314655304\n",
      "Step: 8000  \tTraining loss: 0.33842822909355164\n",
      "Step: 8000  \tTraining accuracy: 0.8411351442337036\n",
      "Step: 8000  \tValid loss: 0.34209951758384705\n",
      "Step: 8100  \tTraining loss: 0.3382158875465393\n",
      "Step: 8100  \tTraining accuracy: 0.8411872386932373\n",
      "Step: 8100  \tValid loss: 0.3420906662940979\n",
      "Step: 8200  \tTraining loss: 0.33800509572029114\n",
      "Step: 8200  \tTraining accuracy: 0.8412441611289978\n",
      "Step: 8200  \tValid loss: 0.34208589792251587\n",
      "Step: 8300  \tTraining loss: 0.33778029680252075\n",
      "Step: 8300  \tTraining accuracy: 0.8413004279136658\n",
      "Step: 8300  \tValid loss: 0.3420534133911133\n",
      "Step: 8400  \tTraining loss: 0.33755549788475037\n",
      "Step: 8400  \tTraining accuracy: 0.8413519859313965\n",
      "Step: 8400  \tValid loss: 0.3420208692550659\n",
      "Step: 8500  \tTraining loss: 0.3373223543167114\n",
      "Step: 8500  \tTraining accuracy: 0.8414043188095093\n",
      "Step: 8500  \tValid loss: 0.341981440782547\n",
      "Step: 8600  \tTraining loss: 0.3369812071323395\n",
      "Step: 8600  \tTraining accuracy: 0.8414528369903564\n",
      "Step: 8600  \tValid loss: 0.34173697233200073\n",
      "Step: 8700  \tTraining loss: 0.3366456925868988\n",
      "Step: 8700  \tTraining accuracy: 0.8415066003799438\n",
      "Step: 8700  \tValid loss: 0.34150052070617676\n",
      "Step: 8800  \tTraining loss: 0.33635413646698\n",
      "Step: 8800  \tTraining accuracy: 0.8415642976760864\n",
      "Step: 8800  \tValid loss: 0.34150803089141846\n",
      "Step: 8900  \tTraining loss: 0.3360905051231384\n",
      "Step: 8900  \tTraining accuracy: 0.8416206240653992\n",
      "Step: 8900  \tValid loss: 0.3415158987045288\n",
      "Step: 9000  \tTraining loss: 0.33583638072013855\n",
      "Step: 9000  \tTraining accuracy: 0.8416738510131836\n",
      "Step: 9000  \tValid loss: 0.3415251076221466\n",
      "Step: 9100  \tTraining loss: 0.3355999290943146\n",
      "Step: 9100  \tTraining accuracy: 0.8417271375656128\n",
      "Step: 9100  \tValid loss: 0.3415461778640747\n",
      "Step: 9200  \tTraining loss: 0.3353848457336426\n",
      "Step: 9200  \tTraining accuracy: 0.8417835235595703\n",
      "Step: 9200  \tValid loss: 0.34158962965011597\n",
      "Step: 9300  \tTraining loss: 0.33518123626708984\n",
      "Step: 9300  \tTraining accuracy: 0.8418458700180054\n",
      "Step: 9300  \tValid loss: 0.34163492918014526\n",
      "Step: 9400  \tTraining loss: 0.3349877893924713\n",
      "Step: 9400  \tTraining accuracy: 0.8419140577316284\n",
      "Step: 9400  \tValid loss: 0.3416614234447479\n",
      "Step: 9500  \tTraining loss: 0.3348029851913452\n",
      "Step: 9500  \tTraining accuracy: 0.8419843316078186\n",
      "Step: 9500  \tValid loss: 0.34169113636016846\n",
      "Step: 9600  \tTraining loss: 0.3346235156059265\n",
      "Step: 9600  \tTraining accuracy: 0.8420519828796387\n",
      "Step: 9600  \tValid loss: 0.34170135855674744\n",
      "Step: 9700  \tTraining loss: 0.3344518840312958\n",
      "Step: 9700  \tTraining accuracy: 0.8421221971511841\n",
      "Step: 9700  \tValid loss: 0.34170016646385193\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.84219164\n",
      "Precision: 0.8589065\n",
      "Recall: 0.84201425\n",
      "F1 score: 0.8338276\n",
      "AUC: 0.84766316\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.842192   0.858907  0.842014  0.833828  0.847663  0.334432      0.842164   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.341461       0.842143   0.319013      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  9711.0  \n",
      "30\n",
      "(11890, 8)\n",
      "(11890, 1)\n",
      "(6560, 8)\n",
      "(6560, 1)\n",
      "(5330, 8)\n",
      "(5330, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6511139869689941\n",
      "Step: 100  \tTraining accuracy: 0.6182506084442139\n",
      "Step: 100  \tValid loss: 0.6533347368240356\n",
      "Step: 200  \tTraining loss: 0.6079298853874207\n",
      "Step: 200  \tTraining accuracy: 0.6458088159561157\n",
      "Step: 200  \tValid loss: 0.6108940243721008\n",
      "Step: 300  \tTraining loss: 0.5663985013961792\n",
      "Step: 300  \tTraining accuracy: 0.6710849404335022\n",
      "Step: 300  \tValid loss: 0.5714157223701477\n",
      "Step: 400  \tTraining loss: 0.5496236085891724\n",
      "Step: 400  \tTraining accuracy: 0.6861708760261536\n",
      "Step: 400  \tValid loss: 0.5570465922355652\n",
      "Step: 500  \tTraining loss: 0.5419345498085022\n",
      "Step: 500  \tTraining accuracy: 0.6960564255714417\n",
      "Step: 500  \tValid loss: 0.5515079498291016\n",
      "Step: 600  \tTraining loss: 0.5374424457550049\n",
      "Step: 600  \tTraining accuracy: 0.7032418251037598\n",
      "Step: 600  \tValid loss: 0.5488551259040833\n",
      "Step: 700  \tTraining loss: 0.534162700176239\n",
      "Step: 700  \tTraining accuracy: 0.7083457112312317\n",
      "Step: 700  \tValid loss: 0.5468029975891113\n",
      "Step: 800  \tTraining loss: 0.5318938493728638\n",
      "Step: 800  \tTraining accuracy: 0.7124530673027039\n",
      "Step: 800  \tValid loss: 0.5455803275108337\n",
      "Step: 900  \tTraining loss: 0.5294163227081299\n",
      "Step: 900  \tTraining accuracy: 0.7157027721405029\n",
      "Step: 900  \tValid loss: 0.5440073609352112\n",
      "Step: 1000  \tTraining loss: 0.5276439785957336\n",
      "Step: 1000  \tTraining accuracy: 0.7184365391731262\n",
      "Step: 1000  \tValid loss: 0.5429661273956299\n",
      "Step: 1100  \tTraining loss: 0.5260400772094727\n",
      "Step: 1100  \tTraining accuracy: 0.7207977771759033\n",
      "Step: 1100  \tValid loss: 0.5418230295181274\n",
      "Step: 1200  \tTraining loss: 0.5247617959976196\n",
      "Step: 1200  \tTraining accuracy: 0.7228105664253235\n",
      "Step: 1200  \tValid loss: 0.5409630537033081\n",
      "Step: 1300  \tTraining loss: 0.5236250758171082\n",
      "Step: 1300  \tTraining accuracy: 0.7245652079582214\n",
      "Step: 1300  \tValid loss: 0.5399777889251709\n",
      "Step: 1400  \tTraining loss: 0.5226430296897888\n",
      "Step: 1400  \tTraining accuracy: 0.7260193824768066\n",
      "Step: 1400  \tValid loss: 0.5391235947608948\n",
      "Step: 1500  \tTraining loss: 0.5217821598052979\n",
      "Step: 1500  \tTraining accuracy: 0.727362871170044\n",
      "Step: 1500  \tValid loss: 0.5383219122886658\n",
      "Step: 1600  \tTraining loss: 0.5210412740707397\n",
      "Step: 1600  \tTraining accuracy: 0.7285927534103394\n",
      "Step: 1600  \tValid loss: 0.537553071975708\n",
      "Step: 1700  \tTraining loss: 0.5204190015792847\n",
      "Step: 1700  \tTraining accuracy: 0.7297066450119019\n",
      "Step: 1700  \tValid loss: 0.5370184183120728\n",
      "Step: 1800  \tTraining loss: 0.5198438167572021\n",
      "Step: 1800  \tTraining accuracy: 0.7306764125823975\n",
      "Step: 1800  \tValid loss: 0.5363854169845581\n",
      "Step: 1900  \tTraining loss: 0.5193169713020325\n",
      "Step: 1900  \tTraining accuracy: 0.7315232157707214\n",
      "Step: 1900  \tValid loss: 0.5358495712280273\n",
      "Step: 2000  \tTraining loss: 0.5188323259353638\n",
      "Step: 2000  \tTraining accuracy: 0.7323111295700073\n",
      "Step: 2000  \tValid loss: 0.5351983308792114\n",
      "Step: 2100  \tTraining loss: 0.5183799862861633\n",
      "Step: 2100  \tTraining accuracy: 0.7330406904220581\n",
      "Step: 2100  \tValid loss: 0.5347433686256409\n",
      "Step: 2200  \tTraining loss: 0.5177865624427795\n",
      "Step: 2200  \tTraining accuracy: 0.7336671352386475\n",
      "Step: 2200  \tValid loss: 0.5343798398971558\n",
      "Step: 2300  \tTraining loss: 0.5172964334487915\n",
      "Step: 2300  \tTraining accuracy: 0.7342211008071899\n",
      "Step: 2300  \tValid loss: 0.5338756442070007\n",
      "Step: 2400  \tTraining loss: 0.5168019533157349\n",
      "Step: 2400  \tTraining accuracy: 0.7347154021263123\n",
      "Step: 2400  \tValid loss: 0.5333958864212036\n",
      "Step: 2500  \tTraining loss: 0.5162509679794312\n",
      "Step: 2500  \tTraining accuracy: 0.7352105379104614\n",
      "Step: 2500  \tValid loss: 0.5328112840652466\n",
      "Step: 2600  \tTraining loss: 0.5152801275253296\n",
      "Step: 2600  \tTraining accuracy: 0.7356849312782288\n",
      "Step: 2600  \tValid loss: 0.5322533845901489\n",
      "Step: 2700  \tTraining loss: 0.5146562457084656\n",
      "Step: 2700  \tTraining accuracy: 0.7360997796058655\n",
      "Step: 2700  \tValid loss: 0.531771183013916\n",
      "Step: 2800  \tTraining loss: 0.5141725540161133\n",
      "Step: 2800  \tTraining accuracy: 0.7364875078201294\n",
      "Step: 2800  \tValid loss: 0.531572163105011\n",
      "Step: 2900  \tTraining loss: 0.5136781334877014\n",
      "Step: 2900  \tTraining accuracy: 0.7368243932723999\n",
      "Step: 2900  \tValid loss: 0.5314112901687622\n",
      "Step: 3000  \tTraining loss: 0.5131813883781433\n",
      "Step: 3000  \tTraining accuracy: 0.7371456027030945\n",
      "Step: 3000  \tValid loss: 0.5312197208404541\n",
      "Step: 3100  \tTraining loss: 0.512721061706543\n",
      "Step: 3100  \tTraining accuracy: 0.7374650239944458\n",
      "Step: 3100  \tValid loss: 0.5309398174285889\n",
      "Step: 3200  \tTraining loss: 0.5122835636138916\n",
      "Step: 3200  \tTraining accuracy: 0.7377855181694031\n",
      "Step: 3200  \tValid loss: 0.5306277275085449\n",
      "Step: 3300  \tTraining loss: 0.5118480920791626\n",
      "Step: 3300  \tTraining accuracy: 0.7381134629249573\n",
      "Step: 3300  \tValid loss: 0.5304428935050964\n",
      "Step: 3400  \tTraining loss: 0.5114207863807678\n",
      "Step: 3400  \tTraining accuracy: 0.7384344339370728\n",
      "Step: 3400  \tValid loss: 0.5302267670631409\n",
      "Step: 3500  \tTraining loss: 0.5110428929328918\n",
      "Step: 3500  \tTraining accuracy: 0.7387403845787048\n",
      "Step: 3500  \tValid loss: 0.530182421207428\n",
      "Step: 3600  \tTraining loss: 0.5105882883071899\n",
      "Step: 3600  \tTraining accuracy: 0.7390373945236206\n",
      "Step: 3600  \tValid loss: 0.5298619866371155\n",
      "Step: 3700  \tTraining loss: 0.5101050734519958\n",
      "Step: 3700  \tTraining accuracy: 0.739306628704071\n",
      "Step: 3700  \tValid loss: 0.5295324325561523\n",
      "Step: 3800  \tTraining loss: 0.5097362399101257\n",
      "Step: 3800  \tTraining accuracy: 0.7395738959312439\n",
      "Step: 3800  \tValid loss: 0.5294284224510193\n",
      "Step: 3900  \tTraining loss: 0.5094069242477417\n",
      "Step: 3900  \tTraining accuracy: 0.7398228645324707\n",
      "Step: 3900  \tValid loss: 0.5292271971702576\n",
      "Step: 4000  \tTraining loss: 0.5090832710266113\n",
      "Step: 4000  \tTraining accuracy: 0.740066647529602\n",
      "Step: 4000  \tValid loss: 0.5289485454559326\n",
      "Step: 4100  \tTraining loss: 0.5087809562683105\n",
      "Step: 4100  \tTraining accuracy: 0.7403233051300049\n",
      "Step: 4100  \tValid loss: 0.5286538600921631\n",
      "Step: 4200  \tTraining loss: 0.5084860920906067\n",
      "Step: 4200  \tTraining accuracy: 0.7405797839164734\n",
      "Step: 4200  \tValid loss: 0.5287027955055237\n",
      "Step: 4300  \tTraining loss: 0.50822913646698\n",
      "Step: 4300  \tTraining accuracy: 0.7408242225646973\n",
      "Step: 4300  \tValid loss: 0.5287262797355652\n",
      "Step: 4400  \tTraining loss: 0.5079888701438904\n",
      "Step: 4400  \tTraining accuracy: 0.7410564422607422\n",
      "Step: 4400  \tValid loss: 0.5284704566001892\n",
      "Step: 4500  \tTraining loss: 0.5077508091926575\n",
      "Step: 4500  \tTraining accuracy: 0.7412876486778259\n",
      "Step: 4500  \tValid loss: 0.528296172618866\n",
      "Step: 4600  \tTraining loss: 0.5075029134750366\n",
      "Step: 4600  \tTraining accuracy: 0.7415013313293457\n",
      "Step: 4600  \tValid loss: 0.5281298160552979\n",
      "Step: 4700  \tTraining loss: 0.5072664022445679\n",
      "Step: 4700  \tTraining accuracy: 0.7416895031929016\n",
      "Step: 4700  \tValid loss: 0.5280460715293884\n",
      "Step: 4800  \tTraining loss: 0.5070412158966064\n",
      "Step: 4800  \tTraining accuracy: 0.741860032081604\n",
      "Step: 4800  \tValid loss: 0.5279784798622131\n",
      "Step: 4900  \tTraining loss: 0.506827712059021\n",
      "Step: 4900  \tTraining accuracy: 0.742022693157196\n",
      "Step: 4900  \tValid loss: 0.5278485417366028\n",
      "Step: 5000  \tTraining loss: 0.5066156387329102\n",
      "Step: 5000  \tTraining accuracy: 0.7421914935112\n",
      "Step: 5000  \tValid loss: 0.52776700258255\n",
      "Step: 5100  \tTraining loss: 0.5063982605934143\n",
      "Step: 5100  \tTraining accuracy: 0.7423519492149353\n",
      "Step: 5100  \tValid loss: 0.5276803970336914\n",
      "Step: 5200  \tTraining loss: 0.5061917901039124\n",
      "Step: 5200  \tTraining accuracy: 0.742510199546814\n",
      "Step: 5200  \tValid loss: 0.5276463031768799\n",
      "Step: 5300  \tTraining loss: 0.506004810333252\n",
      "Step: 5300  \tTraining accuracy: 0.7426601052284241\n",
      "Step: 5300  \tValid loss: 0.5276084542274475\n",
      "Step: 5400  \tTraining loss: 0.5058258771896362\n",
      "Step: 5400  \tTraining accuracy: 0.7427980899810791\n",
      "Step: 5400  \tValid loss: 0.5276209712028503\n",
      "Step: 5500  \tTraining loss: 0.5056625604629517\n",
      "Step: 5500  \tTraining accuracy: 0.7429279088973999\n",
      "Step: 5500  \tValid loss: 0.5276265740394592\n",
      "Step: 5600  \tTraining loss: 0.5055047273635864\n",
      "Step: 5600  \tTraining accuracy: 0.7430523037910461\n",
      "Step: 5600  \tValid loss: 0.5276238322257996\n",
      "Step: 5700  \tTraining loss: 0.5053571462631226\n",
      "Step: 5700  \tTraining accuracy: 0.7431693077087402\n",
      "Step: 5700  \tValid loss: 0.5276342034339905\n",
      "Step: 5800  \tTraining loss: 0.5052149891853333\n",
      "Step: 5800  \tTraining accuracy: 0.7432661652565002\n",
      "Step: 5800  \tValid loss: 0.5276132225990295\n",
      "Step: 5900  \tTraining loss: 0.5050795078277588\n",
      "Step: 5900  \tTraining accuracy: 0.743355393409729\n",
      "Step: 5900  \tValid loss: 0.5275619029998779\n",
      "Step: 6000  \tTraining loss: 0.5049463510513306\n",
      "Step: 6000  \tTraining accuracy: 0.743452250957489\n",
      "Step: 6000  \tValid loss: 0.5276069641113281\n",
      "Step: 6100  \tTraining loss: 0.504818320274353\n",
      "Step: 6100  \tTraining accuracy: 0.7435465455055237\n",
      "Step: 6100  \tValid loss: 0.5275920033454895\n",
      "Step: 6200  \tTraining loss: 0.5046972036361694\n",
      "Step: 6200  \tTraining accuracy: 0.7436350584030151\n",
      "Step: 6200  \tValid loss: 0.5275432467460632\n",
      "Step: 6300  \tTraining loss: 0.5045775175094604\n",
      "Step: 6300  \tTraining accuracy: 0.7437288761138916\n",
      "Step: 6300  \tValid loss: 0.5275614261627197\n",
      "Step: 6400  \tTraining loss: 0.504450261592865\n",
      "Step: 6400  \tTraining accuracy: 0.7438163757324219\n",
      "Step: 6400  \tValid loss: 0.5274278521537781\n",
      "Step: 6500  \tTraining loss: 0.504279613494873\n",
      "Step: 6500  \tTraining accuracy: 0.743894636631012\n",
      "Step: 6500  \tValid loss: 0.5271127223968506\n",
      "Step: 6600  \tTraining loss: 0.5041167736053467\n",
      "Step: 6600  \tTraining accuracy: 0.7439666390419006\n",
      "Step: 6600  \tValid loss: 0.5270376801490784\n",
      "Step: 6700  \tTraining loss: 0.5039573907852173\n",
      "Step: 6700  \tTraining accuracy: 0.7440326809883118\n",
      "Step: 6700  \tValid loss: 0.526959240436554\n",
      "Step: 6800  \tTraining loss: 0.5038152933120728\n",
      "Step: 6800  \tTraining accuracy: 0.7440962195396423\n",
      "Step: 6800  \tValid loss: 0.527010440826416\n",
      "Step: 6900  \tTraining loss: 0.503669023513794\n",
      "Step: 6900  \tTraining accuracy: 0.7441688776016235\n",
      "Step: 6900  \tValid loss: 0.5268520712852478\n",
      "Step: 7000  \tTraining loss: 0.5035268068313599\n",
      "Step: 7000  \tTraining accuracy: 0.7442449331283569\n",
      "Step: 7000  \tValid loss: 0.5267595052719116\n",
      "Step: 7100  \tTraining loss: 0.5033921003341675\n",
      "Step: 7100  \tTraining accuracy: 0.7443128228187561\n",
      "Step: 7100  \tValid loss: 0.5267162322998047\n",
      "Step: 7200  \tTraining loss: 0.5032650232315063\n",
      "Step: 7200  \tTraining accuracy: 0.7443752884864807\n",
      "Step: 7200  \tValid loss: 0.5265856981277466\n",
      "Step: 7300  \tTraining loss: 0.5031428337097168\n",
      "Step: 7300  \tTraining accuracy: 0.7444325685501099\n",
      "Step: 7300  \tValid loss: 0.5266095399856567\n",
      "Step: 7400  \tTraining loss: 0.5030248761177063\n",
      "Step: 7400  \tTraining accuracy: 0.7444894313812256\n",
      "Step: 7400  \tValid loss: 0.526615560054779\n",
      "Step: 7500  \tTraining loss: 0.5029049515724182\n",
      "Step: 7500  \tTraining accuracy: 0.7445448040962219\n",
      "Step: 7500  \tValid loss: 0.5265259742736816\n",
      "Step: 7600  \tTraining loss: 0.5027901530265808\n",
      "Step: 7600  \tTraining accuracy: 0.7445986866950989\n",
      "Step: 7600  \tValid loss: 0.5265634059906006\n",
      "Step: 7700  \tTraining loss: 0.5026766061782837\n",
      "Step: 7700  \tTraining accuracy: 0.7446478605270386\n",
      "Step: 7700  \tValid loss: 0.5265359878540039\n",
      "Step: 7800  \tTraining loss: 0.5025661587715149\n",
      "Step: 7800  \tTraining accuracy: 0.7447016835212708\n",
      "Step: 7800  \tValid loss: 0.5265561938285828\n",
      "Step: 7900  \tTraining loss: 0.5024515986442566\n",
      "Step: 7900  \tTraining accuracy: 0.7447552680969238\n",
      "Step: 7900  \tValid loss: 0.526503324508667\n",
      "Step: 8000  \tTraining loss: 0.5023357272148132\n",
      "Step: 8000  \tTraining accuracy: 0.744803786277771\n",
      "Step: 8000  \tValid loss: 0.5265156030654907\n",
      "Step: 8100  \tTraining loss: 0.5022277235984802\n",
      "Step: 8100  \tTraining accuracy: 0.7448495030403137\n",
      "Step: 8100  \tValid loss: 0.5264790654182434\n",
      "Step: 8200  \tTraining loss: 0.5021187663078308\n",
      "Step: 8200  \tTraining accuracy: 0.7449008822441101\n",
      "Step: 8200  \tValid loss: 0.5264478921890259\n",
      "Step: 8300  \tTraining loss: 0.5020159482955933\n",
      "Step: 8300  \tTraining accuracy: 0.7449499368667603\n",
      "Step: 8300  \tValid loss: 0.5263980627059937\n",
      "Step: 8400  \tTraining loss: 0.5019146800041199\n",
      "Step: 8400  \tTraining accuracy: 0.7450008392333984\n",
      "Step: 8400  \tValid loss: 0.5265284776687622\n",
      "Step: 8500  \tTraining loss: 0.5018057227134705\n",
      "Step: 8500  \tTraining accuracy: 0.745046079158783\n",
      "Step: 8500  \tValid loss: 0.5264226198196411\n",
      "Step: 8600  \tTraining loss: 0.5017048120498657\n",
      "Step: 8600  \tTraining accuracy: 0.7450916767120361\n",
      "Step: 8600  \tValid loss: 0.5263891220092773\n",
      "Step: 8700  \tTraining loss: 0.5016031265258789\n",
      "Step: 8700  \tTraining accuracy: 0.7451382279396057\n",
      "Step: 8700  \tValid loss: 0.5263077020645142\n",
      "Step: 8800  \tTraining loss: 0.5015024542808533\n",
      "Step: 8800  \tTraining accuracy: 0.7451798915863037\n",
      "Step: 8800  \tValid loss: 0.5262781381607056\n",
      "Step: 8900  \tTraining loss: 0.5014044642448425\n",
      "Step: 8900  \tTraining accuracy: 0.7452191114425659\n",
      "Step: 8900  \tValid loss: 0.5262166261672974\n",
      "Step: 9000  \tTraining loss: 0.5013061761856079\n",
      "Step: 9000  \tTraining accuracy: 0.7452584505081177\n",
      "Step: 9000  \tValid loss: 0.5262349843978882\n",
      "Step: 9100  \tTraining loss: 0.5012090802192688\n",
      "Step: 9100  \tTraining accuracy: 0.7452978491783142\n",
      "Step: 9100  \tValid loss: 0.5262604355812073\n",
      "Step: 9200  \tTraining loss: 0.5011200904846191\n",
      "Step: 9200  \tTraining accuracy: 0.7453354001045227\n",
      "Step: 9200  \tValid loss: 0.526299774646759\n",
      "Step: 9300  \tTraining loss: 0.5010164976119995\n",
      "Step: 9300  \tTraining accuracy: 0.7453726530075073\n",
      "Step: 9300  \tValid loss: 0.5262052416801453\n",
      "Step: 9400  \tTraining loss: 0.50091952085495\n",
      "Step: 9400  \tTraining accuracy: 0.745406448841095\n",
      "Step: 9400  \tValid loss: 0.5262791514396667\n",
      "Step: 9500  \tTraining loss: 0.5008275508880615\n",
      "Step: 9500  \tTraining accuracy: 0.7454407811164856\n",
      "Step: 9500  \tValid loss: 0.5262822508811951\n",
      "Step: 9600  \tTraining loss: 0.5007402896881104\n",
      "Step: 9600  \tTraining accuracy: 0.7454757690429688\n",
      "Step: 9600  \tValid loss: 0.5261813998222351\n",
      "Step: 9700  \tTraining loss: 0.5006545186042786\n",
      "Step: 9700  \tTraining accuracy: 0.7455130815505981\n",
      "Step: 9700  \tValid loss: 0.5262976288795471\n",
      "Step: 9800  \tTraining loss: 0.5005695223808289\n",
      "Step: 9800  \tTraining accuracy: 0.7455495595932007\n",
      "Step: 9800  \tValid loss: 0.5261570811271667\n",
      "Step: 9900  \tTraining loss: 0.5004826188087463\n",
      "Step: 9900  \tTraining accuracy: 0.7455806732177734\n",
      "Step: 9900  \tValid loss: 0.5261764526367188\n",
      "Step: 10000  \tTraining loss: 0.5003981590270996\n",
      "Step: 10000  \tTraining accuracy: 0.7456145286560059\n",
      "Step: 10000  \tValid loss: 0.5262482166290283\n",
      "Step: 10100  \tTraining loss: 0.5003132820129395\n",
      "Step: 10100  \tTraining accuracy: 0.7456493973731995\n",
      "Step: 10100  \tValid loss: 0.5261127352714539\n",
      "Step: 10200  \tTraining loss: 0.5002283453941345\n",
      "Step: 10200  \tTraining accuracy: 0.7456839680671692\n",
      "Step: 10200  \tValid loss: 0.5261223316192627\n",
      "Step: 10300  \tTraining loss: 0.5001446604728699\n",
      "Step: 10300  \tTraining accuracy: 0.7457186579704285\n",
      "Step: 10300  \tValid loss: 0.5261663198471069\n",
      "Step: 10400  \tTraining loss: 0.5000592470169067\n",
      "Step: 10400  \tTraining accuracy: 0.7457515001296997\n",
      "Step: 10400  \tValid loss: 0.5261496305465698\n",
      "Step: 10500  \tTraining loss: 0.4999748766422272\n",
      "Step: 10500  \tTraining accuracy: 0.7457845211029053\n",
      "Step: 10500  \tValid loss: 0.5260078310966492\n",
      "Step: 10600  \tTraining loss: 0.49989044666290283\n",
      "Step: 10600  \tTraining accuracy: 0.7458168864250183\n",
      "Step: 10600  \tValid loss: 0.5260010361671448\n",
      "Step: 10700  \tTraining loss: 0.4998079240322113\n",
      "Step: 10700  \tTraining accuracy: 0.7458526492118835\n",
      "Step: 10700  \tValid loss: 0.5259642004966736\n",
      "Step: 10800  \tTraining loss: 0.4997270703315735\n",
      "Step: 10800  \tTraining accuracy: 0.7458907961845398\n",
      "Step: 10800  \tValid loss: 0.5258630514144897\n",
      "Step: 10900  \tTraining loss: 0.499644935131073\n",
      "Step: 10900  \tTraining accuracy: 0.745929479598999\n",
      "Step: 10900  \tValid loss: 0.525815486907959\n",
      "Step: 11000  \tTraining loss: 0.4995637834072113\n",
      "Step: 11000  \tTraining accuracy: 0.7459666132926941\n",
      "Step: 11000  \tValid loss: 0.5258384943008423\n",
      "Step: 11100  \tTraining loss: 0.4994819760322571\n",
      "Step: 11100  \tTraining accuracy: 0.7460016012191772\n",
      "Step: 11100  \tValid loss: 0.5257588624954224\n",
      "Step: 11200  \tTraining loss: 0.49939894676208496\n",
      "Step: 11200  \tTraining accuracy: 0.7460371255874634\n",
      "Step: 11200  \tValid loss: 0.5257567167282104\n",
      "Step: 11300  \tTraining loss: 0.4993194341659546\n",
      "Step: 11300  \tTraining accuracy: 0.7460756897926331\n",
      "Step: 11300  \tValid loss: 0.5256937742233276\n",
      "Step: 11400  \tTraining loss: 0.49923816323280334\n",
      "Step: 11400  \tTraining accuracy: 0.746116578578949\n",
      "Step: 11400  \tValid loss: 0.5258201956748962\n",
      "Step: 11500  \tTraining loss: 0.49915793538093567\n",
      "Step: 11500  \tTraining accuracy: 0.7461563348770142\n",
      "Step: 11500  \tValid loss: 0.5256856679916382\n",
      "Step: 11600  \tTraining loss: 0.4990732967853546\n",
      "Step: 11600  \tTraining accuracy: 0.7462031245231628\n",
      "Step: 11600  \tValid loss: 0.5256662368774414\n",
      "Step: 11700  \tTraining loss: 0.49899041652679443\n",
      "Step: 11700  \tTraining accuracy: 0.7462429404258728\n",
      "Step: 11700  \tValid loss: 0.5255985856056213\n",
      "Step: 11800  \tTraining loss: 0.49890509247779846\n",
      "Step: 11800  \tTraining accuracy: 0.7462809681892395\n",
      "Step: 11800  \tValid loss: 0.5256540179252625\n",
      "Step: 11900  \tTraining loss: 0.4988254904747009\n",
      "Step: 11900  \tTraining accuracy: 0.7463158965110779\n",
      "Step: 11900  \tValid loss: 0.5256020426750183\n",
      "Step: 12000  \tTraining loss: 0.4987430274486542\n",
      "Step: 12000  \tTraining accuracy: 0.7463492155075073\n",
      "Step: 12000  \tValid loss: 0.525641918182373\n",
      "Step: 12100  \tTraining loss: 0.49866312742233276\n",
      "Step: 12100  \tTraining accuracy: 0.7463777661323547\n",
      "Step: 12100  \tValid loss: 0.5256281495094299\n",
      "Step: 12200  \tTraining loss: 0.49858397245407104\n",
      "Step: 12200  \tTraining accuracy: 0.7464092969894409\n",
      "Step: 12200  \tValid loss: 0.5255895256996155\n",
      "Step: 12300  \tTraining loss: 0.49850356578826904\n",
      "Step: 12300  \tTraining accuracy: 0.7464444637298584\n",
      "Step: 12300  \tValid loss: 0.52550208568573\n",
      "Step: 12400  \tTraining loss: 0.4984230101108551\n",
      "Step: 12400  \tTraining accuracy: 0.7464817762374878\n",
      "Step: 12400  \tValid loss: 0.5255463123321533\n",
      "Step: 12500  \tTraining loss: 0.4983428120613098\n",
      "Step: 12500  \tTraining accuracy: 0.7465171217918396\n",
      "Step: 12500  \tValid loss: 0.5255037546157837\n",
      "Step: 12600  \tTraining loss: 0.4982627332210541\n",
      "Step: 12600  \tTraining accuracy: 0.7465552687644958\n",
      "Step: 12600  \tValid loss: 0.5253845453262329\n",
      "Step: 12700  \tTraining loss: 0.4981817305088043\n",
      "Step: 12700  \tTraining accuracy: 0.7465954422950745\n",
      "Step: 12700  \tValid loss: 0.5252996683120728\n",
      "Step: 12800  \tTraining loss: 0.4981018900871277\n",
      "Step: 12800  \tTraining accuracy: 0.7466326951980591\n",
      "Step: 12800  \tValid loss: 0.5252745747566223\n",
      "Step: 12900  \tTraining loss: 0.49801507592201233\n",
      "Step: 12900  \tTraining accuracy: 0.7466697096824646\n",
      "Step: 12900  \tValid loss: 0.5252694487571716\n",
      "Step: 13000  \tTraining loss: 0.497932106256485\n",
      "Step: 13000  \tTraining accuracy: 0.7467077374458313\n",
      "Step: 13000  \tValid loss: 0.5252559185028076\n",
      "Step: 13100  \tTraining loss: 0.4978501796722412\n",
      "Step: 13100  \tTraining accuracy: 0.7467484474182129\n",
      "Step: 13100  \tValid loss: 0.5250913500785828\n",
      "Step: 13200  \tTraining loss: 0.49776938557624817\n",
      "Step: 13200  \tTraining accuracy: 0.7467920184135437\n",
      "Step: 13200  \tValid loss: 0.5250966548919678\n",
      "Step: 13300  \tTraining loss: 0.49769189953804016\n",
      "Step: 13300  \tTraining accuracy: 0.7468340396881104\n",
      "Step: 13300  \tValid loss: 0.5249293446540833\n",
      "Step: 13400  \tTraining loss: 0.4975971281528473\n",
      "Step: 13400  \tTraining accuracy: 0.7468725442886353\n",
      "Step: 13400  \tValid loss: 0.5250424742698669\n",
      "Step: 13500  \tTraining loss: 0.49750298261642456\n",
      "Step: 13500  \tTraining accuracy: 0.7469130158424377\n",
      "Step: 13500  \tValid loss: 0.5249029397964478\n",
      "Step: 13600  \tTraining loss: 0.4974188804626465\n",
      "Step: 13600  \tTraining accuracy: 0.7469534873962402\n",
      "Step: 13600  \tValid loss: 0.5247585773468018\n",
      "Step: 13700  \tTraining loss: 0.49732935428619385\n",
      "Step: 13700  \tTraining accuracy: 0.746993362903595\n",
      "Step: 13700  \tValid loss: 0.5247399210929871\n",
      "Step: 13800  \tTraining loss: 0.49724510312080383\n",
      "Step: 13800  \tTraining accuracy: 0.7470307946205139\n",
      "Step: 13800  \tValid loss: 0.5246992111206055\n",
      "Step: 13900  \tTraining loss: 0.49716222286224365\n",
      "Step: 13900  \tTraining accuracy: 0.7470683455467224\n",
      "Step: 13900  \tValid loss: 0.5246123671531677\n",
      "Step: 14000  \tTraining loss: 0.497079998254776\n",
      "Step: 14000  \tTraining accuracy: 0.7471065521240234\n",
      "Step: 14000  \tValid loss: 0.5245766639709473\n",
      "Step: 14100  \tTraining loss: 0.4969973862171173\n",
      "Step: 14100  \tTraining accuracy: 0.7471483945846558\n",
      "Step: 14100  \tValid loss: 0.5245295166969299\n",
      "Step: 14200  \tTraining loss: 0.4969141483306885\n",
      "Step: 14200  \tTraining accuracy: 0.7471872568130493\n",
      "Step: 14200  \tValid loss: 0.5245451927185059\n",
      "Step: 14300  \tTraining loss: 0.49682843685150146\n",
      "Step: 14300  \tTraining accuracy: 0.7472270727157593\n",
      "Step: 14300  \tValid loss: 0.5245044231414795\n",
      "Step: 14400  \tTraining loss: 0.49674084782600403\n",
      "Step: 14400  \tTraining accuracy: 0.7472671866416931\n",
      "Step: 14400  \tValid loss: 0.5244611501693726\n",
      "Step: 14500  \tTraining loss: 0.4966523349285126\n",
      "Step: 14500  \tTraining accuracy: 0.7473073601722717\n",
      "Step: 14500  \tValid loss: 0.5244092345237732\n",
      "Step: 14600  \tTraining loss: 0.49656206369400024\n",
      "Step: 14600  \tTraining accuracy: 0.7473486661911011\n",
      "Step: 14600  \tValid loss: 0.5243348479270935\n",
      "Step: 14700  \tTraining loss: 0.4964750111103058\n",
      "Step: 14700  \tTraining accuracy: 0.7473897337913513\n",
      "Step: 14700  \tValid loss: 0.5242319107055664\n",
      "Step: 14800  \tTraining loss: 0.49638447165489197\n",
      "Step: 14800  \tTraining accuracy: 0.7474371194839478\n",
      "Step: 14800  \tValid loss: 0.524258553981781\n",
      "Step: 14900  \tTraining loss: 0.4963010847568512\n",
      "Step: 14900  \tTraining accuracy: 0.74748295545578\n",
      "Step: 14900  \tValid loss: 0.5241996645927429\n",
      "Step: 15000  \tTraining loss: 0.49621936678886414\n",
      "Step: 15000  \tTraining accuracy: 0.7475265264511108\n",
      "Step: 15000  \tValid loss: 0.5242447853088379\n",
      "Step: 15100  \tTraining loss: 0.496141254901886\n",
      "Step: 15100  \tTraining accuracy: 0.7475700378417969\n",
      "Step: 15100  \tValid loss: 0.5242443680763245\n",
      "Step: 15200  \tTraining loss: 0.49606287479400635\n",
      "Step: 15200  \tTraining accuracy: 0.747612714767456\n",
      "Step: 15200  \tValid loss: 0.5242100954055786\n",
      "Step: 15300  \tTraining loss: 0.4959874153137207\n",
      "Step: 15300  \tTraining accuracy: 0.747655987739563\n",
      "Step: 15300  \tValid loss: 0.524160623550415\n",
      "Step: 15400  \tTraining loss: 0.4959140419960022\n",
      "Step: 15400  \tTraining accuracy: 0.7476992011070251\n",
      "Step: 15400  \tValid loss: 0.5242205262184143\n",
      "Step: 15500  \tTraining loss: 0.4958401322364807\n",
      "Step: 15500  \tTraining accuracy: 0.7477426528930664\n",
      "Step: 15500  \tValid loss: 0.5240527987480164\n",
      "Step: 15600  \tTraining loss: 0.4957702159881592\n",
      "Step: 15600  \tTraining accuracy: 0.7477852702140808\n",
      "Step: 15600  \tValid loss: 0.5242199897766113\n",
      "Step: 15700  \tTraining loss: 0.49569642543792725\n",
      "Step: 15700  \tTraining accuracy: 0.7478271126747131\n",
      "Step: 15700  \tValid loss: 0.52415531873703\n",
      "Step: 15800  \tTraining loss: 0.4956275224685669\n",
      "Step: 15800  \tTraining accuracy: 0.7478702664375305\n",
      "Step: 15800  \tValid loss: 0.5240238308906555\n",
      "Step: 15900  \tTraining loss: 0.49555593729019165\n",
      "Step: 15900  \tTraining accuracy: 0.7479131817817688\n",
      "Step: 15900  \tValid loss: 0.5241173505783081\n",
      "Step: 16000  \tTraining loss: 0.4954894483089447\n",
      "Step: 16000  \tTraining accuracy: 0.7479557991027832\n",
      "Step: 16000  \tValid loss: 0.5239733457565308\n",
      "Step: 16100  \tTraining loss: 0.49542346596717834\n",
      "Step: 16100  \tTraining accuracy: 0.7479994297027588\n",
      "Step: 16100  \tValid loss: 0.5240673422813416\n",
      "Step: 16200  \tTraining loss: 0.4953603446483612\n",
      "Step: 16200  \tTraining accuracy: 0.748039960861206\n",
      "Step: 16200  \tValid loss: 0.5240147709846497\n",
      "Step: 16300  \tTraining loss: 0.49529731273651123\n",
      "Step: 16300  \tTraining accuracy: 0.7480801939964294\n",
      "Step: 16300  \tValid loss: 0.5241264700889587\n",
      "Step: 16400  \tTraining loss: 0.4952141046524048\n",
      "Step: 16400  \tTraining accuracy: 0.7481231093406677\n",
      "Step: 16400  \tValid loss: 0.5242469906806946\n",
      "Step: 16500  \tTraining loss: 0.4951469600200653\n",
      "Step: 16500  \tTraining accuracy: 0.7481662034988403\n",
      "Step: 16500  \tValid loss: 0.524318516254425\n",
      "Step: 16600  \tTraining loss: 0.49508655071258545\n",
      "Step: 16600  \tTraining accuracy: 0.7482103109359741\n",
      "Step: 16600  \tValid loss: 0.5242822170257568\n",
      "Step: 16700  \tTraining loss: 0.4950258135795593\n",
      "Step: 16700  \tTraining accuracy: 0.7482518553733826\n",
      "Step: 16700  \tValid loss: 0.5242971777915955\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7482942\n",
      "Precision: 0.7280488\n",
      "Recall: 0.63483626\n",
      "F1 score: 0.6971157\n",
      "AUC: 0.73985827\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.748294   0.728049  0.634836  0.697116  0.739858  0.494987      0.748254   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.523937       0.748261   0.502081      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  16761.0  \n",
      "31\n",
      "(5800, 8)\n",
      "(5800, 1)\n",
      "(3120, 8)\n",
      "(3120, 1)\n",
      "(2535, 8)\n",
      "(2535, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5667673945426941\n",
      "Step: 100  \tTraining accuracy: 0.7467241287231445\n",
      "Step: 100  \tValid loss: 0.5853490829467773\n",
      "Step: 200  \tTraining loss: 0.4785322844982147\n",
      "Step: 200  \tTraining accuracy: 0.7586786150932312\n",
      "Step: 200  \tValid loss: 0.5026067495346069\n",
      "Step: 300  \tTraining loss: 0.3984676003456116\n",
      "Step: 300  \tTraining accuracy: 0.7807384133338928\n",
      "Step: 300  \tValid loss: 0.4239291548728943\n",
      "Step: 400  \tTraining loss: 0.3473564684391022\n",
      "Step: 400  \tTraining accuracy: 0.7986804246902466\n",
      "Step: 400  \tValid loss: 0.3774341344833374\n",
      "Step: 500  \tTraining loss: 0.32380154728889465\n",
      "Step: 500  \tTraining accuracy: 0.8133475184440613\n",
      "Step: 500  \tValid loss: 0.3586568236351013\n",
      "Step: 600  \tTraining loss: 0.3100228011608124\n",
      "Step: 600  \tTraining accuracy: 0.8235117197036743\n",
      "Step: 600  \tValid loss: 0.34804537892341614\n",
      "Step: 700  \tTraining loss: 0.29956144094467163\n",
      "Step: 700  \tTraining accuracy: 0.8311418294906616\n",
      "Step: 700  \tValid loss: 0.3396700322628021\n",
      "Step: 800  \tTraining loss: 0.29179325699806213\n",
      "Step: 800  \tTraining accuracy: 0.8373669981956482\n",
      "Step: 800  \tValid loss: 0.33317869901657104\n",
      "Step: 900  \tTraining loss: 0.2864192724227905\n",
      "Step: 900  \tTraining accuracy: 0.8427647948265076\n",
      "Step: 900  \tValid loss: 0.32853439450263977\n",
      "Step: 1000  \tTraining loss: 0.2828097343444824\n",
      "Step: 1000  \tTraining accuracy: 0.8472473621368408\n",
      "Step: 1000  \tValid loss: 0.3253726661205292\n",
      "Step: 1100  \tTraining loss: 0.2804048955440521\n",
      "Step: 1100  \tTraining accuracy: 0.8509015440940857\n",
      "Step: 1100  \tValid loss: 0.3233788013458252\n",
      "Step: 1200  \tTraining loss: 0.2787317633628845\n",
      "Step: 1200  \tTraining accuracy: 0.8539660573005676\n",
      "Step: 1200  \tValid loss: 0.32208526134490967\n",
      "Step: 1300  \tTraining loss: 0.27748218178749084\n",
      "Step: 1300  \tTraining accuracy: 0.8566452860832214\n",
      "Step: 1300  \tValid loss: 0.3211633265018463\n",
      "Step: 1400  \tTraining loss: 0.2764609158039093\n",
      "Step: 1400  \tTraining accuracy: 0.8589794039726257\n",
      "Step: 1400  \tValid loss: 0.3204665780067444\n",
      "Step: 1500  \tTraining loss: 0.27553191781044006\n",
      "Step: 1500  \tTraining accuracy: 0.8610158562660217\n",
      "Step: 1500  \tValid loss: 0.3199416995048523\n",
      "Step: 1600  \tTraining loss: 0.2746290862560272\n",
      "Step: 1600  \tTraining accuracy: 0.8628008365631104\n",
      "Step: 1600  \tValid loss: 0.31957879662513733\n",
      "Step: 1700  \tTraining loss: 0.2737807631492615\n",
      "Step: 1700  \tTraining accuracy: 0.8643590211868286\n",
      "Step: 1700  \tValid loss: 0.3193188011646271\n",
      "Step: 1800  \tTraining loss: 0.2730115056037903\n",
      "Step: 1800  \tTraining accuracy: 0.8657441139221191\n",
      "Step: 1800  \tValid loss: 0.31907394528388977\n",
      "Step: 1900  \tTraining loss: 0.27229928970336914\n",
      "Step: 1900  \tTraining accuracy: 0.8669843077659607\n",
      "Step: 1900  \tValid loss: 0.31880971789360046\n",
      "Step: 2000  \tTraining loss: 0.27160948514938354\n",
      "Step: 2000  \tTraining accuracy: 0.8680973052978516\n",
      "Step: 2000  \tValid loss: 0.31851089000701904\n",
      "Step: 2100  \tTraining loss: 0.2709025740623474\n",
      "Step: 2100  \tTraining accuracy: 0.8691230416297913\n",
      "Step: 2100  \tValid loss: 0.3181644082069397\n",
      "Step: 2200  \tTraining loss: 0.2701372802257538\n",
      "Step: 2200  \tTraining accuracy: 0.8700452446937561\n",
      "Step: 2200  \tValid loss: 0.31778132915496826\n",
      "Step: 2300  \tTraining loss: 0.26929229497909546\n",
      "Step: 2300  \tTraining accuracy: 0.8708971738815308\n",
      "Step: 2300  \tValid loss: 0.3174441158771515\n",
      "Step: 2400  \tTraining loss: 0.26841291785240173\n",
      "Step: 2400  \tTraining accuracy: 0.8716914653778076\n",
      "Step: 2400  \tValid loss: 0.3172541558742523\n",
      "Step: 2500  \tTraining loss: 0.26757481694221497\n",
      "Step: 2500  \tTraining accuracy: 0.8724636435508728\n",
      "Step: 2500  \tValid loss: 0.317116379737854\n",
      "Step: 2600  \tTraining loss: 0.26679956912994385\n",
      "Step: 2600  \tTraining accuracy: 0.873212993144989\n",
      "Step: 2600  \tValid loss: 0.31690719723701477\n",
      "Step: 2700  \tTraining loss: 0.266078382730484\n",
      "Step: 2700  \tTraining accuracy: 0.873902440071106\n",
      "Step: 2700  \tValid loss: 0.3166111409664154\n",
      "Step: 2800  \tTraining loss: 0.2654045522212982\n",
      "Step: 2800  \tTraining accuracy: 0.8745132088661194\n",
      "Step: 2800  \tValid loss: 0.31627941131591797\n",
      "Step: 2900  \tTraining loss: 0.26477640867233276\n",
      "Step: 2900  \tTraining accuracy: 0.8750811815261841\n",
      "Step: 2900  \tValid loss: 0.31598392128944397\n",
      "Step: 3000  \tTraining loss: 0.26419252157211304\n",
      "Step: 3000  \tTraining accuracy: 0.8756076097488403\n",
      "Step: 3000  \tValid loss: 0.31574535369873047\n",
      "Step: 3100  \tTraining loss: 0.26364848017692566\n",
      "Step: 3100  \tTraining accuracy: 0.8761138916015625\n",
      "Step: 3100  \tValid loss: 0.3155525028705597\n",
      "Step: 3200  \tTraining loss: 0.2631389796733856\n",
      "Step: 3200  \tTraining accuracy: 0.8765714168548584\n",
      "Step: 3200  \tValid loss: 0.3153887689113617\n",
      "Step: 3300  \tTraining loss: 0.26266124844551086\n",
      "Step: 3300  \tTraining accuracy: 0.8769927024841309\n",
      "Step: 3300  \tValid loss: 0.31522831320762634\n",
      "Step: 3400  \tTraining loss: 0.26220980286598206\n",
      "Step: 3400  \tTraining accuracy: 0.8773888349533081\n",
      "Step: 3400  \tValid loss: 0.31506234407424927\n",
      "Step: 3500  \tTraining loss: 0.2617814540863037\n",
      "Step: 3500  \tTraining accuracy: 0.877774715423584\n",
      "Step: 3500  \tValid loss: 0.31487399339675903\n",
      "Step: 3600  \tTraining loss: 0.26137325167655945\n",
      "Step: 3600  \tTraining accuracy: 0.8781289458274841\n",
      "Step: 3600  \tValid loss: 0.3146614134311676\n",
      "Step: 3700  \tTraining loss: 0.2609834372997284\n",
      "Step: 3700  \tTraining accuracy: 0.8784542679786682\n",
      "Step: 3700  \tValid loss: 0.3144223392009735\n",
      "Step: 3800  \tTraining loss: 0.2606087625026703\n",
      "Step: 3800  \tTraining accuracy: 0.8787552118301392\n",
      "Step: 3800  \tValid loss: 0.3141542077064514\n",
      "Step: 3900  \tTraining loss: 0.2602445185184479\n",
      "Step: 3900  \tTraining accuracy: 0.8790382742881775\n",
      "Step: 3900  \tValid loss: 0.3138490617275238\n",
      "Step: 4000  \tTraining loss: 0.25988104939460754\n",
      "Step: 4000  \tTraining accuracy: 0.8793070316314697\n",
      "Step: 4000  \tValid loss: 0.31348007917404175\n",
      "Step: 4100  \tTraining loss: 0.2595024108886719\n",
      "Step: 4100  \tTraining accuracy: 0.8795560598373413\n",
      "Step: 4100  \tValid loss: 0.31298524141311646\n",
      "Step: 4200  \tTraining loss: 0.25908371806144714\n",
      "Step: 4200  \tTraining accuracy: 0.8797930479049683\n",
      "Step: 4200  \tValid loss: 0.3122517168521881\n",
      "Step: 4300  \tTraining loss: 0.25861766934394836\n",
      "Step: 4300  \tTraining accuracy: 0.8800312280654907\n",
      "Step: 4300  \tValid loss: 0.3112424314022064\n",
      "Step: 4400  \tTraining loss: 0.25814953446388245\n",
      "Step: 4400  \tTraining accuracy: 0.8802544474601746\n",
      "Step: 4400  \tValid loss: 0.3102641999721527\n",
      "Step: 4500  \tTraining loss: 0.25770652294158936\n",
      "Step: 4500  \tTraining accuracy: 0.8804774284362793\n",
      "Step: 4500  \tValid loss: 0.30948516726493835\n",
      "Step: 4600  \tTraining loss: 0.2572847008705139\n",
      "Step: 4600  \tTraining accuracy: 0.8806906342506409\n",
      "Step: 4600  \tValid loss: 0.30881476402282715\n",
      "Step: 4700  \tTraining loss: 0.25687888264656067\n",
      "Step: 4700  \tTraining accuracy: 0.8809040188789368\n",
      "Step: 4700  \tValid loss: 0.30819520354270935\n",
      "Step: 4800  \tTraining loss: 0.25648871064186096\n",
      "Step: 4800  \tTraining accuracy: 0.8811157941818237\n",
      "Step: 4800  \tValid loss: 0.3076121509075165\n",
      "Step: 4900  \tTraining loss: 0.2561142146587372\n",
      "Step: 4900  \tTraining accuracy: 0.8813080191612244\n",
      "Step: 4900  \tValid loss: 0.3070606291294098\n",
      "Step: 5000  \tTraining loss: 0.25575411319732666\n",
      "Step: 5000  \tTraining accuracy: 0.8815101385116577\n",
      "Step: 5000  \tValid loss: 0.30652496218681335\n",
      "Step: 5100  \tTraining loss: 0.25540632009506226\n",
      "Step: 5100  \tTraining accuracy: 0.8817111849784851\n",
      "Step: 5100  \tValid loss: 0.3059929311275482\n",
      "Step: 5200  \tTraining loss: 0.2550694942474365\n",
      "Step: 5200  \tTraining accuracy: 0.881912887096405\n",
      "Step: 5200  \tValid loss: 0.3054558336734772\n",
      "Step: 5300  \tTraining loss: 0.2547417879104614\n",
      "Step: 5300  \tTraining accuracy: 0.882113516330719\n",
      "Step: 5300  \tValid loss: 0.30491095781326294\n",
      "Step: 5400  \tTraining loss: 0.25442230701446533\n",
      "Step: 5400  \tTraining accuracy: 0.882306694984436\n",
      "Step: 5400  \tValid loss: 0.3043563961982727\n",
      "Step: 5500  \tTraining loss: 0.25411027669906616\n",
      "Step: 5500  \tTraining accuracy: 0.8824975490570068\n",
      "Step: 5500  \tValid loss: 0.30379167199134827\n",
      "Step: 5600  \tTraining loss: 0.2538047134876251\n",
      "Step: 5600  \tTraining accuracy: 0.8826799988746643\n",
      "Step: 5600  \tValid loss: 0.30321556329727173\n",
      "Step: 5700  \tTraining loss: 0.25350505113601685\n",
      "Step: 5700  \tTraining accuracy: 0.8828559517860413\n",
      "Step: 5700  \tValid loss: 0.30262741446495056\n",
      "Step: 5800  \tTraining loss: 0.2532110810279846\n",
      "Step: 5800  \tTraining accuracy: 0.8830318450927734\n",
      "Step: 5800  \tValid loss: 0.3020291030406952\n",
      "Step: 5900  \tTraining loss: 0.2529221177101135\n",
      "Step: 5900  \tTraining accuracy: 0.8831987977027893\n",
      "Step: 5900  \tValid loss: 0.30141985416412354\n",
      "Step: 6000  \tTraining loss: 0.2526378333568573\n",
      "Step: 6000  \tTraining accuracy: 0.8833586573600769\n",
      "Step: 6000  \tValid loss: 0.30080223083496094\n",
      "Step: 6100  \tTraining loss: 0.2523578703403473\n",
      "Step: 6100  \tTraining accuracy: 0.8835262060165405\n",
      "Step: 6100  \tValid loss: 0.3001789152622223\n",
      "Step: 6200  \tTraining loss: 0.2520820200443268\n",
      "Step: 6200  \tTraining accuracy: 0.8836840391159058\n",
      "Step: 6200  \tValid loss: 0.2995522618293762\n",
      "Step: 6300  \tTraining loss: 0.2518097162246704\n",
      "Step: 6300  \tTraining accuracy: 0.8838340044021606\n",
      "Step: 6300  \tValid loss: 0.29892611503601074\n",
      "Step: 6400  \tTraining loss: 0.2515413165092468\n",
      "Step: 6400  \tTraining accuracy: 0.8839751482009888\n",
      "Step: 6400  \tValid loss: 0.2983057200908661\n",
      "Step: 6500  \tTraining loss: 0.25127658247947693\n",
      "Step: 6500  \tTraining accuracy: 0.8841119408607483\n",
      "Step: 6500  \tValid loss: 0.29769331216812134\n",
      "Step: 6600  \tTraining loss: 0.2510150671005249\n",
      "Step: 6600  \tTraining accuracy: 0.8842431902885437\n",
      "Step: 6600  \tValid loss: 0.2970939874649048\n",
      "Step: 6700  \tTraining loss: 0.2507568299770355\n",
      "Step: 6700  \tTraining accuracy: 0.8843665719032288\n",
      "Step: 6700  \tValid loss: 0.296510249376297\n",
      "Step: 6800  \tTraining loss: 0.250501424074173\n",
      "Step: 6800  \tTraining accuracy: 0.8844953775405884\n",
      "Step: 6800  \tValid loss: 0.29594457149505615\n",
      "Step: 6900  \tTraining loss: 0.25024983286857605\n",
      "Step: 6900  \tTraining accuracy: 0.8846254944801331\n",
      "Step: 6900  \tValid loss: 0.2954026162624359\n",
      "Step: 7000  \tTraining loss: 0.2500011920928955\n",
      "Step: 7000  \tTraining accuracy: 0.884756863117218\n",
      "Step: 7000  \tValid loss: 0.2948806881904602\n",
      "Step: 7100  \tTraining loss: 0.24975477159023285\n",
      "Step: 7100  \tTraining accuracy: 0.8848857879638672\n",
      "Step: 7100  \tValid loss: 0.29438164830207825\n",
      "Step: 7200  \tTraining loss: 0.2495102435350418\n",
      "Step: 7200  \tTraining accuracy: 0.8850110769271851\n",
      "Step: 7200  \tValid loss: 0.29390621185302734\n",
      "Step: 7300  \tTraining loss: 0.24926723539829254\n",
      "Step: 7300  \tTraining accuracy: 0.8851293325424194\n",
      "Step: 7300  \tValid loss: 0.293453574180603\n",
      "Step: 7400  \tTraining loss: 0.2490251660346985\n",
      "Step: 7400  \tTraining accuracy: 0.8852396011352539\n",
      "Step: 7400  \tValid loss: 0.29302337765693665\n",
      "Step: 7500  \tTraining loss: 0.24878384172916412\n",
      "Step: 7500  \tTraining accuracy: 0.8853456974029541\n",
      "Step: 7500  \tValid loss: 0.2926154136657715\n",
      "Step: 7600  \tTraining loss: 0.24854272603988647\n",
      "Step: 7600  \tTraining accuracy: 0.8854513168334961\n",
      "Step: 7600  \tValid loss: 0.29222893714904785\n",
      "Step: 7700  \tTraining loss: 0.24830195307731628\n",
      "Step: 7700  \tTraining accuracy: 0.8855565190315247\n",
      "Step: 7700  \tValid loss: 0.29186275601387024\n",
      "Step: 7800  \tTraining loss: 0.2480611652135849\n",
      "Step: 7800  \tTraining accuracy: 0.8856645822525024\n",
      "Step: 7800  \tValid loss: 0.29151633381843567\n",
      "Step: 7900  \tTraining loss: 0.24782054126262665\n",
      "Step: 7900  \tTraining accuracy: 0.8857809901237488\n",
      "Step: 7900  \tValid loss: 0.29118943214416504\n",
      "Step: 8000  \tTraining loss: 0.24758031964302063\n",
      "Step: 8000  \tTraining accuracy: 0.8858956098556519\n",
      "Step: 8000  \tValid loss: 0.2908811867237091\n",
      "Step: 8100  \tTraining loss: 0.24734078347682953\n",
      "Step: 8100  \tTraining accuracy: 0.8860095143318176\n",
      "Step: 8100  \tValid loss: 0.2905917465686798\n",
      "Step: 8200  \tTraining loss: 0.24710260331630707\n",
      "Step: 8200  \tTraining accuracy: 0.8861206769943237\n",
      "Step: 8200  \tValid loss: 0.29032161831855774\n",
      "Step: 8300  \tTraining loss: 0.24686603248119354\n",
      "Step: 8300  \tTraining accuracy: 0.886228084564209\n",
      "Step: 8300  \tValid loss: 0.29007023572921753\n",
      "Step: 8400  \tTraining loss: 0.2466316968202591\n",
      "Step: 8400  \tTraining accuracy: 0.8863286972045898\n",
      "Step: 8400  \tValid loss: 0.28983744978904724\n",
      "Step: 8500  \tTraining loss: 0.2463998943567276\n",
      "Step: 8500  \tTraining accuracy: 0.8864269256591797\n",
      "Step: 8500  \tValid loss: 0.2896227240562439\n",
      "Step: 8600  \tTraining loss: 0.2461707592010498\n",
      "Step: 8600  \tTraining accuracy: 0.8865228891372681\n",
      "Step: 8600  \tValid loss: 0.2894255518913269\n",
      "Step: 8700  \tTraining loss: 0.2459445297718048\n",
      "Step: 8700  \tTraining accuracy: 0.8866155743598938\n",
      "Step: 8700  \tValid loss: 0.28924456238746643\n",
      "Step: 8800  \tTraining loss: 0.2457212656736374\n",
      "Step: 8800  \tTraining accuracy: 0.8867071866989136\n",
      "Step: 8800  \tValid loss: 0.28907838463783264\n",
      "Step: 8900  \tTraining loss: 0.2455012947320938\n",
      "Step: 8900  \tTraining accuracy: 0.8867937922477722\n",
      "Step: 8900  \tValid loss: 0.28892531991004944\n",
      "Step: 9000  \tTraining loss: 0.24528458714485168\n",
      "Step: 9000  \tTraining accuracy: 0.8868784308433533\n",
      "Step: 9000  \tValid loss: 0.28878381848335266\n",
      "Step: 9100  \tTraining loss: 0.24507100880146027\n",
      "Step: 9100  \tTraining accuracy: 0.8869631290435791\n",
      "Step: 9100  \tValid loss: 0.2886520028114319\n",
      "Step: 9200  \tTraining loss: 0.24486088752746582\n",
      "Step: 9200  \tTraining accuracy: 0.8870488405227661\n",
      "Step: 9200  \tValid loss: 0.2885286211967468\n",
      "Step: 9300  \tTraining loss: 0.2446538656949997\n",
      "Step: 9300  \tTraining accuracy: 0.8871411681175232\n",
      "Step: 9300  \tValid loss: 0.2884117364883423\n",
      "Step: 9400  \tTraining loss: 0.24444976449012756\n",
      "Step: 9400  \tTraining accuracy: 0.8872315287590027\n",
      "Step: 9400  \tValid loss: 0.2883011996746063\n",
      "Step: 9500  \tTraining loss: 0.2442484200000763\n",
      "Step: 9500  \tTraining accuracy: 0.8873190879821777\n",
      "Step: 9500  \tValid loss: 0.28819572925567627\n",
      "Step: 9600  \tTraining loss: 0.24404974281787872\n",
      "Step: 9600  \tTraining accuracy: 0.88740473985672\n",
      "Step: 9600  \tValid loss: 0.2880948483943939\n",
      "Step: 9700  \tTraining loss: 0.24385346472263336\n",
      "Step: 9700  \tTraining accuracy: 0.8874941468238831\n",
      "Step: 9700  \tValid loss: 0.2879977226257324\n",
      "Step: 9800  \tTraining loss: 0.24365916848182678\n",
      "Step: 9800  \tTraining accuracy: 0.8875852227210999\n",
      "Step: 9800  \tValid loss: 0.2879040539264679\n",
      "Step: 9900  \tTraining loss: 0.2434668391942978\n",
      "Step: 9900  \tTraining accuracy: 0.8876753449440002\n",
      "Step: 9900  \tValid loss: 0.2878137528896332\n",
      "Step: 10000  \tTraining loss: 0.24327626824378967\n",
      "Step: 10000  \tTraining accuracy: 0.8877689242362976\n",
      "Step: 10000  \tValid loss: 0.2877260148525238\n",
      "Step: 10100  \tTraining loss: 0.24308715760707855\n",
      "Step: 10100  \tTraining accuracy: 0.8878563642501831\n",
      "Step: 10100  \tValid loss: 0.2876411974430084\n",
      "Step: 10200  \tTraining loss: 0.24289949238300323\n",
      "Step: 10200  \tTraining accuracy: 0.8879393935203552\n",
      "Step: 10200  \tValid loss: 0.28755924105644226\n",
      "Step: 10300  \tTraining loss: 0.24271297454833984\n",
      "Step: 10300  \tTraining accuracy: 0.8880192041397095\n",
      "Step: 10300  \tValid loss: 0.28747984766960144\n",
      "Step: 10400  \tTraining loss: 0.24252761900424957\n",
      "Step: 10400  \tTraining accuracy: 0.8880974054336548\n",
      "Step: 10400  \tValid loss: 0.2874031662940979\n",
      "Step: 10500  \tTraining loss: 0.24234339594841003\n",
      "Step: 10500  \tTraining accuracy: 0.8881741166114807\n",
      "Step: 10500  \tValid loss: 0.28732946515083313\n",
      "Step: 10600  \tTraining loss: 0.2421603500843048\n",
      "Step: 10600  \tTraining accuracy: 0.888249397277832\n",
      "Step: 10600  \tValid loss: 0.28725844621658325\n",
      "Step: 10700  \tTraining loss: 0.24197843670845032\n",
      "Step: 10700  \tTraining accuracy: 0.8883224129676819\n",
      "Step: 10700  \tValid loss: 0.2871902883052826\n",
      "Step: 10800  \tTraining loss: 0.2417978048324585\n",
      "Step: 10800  \tTraining accuracy: 0.8883948922157288\n",
      "Step: 10800  \tValid loss: 0.28712525963783264\n",
      "Step: 10900  \tTraining loss: 0.24161849915981293\n",
      "Step: 10900  \tTraining accuracy: 0.8884660601615906\n",
      "Step: 10900  \tValid loss: 0.28706300258636475\n",
      "Step: 11000  \tTraining loss: 0.241440549492836\n",
      "Step: 11000  \tTraining accuracy: 0.8885359168052673\n",
      "Step: 11000  \tValid loss: 0.28700345754623413\n",
      "Step: 11100  \tTraining loss: 0.2412637621164322\n",
      "Step: 11100  \tTraining accuracy: 0.8886045217514038\n",
      "Step: 11100  \tValid loss: 0.2869471311569214\n",
      "Step: 11200  \tTraining loss: 0.24108849465847015\n",
      "Step: 11200  \tTraining accuracy: 0.888671875\n",
      "Step: 11200  \tValid loss: 0.2868936061859131\n",
      "Step: 11300  \tTraining loss: 0.24091404676437378\n",
      "Step: 11300  \tTraining accuracy: 0.8887372612953186\n",
      "Step: 11300  \tValid loss: 0.2868427336215973\n",
      "Step: 11400  \tTraining loss: 0.24074088037014008\n",
      "Step: 11400  \tTraining accuracy: 0.8887984156608582\n",
      "Step: 11400  \tValid loss: 0.2867942154407501\n",
      "Step: 11500  \tTraining loss: 0.24056847393512726\n",
      "Step: 11500  \tTraining accuracy: 0.8888569474220276\n",
      "Step: 11500  \tValid loss: 0.28674787282943726\n",
      "Step: 11600  \tTraining loss: 0.24039702117443085\n",
      "Step: 11600  \tTraining accuracy: 0.8889167904853821\n",
      "Step: 11600  \tValid loss: 0.28670385479927063\n",
      "Step: 11700  \tTraining loss: 0.24022625386714935\n",
      "Step: 11700  \tTraining accuracy: 0.88897705078125\n",
      "Step: 11700  \tValid loss: 0.2866617739200592\n",
      "Step: 11800  \tTraining loss: 0.24005606770515442\n",
      "Step: 11800  \tTraining accuracy: 0.8890362977981567\n",
      "Step: 11800  \tValid loss: 0.28662171959877014\n",
      "Step: 11900  \tTraining loss: 0.23988638818264008\n",
      "Step: 11900  \tTraining accuracy: 0.8890982866287231\n",
      "Step: 11900  \tValid loss: 0.2865833342075348\n",
      "Step: 12000  \tTraining loss: 0.23971714079380035\n",
      "Step: 12000  \tTraining accuracy: 0.8891620635986328\n",
      "Step: 12000  \tValid loss: 0.2865467071533203\n",
      "Step: 12100  \tTraining loss: 0.23954804241657257\n",
      "Step: 12100  \tTraining accuracy: 0.8892277479171753\n",
      "Step: 12100  \tValid loss: 0.2865113317966461\n",
      "Step: 12200  \tTraining loss: 0.2393791228532791\n",
      "Step: 12200  \tTraining accuracy: 0.8892923593521118\n",
      "Step: 12200  \tValid loss: 0.2864772379398346\n",
      "Step: 12300  \tTraining loss: 0.2392103523015976\n",
      "Step: 12300  \tTraining accuracy: 0.8893558979034424\n",
      "Step: 12300  \tValid loss: 0.2864440381526947\n",
      "Step: 12400  \tTraining loss: 0.2390415370464325\n",
      "Step: 12400  \tTraining accuracy: 0.8894190788269043\n",
      "Step: 12400  \tValid loss: 0.2864118814468384\n",
      "Step: 12500  \tTraining loss: 0.23887278139591217\n",
      "Step: 12500  \tTraining accuracy: 0.8894826769828796\n",
      "Step: 12500  \tValid loss: 0.2863803505897522\n",
      "Step: 12600  \tTraining loss: 0.2387036830186844\n",
      "Step: 12600  \tTraining accuracy: 0.8895466923713684\n",
      "Step: 12600  \tValid loss: 0.2863493263721466\n",
      "Step: 12700  \tTraining loss: 0.23853464424610138\n",
      "Step: 12700  \tTraining accuracy: 0.8896102905273438\n",
      "Step: 12700  \tValid loss: 0.28631895780563354\n",
      "Step: 12800  \tTraining loss: 0.23836533725261688\n",
      "Step: 12800  \tTraining accuracy: 0.8896729946136475\n",
      "Step: 12800  \tValid loss: 0.2862882912158966\n",
      "Step: 12900  \tTraining loss: 0.23819571733474731\n",
      "Step: 12900  \tTraining accuracy: 0.8897346258163452\n",
      "Step: 12900  \tValid loss: 0.28625792264938354\n",
      "Step: 13000  \tTraining loss: 0.23802617192268372\n",
      "Step: 13000  \tTraining accuracy: 0.8897940516471863\n",
      "Step: 13000  \tValid loss: 0.2862277030944824\n",
      "Step: 13100  \tTraining loss: 0.23785614967346191\n",
      "Step: 13100  \tTraining accuracy: 0.8898511528968811\n",
      "Step: 13100  \tValid loss: 0.28619691729545593\n",
      "Step: 13200  \tTraining loss: 0.23768605291843414\n",
      "Step: 13200  \tTraining accuracy: 0.8899120688438416\n",
      "Step: 13200  \tValid loss: 0.28616610169410706\n",
      "Step: 13300  \tTraining loss: 0.23751595616340637\n",
      "Step: 13300  \tTraining accuracy: 0.8899727463722229\n",
      "Step: 13300  \tValid loss: 0.28613463044166565\n",
      "Step: 13400  \tTraining loss: 0.23734572529792786\n",
      "Step: 13400  \tTraining accuracy: 0.8900337815284729\n",
      "Step: 13400  \tValid loss: 0.2861025929450989\n",
      "Step: 13500  \tTraining loss: 0.23717546463012695\n",
      "Step: 13500  \tTraining accuracy: 0.8900964856147766\n",
      "Step: 13500  \tValid loss: 0.2860698997974396\n",
      "Step: 13600  \tTraining loss: 0.2370053231716156\n",
      "Step: 13600  \tTraining accuracy: 0.8901628255844116\n",
      "Step: 13600  \tValid loss: 0.2860363721847534\n",
      "Step: 13700  \tTraining loss: 0.23683540523052216\n",
      "Step: 13700  \tTraining accuracy: 0.890230119228363\n",
      "Step: 13700  \tValid loss: 0.28600195050239563\n",
      "Step: 13800  \tTraining loss: 0.23666585981845856\n",
      "Step: 13800  \tTraining accuracy: 0.890296995639801\n",
      "Step: 13800  \tValid loss: 0.28596624732017517\n",
      "Step: 13900  \tTraining loss: 0.2364967316389084\n",
      "Step: 13900  \tTraining accuracy: 0.890365481376648\n",
      "Step: 13900  \tValid loss: 0.285928875207901\n",
      "Step: 14000  \tTraining loss: 0.23632802069187164\n",
      "Step: 14000  \tTraining accuracy: 0.8904330134391785\n",
      "Step: 14000  \tValid loss: 0.2858894169330597\n",
      "Step: 14100  \tTraining loss: 0.23616012930870056\n",
      "Step: 14100  \tTraining accuracy: 0.8905020356178284\n",
      "Step: 14100  \tValid loss: 0.28584790229797363\n",
      "Step: 14200  \tTraining loss: 0.23599298298358917\n",
      "Step: 14200  \tTraining accuracy: 0.8905712962150574\n",
      "Step: 14200  \tValid loss: 0.2858039438724518\n",
      "Step: 14300  \tTraining loss: 0.2358267456293106\n",
      "Step: 14300  \tTraining accuracy: 0.89063960313797\n",
      "Step: 14300  \tValid loss: 0.28575751185417175\n",
      "Step: 14400  \tTraining loss: 0.23566149175167084\n",
      "Step: 14400  \tTraining accuracy: 0.8907069563865662\n",
      "Step: 14400  \tValid loss: 0.28570878505706787\n",
      "Step: 14500  \tTraining loss: 0.23549725115299225\n",
      "Step: 14500  \tTraining accuracy: 0.8907769918441772\n",
      "Step: 14500  \tValid loss: 0.28565770387649536\n",
      "Step: 14600  \tTraining loss: 0.2353343814611435\n",
      "Step: 14600  \tTraining accuracy: 0.8908514976501465\n",
      "Step: 14600  \tValid loss: 0.2856040894985199\n",
      "Step: 14700  \tTraining loss: 0.23517277836799622\n",
      "Step: 14700  \tTraining accuracy: 0.8909267783164978\n",
      "Step: 14700  \tValid loss: 0.2855496406555176\n",
      "Step: 14800  \tTraining loss: 0.23501241207122803\n",
      "Step: 14800  \tTraining accuracy: 0.8910009860992432\n",
      "Step: 14800  \tValid loss: 0.2854926288127899\n",
      "Step: 14900  \tTraining loss: 0.23485364019870758\n",
      "Step: 14900  \tTraining accuracy: 0.8910742402076721\n",
      "Step: 14900  \tValid loss: 0.28543350100517273\n",
      "Step: 15000  \tTraining loss: 0.2346964180469513\n",
      "Step: 15000  \tTraining accuracy: 0.8911464810371399\n",
      "Step: 15000  \tValid loss: 0.2853724956512451\n",
      "Step: 15100  \tTraining loss: 0.2345409095287323\n",
      "Step: 15100  \tTraining accuracy: 0.8912177681922913\n",
      "Step: 15100  \tValid loss: 0.2853104770183563\n",
      "Step: 15200  \tTraining loss: 0.23438695073127747\n",
      "Step: 15200  \tTraining accuracy: 0.8912881016731262\n",
      "Step: 15200  \tValid loss: 0.2852478325366974\n",
      "Step: 15300  \tTraining loss: 0.2342347800731659\n",
      "Step: 15300  \tTraining accuracy: 0.8913575410842896\n",
      "Step: 15300  \tValid loss: 0.2851853370666504\n",
      "Step: 15400  \tTraining loss: 0.2340846061706543\n",
      "Step: 15400  \tTraining accuracy: 0.8914260864257812\n",
      "Step: 15400  \tValid loss: 0.2851237654685974\n",
      "Step: 15500  \tTraining loss: 0.23393601179122925\n",
      "Step: 15500  \tTraining accuracy: 0.8914931416511536\n",
      "Step: 15500  \tValid loss: 0.28506359457969666\n",
      "Step: 15600  \tTraining loss: 0.2337893545627594\n",
      "Step: 15600  \tTraining accuracy: 0.891558825969696\n",
      "Step: 15600  \tValid loss: 0.28500500321388245\n",
      "Step: 15700  \tTraining loss: 0.23364455997943878\n",
      "Step: 15700  \tTraining accuracy: 0.8916236162185669\n",
      "Step: 15700  \tValid loss: 0.2849484086036682\n",
      "Step: 15800  \tTraining loss: 0.2335016429424286\n",
      "Step: 15800  \tTraining accuracy: 0.8916898369789124\n",
      "Step: 15800  \tValid loss: 0.2848941385746002\n",
      "Step: 15900  \tTraining loss: 0.2333606332540512\n",
      "Step: 15900  \tTraining accuracy: 0.8917568325996399\n",
      "Step: 15900  \tValid loss: 0.28484249114990234\n",
      "Step: 16000  \tTraining loss: 0.23322156071662903\n",
      "Step: 16000  \tTraining accuracy: 0.8918246626853943\n",
      "Step: 16000  \tValid loss: 0.2847937047481537\n",
      "Step: 16100  \tTraining loss: 0.2330843210220337\n",
      "Step: 16100  \tTraining accuracy: 0.8918894529342651\n",
      "Step: 16100  \tValid loss: 0.28474751114845276\n",
      "Step: 16200  \tTraining loss: 0.23294900357723236\n",
      "Step: 16200  \tTraining accuracy: 0.891950786113739\n",
      "Step: 16200  \tValid loss: 0.2847041189670563\n",
      "Step: 16300  \tTraining loss: 0.23281578719615936\n",
      "Step: 16300  \tTraining accuracy: 0.892007052898407\n",
      "Step: 16300  \tValid loss: 0.2846633493900299\n",
      "Step: 16400  \tTraining loss: 0.23268437385559082\n",
      "Step: 16400  \tTraining accuracy: 0.8920593857765198\n",
      "Step: 16400  \tValid loss: 0.2846253514289856\n",
      "Step: 16500  \tTraining loss: 0.2325550764799118\n",
      "Step: 16500  \tTraining accuracy: 0.892112672328949\n",
      "Step: 16500  \tValid loss: 0.28459009528160095\n",
      "Step: 16600  \tTraining loss: 0.23242764174938202\n",
      "Step: 16600  \tTraining accuracy: 0.8921664357185364\n",
      "Step: 16600  \tValid loss: 0.2845577597618103\n",
      "Step: 16700  \tTraining loss: 0.23230215907096863\n",
      "Step: 16700  \tTraining accuracy: 0.8922221064567566\n",
      "Step: 16700  \tValid loss: 0.2845282256603241\n",
      "Step: 16800  \tTraining loss: 0.23217865824699402\n",
      "Step: 16800  \tTraining accuracy: 0.8922792077064514\n",
      "Step: 16800  \tValid loss: 0.28450170159339905\n",
      "Step: 16900  \tTraining loss: 0.23205704987049103\n",
      "Step: 16900  \tTraining accuracy: 0.8923356533050537\n",
      "Step: 16900  \tValid loss: 0.2844785749912262\n",
      "Step: 17000  \tTraining loss: 0.2319372594356537\n",
      "Step: 17000  \tTraining accuracy: 0.89239501953125\n",
      "Step: 17000  \tValid loss: 0.28445836901664734\n",
      "Step: 17100  \tTraining loss: 0.23181934654712677\n",
      "Step: 17100  \tTraining accuracy: 0.8924557566642761\n",
      "Step: 17100  \tValid loss: 0.28444141149520874\n",
      "Step: 17200  \tTraining loss: 0.23170308768749237\n",
      "Step: 17200  \tTraining accuracy: 0.8925163149833679\n",
      "Step: 17200  \tValid loss: 0.2844274640083313\n",
      "Step: 17300  \tTraining loss: 0.23158860206604004\n",
      "Step: 17300  \tTraining accuracy: 0.8925761580467224\n",
      "Step: 17300  \tValid loss: 0.28441646695137024\n",
      "Step: 17400  \tTraining loss: 0.23147568106651306\n",
      "Step: 17400  \tTraining accuracy: 0.8926352858543396\n",
      "Step: 17400  \tValid loss: 0.28440868854522705\n",
      "Step: 17500  \tTraining loss: 0.23136447370052338\n",
      "Step: 17500  \tTraining accuracy: 0.8926927447319031\n",
      "Step: 17500  \tValid loss: 0.28440383076667786\n",
      "Step: 17600  \tTraining loss: 0.23125475645065308\n",
      "Step: 17600  \tTraining accuracy: 0.8927470445632935\n",
      "Step: 17600  \tValid loss: 0.2844022214412689\n",
      "Step: 17700  \tTraining loss: 0.23114663362503052\n",
      "Step: 17700  \tTraining accuracy: 0.8928012251853943\n",
      "Step: 17700  \tValid loss: 0.284403920173645\n",
      "Step: 17800  \tTraining loss: 0.23103979229927063\n",
      "Step: 17800  \tTraining accuracy: 0.8928568363189697\n",
      "Step: 17800  \tValid loss: 0.2844083309173584\n",
      "Step: 17900  \tTraining loss: 0.23093436658382416\n",
      "Step: 17900  \tTraining accuracy: 0.8929117321968079\n",
      "Step: 17900  \tValid loss: 0.28441646695137024\n",
      "Step: 18000  \tTraining loss: 0.2308303713798523\n",
      "Step: 18000  \tTraining accuracy: 0.892966091632843\n",
      "Step: 18000  \tValid loss: 0.2844282388687134\n",
      "Step: 18100  \tTraining loss: 0.23072749376296997\n",
      "Step: 18100  \tTraining accuracy: 0.8930197954177856\n",
      "Step: 18100  \tValid loss: 0.2844437062740326\n",
      "Step: 18200  \tTraining loss: 0.2306259721517563\n",
      "Step: 18200  \tTraining accuracy: 0.8930729031562805\n",
      "Step: 18200  \tValid loss: 0.28446242213249207\n",
      "Step: 18300  \tTraining loss: 0.2305254489183426\n",
      "Step: 18300  \tTraining accuracy: 0.8931254744529724\n",
      "Step: 18300  \tValid loss: 0.28448426723480225\n",
      "Step: 18400  \tTraining loss: 0.23042628169059753\n",
      "Step: 18400  \tTraining accuracy: 0.893178403377533\n",
      "Step: 18400  \tValid loss: 0.2845093905925751\n",
      "Step: 18500  \tTraining loss: 0.23032821714878082\n",
      "Step: 18500  \tTraining accuracy: 0.8932307362556458\n",
      "Step: 18500  \tValid loss: 0.28453701734542847\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.89328253\n",
      "Precision: 0.88294315\n",
      "Recall: 0.8922687\n",
      "F1 score: 0.89797515\n",
      "AUC: 0.90535367\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.893283   0.882943  0.892269  0.897975  0.905354  0.230247      0.893243   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.284402       0.893245   0.241194      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  18582.0  \n",
      "32\n",
      "(4205, 8)\n",
      "(4205, 1)\n",
      "(2240, 8)\n",
      "(2240, 1)\n",
      "(1820, 8)\n",
      "(1820, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6311945915222168\n",
      "Step: 100  \tTraining accuracy: 0.6461355686187744\n",
      "Step: 100  \tValid loss: 0.6361020803451538\n",
      "Step: 200  \tTraining loss: 0.5753709077835083\n",
      "Step: 200  \tTraining accuracy: 0.6653568744659424\n",
      "Step: 200  \tValid loss: 0.5754144787788391\n",
      "Step: 300  \tTraining loss: 0.4965296685695648\n",
      "Step: 300  \tTraining accuracy: 0.6952013373374939\n",
      "Step: 300  \tValid loss: 0.486482173204422\n",
      "Step: 400  \tTraining loss: 0.43722131848335266\n",
      "Step: 400  \tTraining accuracy: 0.724965512752533\n",
      "Step: 400  \tValid loss: 0.4169849753379822\n",
      "Step: 500  \tTraining loss: 0.40173158049583435\n",
      "Step: 500  \tTraining accuracy: 0.7486917972564697\n",
      "Step: 500  \tValid loss: 0.37237247824668884\n",
      "Step: 600  \tTraining loss: 0.3820927143096924\n",
      "Step: 600  \tTraining accuracy: 0.7660883069038391\n",
      "Step: 600  \tValid loss: 0.34560203552246094\n",
      "Step: 700  \tTraining loss: 0.37198537588119507\n",
      "Step: 700  \tTraining accuracy: 0.7790501117706299\n",
      "Step: 700  \tValid loss: 0.3302954137325287\n",
      "Step: 800  \tTraining loss: 0.3654618561267853\n",
      "Step: 800  \tTraining accuracy: 0.7889139652252197\n",
      "Step: 800  \tValid loss: 0.3206922113895416\n",
      "Step: 900  \tTraining loss: 0.36206430196762085\n",
      "Step: 900  \tTraining accuracy: 0.79654461145401\n",
      "Step: 900  \tValid loss: 0.31560006737709045\n",
      "Step: 1000  \tTraining loss: 0.36010435223579407\n",
      "Step: 1000  \tTraining accuracy: 0.8024939298629761\n",
      "Step: 1000  \tValid loss: 0.31250613927841187\n",
      "Step: 1100  \tTraining loss: 0.358689546585083\n",
      "Step: 1100  \tTraining accuracy: 0.8072879910469055\n",
      "Step: 1100  \tValid loss: 0.3106445372104645\n",
      "Step: 1200  \tTraining loss: 0.3575170934200287\n",
      "Step: 1200  \tTraining accuracy: 0.8115538358688354\n",
      "Step: 1200  \tValid loss: 0.30939924716949463\n",
      "Step: 1300  \tTraining loss: 0.35644814372062683\n",
      "Step: 1300  \tTraining accuracy: 0.8153600692749023\n",
      "Step: 1300  \tValid loss: 0.30845528841018677\n",
      "Step: 1400  \tTraining loss: 0.3554368317127228\n",
      "Step: 1400  \tTraining accuracy: 0.8187819123268127\n",
      "Step: 1400  \tValid loss: 0.30760154128074646\n",
      "Step: 1500  \tTraining loss: 0.35444363951683044\n",
      "Step: 1500  \tTraining accuracy: 0.82182377576828\n",
      "Step: 1500  \tValid loss: 0.30678367614746094\n",
      "Step: 1600  \tTraining loss: 0.3535310626029968\n",
      "Step: 1600  \tTraining accuracy: 0.8244889974594116\n",
      "Step: 1600  \tValid loss: 0.30602356791496277\n",
      "Step: 1700  \tTraining loss: 0.35268691182136536\n",
      "Step: 1700  \tTraining accuracy: 0.8268166780471802\n",
      "Step: 1700  \tValid loss: 0.3053343892097473\n",
      "Step: 1800  \tTraining loss: 0.35186585783958435\n",
      "Step: 1800  \tTraining accuracy: 0.8288300633430481\n",
      "Step: 1800  \tValid loss: 0.30475977063179016\n",
      "Step: 1900  \tTraining loss: 0.3510843813419342\n",
      "Step: 1900  \tTraining accuracy: 0.8305801749229431\n",
      "Step: 1900  \tValid loss: 0.30411818623542786\n",
      "Step: 2000  \tTraining loss: 0.35036665201187134\n",
      "Step: 2000  \tTraining accuracy: 0.8321384191513062\n",
      "Step: 2000  \tValid loss: 0.3036074936389923\n",
      "Step: 2100  \tTraining loss: 0.3496716022491455\n",
      "Step: 2100  \tTraining accuracy: 0.8335506319999695\n",
      "Step: 2100  \tValid loss: 0.30315759778022766\n",
      "Step: 2200  \tTraining loss: 0.3489932417869568\n",
      "Step: 2200  \tTraining accuracy: 0.8348821401596069\n",
      "Step: 2200  \tValid loss: 0.3027229309082031\n",
      "Step: 2300  \tTraining loss: 0.34832659363746643\n",
      "Step: 2300  \tTraining accuracy: 0.8360953330993652\n",
      "Step: 2300  \tValid loss: 0.3023025095462799\n",
      "Step: 2400  \tTraining loss: 0.3476680517196655\n",
      "Step: 2400  \tTraining accuracy: 0.8372002243995667\n",
      "Step: 2400  \tValid loss: 0.30189773440361023\n",
      "Step: 2500  \tTraining loss: 0.34702327847480774\n",
      "Step: 2500  \tTraining accuracy: 0.8382444977760315\n",
      "Step: 2500  \tValid loss: 0.30151447653770447\n",
      "Step: 2600  \tTraining loss: 0.3463990092277527\n",
      "Step: 2600  \tTraining accuracy: 0.8392354249954224\n",
      "Step: 2600  \tValid loss: 0.30115073919296265\n",
      "Step: 2700  \tTraining loss: 0.34580355882644653\n",
      "Step: 2700  \tTraining accuracy: 0.840178906917572\n",
      "Step: 2700  \tValid loss: 0.3007851541042328\n",
      "Step: 2800  \tTraining loss: 0.3452320396900177\n",
      "Step: 2800  \tTraining accuracy: 0.8410362601280212\n",
      "Step: 2800  \tValid loss: 0.30045339465141296\n",
      "Step: 2900  \tTraining loss: 0.34467795491218567\n",
      "Step: 2900  \tTraining accuracy: 0.8418164253234863\n",
      "Step: 2900  \tValid loss: 0.3001267910003662\n",
      "Step: 3000  \tTraining loss: 0.34411337971687317\n",
      "Step: 3000  \tTraining accuracy: 0.8425806760787964\n",
      "Step: 3000  \tValid loss: 0.2997804582118988\n",
      "Step: 3100  \tTraining loss: 0.34350118041038513\n",
      "Step: 3100  \tTraining accuracy: 0.8433067202568054\n",
      "Step: 3100  \tValid loss: 0.29933300614356995\n",
      "Step: 3200  \tTraining loss: 0.3428569734096527\n",
      "Step: 3200  \tTraining accuracy: 0.844013512134552\n",
      "Step: 3200  \tValid loss: 0.2986990213394165\n",
      "Step: 3300  \tTraining loss: 0.34221938252449036\n",
      "Step: 3300  \tTraining accuracy: 0.8446619510650635\n",
      "Step: 3300  \tValid loss: 0.2980882227420807\n",
      "Step: 3400  \tTraining loss: 0.34160080552101135\n",
      "Step: 3400  \tTraining accuracy: 0.845275342464447\n",
      "Step: 3400  \tValid loss: 0.2975030839443207\n",
      "Step: 3500  \tTraining loss: 0.3409954607486725\n",
      "Step: 3500  \tTraining accuracy: 0.8458881974220276\n",
      "Step: 3500  \tValid loss: 0.29699283838272095\n",
      "Step: 3600  \tTraining loss: 0.340400367975235\n",
      "Step: 3600  \tTraining accuracy: 0.8464971780776978\n",
      "Step: 3600  \tValid loss: 0.29654422402381897\n",
      "Step: 3700  \tTraining loss: 0.3398122787475586\n",
      "Step: 3700  \tTraining accuracy: 0.8471192717552185\n",
      "Step: 3700  \tValid loss: 0.2960842251777649\n",
      "Step: 3800  \tTraining loss: 0.33923202753067017\n",
      "Step: 3800  \tTraining accuracy: 0.8477048873901367\n",
      "Step: 3800  \tValid loss: 0.29566463828086853\n",
      "Step: 3900  \tTraining loss: 0.33865776658058167\n",
      "Step: 3900  \tTraining accuracy: 0.8482633233070374\n",
      "Step: 3900  \tValid loss: 0.29523348808288574\n",
      "Step: 4000  \tTraining loss: 0.3380950391292572\n",
      "Step: 4000  \tTraining accuracy: 0.8487995266914368\n",
      "Step: 4000  \tValid loss: 0.2948167324066162\n",
      "Step: 4100  \tTraining loss: 0.3375445306301117\n",
      "Step: 4100  \tTraining accuracy: 0.8493033051490784\n",
      "Step: 4100  \tValid loss: 0.29437753558158875\n",
      "Step: 4200  \tTraining loss: 0.33700308203697205\n",
      "Step: 4200  \tTraining accuracy: 0.8497857451438904\n",
      "Step: 4200  \tValid loss: 0.2940138280391693\n",
      "Step: 4300  \tTraining loss: 0.3364783823490143\n",
      "Step: 4300  \tTraining accuracy: 0.850245475769043\n",
      "Step: 4300  \tValid loss: 0.293660044670105\n",
      "Step: 4400  \tTraining loss: 0.335940957069397\n",
      "Step: 4400  \tTraining accuracy: 0.850656270980835\n",
      "Step: 4400  \tValid loss: 0.29335883259773254\n",
      "Step: 4500  \tTraining loss: 0.33526429533958435\n",
      "Step: 4500  \tTraining accuracy: 0.8510241508483887\n",
      "Step: 4500  \tValid loss: 0.29325050115585327\n",
      "Step: 4600  \tTraining loss: 0.3343161940574646\n",
      "Step: 4600  \tTraining accuracy: 0.8513891696929932\n",
      "Step: 4600  \tValid loss: 0.2934703230857849\n",
      "Step: 4700  \tTraining loss: 0.3334428668022156\n",
      "Step: 4700  \tTraining accuracy: 0.8517462611198425\n",
      "Step: 4700  \tValid loss: 0.29337233304977417\n",
      "Step: 4800  \tTraining loss: 0.332721084356308\n",
      "Step: 4800  \tTraining accuracy: 0.8520883321762085\n",
      "Step: 4800  \tValid loss: 0.2931160628795624\n",
      "Step: 4900  \tTraining loss: 0.332064688205719\n",
      "Step: 4900  \tTraining accuracy: 0.8524187803268433\n",
      "Step: 4900  \tValid loss: 0.2927796542644501\n",
      "Step: 5000  \tTraining loss: 0.3314518630504608\n",
      "Step: 5000  \tTraining accuracy: 0.8527432084083557\n",
      "Step: 5000  \tValid loss: 0.2924645245075226\n",
      "Step: 5100  \tTraining loss: 0.3308456540107727\n",
      "Step: 5100  \tTraining accuracy: 0.8530500531196594\n",
      "Step: 5100  \tValid loss: 0.2921559512615204\n",
      "Step: 5200  \tTraining loss: 0.33027204871177673\n",
      "Step: 5200  \tTraining accuracy: 0.8533519506454468\n",
      "Step: 5200  \tValid loss: 0.29188069701194763\n",
      "Step: 5300  \tTraining loss: 0.3296998143196106\n",
      "Step: 5300  \tTraining accuracy: 0.853640079498291\n",
      "Step: 5300  \tValid loss: 0.2916731536388397\n",
      "Step: 5400  \tTraining loss: 0.32911422848701477\n",
      "Step: 5400  \tTraining accuracy: 0.8539106845855713\n",
      "Step: 5400  \tValid loss: 0.291520893573761\n",
      "Step: 5500  \tTraining loss: 0.32853662967681885\n",
      "Step: 5500  \tTraining accuracy: 0.8541713356971741\n",
      "Step: 5500  \tValid loss: 0.29137709736824036\n",
      "Step: 5600  \tTraining loss: 0.32796549797058105\n",
      "Step: 5600  \tTraining accuracy: 0.8544269800186157\n",
      "Step: 5600  \tValid loss: 0.2911568880081177\n",
      "Step: 5700  \tTraining loss: 0.3274107277393341\n",
      "Step: 5700  \tTraining accuracy: 0.8546585440635681\n",
      "Step: 5700  \tValid loss: 0.29094892740249634\n",
      "Step: 5800  \tTraining loss: 0.3268719017505646\n",
      "Step: 5800  \tTraining accuracy: 0.854888379573822\n",
      "Step: 5800  \tValid loss: 0.2907411754131317\n",
      "Step: 5900  \tTraining loss: 0.3263472616672516\n",
      "Step: 5900  \tTraining accuracy: 0.8551083207130432\n",
      "Step: 5900  \tValid loss: 0.2904973030090332\n",
      "Step: 6000  \tTraining loss: 0.3258359432220459\n",
      "Step: 6000  \tTraining accuracy: 0.8553310036659241\n",
      "Step: 6000  \tValid loss: 0.29034125804901123\n",
      "Step: 6100  \tTraining loss: 0.3253375291824341\n",
      "Step: 6100  \tTraining accuracy: 0.855560302734375\n",
      "Step: 6100  \tValid loss: 0.29012376070022583\n",
      "Step: 6200  \tTraining loss: 0.3248518407344818\n",
      "Step: 6200  \tTraining accuracy: 0.8557841777801514\n",
      "Step: 6200  \tValid loss: 0.28991732001304626\n",
      "Step: 6300  \tTraining loss: 0.3243783116340637\n",
      "Step: 6300  \tTraining accuracy: 0.8559756875038147\n",
      "Step: 6300  \tValid loss: 0.2897498607635498\n",
      "Step: 6400  \tTraining loss: 0.3239053785800934\n",
      "Step: 6400  \tTraining accuracy: 0.8561554551124573\n",
      "Step: 6400  \tValid loss: 0.2895156443119049\n",
      "Step: 6500  \tTraining loss: 0.32345065474510193\n",
      "Step: 6500  \tTraining accuracy: 0.8563259243965149\n",
      "Step: 6500  \tValid loss: 0.2893061637878418\n",
      "Step: 6600  \tTraining loss: 0.32299143075942993\n",
      "Step: 6600  \tTraining accuracy: 0.8565040826797485\n",
      "Step: 6600  \tValid loss: 0.2891160547733307\n",
      "Step: 6700  \tTraining loss: 0.32254716753959656\n",
      "Step: 6700  \tTraining accuracy: 0.8566805124282837\n",
      "Step: 6700  \tValid loss: 0.28887030482292175\n",
      "Step: 6800  \tTraining loss: 0.32211020588874817\n",
      "Step: 6800  \tTraining accuracy: 0.8568678498268127\n",
      "Step: 6800  \tValid loss: 0.28864118456840515\n",
      "Step: 6900  \tTraining loss: 0.3216772675514221\n",
      "Step: 6900  \tTraining accuracy: 0.8570462465286255\n",
      "Step: 6900  \tValid loss: 0.28848984837532043\n",
      "Step: 7000  \tTraining loss: 0.32124778628349304\n",
      "Step: 7000  \tTraining accuracy: 0.8572142124176025\n",
      "Step: 7000  \tValid loss: 0.28823497891426086\n",
      "Step: 7100  \tTraining loss: 0.32082951068878174\n",
      "Step: 7100  \tTraining accuracy: 0.8573791980743408\n",
      "Step: 7100  \tValid loss: 0.28801673650741577\n",
      "Step: 7200  \tTraining loss: 0.320421040058136\n",
      "Step: 7200  \tTraining accuracy: 0.857542872428894\n",
      "Step: 7200  \tValid loss: 0.28775474429130554\n",
      "Step: 7300  \tTraining loss: 0.32001811265945435\n",
      "Step: 7300  \tTraining accuracy: 0.8576954007148743\n",
      "Step: 7300  \tValid loss: 0.28747865557670593\n",
      "Step: 7400  \tTraining loss: 0.3196299374103546\n",
      "Step: 7400  \tTraining accuracy: 0.857835590839386\n",
      "Step: 7400  \tValid loss: 0.28721916675567627\n",
      "Step: 7500  \tTraining loss: 0.319246381521225\n",
      "Step: 7500  \tTraining accuracy: 0.8579719662666321\n",
      "Step: 7500  \tValid loss: 0.28700169920921326\n",
      "Step: 7600  \tTraining loss: 0.31887537240982056\n",
      "Step: 7600  \tTraining accuracy: 0.8581031560897827\n",
      "Step: 7600  \tValid loss: 0.28674498200416565\n",
      "Step: 7700  \tTraining loss: 0.3185216188430786\n",
      "Step: 7700  \tTraining accuracy: 0.8582340478897095\n",
      "Step: 7700  \tValid loss: 0.2865169942378998\n",
      "Step: 7800  \tTraining loss: 0.3181602954864502\n",
      "Step: 7800  \tTraining accuracy: 0.8583787083625793\n",
      "Step: 7800  \tValid loss: 0.28625696897506714\n",
      "Step: 7900  \tTraining loss: 0.31782394647598267\n",
      "Step: 7900  \tTraining accuracy: 0.8584997057914734\n",
      "Step: 7900  \tValid loss: 0.28604719042778015\n",
      "Step: 8000  \tTraining loss: 0.31748634576797485\n",
      "Step: 8000  \tTraining accuracy: 0.8586100339889526\n",
      "Step: 8000  \tValid loss: 0.2858400344848633\n",
      "Step: 8100  \tTraining loss: 0.3171480596065521\n",
      "Step: 8100  \tTraining accuracy: 0.8587251305580139\n",
      "Step: 8100  \tValid loss: 0.2856234908103943\n",
      "Step: 8200  \tTraining loss: 0.31682124733924866\n",
      "Step: 8200  \tTraining accuracy: 0.8588581681251526\n",
      "Step: 8200  \tValid loss: 0.28537580370903015\n",
      "Step: 8300  \tTraining loss: 0.31650424003601074\n",
      "Step: 8300  \tTraining accuracy: 0.8589923977851868\n",
      "Step: 8300  \tValid loss: 0.2851441502571106\n",
      "Step: 8400  \tTraining loss: 0.31619828939437866\n",
      "Step: 8400  \tTraining accuracy: 0.8591234683990479\n",
      "Step: 8400  \tValid loss: 0.28494590520858765\n",
      "Step: 8500  \tTraining loss: 0.3158874213695526\n",
      "Step: 8500  \tTraining accuracy: 0.8592513799667358\n",
      "Step: 8500  \tValid loss: 0.28470173478126526\n",
      "Step: 8600  \tTraining loss: 0.315582275390625\n",
      "Step: 8600  \tTraining accuracy: 0.8593776822090149\n",
      "Step: 8600  \tValid loss: 0.28449195623397827\n",
      "Step: 8700  \tTraining loss: 0.31527426838874817\n",
      "Step: 8700  \tTraining accuracy: 0.8595108985900879\n",
      "Step: 8700  \tValid loss: 0.28427767753601074\n",
      "Step: 8800  \tTraining loss: 0.31494665145874023\n",
      "Step: 8800  \tTraining accuracy: 0.859643816947937\n",
      "Step: 8800  \tValid loss: 0.2840215265750885\n",
      "Step: 8900  \tTraining loss: 0.3145907521247864\n",
      "Step: 8900  \tTraining accuracy: 0.8597737550735474\n",
      "Step: 8900  \tValid loss: 0.28377485275268555\n",
      "Step: 9000  \tTraining loss: 0.3142280876636505\n",
      "Step: 9000  \tTraining accuracy: 0.8598994016647339\n",
      "Step: 9000  \tValid loss: 0.2834462821483612\n",
      "Step: 9100  \tTraining loss: 0.31387820839881897\n",
      "Step: 9100  \tTraining accuracy: 0.8600022792816162\n",
      "Step: 9100  \tValid loss: 0.28307902812957764\n",
      "Step: 9200  \tTraining loss: 0.313544362783432\n",
      "Step: 9200  \tTraining accuracy: 0.8600949048995972\n",
      "Step: 9200  \tValid loss: 0.2827448844909668\n",
      "Step: 9300  \tTraining loss: 0.3132050335407257\n",
      "Step: 9300  \tTraining accuracy: 0.8601830005645752\n",
      "Step: 9300  \tValid loss: 0.28233686089515686\n",
      "Step: 9400  \tTraining loss: 0.3128531277179718\n",
      "Step: 9400  \tTraining accuracy: 0.8602639436721802\n",
      "Step: 9400  \tValid loss: 0.28189533948898315\n",
      "Step: 9500  \tTraining loss: 0.312521755695343\n",
      "Step: 9500  \tTraining accuracy: 0.8603470921516418\n",
      "Step: 9500  \tValid loss: 0.2815299928188324\n",
      "Step: 9600  \tTraining loss: 0.312200129032135\n",
      "Step: 9600  \tTraining accuracy: 0.8604157567024231\n",
      "Step: 9600  \tValid loss: 0.28118395805358887\n",
      "Step: 9700  \tTraining loss: 0.3118857145309448\n",
      "Step: 9700  \tTraining accuracy: 0.8604767918586731\n",
      "Step: 9700  \tValid loss: 0.2808449864387512\n",
      "Step: 9800  \tTraining loss: 0.31157732009887695\n",
      "Step: 9800  \tTraining accuracy: 0.860536515712738\n",
      "Step: 9800  \tValid loss: 0.2805172801017761\n",
      "Step: 9900  \tTraining loss: 0.3112739026546478\n",
      "Step: 9900  \tTraining accuracy: 0.860597550868988\n",
      "Step: 9900  \tValid loss: 0.28020238876342773\n",
      "Step: 10000  \tTraining loss: 0.3109748959541321\n",
      "Step: 10000  \tTraining accuracy: 0.8606634140014648\n",
      "Step: 10000  \tValid loss: 0.2798987925052643\n",
      "Step: 10100  \tTraining loss: 0.31067949533462524\n",
      "Step: 10100  \tTraining accuracy: 0.8607279062271118\n",
      "Step: 10100  \tValid loss: 0.2796114981174469\n",
      "Step: 10200  \tTraining loss: 0.31038784980773926\n",
      "Step: 10200  \tTraining accuracy: 0.8607816696166992\n",
      "Step: 10200  \tValid loss: 0.27930948138237\n",
      "Step: 10300  \tTraining loss: 0.31010183691978455\n",
      "Step: 10300  \tTraining accuracy: 0.8608390688896179\n",
      "Step: 10300  \tValid loss: 0.2790201008319855\n",
      "Step: 10400  \tTraining loss: 0.3098212480545044\n",
      "Step: 10400  \tTraining accuracy: 0.8609047532081604\n",
      "Step: 10400  \tValid loss: 0.27874675393104553\n",
      "Step: 10500  \tTraining loss: 0.30954116582870483\n",
      "Step: 10500  \tTraining accuracy: 0.8609691262245178\n",
      "Step: 10500  \tValid loss: 0.27847450971603394\n",
      "Step: 10600  \tTraining loss: 0.30921992659568787\n",
      "Step: 10600  \tTraining accuracy: 0.8610391616821289\n",
      "Step: 10600  \tValid loss: 0.2781312167644501\n",
      "Step: 10700  \tTraining loss: 0.3088800311088562\n",
      "Step: 10700  \tTraining accuracy: 0.8611056804656982\n",
      "Step: 10700  \tValid loss: 0.27785587310791016\n",
      "Step: 10800  \tTraining loss: 0.3085842728614807\n",
      "Step: 10800  \tTraining accuracy: 0.8611607551574707\n",
      "Step: 10800  \tValid loss: 0.2777109146118164\n",
      "Step: 10900  \tTraining loss: 0.3083045184612274\n",
      "Step: 10900  \tTraining accuracy: 0.8612081408500671\n",
      "Step: 10900  \tValid loss: 0.27749258279800415\n",
      "Step: 11000  \tTraining loss: 0.30802804231643677\n",
      "Step: 11000  \tTraining accuracy: 0.861253559589386\n",
      "Step: 11000  \tValid loss: 0.27724480628967285\n",
      "Step: 11100  \tTraining loss: 0.30775752663612366\n",
      "Step: 11100  \tTraining accuracy: 0.8612915873527527\n",
      "Step: 11100  \tValid loss: 0.2770475745201111\n",
      "Step: 11200  \tTraining loss: 0.3074989914894104\n",
      "Step: 11200  \tTraining accuracy: 0.8613300323486328\n",
      "Step: 11200  \tValid loss: 0.27681300044059753\n",
      "Step: 11300  \tTraining loss: 0.3072391748428345\n",
      "Step: 11300  \tTraining accuracy: 0.8613688945770264\n",
      "Step: 11300  \tValid loss: 0.2765938341617584\n",
      "Step: 11400  \tTraining loss: 0.30698052048683167\n",
      "Step: 11400  \tTraining accuracy: 0.8614038228988647\n",
      "Step: 11400  \tValid loss: 0.27639439702033997\n",
      "Step: 11500  \tTraining loss: 0.30672118067741394\n",
      "Step: 11500  \tTraining accuracy: 0.8614455461502075\n",
      "Step: 11500  \tValid loss: 0.2761630415916443\n",
      "Step: 11600  \tTraining loss: 0.30647170543670654\n",
      "Step: 11600  \tTraining accuracy: 0.861487627029419\n",
      "Step: 11600  \tValid loss: 0.2759968340396881\n",
      "Step: 11700  \tTraining loss: 0.30620530247688293\n",
      "Step: 11700  \tTraining accuracy: 0.8615289330482483\n",
      "Step: 11700  \tValid loss: 0.27575305104255676\n",
      "Step: 11800  \tTraining loss: 0.30594345927238464\n",
      "Step: 11800  \tTraining accuracy: 0.8615695834159851\n",
      "Step: 11800  \tValid loss: 0.27555176615715027\n",
      "Step: 11900  \tTraining loss: 0.30567747354507446\n",
      "Step: 11900  \tTraining accuracy: 0.8616095185279846\n",
      "Step: 11900  \tValid loss: 0.27533069252967834\n",
      "Step: 12000  \tTraining loss: 0.30540961027145386\n",
      "Step: 12000  \tTraining accuracy: 0.861650824546814\n",
      "Step: 12000  \tValid loss: 0.27513617277145386\n",
      "Step: 12100  \tTraining loss: 0.3051372766494751\n",
      "Step: 12100  \tTraining accuracy: 0.8616944551467896\n",
      "Step: 12100  \tValid loss: 0.27492931485176086\n",
      "Step: 12200  \tTraining loss: 0.3048607409000397\n",
      "Step: 12200  \tTraining accuracy: 0.8617343902587891\n",
      "Step: 12200  \tValid loss: 0.27471864223480225\n",
      "Step: 12300  \tTraining loss: 0.30457863211631775\n",
      "Step: 12300  \tTraining accuracy: 0.861773669719696\n",
      "Step: 12300  \tValid loss: 0.2745307385921478\n",
      "Step: 12400  \tTraining loss: 0.3042954206466675\n",
      "Step: 12400  \tTraining accuracy: 0.8618152141571045\n",
      "Step: 12400  \tValid loss: 0.2743160128593445\n",
      "Step: 12500  \tTraining loss: 0.3040149509906769\n",
      "Step: 12500  \tTraining accuracy: 0.8618561625480652\n",
      "Step: 12500  \tValid loss: 0.2740972638130188\n",
      "Step: 12600  \tTraining loss: 0.3037344217300415\n",
      "Step: 12600  \tTraining accuracy: 0.8618925213813782\n",
      "Step: 12600  \tValid loss: 0.2739122211933136\n",
      "Step: 12700  \tTraining loss: 0.3034515082836151\n",
      "Step: 12700  \tTraining accuracy: 0.8619293570518494\n",
      "Step: 12700  \tValid loss: 0.2737087309360504\n",
      "Step: 12800  \tTraining loss: 0.30316323041915894\n",
      "Step: 12800  \tTraining accuracy: 0.8619721531867981\n",
      "Step: 12800  \tValid loss: 0.27349722385406494\n",
      "Step: 12900  \tTraining loss: 0.30287662148475647\n",
      "Step: 12900  \tTraining accuracy: 0.8620171546936035\n",
      "Step: 12900  \tValid loss: 0.2732953727245331\n",
      "Step: 13000  \tTraining loss: 0.30258673429489136\n",
      "Step: 13000  \tTraining accuracy: 0.8620661497116089\n",
      "Step: 13000  \tValid loss: 0.273086279630661\n",
      "Step: 13100  \tTraining loss: 0.3022977113723755\n",
      "Step: 13100  \tTraining accuracy: 0.8621153235435486\n",
      "Step: 13100  \tValid loss: 0.2728917598724365\n",
      "Step: 13200  \tTraining loss: 0.302005797624588\n",
      "Step: 13200  \tTraining accuracy: 0.8621628284454346\n",
      "Step: 13200  \tValid loss: 0.2727169096469879\n",
      "Step: 13300  \tTraining loss: 0.30171796679496765\n",
      "Step: 13300  \tTraining accuracy: 0.8622123003005981\n",
      "Step: 13300  \tValid loss: 0.2725125253200531\n",
      "Step: 13400  \tTraining loss: 0.30143240094184875\n",
      "Step: 13400  \tTraining accuracy: 0.8622646927833557\n",
      "Step: 13400  \tValid loss: 0.27232804894447327\n",
      "Step: 13500  \tTraining loss: 0.30114737153053284\n",
      "Step: 13500  \tTraining accuracy: 0.8623154163360596\n",
      "Step: 13500  \tValid loss: 0.272182434797287\n",
      "Step: 13600  \tTraining loss: 0.3008652627468109\n",
      "Step: 13600  \tTraining accuracy: 0.8623591661453247\n",
      "Step: 13600  \tValid loss: 0.27201923727989197\n",
      "Step: 13700  \tTraining loss: 0.3005855083465576\n",
      "Step: 13700  \tTraining accuracy: 0.8624048829078674\n",
      "Step: 13700  \tValid loss: 0.2718530297279358\n",
      "Step: 13800  \tTraining loss: 0.3003093898296356\n",
      "Step: 13800  \tTraining accuracy: 0.8624499440193176\n",
      "Step: 13800  \tValid loss: 0.2717091739177704\n",
      "Step: 13900  \tTraining loss: 0.30003246665000916\n",
      "Step: 13900  \tTraining accuracy: 0.8624899983406067\n",
      "Step: 13900  \tValid loss: 0.27157315611839294\n",
      "Step: 14000  \tTraining loss: 0.2997584939002991\n",
      "Step: 14000  \tTraining accuracy: 0.8625277280807495\n",
      "Step: 14000  \tValid loss: 0.27141764760017395\n",
      "Step: 14100  \tTraining loss: 0.29948917031288147\n",
      "Step: 14100  \tTraining accuracy: 0.862565815448761\n",
      "Step: 14100  \tValid loss: 0.2712712585926056\n",
      "Step: 14200  \tTraining loss: 0.29922470450401306\n",
      "Step: 14200  \tTraining accuracy: 0.8626050353050232\n",
      "Step: 14200  \tValid loss: 0.2711665630340576\n",
      "Step: 14300  \tTraining loss: 0.29896441102027893\n",
      "Step: 14300  \tTraining accuracy: 0.8626488447189331\n",
      "Step: 14300  \tValid loss: 0.2710353434085846\n",
      "Step: 14400  \tTraining loss: 0.29870954155921936\n",
      "Step: 14400  \tTraining accuracy: 0.8626953959465027\n",
      "Step: 14400  \tValid loss: 0.2709101140499115\n",
      "Step: 14500  \tTraining loss: 0.29845789074897766\n",
      "Step: 14500  \tTraining accuracy: 0.8627412915229797\n",
      "Step: 14500  \tValid loss: 0.27079683542251587\n",
      "Step: 14600  \tTraining loss: 0.29821351170539856\n",
      "Step: 14600  \tTraining accuracy: 0.8627932071685791\n",
      "Step: 14600  \tValid loss: 0.27069154381752014\n",
      "Step: 14700  \tTraining loss: 0.2979714572429657\n",
      "Step: 14700  \tTraining accuracy: 0.862845242023468\n",
      "Step: 14700  \tValid loss: 0.270572304725647\n",
      "Step: 14800  \tTraining loss: 0.2977345287799835\n",
      "Step: 14800  \tTraining accuracy: 0.862893283367157\n",
      "Step: 14800  \tValid loss: 0.27044275403022766\n",
      "Step: 14900  \tTraining loss: 0.297505259513855\n",
      "Step: 14900  \tTraining accuracy: 0.8629390597343445\n",
      "Step: 14900  \tValid loss: 0.27035948634147644\n",
      "Step: 15000  \tTraining loss: 0.29727908968925476\n",
      "Step: 15000  \tTraining accuracy: 0.8629817962646484\n",
      "Step: 15000  \tValid loss: 0.27024325728416443\n",
      "Step: 15100  \tTraining loss: 0.2970575988292694\n",
      "Step: 15100  \tTraining accuracy: 0.8630256056785583\n",
      "Step: 15100  \tValid loss: 0.27013882994651794\n",
      "Step: 15200  \tTraining loss: 0.29684311151504517\n",
      "Step: 15200  \tTraining accuracy: 0.8630727529525757\n",
      "Step: 15200  \tValid loss: 0.27003058791160583\n",
      "Step: 15300  \tTraining loss: 0.2966372072696686\n",
      "Step: 15300  \tTraining accuracy: 0.863121747970581\n",
      "Step: 15300  \tValid loss: 0.26992666721343994\n",
      "Step: 15400  \tTraining loss: 0.29643863439559937\n",
      "Step: 15400  \tTraining accuracy: 0.863170862197876\n",
      "Step: 15400  \tValid loss: 0.26983755826950073\n",
      "Step: 15500  \tTraining loss: 0.2962474226951599\n",
      "Step: 15500  \tTraining accuracy: 0.8632208704948425\n",
      "Step: 15500  \tValid loss: 0.26972177624702454\n",
      "Step: 15600  \tTraining loss: 0.2960447371006012\n",
      "Step: 15600  \tTraining accuracy: 0.8632695078849792\n",
      "Step: 15600  \tValid loss: 0.2695502042770386\n",
      "Step: 15700  \tTraining loss: 0.2958291172981262\n",
      "Step: 15700  \tTraining accuracy: 0.8633182644844055\n",
      "Step: 15700  \tValid loss: 0.2691299021244049\n",
      "Step: 15800  \tTraining loss: 0.29561713337898254\n",
      "Step: 15800  \tTraining accuracy: 0.8633648753166199\n",
      "Step: 15800  \tValid loss: 0.26885223388671875\n",
      "Step: 15900  \tTraining loss: 0.2953864634037018\n",
      "Step: 15900  \tTraining accuracy: 0.8634108901023865\n",
      "Step: 15900  \tValid loss: 0.2686920762062073\n",
      "Step: 16000  \tTraining loss: 0.29517799615859985\n",
      "Step: 16000  \tTraining accuracy: 0.8634623885154724\n",
      "Step: 16000  \tValid loss: 0.26836106181144714\n",
      "Step: 16100  \tTraining loss: 0.29499581456184387\n",
      "Step: 16100  \tTraining accuracy: 0.8635140061378479\n",
      "Step: 16100  \tValid loss: 0.2681504786014557\n",
      "Step: 16200  \tTraining loss: 0.2948218584060669\n",
      "Step: 16200  \tTraining accuracy: 0.863566517829895\n",
      "Step: 16200  \tValid loss: 0.2679416239261627\n",
      "Step: 16300  \tTraining loss: 0.29465484619140625\n",
      "Step: 16300  \tTraining accuracy: 0.8636205792427063\n",
      "Step: 16300  \tValid loss: 0.2678191363811493\n",
      "Step: 16400  \tTraining loss: 0.2944999635219574\n",
      "Step: 16400  \tTraining accuracy: 0.863673985004425\n",
      "Step: 16400  \tValid loss: 0.26780569553375244\n",
      "Step: 16500  \tTraining loss: 0.29434117674827576\n",
      "Step: 16500  \tTraining accuracy: 0.8637230396270752\n",
      "Step: 16500  \tValid loss: 0.26763835549354553\n",
      "Step: 16600  \tTraining loss: 0.29418760538101196\n",
      "Step: 16600  \tTraining accuracy: 0.8637715578079224\n",
      "Step: 16600  \tValid loss: 0.26749110221862793\n",
      "Step: 16700  \tTraining loss: 0.2940361797809601\n",
      "Step: 16700  \tTraining accuracy: 0.863819420337677\n",
      "Step: 16700  \tValid loss: 0.26739564538002014\n",
      "Step: 16800  \tTraining loss: 0.29388582706451416\n",
      "Step: 16800  \tTraining accuracy: 0.8638668060302734\n",
      "Step: 16800  \tValid loss: 0.26724788546562195\n",
      "Step: 16900  \tTraining loss: 0.29373699426651\n",
      "Step: 16900  \tTraining accuracy: 0.8639135360717773\n",
      "Step: 16900  \tValid loss: 0.2671964168548584\n",
      "Step: 17000  \tTraining loss: 0.2935967743396759\n",
      "Step: 17000  \tTraining accuracy: 0.8639576435089111\n",
      "Step: 17000  \tValid loss: 0.2670697867870331\n",
      "Step: 17100  \tTraining loss: 0.2934550940990448\n",
      "Step: 17100  \tTraining accuracy: 0.8639997839927673\n",
      "Step: 17100  \tValid loss: 0.26696890592575073\n",
      "Step: 17200  \tTraining loss: 0.29331424832344055\n",
      "Step: 17200  \tTraining accuracy: 0.8640414476394653\n",
      "Step: 17200  \tValid loss: 0.26690080761909485\n",
      "Step: 17300  \tTraining loss: 0.2931767702102661\n",
      "Step: 17300  \tTraining accuracy: 0.8640825748443604\n",
      "Step: 17300  \tValid loss: 0.2668182849884033\n",
      "Step: 17400  \tTraining loss: 0.29303890466690063\n",
      "Step: 17400  \tTraining accuracy: 0.8641246557235718\n",
      "Step: 17400  \tValid loss: 0.26675230264663696\n",
      "Step: 17500  \tTraining loss: 0.29290542006492615\n",
      "Step: 17500  \tTraining accuracy: 0.8641704320907593\n",
      "Step: 17500  \tValid loss: 0.26666632294654846\n",
      "Step: 17600  \tTraining loss: 0.29276803135871887\n",
      "Step: 17600  \tTraining accuracy: 0.8642212152481079\n",
      "Step: 17600  \tValid loss: 0.26663750410079956\n",
      "Step: 17700  \tTraining loss: 0.29262834787368774\n",
      "Step: 17700  \tTraining accuracy: 0.8642720580101013\n",
      "Step: 17700  \tValid loss: 0.2663927972316742\n",
      "Step: 17800  \tTraining loss: 0.29247185587882996\n",
      "Step: 17800  \tTraining accuracy: 0.8643237352371216\n",
      "Step: 17800  \tValid loss: 0.26627153158187866\n",
      "Step: 17900  \tTraining loss: 0.29234158992767334\n",
      "Step: 17900  \tTraining accuracy: 0.8643748164176941\n",
      "Step: 17900  \tValid loss: 0.26611825823783875\n",
      "Step: 18000  \tTraining loss: 0.29220619797706604\n",
      "Step: 18000  \tTraining accuracy: 0.864422619342804\n",
      "Step: 18000  \tValid loss: 0.2659754753112793\n",
      "Step: 18100  \tTraining loss: 0.29191723465919495\n",
      "Step: 18100  \tTraining accuracy: 0.8644779920578003\n",
      "Step: 18100  \tValid loss: 0.2652280032634735\n",
      "Step: 18200  \tTraining loss: 0.29174497723579407\n",
      "Step: 18200  \tTraining accuracy: 0.8645413517951965\n",
      "Step: 18200  \tValid loss: 0.26497507095336914\n",
      "Step: 18300  \tTraining loss: 0.2915949523448944\n",
      "Step: 18300  \tTraining accuracy: 0.8645987510681152\n",
      "Step: 18300  \tValid loss: 0.2646618187427521\n",
      "Step: 18400  \tTraining loss: 0.2914316654205322\n",
      "Step: 18400  \tTraining accuracy: 0.8646581172943115\n",
      "Step: 18400  \tValid loss: 0.2645270526409149\n",
      "Step: 18500  \tTraining loss: 0.29124781489372253\n",
      "Step: 18500  \tTraining accuracy: 0.8647162318229675\n",
      "Step: 18500  \tValid loss: 0.26435786485671997\n",
      "Step: 18600  \tTraining loss: 0.2910808324813843\n",
      "Step: 18600  \tTraining accuracy: 0.8647698163986206\n",
      "Step: 18600  \tValid loss: 0.26420363783836365\n",
      "Step: 18700  \tTraining loss: 0.2909258306026459\n",
      "Step: 18700  \tTraining accuracy: 0.8648234605789185\n",
      "Step: 18700  \tValid loss: 0.26407572627067566\n",
      "Step: 18800  \tTraining loss: 0.29077601432800293\n",
      "Step: 18800  \tTraining accuracy: 0.8648810386657715\n",
      "Step: 18800  \tValid loss: 0.26388439536094666\n",
      "Step: 18900  \tTraining loss: 0.2906365990638733\n",
      "Step: 18900  \tTraining accuracy: 0.864939272403717\n",
      "Step: 18900  \tValid loss: 0.26387280225753784\n",
      "Step: 19000  \tTraining loss: 0.29047757387161255\n",
      "Step: 19000  \tTraining accuracy: 0.8650007247924805\n",
      "Step: 19000  \tValid loss: 0.2636299431324005\n",
      "Step: 19100  \tTraining loss: 0.29032838344573975\n",
      "Step: 19100  \tTraining accuracy: 0.8650628328323364\n",
      "Step: 19100  \tValid loss: 0.2635090947151184\n",
      "Step: 19200  \tTraining loss: 0.2901870012283325\n",
      "Step: 19200  \tTraining accuracy: 0.8651211261749268\n",
      "Step: 19200  \tValid loss: 0.2634550631046295\n",
      "Step: 19300  \tTraining loss: 0.2900597155094147\n",
      "Step: 19300  \tTraining accuracy: 0.8651781678199768\n",
      "Step: 19300  \tValid loss: 0.26335567235946655\n",
      "Step: 19400  \tTraining loss: 0.2899194061756134\n",
      "Step: 19400  \tTraining accuracy: 0.8652377724647522\n",
      "Step: 19400  \tValid loss: 0.26324763894081116\n",
      "Step: 19500  \tTraining loss: 0.2897908091545105\n",
      "Step: 19500  \tTraining accuracy: 0.8652954697608948\n",
      "Step: 19500  \tValid loss: 0.26323142647743225\n",
      "Step: 19600  \tTraining loss: 0.28966224193573\n",
      "Step: 19600  \tTraining accuracy: 0.8653538823127747\n",
      "Step: 19600  \tValid loss: 0.2631058692932129\n",
      "Step: 19700  \tTraining loss: 0.28953540325164795\n",
      "Step: 19700  \tTraining accuracy: 0.8654128909111023\n",
      "Step: 19700  \tValid loss: 0.2630675733089447\n",
      "Step: 19800  \tTraining loss: 0.2894071936607361\n",
      "Step: 19800  \tTraining accuracy: 0.8654724955558777\n",
      "Step: 19800  \tValid loss: 0.26309481263160706\n",
      "Step: 19900  \tTraining loss: 0.28927499055862427\n",
      "Step: 19900  \tTraining accuracy: 0.8655309677124023\n",
      "Step: 19900  \tValid loss: 0.26298075914382935\n",
      "Step: 20000  \tTraining loss: 0.28915882110595703\n",
      "Step: 20000  \tTraining accuracy: 0.8655893802642822\n",
      "Step: 20000  \tValid loss: 0.26287591457366943\n",
      "Step: 20100  \tTraining loss: 0.28902557492256165\n",
      "Step: 20100  \tTraining accuracy: 0.8656479120254517\n",
      "Step: 20100  \tValid loss: 0.2628582715988159\n",
      "Step: 20200  \tTraining loss: 0.28889065980911255\n",
      "Step: 20200  \tTraining accuracy: 0.8657034039497375\n",
      "Step: 20200  \tValid loss: 0.26278817653656006\n",
      "Step: 20300  \tTraining loss: 0.2887686789035797\n",
      "Step: 20300  \tTraining accuracy: 0.8657582998275757\n",
      "Step: 20300  \tValid loss: 0.2626937925815582\n",
      "Step: 20400  \tTraining loss: 0.28863057494163513\n",
      "Step: 20400  \tTraining accuracy: 0.8658145070075989\n",
      "Step: 20400  \tValid loss: 0.2626158595085144\n",
      "Step: 20500  \tTraining loss: 0.28849896788597107\n",
      "Step: 20500  \tTraining accuracy: 0.8658701181411743\n",
      "Step: 20500  \tValid loss: 0.2625972628593445\n",
      "Step: 20600  \tTraining loss: 0.28837332129478455\n",
      "Step: 20600  \tTraining accuracy: 0.8659252524375916\n",
      "Step: 20600  \tValid loss: 0.26244595646858215\n",
      "Step: 20700  \tTraining loss: 0.2882426977157593\n",
      "Step: 20700  \tTraining accuracy: 0.8659786581993103\n",
      "Step: 20700  \tValid loss: 0.26238521933555603\n",
      "Step: 20800  \tTraining loss: 0.2881135940551758\n",
      "Step: 20800  \tTraining accuracy: 0.8660309314727783\n",
      "Step: 20800  \tValid loss: 0.262311190366745\n",
      "Step: 20900  \tTraining loss: 0.28798559308052063\n",
      "Step: 20900  \tTraining accuracy: 0.8660833239555359\n",
      "Step: 20900  \tValid loss: 0.2623125910758972\n",
      "Step: 21000  \tTraining loss: 0.2878524661064148\n",
      "Step: 21000  \tTraining accuracy: 0.8661345839500427\n",
      "Step: 21000  \tValid loss: 0.26214611530303955\n",
      "Step: 21100  \tTraining loss: 0.28772464394569397\n",
      "Step: 21100  \tTraining accuracy: 0.8661888241767883\n",
      "Step: 21100  \tValid loss: 0.2621346712112427\n",
      "Step: 21200  \tTraining loss: 0.2875964045524597\n",
      "Step: 21200  \tTraining accuracy: 0.8662437200546265\n",
      "Step: 21200  \tValid loss: 0.26196959614753723\n",
      "Step: 21300  \tTraining loss: 0.28747689723968506\n",
      "Step: 21300  \tTraining accuracy: 0.8662992715835571\n",
      "Step: 21300  \tValid loss: 0.26203644275665283\n",
      "Step: 21400  \tTraining loss: 0.2873450815677643\n",
      "Step: 21400  \tTraining accuracy: 0.8663548231124878\n",
      "Step: 21400  \tValid loss: 0.2618299722671509\n",
      "Step: 21500  \tTraining loss: 0.2872104048728943\n",
      "Step: 21500  \tTraining accuracy: 0.8664081692695618\n",
      "Step: 21500  \tValid loss: 0.2617928087711334\n",
      "Step: 21600  \tTraining loss: 0.2870880663394928\n",
      "Step: 21600  \tTraining accuracy: 0.8664609789848328\n",
      "Step: 21600  \tValid loss: 0.26165154576301575\n",
      "Step: 21700  \tTraining loss: 0.28696009516716003\n",
      "Step: 21700  \tTraining accuracy: 0.866515576839447\n",
      "Step: 21700  \tValid loss: 0.2616903483867645\n",
      "Step: 21800  \tTraining loss: 0.28682181239128113\n",
      "Step: 21800  \tTraining accuracy: 0.8665730357170105\n",
      "Step: 21800  \tValid loss: 0.26158401370048523\n",
      "Step: 21900  \tTraining loss: 0.2866865396499634\n",
      "Step: 21900  \tTraining accuracy: 0.8666288256645203\n",
      "Step: 21900  \tValid loss: 0.26147985458374023\n",
      "Step: 22000  \tTraining loss: 0.2865559458732605\n",
      "Step: 22000  \tTraining accuracy: 0.8666835427284241\n",
      "Step: 22000  \tValid loss: 0.26143500208854675\n",
      "Step: 22100  \tTraining loss: 0.28640902042388916\n",
      "Step: 22100  \tTraining accuracy: 0.8667361736297607\n",
      "Step: 22100  \tValid loss: 0.26139387488365173\n",
      "Step: 22200  \tTraining loss: 0.286272794008255\n",
      "Step: 22200  \tTraining accuracy: 0.8667871952056885\n",
      "Step: 22200  \tValid loss: 0.2612135410308838\n",
      "Step: 22300  \tTraining loss: 0.28612974286079407\n",
      "Step: 22300  \tTraining accuracy: 0.8668345212936401\n",
      "Step: 22300  \tValid loss: 0.2611504793167114\n",
      "Step: 22400  \tTraining loss: 0.2859856188297272\n",
      "Step: 22400  \tTraining accuracy: 0.8668830394744873\n",
      "Step: 22400  \tValid loss: 0.2610766887664795\n",
      "Step: 22500  \tTraining loss: 0.2858467996120453\n",
      "Step: 22500  \tTraining accuracy: 0.8669321537017822\n",
      "Step: 22500  \tValid loss: 0.2609488070011139\n",
      "Step: 22600  \tTraining loss: 0.2857057452201843\n",
      "Step: 22600  \tTraining accuracy: 0.8669798374176025\n",
      "Step: 22600  \tValid loss: 0.2608906328678131\n",
      "Step: 22700  \tTraining loss: 0.2855643332004547\n",
      "Step: 22700  \tTraining accuracy: 0.8670297265052795\n",
      "Step: 22700  \tValid loss: 0.2607721984386444\n",
      "Step: 22800  \tTraining loss: 0.2854335308074951\n",
      "Step: 22800  \tTraining accuracy: 0.8670834302902222\n",
      "Step: 22800  \tValid loss: 0.26077035069465637\n",
      "Step: 22900  \tTraining loss: 0.28529292345046997\n",
      "Step: 22900  \tTraining accuracy: 0.8671334981918335\n",
      "Step: 22900  \tValid loss: 0.260659396648407\n",
      "Step: 23000  \tTraining loss: 0.2851462662220001\n",
      "Step: 23000  \tTraining accuracy: 0.8671815395355225\n",
      "Step: 23000  \tValid loss: 0.2605742812156677\n",
      "Step: 23100  \tTraining loss: 0.2850085496902466\n",
      "Step: 23100  \tTraining accuracy: 0.8672276139259338\n",
      "Step: 23100  \tValid loss: 0.2604941427707672\n",
      "Step: 23200  \tTraining loss: 0.28486356139183044\n",
      "Step: 23200  \tTraining accuracy: 0.8672732710838318\n",
      "Step: 23200  \tValid loss: 0.2603921890258789\n",
      "Step: 23300  \tTraining loss: 0.28472474217414856\n",
      "Step: 23300  \tTraining accuracy: 0.8673174977302551\n",
      "Step: 23300  \tValid loss: 0.2603144645690918\n",
      "Step: 23400  \tTraining loss: 0.284588485956192\n",
      "Step: 23400  \tTraining accuracy: 0.8673608303070068\n",
      "Step: 23400  \tValid loss: 0.2602444291114807\n",
      "Step: 23500  \tTraining loss: 0.28445157408714294\n",
      "Step: 23500  \tTraining accuracy: 0.8674089312553406\n",
      "Step: 23500  \tValid loss: 0.2601599097251892\n",
      "Step: 23600  \tTraining loss: 0.28432223200798035\n",
      "Step: 23600  \tTraining accuracy: 0.867459237575531\n",
      "Step: 23600  \tValid loss: 0.2600378096103668\n",
      "Step: 23700  \tTraining loss: 0.28418484330177307\n",
      "Step: 23700  \tTraining accuracy: 0.8675095438957214\n",
      "Step: 23700  \tValid loss: 0.25996387004852295\n",
      "Step: 23800  \tTraining loss: 0.2840638756752014\n",
      "Step: 23800  \tTraining accuracy: 0.867559015750885\n",
      "Step: 23800  \tValid loss: 0.25995248556137085\n",
      "Step: 23900  \tTraining loss: 0.2839259207248688\n",
      "Step: 23900  \tTraining accuracy: 0.867606520652771\n",
      "Step: 23900  \tValid loss: 0.25981375575065613\n",
      "Step: 24000  \tTraining loss: 0.28379520773887634\n",
      "Step: 24000  \tTraining accuracy: 0.8676515817642212\n",
      "Step: 24000  \tValid loss: 0.2597927749156952\n",
      "Step: 24100  \tTraining loss: 0.2836703360080719\n",
      "Step: 24100  \tTraining accuracy: 0.8676952719688416\n",
      "Step: 24100  \tValid loss: 0.25971874594688416\n",
      "Step: 24200  \tTraining loss: 0.2835453152656555\n",
      "Step: 24200  \tTraining accuracy: 0.8677371144294739\n",
      "Step: 24200  \tValid loss: 0.2596052587032318\n",
      "Step: 24300  \tTraining loss: 0.2834148108959198\n",
      "Step: 24300  \tTraining accuracy: 0.8677825927734375\n",
      "Step: 24300  \tValid loss: 0.2596336603164673\n",
      "Step: 24400  \tTraining loss: 0.2832939028739929\n",
      "Step: 24400  \tTraining accuracy: 0.8678267002105713\n",
      "Step: 24400  \tValid loss: 0.25954023003578186\n",
      "Step: 24500  \tTraining loss: 0.28316956758499146\n",
      "Step: 24500  \tTraining accuracy: 0.8678719401359558\n",
      "Step: 24500  \tValid loss: 0.25946831703186035\n",
      "Step: 24600  \tTraining loss: 0.28304946422576904\n",
      "Step: 24600  \tTraining accuracy: 0.8679153323173523\n",
      "Step: 24600  \tValid loss: 0.25934404134750366\n",
      "Step: 24700  \tTraining loss: 0.28292733430862427\n",
      "Step: 24700  \tTraining accuracy: 0.8679578304290771\n",
      "Step: 24700  \tValid loss: 0.2592830955982208\n",
      "Step: 24800  \tTraining loss: 0.2828066051006317\n",
      "Step: 24800  \tTraining accuracy: 0.8680019974708557\n",
      "Step: 24800  \tValid loss: 0.2592414915561676\n",
      "Step: 24900  \tTraining loss: 0.2826875150203705\n",
      "Step: 24900  \tTraining accuracy: 0.8680462837219238\n",
      "Step: 24900  \tValid loss: 0.2591678500175476\n",
      "Step: 25000  \tTraining loss: 0.28257060050964355\n",
      "Step: 25000  \tTraining accuracy: 0.8680902123451233\n",
      "Step: 25000  \tValid loss: 0.2591056227684021\n",
      "Step: 25100  \tTraining loss: 0.2824539244174957\n",
      "Step: 25100  \tTraining accuracy: 0.8681318759918213\n",
      "Step: 25100  \tValid loss: 0.25900089740753174\n",
      "Step: 25200  \tTraining loss: 0.282339483499527\n",
      "Step: 25200  \tTraining accuracy: 0.8681736588478088\n",
      "Step: 25200  \tValid loss: 0.25889021158218384\n",
      "Step: 25300  \tTraining loss: 0.2822267413139343\n",
      "Step: 25300  \tTraining accuracy: 0.8682184815406799\n",
      "Step: 25300  \tValid loss: 0.25887832045555115\n",
      "Step: 25400  \tTraining loss: 0.2821078300476074\n",
      "Step: 25400  \tTraining accuracy: 0.8682615160942078\n",
      "Step: 25400  \tValid loss: 0.25878486037254333\n",
      "Step: 25500  \tTraining loss: 0.28200268745422363\n",
      "Step: 25500  \tTraining accuracy: 0.8683056831359863\n",
      "Step: 25500  \tValid loss: 0.2587299346923828\n",
      "Step: 25600  \tTraining loss: 0.28188556432724\n",
      "Step: 25600  \tTraining accuracy: 0.8683480024337769\n",
      "Step: 25600  \tValid loss: 0.2586851418018341\n",
      "Step: 25700  \tTraining loss: 0.28177306056022644\n",
      "Step: 25700  \tTraining accuracy: 0.8683896064758301\n",
      "Step: 25700  \tValid loss: 0.25859183073043823\n",
      "Step: 25800  \tTraining loss: 0.28166067600250244\n",
      "Step: 25800  \tTraining accuracy: 0.8684307932853699\n",
      "Step: 25800  \tValid loss: 0.2585769593715668\n",
      "Step: 25900  \tTraining loss: 0.28155040740966797\n",
      "Step: 25900  \tTraining accuracy: 0.8684717416763306\n",
      "Step: 25900  \tValid loss: 0.2585160434246063\n",
      "Step: 26000  \tTraining loss: 0.2814392149448395\n",
      "Step: 26000  \tTraining accuracy: 0.8685123324394226\n",
      "Step: 26000  \tValid loss: 0.25847554206848145\n",
      "Step: 26100  \tTraining loss: 0.28133291006088257\n",
      "Step: 26100  \tTraining accuracy: 0.8685507774353027\n",
      "Step: 26100  \tValid loss: 0.2584554851055145\n",
      "Step: 26200  \tTraining loss: 0.28122106194496155\n",
      "Step: 26200  \tTraining accuracy: 0.8685893416404724\n",
      "Step: 26200  \tValid loss: 0.2583695948123932\n",
      "Step: 26300  \tTraining loss: 0.2811157703399658\n",
      "Step: 26300  \tTraining accuracy: 0.8686304092407227\n",
      "Step: 26300  \tValid loss: 0.2583281695842743\n",
      "Step: 26400  \tTraining loss: 0.2810100317001343\n",
      "Step: 26400  \tTraining accuracy: 0.8686735033988953\n",
      "Step: 26400  \tValid loss: 0.2583051919937134\n",
      "Step: 26500  \tTraining loss: 0.2809002995491028\n",
      "Step: 26500  \tTraining accuracy: 0.8687148690223694\n",
      "Step: 26500  \tValid loss: 0.25823065638542175\n",
      "Step: 26600  \tTraining loss: 0.28079915046691895\n",
      "Step: 26600  \tTraining accuracy: 0.8687540888786316\n",
      "Step: 26600  \tValid loss: 0.2581692337989807\n",
      "Step: 26700  \tTraining loss: 0.2806922495365143\n",
      "Step: 26700  \tTraining accuracy: 0.8687911629676819\n",
      "Step: 26700  \tValid loss: 0.2581688463687897\n",
      "Step: 26800  \tTraining loss: 0.28059613704681396\n",
      "Step: 26800  \tTraining accuracy: 0.8688307404518127\n",
      "Step: 26800  \tValid loss: 0.2581191956996918\n",
      "Step: 26900  \tTraining loss: 0.28048568964004517\n",
      "Step: 26900  \tTraining accuracy: 0.8688672780990601\n",
      "Step: 26900  \tValid loss: 0.25802889466285706\n",
      "Step: 27000  \tTraining loss: 0.28038814663887024\n",
      "Step: 27000  \tTraining accuracy: 0.8689062595367432\n",
      "Step: 27000  \tValid loss: 0.2579413950443268\n",
      "Step: 27100  \tTraining loss: 0.2802887558937073\n",
      "Step: 27100  \tTraining accuracy: 0.868945837020874\n",
      "Step: 27100  \tValid loss: 0.2578997015953064\n",
      "Step: 27200  \tTraining loss: 0.2801855802536011\n",
      "Step: 27200  \tTraining accuracy: 0.868985116481781\n",
      "Step: 27200  \tValid loss: 0.2578532099723816\n",
      "Step: 27300  \tTraining loss: 0.28008219599723816\n",
      "Step: 27300  \tTraining accuracy: 0.8690240979194641\n",
      "Step: 27300  \tValid loss: 0.2578025162220001\n",
      "Step: 27400  \tTraining loss: 0.2799786329269409\n",
      "Step: 27400  \tTraining accuracy: 0.8690628409385681\n",
      "Step: 27400  \tValid loss: 0.2577558755874634\n",
      "Step: 27500  \tTraining loss: 0.27987512946128845\n",
      "Step: 27500  \tTraining accuracy: 0.8690999746322632\n",
      "Step: 27500  \tValid loss: 0.25774621963500977\n",
      "Step: 27600  \tTraining loss: 0.2797776758670807\n",
      "Step: 27600  \tTraining accuracy: 0.8691359162330627\n",
      "Step: 27600  \tValid loss: 0.257764607667923\n",
      "Step: 27700  \tTraining loss: 0.2796746790409088\n",
      "Step: 27700  \tTraining accuracy: 0.869175136089325\n",
      "Step: 27700  \tValid loss: 0.25762927532196045\n",
      "Step: 27800  \tTraining loss: 0.27957388758659363\n",
      "Step: 27800  \tTraining accuracy: 0.8692096471786499\n",
      "Step: 27800  \tValid loss: 0.2576111853122711\n",
      "Step: 27900  \tTraining loss: 0.27947402000427246\n",
      "Step: 27900  \tTraining accuracy: 0.869245707988739\n",
      "Step: 27900  \tValid loss: 0.2575759291648865\n",
      "Step: 28000  \tTraining loss: 0.27938738465309143\n",
      "Step: 28000  \tTraining accuracy: 0.869284987449646\n",
      "Step: 28000  \tValid loss: 0.2575298547744751\n",
      "Step: 28100  \tTraining loss: 0.2792820632457733\n",
      "Step: 28100  \tTraining accuracy: 0.8693222403526306\n",
      "Step: 28100  \tValid loss: 0.2574688494205475\n",
      "Step: 28200  \tTraining loss: 0.2791840434074402\n",
      "Step: 28200  \tTraining accuracy: 0.8693570494651794\n",
      "Step: 28200  \tValid loss: 0.25745582580566406\n",
      "Step: 28300  \tTraining loss: 0.2790881395339966\n",
      "Step: 28300  \tTraining accuracy: 0.8693929314613342\n",
      "Step: 28300  \tValid loss: 0.25741544365882874\n",
      "Step: 28400  \tTraining loss: 0.2789928615093231\n",
      "Step: 28400  \tTraining accuracy: 0.8694294095039368\n",
      "Step: 28400  \tValid loss: 0.25738731026649475\n",
      "Step: 28500  \tTraining loss: 0.27888157963752747\n",
      "Step: 28500  \tTraining accuracy: 0.8694660663604736\n",
      "Step: 28500  \tValid loss: 0.25722816586494446\n",
      "Step: 28600  \tTraining loss: 0.2787790894508362\n",
      "Step: 28600  \tTraining accuracy: 0.8695020079612732\n",
      "Step: 28600  \tValid loss: 0.2572525143623352\n",
      "Step: 28700  \tTraining loss: 0.2786748707294464\n",
      "Step: 28700  \tTraining accuracy: 0.8695423603057861\n",
      "Step: 28700  \tValid loss: 0.25717782974243164\n",
      "Step: 28800  \tTraining loss: 0.27857959270477295\n",
      "Step: 28800  \tTraining accuracy: 0.8695816397666931\n",
      "Step: 28800  \tValid loss: 0.2571636736392975\n",
      "Step: 28900  \tTraining loss: 0.278484970331192\n",
      "Step: 28900  \tTraining accuracy: 0.8696222305297852\n",
      "Step: 28900  \tValid loss: 0.25711724162101746\n",
      "Step: 29000  \tTraining loss: 0.27839475870132446\n",
      "Step: 29000  \tTraining accuracy: 0.8696630001068115\n",
      "Step: 29000  \tValid loss: 0.2570834457874298\n",
      "Step: 29100  \tTraining loss: 0.2782963812351227\n",
      "Step: 29100  \tTraining accuracy: 0.8697043657302856\n",
      "Step: 29100  \tValid loss: 0.25712329149246216\n",
      "Step: 29200  \tTraining loss: 0.27820470929145813\n",
      "Step: 29200  \tTraining accuracy: 0.8697470426559448\n",
      "Step: 29200  \tValid loss: 0.25709548592567444\n",
      "Step: 29300  \tTraining loss: 0.2781197726726532\n",
      "Step: 29300  \tTraining accuracy: 0.8697890639305115\n",
      "Step: 29300  \tValid loss: 0.25699418783187866\n",
      "Step: 29400  \tTraining loss: 0.27802982926368713\n",
      "Step: 29400  \tTraining accuracy: 0.8698275089263916\n",
      "Step: 29400  \tValid loss: 0.25698545575141907\n",
      "Step: 29500  \tTraining loss: 0.27793338894844055\n",
      "Step: 29500  \tTraining accuracy: 0.8698668479919434\n",
      "Step: 29500  \tValid loss: 0.25695452094078064\n",
      "Step: 29600  \tTraining loss: 0.2778415083885193\n",
      "Step: 29600  \tTraining accuracy: 0.8699064254760742\n",
      "Step: 29600  \tValid loss: 0.25697430968284607\n",
      "Step: 29700  \tTraining loss: 0.2777588963508606\n",
      "Step: 29700  \tTraining accuracy: 0.8699477314949036\n",
      "Step: 29700  \tValid loss: 0.2569159269332886\n",
      "Step: 29800  \tTraining loss: 0.2776694595813751\n",
      "Step: 29800  \tTraining accuracy: 0.869988739490509\n",
      "Step: 29800  \tValid loss: 0.25690707564353943\n",
      "Step: 29900  \tTraining loss: 0.2775731384754181\n",
      "Step: 29900  \tTraining accuracy: 0.8700302839279175\n",
      "Step: 29900  \tValid loss: 0.2568961977958679\n",
      "Step: 30000  \tTraining loss: 0.27748551964759827\n",
      "Step: 30000  \tTraining accuracy: 0.8700703978538513\n",
      "Step: 30000  \tValid loss: 0.25677964091300964\n",
      "Step: 30100  \tTraining loss: 0.27740123867988586\n",
      "Step: 30100  \tTraining accuracy: 0.8701109886169434\n",
      "Step: 30100  \tValid loss: 0.25677424669265747\n",
      "Step: 30200  \tTraining loss: 0.2773226797580719\n",
      "Step: 30200  \tTraining accuracy: 0.8701469302177429\n",
      "Step: 30200  \tValid loss: 0.2566838264465332\n",
      "Step: 30300  \tTraining loss: 0.2772253155708313\n",
      "Step: 30300  \tTraining accuracy: 0.87018221616745\n",
      "Step: 30300  \tValid loss: 0.2566767632961273\n",
      "Step: 30400  \tTraining loss: 0.2771344780921936\n",
      "Step: 30400  \tTraining accuracy: 0.8702212572097778\n",
      "Step: 30400  \tValid loss: 0.25669848918914795\n",
      "Step: 30500  \tTraining loss: 0.27704480290412903\n",
      "Step: 30500  \tTraining accuracy: 0.8702588677406311\n",
      "Step: 30500  \tValid loss: 0.2566402554512024\n",
      "Step: 30600  \tTraining loss: 0.2769564688205719\n",
      "Step: 30600  \tTraining accuracy: 0.8702954053878784\n",
      "Step: 30600  \tValid loss: 0.25661778450012207\n",
      "Step: 30700  \tTraining loss: 0.27687057852745056\n",
      "Step: 30700  \tTraining accuracy: 0.8703305125236511\n",
      "Step: 30700  \tValid loss: 0.2565438151359558\n",
      "Step: 30800  \tTraining loss: 0.27679377794265747\n",
      "Step: 30800  \tTraining accuracy: 0.8703658580780029\n",
      "Step: 30800  \tValid loss: 0.25645849108695984\n",
      "Step: 30900  \tTraining loss: 0.27670159935951233\n",
      "Step: 30900  \tTraining accuracy: 0.8704004883766174\n",
      "Step: 30900  \tValid loss: 0.25644540786743164\n",
      "Step: 31000  \tTraining loss: 0.276621013879776\n",
      "Step: 31000  \tTraining accuracy: 0.870435357093811\n",
      "Step: 31000  \tValid loss: 0.2564113736152649\n",
      "Step: 31100  \tTraining loss: 0.2765490710735321\n",
      "Step: 31100  \tTraining accuracy: 0.8704707622528076\n",
      "Step: 31100  \tValid loss: 0.25635409355163574\n",
      "Step: 31200  \tTraining loss: 0.2764538824558258\n",
      "Step: 31200  \tTraining accuracy: 0.8705047965049744\n",
      "Step: 31200  \tValid loss: 0.2563052177429199\n",
      "Step: 31300  \tTraining loss: 0.2763765752315521\n",
      "Step: 31300  \tTraining accuracy: 0.8705389499664307\n",
      "Step: 31300  \tValid loss: 0.2562577426433563\n",
      "Step: 31400  \tTraining loss: 0.2762930691242218\n",
      "Step: 31400  \tTraining accuracy: 0.8705736994743347\n",
      "Step: 31400  \tValid loss: 0.25618746876716614\n",
      "Step: 31500  \tTraining loss: 0.27621030807495117\n",
      "Step: 31500  \tTraining accuracy: 0.8706082105636597\n",
      "Step: 31500  \tValid loss: 0.2561385929584503\n",
      "Step: 31600  \tTraining loss: 0.2761326730251312\n",
      "Step: 31600  \tTraining accuracy: 0.8706413507461548\n",
      "Step: 31600  \tValid loss: 0.2560928463935852\n",
      "Step: 31700  \tTraining loss: 0.27605506777763367\n",
      "Step: 31700  \tTraining accuracy: 0.8706746697425842\n",
      "Step: 31700  \tValid loss: 0.2560969293117523\n",
      "Step: 31800  \tTraining loss: 0.2759743332862854\n",
      "Step: 31800  \tTraining accuracy: 0.8707085251808167\n",
      "Step: 31800  \tValid loss: 0.25601449608802795\n",
      "Step: 31900  \tTraining loss: 0.27589863538742065\n",
      "Step: 31900  \tTraining accuracy: 0.8707422018051147\n",
      "Step: 31900  \tValid loss: 0.25593623518943787\n",
      "Step: 32000  \tTraining loss: 0.27582404017448425\n",
      "Step: 32000  \tTraining accuracy: 0.870774507522583\n",
      "Step: 32000  \tValid loss: 0.2559003531932831\n",
      "Step: 32100  \tTraining loss: 0.2757430970668793\n",
      "Step: 32100  \tTraining accuracy: 0.8708069920539856\n",
      "Step: 32100  \tValid loss: 0.25586655735969543\n",
      "Step: 32200  \tTraining loss: 0.27567076683044434\n",
      "Step: 32200  \tTraining accuracy: 0.8708400130271912\n",
      "Step: 32200  \tValid loss: 0.2557675540447235\n",
      "Step: 32300  \tTraining loss: 0.2755907475948334\n",
      "Step: 32300  \tTraining accuracy: 0.8708728551864624\n",
      "Step: 32300  \tValid loss: 0.25572243332862854\n",
      "Step: 32400  \tTraining loss: 0.2755216956138611\n",
      "Step: 32400  \tTraining accuracy: 0.8709039688110352\n",
      "Step: 32400  \tValid loss: 0.25563541054725647\n",
      "Step: 32500  \tTraining loss: 0.2754404544830322\n",
      "Step: 32500  \tTraining accuracy: 0.8709341287612915\n",
      "Step: 32500  \tValid loss: 0.2556546926498413\n",
      "Step: 32600  \tTraining loss: 0.2753681540489197\n",
      "Step: 32600  \tTraining accuracy: 0.8709671497344971\n",
      "Step: 32600  \tValid loss: 0.2556086480617523\n",
      "Step: 32700  \tTraining loss: 0.27529528737068176\n",
      "Step: 32700  \tTraining accuracy: 0.8710002899169922\n",
      "Step: 32700  \tValid loss: 0.2554941773414612\n",
      "Step: 32800  \tTraining loss: 0.2752198874950409\n",
      "Step: 32800  \tTraining accuracy: 0.8710328340530396\n",
      "Step: 32800  \tValid loss: 0.2555340826511383\n",
      "Step: 32900  \tTraining loss: 0.2751480042934418\n",
      "Step: 32900  \tTraining accuracy: 0.8710663318634033\n",
      "Step: 32900  \tValid loss: 0.2554151117801666\n",
      "Step: 33000  \tTraining loss: 0.27507415413856506\n",
      "Step: 33000  \tTraining accuracy: 0.8710996508598328\n",
      "Step: 33000  \tValid loss: 0.2553800940513611\n",
      "Step: 33100  \tTraining loss: 0.2750012278556824\n",
      "Step: 33100  \tTraining accuracy: 0.8711330890655518\n",
      "Step: 33100  \tValid loss: 0.2553534507751465\n",
      "Step: 33200  \tTraining loss: 0.27492642402648926\n",
      "Step: 33200  \tTraining accuracy: 0.871167778968811\n",
      "Step: 33200  \tValid loss: 0.2552930414676666\n",
      "Step: 33300  \tTraining loss: 0.27485421299934387\n",
      "Step: 33300  \tTraining accuracy: 0.8712000846862793\n",
      "Step: 33300  \tValid loss: 0.25529944896698\n",
      "Step: 33400  \tTraining loss: 0.27478477358818054\n",
      "Step: 33400  \tTraining accuracy: 0.8712318539619446\n",
      "Step: 33400  \tValid loss: 0.2551995515823364\n",
      "Step: 33500  \tTraining loss: 0.2747093439102173\n",
      "Step: 33500  \tTraining accuracy: 0.8712645173072815\n",
      "Step: 33500  \tValid loss: 0.25519514083862305\n",
      "Step: 33600  \tTraining loss: 0.2746407687664032\n",
      "Step: 33600  \tTraining accuracy: 0.871297299861908\n",
      "Step: 33600  \tValid loss: 0.2551541030406952\n",
      "Step: 33700  \tTraining loss: 0.27457156777381897\n",
      "Step: 33700  \tTraining accuracy: 0.8713302612304688\n",
      "Step: 33700  \tValid loss: 0.2550475001335144\n",
      "Step: 33800  \tTraining loss: 0.27450254559516907\n",
      "Step: 33800  \tTraining accuracy: 0.8713637590408325\n",
      "Step: 33800  \tValid loss: 0.2550068795681\n",
      "Step: 33900  \tTraining loss: 0.2744320333003998\n",
      "Step: 33900  \tTraining accuracy: 0.871397078037262\n",
      "Step: 33900  \tValid loss: 0.2549501657485962\n",
      "Step: 34000  \tTraining loss: 0.27436280250549316\n",
      "Step: 34000  \tTraining accuracy: 0.8714302182197571\n",
      "Step: 34000  \tValid loss: 0.2549217641353607\n",
      "Step: 34100  \tTraining loss: 0.2742938697338104\n",
      "Step: 34100  \tTraining accuracy: 0.8714649081230164\n",
      "Step: 34100  \tValid loss: 0.25484901666641235\n",
      "Step: 34200  \tTraining loss: 0.27422595024108887\n",
      "Step: 34200  \tTraining accuracy: 0.8714993596076965\n",
      "Step: 34200  \tValid loss: 0.25480690598487854\n",
      "Step: 34300  \tTraining loss: 0.2741556465625763\n",
      "Step: 34300  \tTraining accuracy: 0.8715336918830872\n",
      "Step: 34300  \tValid loss: 0.25482097268104553\n",
      "Step: 34400  \tTraining loss: 0.27409034967422485\n",
      "Step: 34400  \tTraining accuracy: 0.8715645670890808\n",
      "Step: 34400  \tValid loss: 0.25476574897766113\n",
      "Step: 34500  \tTraining loss: 0.2740216851234436\n",
      "Step: 34500  \tTraining accuracy: 0.8715953230857849\n",
      "Step: 34500  \tValid loss: 0.2547224462032318\n",
      "Step: 34600  \tTraining loss: 0.27395737171173096\n",
      "Step: 34600  \tTraining accuracy: 0.8716272711753845\n",
      "Step: 34600  \tValid loss: 0.25469136238098145\n",
      "Step: 34700  \tTraining loss: 0.27388641238212585\n",
      "Step: 34700  \tTraining accuracy: 0.8716590404510498\n",
      "Step: 34700  \tValid loss: 0.2546009123325348\n",
      "Step: 34800  \tTraining loss: 0.27382490038871765\n",
      "Step: 34800  \tTraining accuracy: 0.8716906309127808\n",
      "Step: 34800  \tValid loss: 0.254572331905365\n",
      "Step: 34900  \tTraining loss: 0.2737603485584259\n",
      "Step: 34900  \tTraining accuracy: 0.8717220425605774\n",
      "Step: 34900  \tValid loss: 0.25444912910461426\n",
      "Step: 35000  \tTraining loss: 0.27369701862335205\n",
      "Step: 35000  \tTraining accuracy: 0.8717532753944397\n",
      "Step: 35000  \tValid loss: 0.2544029653072357\n",
      "Step: 35100  \tTraining loss: 0.2736327052116394\n",
      "Step: 35100  \tTraining accuracy: 0.8717843294143677\n",
      "Step: 35100  \tValid loss: 0.2544187903404236\n",
      "Step: 35200  \tTraining loss: 0.27356910705566406\n",
      "Step: 35200  \tTraining accuracy: 0.8718148469924927\n",
      "Step: 35200  \tValid loss: 0.25432097911834717\n",
      "Step: 35300  \tTraining loss: 0.27350813150405884\n",
      "Step: 35300  \tTraining accuracy: 0.8718445301055908\n",
      "Step: 35300  \tValid loss: 0.2543317973613739\n",
      "Step: 35400  \tTraining loss: 0.27344366908073425\n",
      "Step: 35400  \tTraining accuracy: 0.8718750476837158\n",
      "Step: 35400  \tValid loss: 0.2542290687561035\n",
      "Step: 35500  \tTraining loss: 0.27338090538978577\n",
      "Step: 35500  \tTraining accuracy: 0.8719054460525513\n",
      "Step: 35500  \tValid loss: 0.254182904958725\n",
      "Step: 35600  \tTraining loss: 0.27331987023353577\n",
      "Step: 35600  \tTraining accuracy: 0.8719363212585449\n",
      "Step: 35600  \tValid loss: 0.25412651896476746\n",
      "Step: 35700  \tTraining loss: 0.2732582688331604\n",
      "Step: 35700  \tTraining accuracy: 0.8719673156738281\n",
      "Step: 35700  \tValid loss: 0.2541021704673767\n",
      "Step: 35800  \tTraining loss: 0.27319636940956116\n",
      "Step: 35800  \tTraining accuracy: 0.8719971776008606\n",
      "Step: 35800  \tValid loss: 0.25407153367996216\n",
      "Step: 35900  \tTraining loss: 0.27313676476478577\n",
      "Step: 35900  \tTraining accuracy: 0.8720268607139587\n",
      "Step: 35900  \tValid loss: 0.25398847460746765\n",
      "Step: 36000  \tTraining loss: 0.2730753719806671\n",
      "Step: 36000  \tTraining accuracy: 0.8720563650131226\n",
      "Step: 36000  \tValid loss: 0.2539926767349243\n",
      "Step: 36100  \tTraining loss: 0.273013710975647\n",
      "Step: 36100  \tTraining accuracy: 0.8720850348472595\n",
      "Step: 36100  \tValid loss: 0.2538565397262573\n",
      "Step: 36200  \tTraining loss: 0.27295196056365967\n",
      "Step: 36200  \tTraining accuracy: 0.8721132278442383\n",
      "Step: 36200  \tValid loss: 0.25381362438201904\n",
      "Step: 36300  \tTraining loss: 0.27289247512817383\n",
      "Step: 36300  \tTraining accuracy: 0.8721422553062439\n",
      "Step: 36300  \tValid loss: 0.2537590563297272\n",
      "Step: 36400  \tTraining loss: 0.2728345990180969\n",
      "Step: 36400  \tTraining accuracy: 0.87217116355896\n",
      "Step: 36400  \tValid loss: 0.25373128056526184\n",
      "Step: 36500  \tTraining loss: 0.2727762460708618\n",
      "Step: 36500  \tTraining accuracy: 0.8721998333930969\n",
      "Step: 36500  \tValid loss: 0.25368040800094604\n",
      "Step: 36600  \tTraining loss: 0.2727135717868805\n",
      "Step: 36600  \tTraining accuracy: 0.8722261190414429\n",
      "Step: 36600  \tValid loss: 0.25364768505096436\n",
      "Step: 36700  \tTraining loss: 0.27265438437461853\n",
      "Step: 36700  \tTraining accuracy: 0.872252881526947\n",
      "Step: 36700  \tValid loss: 0.2536063492298126\n",
      "Step: 36800  \tTraining loss: 0.27259430289268494\n",
      "Step: 36800  \tTraining accuracy: 0.8722794651985168\n",
      "Step: 36800  \tValid loss: 0.25355836749076843\n",
      "Step: 36900  \tTraining loss: 0.27254047989845276\n",
      "Step: 36900  \tTraining accuracy: 0.8723065853118896\n",
      "Step: 36900  \tValid loss: 0.2535814642906189\n",
      "Step: 37000  \tTraining loss: 0.27247315645217896\n",
      "Step: 37000  \tTraining accuracy: 0.8723338842391968\n",
      "Step: 37000  \tValid loss: 0.2534560561180115\n",
      "Step: 37100  \tTraining loss: 0.27242034673690796\n",
      "Step: 37100  \tTraining accuracy: 0.8723610639572144\n",
      "Step: 37100  \tValid loss: 0.25342756509780884\n",
      "Step: 37200  \tTraining loss: 0.27235665917396545\n",
      "Step: 37200  \tTraining accuracy: 0.8723877668380737\n",
      "Step: 37200  \tValid loss: 0.2533459961414337\n",
      "Step: 37300  \tTraining loss: 0.272296279668808\n",
      "Step: 37300  \tTraining accuracy: 0.8724136352539062\n",
      "Step: 37300  \tValid loss: 0.2533266842365265\n",
      "Step: 37400  \tTraining loss: 0.272236704826355\n",
      "Step: 37400  \tTraining accuracy: 0.8724396824836731\n",
      "Step: 37400  \tValid loss: 0.25327563285827637\n",
      "Step: 37500  \tTraining loss: 0.27217724919319153\n",
      "Step: 37500  \tTraining accuracy: 0.8724653124809265\n",
      "Step: 37500  \tValid loss: 0.2532379925251007\n",
      "Step: 37600  \tTraining loss: 0.27212047576904297\n",
      "Step: 37600  \tTraining accuracy: 0.8724908232688904\n",
      "Step: 37600  \tValid loss: 0.25317198038101196\n",
      "Step: 37700  \tTraining loss: 0.2720603346824646\n",
      "Step: 37700  \tTraining accuracy: 0.8725171089172363\n",
      "Step: 37700  \tValid loss: 0.2531571388244629\n",
      "Step: 37800  \tTraining loss: 0.2720019221305847\n",
      "Step: 37800  \tTraining accuracy: 0.8725429773330688\n",
      "Step: 37800  \tValid loss: 0.25307607650756836\n",
      "Step: 37900  \tTraining loss: 0.27194276452064514\n",
      "Step: 37900  \tTraining accuracy: 0.8725689649581909\n",
      "Step: 37900  \tValid loss: 0.2530484199523926\n",
      "Step: 38000  \tTraining loss: 0.271884948015213\n",
      "Step: 38000  \tTraining accuracy: 0.872595489025116\n",
      "Step: 38000  \tValid loss: 0.25295883417129517\n",
      "Step: 38100  \tTraining loss: 0.27182862162590027\n",
      "Step: 38100  \tTraining accuracy: 0.8726235032081604\n",
      "Step: 38100  \tValid loss: 0.25293225049972534\n",
      "Step: 38200  \tTraining loss: 0.2717703878879547\n",
      "Step: 38200  \tTraining accuracy: 0.872651994228363\n",
      "Step: 38200  \tValid loss: 0.25287339091300964\n",
      "Step: 38300  \tTraining loss: 0.2717113196849823\n",
      "Step: 38300  \tTraining accuracy: 0.8726806044578552\n",
      "Step: 38300  \tValid loss: 0.25284746289253235\n",
      "Step: 38400  \tTraining loss: 0.27165523171424866\n",
      "Step: 38400  \tTraining accuracy: 0.872708797454834\n",
      "Step: 38400  \tValid loss: 0.25278687477111816\n",
      "Step: 38500  \tTraining loss: 0.27159005403518677\n",
      "Step: 38500  \tTraining accuracy: 0.8727377653121948\n",
      "Step: 38500  \tValid loss: 0.2527334988117218\n",
      "Step: 38600  \tTraining loss: 0.27153128385543823\n",
      "Step: 38600  \tTraining accuracy: 0.8727665543556213\n",
      "Step: 38600  \tValid loss: 0.2526842951774597\n",
      "Step: 38700  \tTraining loss: 0.27147412300109863\n",
      "Step: 38700  \tTraining accuracy: 0.8727958798408508\n",
      "Step: 38700  \tValid loss: 0.25258204340934753\n",
      "Step: 38800  \tTraining loss: 0.27141958475112915\n",
      "Step: 38800  \tTraining accuracy: 0.8728265762329102\n",
      "Step: 38800  \tValid loss: 0.2525615692138672\n",
      "Step: 38900  \tTraining loss: 0.2713547348976135\n",
      "Step: 38900  \tTraining accuracy: 0.8728552460670471\n",
      "Step: 38900  \tValid loss: 0.25252828001976013\n",
      "Step: 39000  \tTraining loss: 0.27130457758903503\n",
      "Step: 39000  \tTraining accuracy: 0.8728846907615662\n",
      "Step: 39000  \tValid loss: 0.25244948267936707\n",
      "Step: 39100  \tTraining loss: 0.2712372839450836\n",
      "Step: 39100  \tTraining accuracy: 0.8729137182235718\n",
      "Step: 39100  \tValid loss: 0.25237610936164856\n",
      "Step: 39200  \tTraining loss: 0.2711790204048157\n",
      "Step: 39200  \tTraining accuracy: 0.8729438185691833\n",
      "Step: 39200  \tValid loss: 0.25232821702957153\n",
      "Step: 39300  \tTraining loss: 0.2711179554462433\n",
      "Step: 39300  \tTraining accuracy: 0.8729724884033203\n",
      "Step: 39300  \tValid loss: 0.25227198004722595\n",
      "Step: 39400  \tTraining loss: 0.2710649073123932\n",
      "Step: 39400  \tTraining accuracy: 0.873002290725708\n",
      "Step: 39400  \tValid loss: 0.2522186040878296\n",
      "Step: 39500  \tTraining loss: 0.27099981904029846\n",
      "Step: 39500  \tTraining accuracy: 0.8730307221412659\n",
      "Step: 39500  \tValid loss: 0.25222474336624146\n",
      "Step: 39600  \tTraining loss: 0.2709426283836365\n",
      "Step: 39600  \tTraining accuracy: 0.8730601668357849\n",
      "Step: 39600  \tValid loss: 0.25211822986602783\n",
      "Step: 39700  \tTraining loss: 0.27088096737861633\n",
      "Step: 39700  \tTraining accuracy: 0.8730883002281189\n",
      "Step: 39700  \tValid loss: 0.25209876894950867\n",
      "Step: 39800  \tTraining loss: 0.2708238959312439\n",
      "Step: 39800  \tTraining accuracy: 0.8731175065040588\n",
      "Step: 39800  \tValid loss: 0.25200778245925903\n",
      "Step: 39900  \tTraining loss: 0.27076271176338196\n",
      "Step: 39900  \tTraining accuracy: 0.8731459379196167\n",
      "Step: 39900  \tValid loss: 0.25194257497787476\n",
      "Step: 40000  \tTraining loss: 0.27070388197898865\n",
      "Step: 40000  \tTraining accuracy: 0.8731751441955566\n",
      "Step: 40000  \tValid loss: 0.2519606947898865\n",
      "Step: 40100  \tTraining loss: 0.2706442177295685\n",
      "Step: 40100  \tTraining accuracy: 0.8732039332389832\n",
      "Step: 40100  \tValid loss: 0.25184088945388794\n",
      "Step: 40200  \tTraining loss: 0.27058646082878113\n",
      "Step: 40200  \tTraining accuracy: 0.8732319474220276\n",
      "Step: 40200  \tValid loss: 0.25181955099105835\n",
      "Step: 40300  \tTraining loss: 0.2705225646495819\n",
      "Step: 40300  \tTraining accuracy: 0.8732598423957825\n",
      "Step: 40300  \tValid loss: 0.25174587965011597\n",
      "Step: 40400  \tTraining loss: 0.2704615890979767\n",
      "Step: 40400  \tTraining accuracy: 0.8732890486717224\n",
      "Step: 40400  \tValid loss: 0.25169846415519714\n",
      "Step: 40500  \tTraining loss: 0.2704034149646759\n",
      "Step: 40500  \tTraining accuracy: 0.8733181357383728\n",
      "Step: 40500  \tValid loss: 0.2516440749168396\n",
      "Step: 40600  \tTraining loss: 0.27033984661102295\n",
      "Step: 40600  \tTraining accuracy: 0.8733456134796143\n",
      "Step: 40600  \tValid loss: 0.25158706307411194\n",
      "Step: 40700  \tTraining loss: 0.2702815532684326\n",
      "Step: 40700  \tTraining accuracy: 0.8733744025230408\n",
      "Step: 40700  \tValid loss: 0.25153079628944397\n",
      "Step: 40800  \tTraining loss: 0.2702181935310364\n",
      "Step: 40800  \tTraining accuracy: 0.8734015822410583\n",
      "Step: 40800  \tValid loss: 0.25147995352745056\n",
      "Step: 40900  \tTraining loss: 0.27015650272369385\n",
      "Step: 40900  \tTraining accuracy: 0.8734286427497864\n",
      "Step: 40900  \tValid loss: 0.2514612078666687\n",
      "Step: 41000  \tTraining loss: 0.2700989544391632\n",
      "Step: 41000  \tTraining accuracy: 0.8734555840492249\n",
      "Step: 41000  \tValid loss: 0.2513868510723114\n",
      "Step: 41100  \tTraining loss: 0.27003151178359985\n",
      "Step: 41100  \tTraining accuracy: 0.873482346534729\n",
      "Step: 41100  \tValid loss: 0.2513289451599121\n",
      "Step: 41200  \tTraining loss: 0.26996755599975586\n",
      "Step: 41200  \tTraining accuracy: 0.8735090494155884\n",
      "Step: 41200  \tValid loss: 0.251266211271286\n",
      "Step: 41300  \tTraining loss: 0.2699073851108551\n",
      "Step: 41300  \tTraining accuracy: 0.8735355734825134\n",
      "Step: 41300  \tValid loss: 0.25122150778770447\n",
      "Step: 41400  \tTraining loss: 0.26983919739723206\n",
      "Step: 41400  \tTraining accuracy: 0.8735619783401489\n",
      "Step: 41400  \tValid loss: 0.25114619731903076\n",
      "Step: 41500  \tTraining loss: 0.2697749733924866\n",
      "Step: 41500  \tTraining accuracy: 0.8735882043838501\n",
      "Step: 41500  \tValid loss: 0.25110551714897156\n",
      "Step: 41600  \tTraining loss: 0.2697104215621948\n",
      "Step: 41600  \tTraining accuracy: 0.8736152648925781\n",
      "Step: 41600  \tValid loss: 0.2510262131690979\n",
      "Step: 41700  \tTraining loss: 0.26964646577835083\n",
      "Step: 41700  \tTraining accuracy: 0.8736438751220703\n",
      "Step: 41700  \tValid loss: 0.25098732113838196\n",
      "Step: 41800  \tTraining loss: 0.2695818841457367\n",
      "Step: 41800  \tTraining accuracy: 0.8736726641654968\n",
      "Step: 41800  \tValid loss: 0.2509312927722931\n",
      "Step: 41900  \tTraining loss: 0.2695154845714569\n",
      "Step: 41900  \tTraining accuracy: 0.8737021684646606\n",
      "Step: 41900  \tValid loss: 0.2508867383003235\n",
      "Step: 42000  \tTraining loss: 0.269450843334198\n",
      "Step: 42000  \tTraining accuracy: 0.8737321496009827\n",
      "Step: 42000  \tValid loss: 0.2507832646369934\n",
      "Step: 42100  \tTraining loss: 0.26938506960868835\n",
      "Step: 42100  \tTraining accuracy: 0.8737619519233704\n",
      "Step: 42100  \tValid loss: 0.25075116753578186\n",
      "Step: 42200  \tTraining loss: 0.2693181037902832\n",
      "Step: 42200  \tTraining accuracy: 0.8737916350364685\n",
      "Step: 42200  \tValid loss: 0.25067731738090515\n",
      "Step: 42300  \tTraining loss: 0.269253671169281\n",
      "Step: 42300  \tTraining accuracy: 0.8738211393356323\n",
      "Step: 42300  \tValid loss: 0.2505979835987091\n",
      "Step: 42400  \tTraining loss: 0.2691862881183624\n",
      "Step: 42400  \tTraining accuracy: 0.8738505244255066\n",
      "Step: 42400  \tValid loss: 0.2505318224430084\n",
      "Step: 42500  \tTraining loss: 0.26912447810173035\n",
      "Step: 42500  \tTraining accuracy: 0.8738797903060913\n",
      "Step: 42500  \tValid loss: 0.25051653385162354\n",
      "Step: 42600  \tTraining loss: 0.2690538763999939\n",
      "Step: 42600  \tTraining accuracy: 0.8739088773727417\n",
      "Step: 42600  \tValid loss: 0.25043344497680664\n",
      "Step: 42700  \tTraining loss: 0.26898565888404846\n",
      "Step: 42700  \tTraining accuracy: 0.8739378452301025\n",
      "Step: 42700  \tValid loss: 0.2503603398799896\n",
      "Step: 42800  \tTraining loss: 0.2689230442047119\n",
      "Step: 42800  \tTraining accuracy: 0.8739678263664246\n",
      "Step: 42800  \tValid loss: 0.2503293752670288\n",
      "Step: 42900  \tTraining loss: 0.2688528895378113\n",
      "Step: 42900  \tTraining accuracy: 0.8739967942237854\n",
      "Step: 42900  \tValid loss: 0.25022217631340027\n",
      "Step: 43000  \tTraining loss: 0.2687849998474121\n",
      "Step: 43000  \tTraining accuracy: 0.8740254044532776\n",
      "Step: 43000  \tValid loss: 0.25016888976097107\n",
      "Step: 43100  \tTraining loss: 0.26871830224990845\n",
      "Step: 43100  \tTraining accuracy: 0.8740538358688354\n",
      "Step: 43100  \tValid loss: 0.25010037422180176\n",
      "Step: 43200  \tTraining loss: 0.26865148544311523\n",
      "Step: 43200  \tTraining accuracy: 0.8740829825401306\n",
      "Step: 43200  \tValid loss: 0.2500470280647278\n",
      "Step: 43300  \tTraining loss: 0.2685863971710205\n",
      "Step: 43300  \tTraining accuracy: 0.8741111159324646\n",
      "Step: 43300  \tValid loss: 0.24998313188552856\n",
      "Step: 43400  \tTraining loss: 0.2685180604457855\n",
      "Step: 43400  \tTraining accuracy: 0.874137818813324\n",
      "Step: 43400  \tValid loss: 0.24990858137607574\n",
      "Step: 43500  \tTraining loss: 0.2684515118598938\n",
      "Step: 43500  \tTraining accuracy: 0.8741657137870789\n",
      "Step: 43500  \tValid loss: 0.24984228610992432\n",
      "Step: 43600  \tTraining loss: 0.26838579773902893\n",
      "Step: 43600  \tTraining accuracy: 0.8741926550865173\n",
      "Step: 43600  \tValid loss: 0.24977868795394897\n",
      "Step: 43700  \tTraining loss: 0.26831746101379395\n",
      "Step: 43700  \tTraining accuracy: 0.8742175698280334\n",
      "Step: 43700  \tValid loss: 0.24978108704090118\n",
      "Step: 43800  \tTraining loss: 0.26824942231178284\n",
      "Step: 43800  \tTraining accuracy: 0.874242901802063\n",
      "Step: 43800  \tValid loss: 0.2497275173664093\n",
      "Step: 43900  \tTraining loss: 0.2681780457496643\n",
      "Step: 43900  \tTraining accuracy: 0.874268114566803\n",
      "Step: 43900  \tValid loss: 0.24969978630542755\n",
      "Step: 44000  \tTraining loss: 0.26810842752456665\n",
      "Step: 44000  \tTraining accuracy: 0.8742940425872803\n",
      "Step: 44000  \tValid loss: 0.24970634281635284\n",
      "Step: 44100  \tTraining loss: 0.2680368423461914\n",
      "Step: 44100  \tTraining accuracy: 0.874319851398468\n",
      "Step: 44100  \tValid loss: 0.24962671101093292\n",
      "Step: 44200  \tTraining loss: 0.26796990633010864\n",
      "Step: 44200  \tTraining accuracy: 0.8743455410003662\n",
      "Step: 44200  \tValid loss: 0.24958468973636627\n",
      "Step: 44300  \tTraining loss: 0.2679010033607483\n",
      "Step: 44300  \tTraining accuracy: 0.874369740486145\n",
      "Step: 44300  \tValid loss: 0.24951204657554626\n",
      "Step: 44400  \tTraining loss: 0.267831027507782\n",
      "Step: 44400  \tTraining accuracy: 0.8743948936462402\n",
      "Step: 44400  \tValid loss: 0.2493888884782791\n",
      "Step: 44500  \tTraining loss: 0.26776808500289917\n",
      "Step: 44500  \tTraining accuracy: 0.8744205236434937\n",
      "Step: 44500  \tValid loss: 0.24932250380516052\n",
      "Step: 44600  \tTraining loss: 0.2677014470100403\n",
      "Step: 44600  \tTraining accuracy: 0.8744444251060486\n",
      "Step: 44600  \tValid loss: 0.24926316738128662\n",
      "Step: 44700  \tTraining loss: 0.2676357328891754\n",
      "Step: 44700  \tTraining accuracy: 0.874468207359314\n",
      "Step: 44700  \tValid loss: 0.24916529655456543\n",
      "Step: 44800  \tTraining loss: 0.26757121086120605\n",
      "Step: 44800  \tTraining accuracy: 0.8744918704032898\n",
      "Step: 44800  \tValid loss: 0.24912351369857788\n",
      "Step: 44900  \tTraining loss: 0.2675081193447113\n",
      "Step: 44900  \tTraining accuracy: 0.8745154142379761\n",
      "Step: 44900  \tValid loss: 0.24901308119297028\n",
      "Step: 45000  \tTraining loss: 0.26744475960731506\n",
      "Step: 45000  \tTraining accuracy: 0.8745388388633728\n",
      "Step: 45000  \tValid loss: 0.2489372044801712\n",
      "Step: 45100  \tTraining loss: 0.2673836648464203\n",
      "Step: 45100  \tTraining accuracy: 0.8745622038841248\n",
      "Step: 45100  \tValid loss: 0.24888883531093597\n",
      "Step: 45200  \tTraining loss: 0.26732173562049866\n",
      "Step: 45200  \tTraining accuracy: 0.8745854496955872\n",
      "Step: 45200  \tValid loss: 0.24881717562675476\n",
      "Step: 45300  \tTraining loss: 0.2672627568244934\n",
      "Step: 45300  \tTraining accuracy: 0.8746094107627869\n",
      "Step: 45300  \tValid loss: 0.2487400323152542\n",
      "Step: 45400  \tTraining loss: 0.26719585061073303\n",
      "Step: 45400  \tTraining accuracy: 0.8746329545974731\n",
      "Step: 45400  \tValid loss: 0.24866054952144623\n",
      "Step: 45500  \tTraining loss: 0.2671361267566681\n",
      "Step: 45500  \tTraining accuracy: 0.8746566772460938\n",
      "Step: 45500  \tValid loss: 0.24857521057128906\n",
      "Step: 45600  \tTraining loss: 0.2670746445655823\n",
      "Step: 45600  \tTraining accuracy: 0.8746808767318726\n",
      "Step: 45600  \tValid loss: 0.24849694967269897\n",
      "Step: 45700  \tTraining loss: 0.26701414585113525\n",
      "Step: 45700  \tTraining accuracy: 0.874704897403717\n",
      "Step: 45700  \tValid loss: 0.24842870235443115\n",
      "Step: 45800  \tTraining loss: 0.26695573329925537\n",
      "Step: 45800  \tTraining accuracy: 0.8747288584709167\n",
      "Step: 45800  \tValid loss: 0.24835117161273956\n",
      "Step: 45900  \tTraining loss: 0.26689451932907104\n",
      "Step: 45900  \tTraining accuracy: 0.8747529983520508\n",
      "Step: 45900  \tValid loss: 0.24827660620212555\n",
      "Step: 46000  \tTraining loss: 0.2668384313583374\n",
      "Step: 46000  \tTraining accuracy: 0.8747788071632385\n",
      "Step: 46000  \tValid loss: 0.24821420013904572\n",
      "Step: 46100  \tTraining loss: 0.2667762339115143\n",
      "Step: 46100  \tTraining accuracy: 0.8748043179512024\n",
      "Step: 46100  \tValid loss: 0.24813973903656006\n",
      "Step: 46200  \tTraining loss: 0.26672083139419556\n",
      "Step: 46200  \tTraining accuracy: 0.8748299479484558\n",
      "Step: 46200  \tValid loss: 0.2480713427066803\n",
      "Step: 46300  \tTraining loss: 0.2666603624820709\n",
      "Step: 46300  \tTraining accuracy: 0.8748551607131958\n",
      "Step: 46300  \tValid loss: 0.24796801805496216\n",
      "Step: 46400  \tTraining loss: 0.2666023075580597\n",
      "Step: 46400  \tTraining accuracy: 0.8748805522918701\n",
      "Step: 46400  \tValid loss: 0.24792997539043427\n",
      "Step: 46500  \tTraining loss: 0.26654309034347534\n",
      "Step: 46500  \tTraining accuracy: 0.8749055862426758\n",
      "Step: 46500  \tValid loss: 0.24785193800926208\n",
      "Step: 46600  \tTraining loss: 0.26648756861686707\n",
      "Step: 46600  \tTraining accuracy: 0.8749300241470337\n",
      "Step: 46600  \tValid loss: 0.24779504537582397\n",
      "Step: 46700  \tTraining loss: 0.266429603099823\n",
      "Step: 46700  \tTraining accuracy: 0.874954342842102\n",
      "Step: 46700  \tValid loss: 0.24771428108215332\n",
      "Step: 46800  \tTraining loss: 0.26637181639671326\n",
      "Step: 46800  \tTraining accuracy: 0.8749793171882629\n",
      "Step: 46800  \tValid loss: 0.24764545261859894\n",
      "Step: 46900  \tTraining loss: 0.26631635427474976\n",
      "Step: 46900  \tTraining accuracy: 0.8750047087669373\n",
      "Step: 46900  \tValid loss: 0.24758990108966827\n",
      "Step: 47000  \tTraining loss: 0.2662593424320221\n",
      "Step: 47000  \tTraining accuracy: 0.8750289082527161\n",
      "Step: 47000  \tValid loss: 0.24750930070877075\n",
      "Step: 47100  \tTraining loss: 0.2662030756473541\n",
      "Step: 47100  \tTraining accuracy: 0.8750538229942322\n",
      "Step: 47100  \tValid loss: 0.24745678901672363\n",
      "Step: 47200  \tTraining loss: 0.26614782214164734\n",
      "Step: 47200  \tTraining accuracy: 0.8750789165496826\n",
      "Step: 47200  \tValid loss: 0.24738463759422302\n",
      "Step: 47300  \tTraining loss: 0.26609086990356445\n",
      "Step: 47300  \tTraining accuracy: 0.8751028180122375\n",
      "Step: 47300  \tValid loss: 0.24731560051441193\n",
      "Step: 47400  \tTraining loss: 0.2660353183746338\n",
      "Step: 47400  \tTraining accuracy: 0.8751264214515686\n",
      "Step: 47400  \tValid loss: 0.24724872410297394\n",
      "Step: 47500  \tTraining loss: 0.2659800350666046\n",
      "Step: 47500  \tTraining accuracy: 0.8751499056816101\n",
      "Step: 47500  \tValid loss: 0.24718260765075684\n",
      "Step: 47600  \tTraining loss: 0.26592546701431274\n",
      "Step: 47600  \tTraining accuracy: 0.8751740455627441\n",
      "Step: 47600  \tValid loss: 0.2471291720867157\n",
      "Step: 47700  \tTraining loss: 0.26586994528770447\n",
      "Step: 47700  \tTraining accuracy: 0.8751978278160095\n",
      "Step: 47700  \tValid loss: 0.24703271687030792\n",
      "Step: 47800  \tTraining loss: 0.26582175493240356\n",
      "Step: 47800  \tTraining accuracy: 0.8752212524414062\n",
      "Step: 47800  \tValid loss: 0.24698393046855927\n",
      "Step: 47900  \tTraining loss: 0.26576125621795654\n",
      "Step: 47900  \tTraining accuracy: 0.8752453327178955\n",
      "Step: 47900  \tValid loss: 0.2469223588705063\n",
      "Step: 48000  \tTraining loss: 0.26570725440979004\n",
      "Step: 48000  \tTraining accuracy: 0.8752678632736206\n",
      "Step: 48000  \tValid loss: 0.2468796819448471\n",
      "Step: 48100  \tTraining loss: 0.26565659046173096\n",
      "Step: 48100  \tTraining accuracy: 0.8752907514572144\n",
      "Step: 48100  \tValid loss: 0.24679791927337646\n",
      "Step: 48200  \tTraining loss: 0.265600323677063\n",
      "Step: 48200  \tTraining accuracy: 0.8753135204315186\n",
      "Step: 48200  \tValid loss: 0.24675790965557098\n",
      "Step: 48300  \tTraining loss: 0.2655467391014099\n",
      "Step: 48300  \tTraining accuracy: 0.875336229801178\n",
      "Step: 48300  \tValid loss: 0.2466864436864853\n",
      "Step: 48400  \tTraining loss: 0.26549214124679565\n",
      "Step: 48400  \tTraining accuracy: 0.8753581047058105\n",
      "Step: 48400  \tValid loss: 0.24662116169929504\n",
      "Step: 48500  \tTraining loss: 0.26544189453125\n",
      "Step: 48500  \tTraining accuracy: 0.8753800988197327\n",
      "Step: 48500  \tValid loss: 0.2465832531452179\n",
      "Step: 48600  \tTraining loss: 0.2653866708278656\n",
      "Step: 48600  \tTraining accuracy: 0.8754017949104309\n",
      "Step: 48600  \tValid loss: 0.24650143086910248\n",
      "Step: 48700  \tTraining loss: 0.26533737778663635\n",
      "Step: 48700  \tTraining accuracy: 0.8754236102104187\n",
      "Step: 48700  \tValid loss: 0.24646449089050293\n",
      "Step: 48800  \tTraining loss: 0.26528069376945496\n",
      "Step: 48800  \tTraining accuracy: 0.8754456043243408\n",
      "Step: 48800  \tValid loss: 0.24640388786792755\n",
      "Step: 48900  \tTraining loss: 0.2652284502983093\n",
      "Step: 48900  \tTraining accuracy: 0.8754684925079346\n",
      "Step: 48900  \tValid loss: 0.24633881449699402\n",
      "Step: 49000  \tTraining loss: 0.26517611742019653\n",
      "Step: 49000  \tTraining accuracy: 0.8754917979240417\n",
      "Step: 49000  \tValid loss: 0.24628280103206635\n",
      "Step: 49100  \tTraining loss: 0.2651241719722748\n",
      "Step: 49100  \tTraining accuracy: 0.8755149841308594\n",
      "Step: 49100  \tValid loss: 0.24622862040996552\n",
      "Step: 49200  \tTraining loss: 0.2650741636753082\n",
      "Step: 49200  \tTraining accuracy: 0.875537633895874\n",
      "Step: 49200  \tValid loss: 0.2461777776479721\n",
      "Step: 49300  \tTraining loss: 0.26502159237861633\n",
      "Step: 49300  \tTraining accuracy: 0.87555992603302\n",
      "Step: 49300  \tValid loss: 0.24612590670585632\n",
      "Step: 49400  \tTraining loss: 0.26496949791908264\n",
      "Step: 49400  \tTraining accuracy: 0.8755820989608765\n",
      "Step: 49400  \tValid loss: 0.24605537950992584\n",
      "Step: 49500  \tTraining loss: 0.26491960883140564\n",
      "Step: 49500  \tTraining accuracy: 0.8756044507026672\n",
      "Step: 49500  \tValid loss: 0.24601738154888153\n",
      "Step: 49600  \tTraining loss: 0.26486673951148987\n",
      "Step: 49600  \tTraining accuracy: 0.8756264448165894\n",
      "Step: 49600  \tValid loss: 0.2459457963705063\n",
      "Step: 49700  \tTraining loss: 0.26481640338897705\n",
      "Step: 49700  \tTraining accuracy: 0.8756486177444458\n",
      "Step: 49700  \tValid loss: 0.24589626491069794\n",
      "Step: 49800  \tTraining loss: 0.2647700905799866\n",
      "Step: 49800  \tTraining accuracy: 0.8756711483001709\n",
      "Step: 49800  \tValid loss: 0.24585320055484772\n",
      "Step: 49900  \tTraining loss: 0.2647149860858917\n",
      "Step: 49900  \tTraining accuracy: 0.8756936192512512\n",
      "Step: 49900  \tValid loss: 0.2457907497882843\n",
      "Step: 50000  \tTraining loss: 0.26466742157936096\n",
      "Step: 50000  \tTraining accuracy: 0.875716507434845\n",
      "Step: 50000  \tValid loss: 0.24575209617614746\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.87574\n",
      "Precision: 0.88582677\n",
      "Recall: 0.83179295\n",
      "F1 score: 0.8481905\n",
      "AUC: 0.8822017\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0   0.87574   0.885827  0.831793   0.84819  0.882202  0.264667      0.875722   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.245734       0.875707   0.373403      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  49999.0  \n",
      "33\n",
      "(4640, 8)\n",
      "(4640, 1)\n",
      "(2560, 8)\n",
      "(2560, 1)\n",
      "(2080, 8)\n",
      "(2080, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5497636198997498\n",
      "Step: 100  \tTraining accuracy: 0.7314655184745789\n",
      "Step: 100  \tValid loss: 0.5513582229614258\n",
      "Step: 200  \tTraining loss: 0.504779040813446\n",
      "Step: 200  \tTraining accuracy: 0.7408764362335205\n",
      "Step: 200  \tValid loss: 0.49923205375671387\n",
      "Step: 300  \tTraining loss: 0.4924089014530182\n",
      "Step: 300  \tTraining accuracy: 0.7487931251525879\n",
      "Step: 300  \tValid loss: 0.4857843518257141\n",
      "Step: 400  \tTraining loss: 0.4831235110759735\n",
      "Step: 400  \tTraining accuracy: 0.7540640234947205\n",
      "Step: 400  \tValid loss: 0.4770960807800293\n",
      "Step: 500  \tTraining loss: 0.4759187400341034\n",
      "Step: 500  \tTraining accuracy: 0.7590996026992798\n",
      "Step: 500  \tValid loss: 0.4712905287742615\n",
      "Step: 600  \tTraining loss: 0.46977493166923523\n",
      "Step: 600  \tTraining accuracy: 0.7631073594093323\n",
      "Step: 600  \tValid loss: 0.46668633818626404\n",
      "Step: 700  \tTraining loss: 0.46207812428474426\n",
      "Step: 700  \tTraining accuracy: 0.7667274475097656\n",
      "Step: 700  \tValid loss: 0.4615984857082367\n",
      "Step: 800  \tTraining loss: 0.4556101858615875\n",
      "Step: 800  \tTraining accuracy: 0.7697557210922241\n",
      "Step: 800  \tValid loss: 0.4570094048976898\n",
      "Step: 900  \tTraining loss: 0.45125091075897217\n",
      "Step: 900  \tTraining accuracy: 0.77265465259552\n",
      "Step: 900  \tValid loss: 0.4524545669555664\n",
      "Step: 1000  \tTraining loss: 0.44727078080177307\n",
      "Step: 1000  \tTraining accuracy: 0.7752155065536499\n",
      "Step: 1000  \tValid loss: 0.448446661233902\n",
      "Step: 1100  \tTraining loss: 0.4435441493988037\n",
      "Step: 1100  \tTraining accuracy: 0.7774220108985901\n",
      "Step: 1100  \tValid loss: 0.4457348883152008\n",
      "Step: 1200  \tTraining loss: 0.4403916597366333\n",
      "Step: 1200  \tTraining accuracy: 0.779319703578949\n",
      "Step: 1200  \tValid loss: 0.4434589743614197\n",
      "Step: 1300  \tTraining loss: 0.43818923830986023\n",
      "Step: 1300  \tTraining accuracy: 0.7809827327728271\n",
      "Step: 1300  \tValid loss: 0.44183483719825745\n",
      "Step: 1400  \tTraining loss: 0.4364815056324005\n",
      "Step: 1400  \tTraining accuracy: 0.7824313640594482\n",
      "Step: 1400  \tValid loss: 0.44107645750045776\n",
      "Step: 1500  \tTraining loss: 0.4336985647678375\n",
      "Step: 1500  \tTraining accuracy: 0.7837098836898804\n",
      "Step: 1500  \tValid loss: 0.4376145303249359\n",
      "Step: 1600  \tTraining loss: 0.4313020408153534\n",
      "Step: 1600  \tTraining accuracy: 0.7848303914070129\n",
      "Step: 1600  \tValid loss: 0.4358838200569153\n",
      "Step: 1700  \tTraining loss: 0.429532527923584\n",
      "Step: 1700  \tTraining accuracy: 0.7857954502105713\n",
      "Step: 1700  \tValid loss: 0.43359923362731934\n",
      "Step: 1800  \tTraining loss: 0.42814579606056213\n",
      "Step: 1800  \tTraining accuracy: 0.7866441011428833\n",
      "Step: 1800  \tValid loss: 0.43241986632347107\n",
      "Step: 1900  \tTraining loss: 0.4268783628940582\n",
      "Step: 1900  \tTraining accuracy: 0.7875000238418579\n",
      "Step: 1900  \tValid loss: 0.4311458468437195\n",
      "Step: 2000  \tTraining loss: 0.4256154000759125\n",
      "Step: 2000  \tTraining accuracy: 0.788317859172821\n",
      "Step: 2000  \tValid loss: 0.4299244284629822\n",
      "Step: 2100  \tTraining loss: 0.4243306517601013\n",
      "Step: 2100  \tTraining accuracy: 0.7890296578407288\n",
      "Step: 2100  \tValid loss: 0.42890411615371704\n",
      "Step: 2200  \tTraining loss: 0.4230378270149231\n",
      "Step: 2200  \tTraining accuracy: 0.7896501421928406\n",
      "Step: 2200  \tValid loss: 0.4277195334434509\n",
      "Step: 2300  \tTraining loss: 0.42163097858428955\n",
      "Step: 2300  \tTraining accuracy: 0.7903017401695251\n",
      "Step: 2300  \tValid loss: 0.426990807056427\n",
      "Step: 2400  \tTraining loss: 0.42029425501823425\n",
      "Step: 2400  \tTraining accuracy: 0.7909941077232361\n",
      "Step: 2400  \tValid loss: 0.42628344893455505\n",
      "Step: 2500  \tTraining loss: 0.41901591420173645\n",
      "Step: 2500  \tTraining accuracy: 0.7915860414505005\n",
      "Step: 2500  \tValid loss: 0.4254012107849121\n",
      "Step: 2600  \tTraining loss: 0.4180678725242615\n",
      "Step: 2600  \tTraining accuracy: 0.7921441793441772\n",
      "Step: 2600  \tValid loss: 0.425258070230484\n",
      "Step: 2700  \tTraining loss: 0.4172499477863312\n",
      "Step: 2700  \tTraining accuracy: 0.7926154732704163\n",
      "Step: 2700  \tValid loss: 0.4250744879245758\n",
      "Step: 2800  \tTraining loss: 0.4164614975452423\n",
      "Step: 2800  \tTraining accuracy: 0.7930681705474854\n",
      "Step: 2800  \tValid loss: 0.4249574542045593\n",
      "Step: 2900  \tTraining loss: 0.41563692688941956\n",
      "Step: 2900  \tTraining accuracy: 0.7934777736663818\n",
      "Step: 2900  \tValid loss: 0.4248540997505188\n",
      "Step: 3000  \tTraining loss: 0.4148678481578827\n",
      "Step: 3000  \tTraining accuracy: 0.7938522696495056\n",
      "Step: 3000  \tValid loss: 0.42488449811935425\n",
      "Step: 3100  \tTraining loss: 0.4141680598258972\n",
      "Step: 3100  \tTraining accuracy: 0.794223427772522\n",
      "Step: 3100  \tValid loss: 0.4250195622444153\n",
      "Step: 3200  \tTraining loss: 0.41351598501205444\n",
      "Step: 3200  \tTraining accuracy: 0.7945812940597534\n",
      "Step: 3200  \tValid loss: 0.42510777711868286\n",
      "Step: 3300  \tTraining loss: 0.4128836393356323\n",
      "Step: 3300  \tTraining accuracy: 0.7949303984642029\n",
      "Step: 3300  \tValid loss: 0.4252120852470398\n",
      "Step: 3400  \tTraining loss: 0.41214263439178467\n",
      "Step: 3400  \tTraining accuracy: 0.7952425479888916\n",
      "Step: 3400  \tValid loss: 0.42544493079185486\n",
      "Step: 3500  \tTraining loss: 0.4114484190940857\n",
      "Step: 3500  \tTraining accuracy: 0.7955178618431091\n",
      "Step: 3500  \tValid loss: 0.4254704415798187\n",
      "Step: 3600  \tTraining loss: 0.4107658565044403\n",
      "Step: 3600  \tTraining accuracy: 0.7957624793052673\n",
      "Step: 3600  \tValid loss: 0.4252120554447174\n",
      "Step: 3700  \tTraining loss: 0.4100692868232727\n",
      "Step: 3700  \tTraining accuracy: 0.7959848642349243\n",
      "Step: 3700  \tValid loss: 0.4252685010433197\n",
      "Step: 3800  \tTraining loss: 0.4094102382659912\n",
      "Step: 3800  \tTraining accuracy: 0.7962327599525452\n",
      "Step: 3800  \tValid loss: 0.42535462975502014\n",
      "Step: 3900  \tTraining loss: 0.4088036119937897\n",
      "Step: 3900  \tTraining accuracy: 0.7964481711387634\n",
      "Step: 3900  \tValid loss: 0.42540183663368225\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7966363\n",
      "Precision: 0.8538109\n",
      "Recall: 0.79766536\n",
      "F1 score: 0.7695529\n",
      "AUC: 0.8140501\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0  0.796636   0.853811  0.797665  0.769553  0.81405  0.408463      0.796436   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.424843       0.796371    0.49384      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3959.0  \n",
      "34\n",
      "(4930, 8)\n",
      "(4930, 1)\n",
      "(2720, 8)\n",
      "(2720, 1)\n",
      "(2210, 8)\n",
      "(2210, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.46629056334495544\n",
      "Step: 100  \tTraining accuracy: 0.820486843585968\n",
      "Step: 100  \tValid loss: 0.45416536927223206\n",
      "Step: 200  \tTraining loss: 0.4405730962753296\n",
      "Step: 200  \tTraining accuracy: 0.8235294222831726\n",
      "Step: 200  \tValid loss: 0.427020400762558\n",
      "Step: 300  \tTraining loss: 0.4091007113456726\n",
      "Step: 300  \tTraining accuracy: 0.8261663317680359\n",
      "Step: 300  \tValid loss: 0.39832332730293274\n",
      "Step: 400  \tTraining loss: 0.38678228855133057\n",
      "Step: 400  \tTraining accuracy: 0.8295276761054993\n",
      "Step: 400  \tValid loss: 0.3779033422470093\n",
      "Step: 500  \tTraining loss: 0.3745153844356537\n",
      "Step: 500  \tTraining accuracy: 0.8330178260803223\n",
      "Step: 500  \tValid loss: 0.3666609525680542\n",
      "Step: 600  \tTraining loss: 0.3680438697338104\n",
      "Step: 600  \tTraining accuracy: 0.8360132575035095\n",
      "Step: 600  \tValid loss: 0.36091896891593933\n",
      "Step: 700  \tTraining loss: 0.36420294642448425\n",
      "Step: 700  \tTraining accuracy: 0.8384615182876587\n",
      "Step: 700  \tValid loss: 0.35778704285621643\n",
      "Step: 800  \tTraining loss: 0.36148905754089355\n",
      "Step: 800  \tTraining accuracy: 0.840540885925293\n",
      "Step: 800  \tValid loss: 0.3558412194252014\n",
      "Step: 900  \tTraining loss: 0.35947445034980774\n",
      "Step: 900  \tTraining accuracy: 0.8422026038169861\n",
      "Step: 900  \tValid loss: 0.3544745147228241\n",
      "Step: 1000  \tTraining loss: 0.35788828134536743\n",
      "Step: 1000  \tTraining accuracy: 0.8434824347496033\n",
      "Step: 1000  \tValid loss: 0.35342976450920105\n",
      "Step: 1100  \tTraining loss: 0.35648438334465027\n",
      "Step: 1100  \tTraining accuracy: 0.8445281386375427\n",
      "Step: 1100  \tValid loss: 0.3525269031524658\n",
      "Step: 1200  \tTraining loss: 0.3550254702568054\n",
      "Step: 1200  \tTraining accuracy: 0.845409631729126\n",
      "Step: 1200  \tValid loss: 0.3515203595161438\n",
      "Step: 1300  \tTraining loss: 0.3531859815120697\n",
      "Step: 1300  \tTraining accuracy: 0.846174418926239\n",
      "Step: 1300  \tValid loss: 0.3498348891735077\n",
      "Step: 1400  \tTraining loss: 0.35120242834091187\n",
      "Step: 1400  \tTraining accuracy: 0.846871018409729\n",
      "Step: 1400  \tValid loss: 0.347601979970932\n",
      "Step: 1500  \tTraining loss: 0.3494417071342468\n",
      "Step: 1500  \tTraining accuracy: 0.8474924564361572\n",
      "Step: 1500  \tValid loss: 0.3458646535873413\n",
      "Step: 1600  \tTraining loss: 0.34783393144607544\n",
      "Step: 1600  \tTraining accuracy: 0.8480337858200073\n",
      "Step: 1600  \tValid loss: 0.34443047642707825\n",
      "Step: 1700  \tTraining loss: 0.3463369607925415\n",
      "Step: 1700  \tTraining accuracy: 0.8485524654388428\n",
      "Step: 1700  \tValid loss: 0.34317222237586975\n",
      "Step: 1800  \tTraining loss: 0.34492024779319763\n",
      "Step: 1800  \tTraining accuracy: 0.849098801612854\n",
      "Step: 1800  \tValid loss: 0.34203916788101196\n",
      "Step: 1900  \tTraining loss: 0.3435589373111725\n",
      "Step: 1900  \tTraining accuracy: 0.8495805859565735\n",
      "Step: 1900  \tValid loss: 0.34101590514183044\n",
      "Step: 2000  \tTraining loss: 0.3422435224056244\n",
      "Step: 2000  \tTraining accuracy: 0.8500598073005676\n",
      "Step: 2000  \tValid loss: 0.34010881185531616\n",
      "Step: 2100  \tTraining loss: 0.3409847617149353\n",
      "Step: 2100  \tTraining accuracy: 0.8505070805549622\n",
      "Step: 2100  \tValid loss: 0.33932963013648987\n",
      "Step: 2200  \tTraining loss: 0.33979952335357666\n",
      "Step: 2200  \tTraining accuracy: 0.8509222269058228\n",
      "Step: 2200  \tValid loss: 0.33867737650871277\n",
      "Step: 2300  \tTraining loss: 0.33869481086730957\n",
      "Step: 2300  \tTraining accuracy: 0.8513184785842896\n",
      "Step: 2300  \tValid loss: 0.3381434381008148\n",
      "Step: 2400  \tTraining loss: 0.33766862750053406\n",
      "Step: 2400  \tTraining accuracy: 0.8516507744789124\n",
      "Step: 2400  \tValid loss: 0.3377154767513275\n",
      "Step: 2500  \tTraining loss: 0.33671513199806213\n",
      "Step: 2500  \tTraining accuracy: 0.8519601225852966\n",
      "Step: 2500  \tValid loss: 0.3373834490776062\n",
      "Step: 2600  \tTraining loss: 0.33582866191864014\n",
      "Step: 2600  \tTraining accuracy: 0.8522689938545227\n",
      "Step: 2600  \tValid loss: 0.337141215801239\n",
      "Step: 2700  \tTraining loss: 0.33500364422798157\n",
      "Step: 2700  \tTraining accuracy: 0.852566123008728\n",
      "Step: 2700  \tValid loss: 0.336982399225235\n",
      "Step: 2800  \tTraining loss: 0.33424118161201477\n",
      "Step: 2800  \tTraining accuracy: 0.8528711199760437\n",
      "Step: 2800  \tValid loss: 0.3369095027446747\n",
      "Step: 2900  \tTraining loss: 0.33354607224464417\n",
      "Step: 2900  \tTraining accuracy: 0.8531582355499268\n",
      "Step: 2900  \tValid loss: 0.3369063138961792\n",
      "Step: 3000  \tTraining loss: 0.3329229950904846\n",
      "Step: 3000  \tTraining accuracy: 0.8534671664237976\n",
      "Step: 3000  \tValid loss: 0.3369567394256592\n",
      "Step: 3100  \tTraining loss: 0.3323706090450287\n",
      "Step: 3100  \tTraining accuracy: 0.8537591695785522\n",
      "Step: 3100  \tValid loss: 0.33704081177711487\n",
      "Step: 3200  \tTraining loss: 0.3318852186203003\n",
      "Step: 3200  \tTraining accuracy: 0.8540294170379639\n",
      "Step: 3200  \tValid loss: 0.3371513783931732\n",
      "Step: 3300  \tTraining loss: 0.33145830035209656\n",
      "Step: 3300  \tTraining accuracy: 0.8542487025260925\n",
      "Step: 3300  \tValid loss: 0.33726975321769714\n",
      "Step: 3400  \tTraining loss: 0.3310796618461609\n",
      "Step: 3400  \tTraining accuracy: 0.8544609546661377\n",
      "Step: 3400  \tValid loss: 0.3373865783214569\n",
      "Step: 3500  \tTraining loss: 0.33073973655700684\n",
      "Step: 3500  \tTraining accuracy: 0.8547079563140869\n",
      "Step: 3500  \tValid loss: 0.33749157190322876\n",
      "Step: 3600  \tTraining loss: 0.3304329812526703\n",
      "Step: 3600  \tTraining accuracy: 0.8549724221229553\n",
      "Step: 3600  \tValid loss: 0.33758875727653503\n",
      "Step: 3700  \tTraining loss: 0.3301520049571991\n",
      "Step: 3700  \tTraining accuracy: 0.8552335500717163\n",
      "Step: 3700  \tValid loss: 0.3376712203025818\n",
      "Step: 3800  \tTraining loss: 0.3298908770084381\n",
      "Step: 3800  \tTraining accuracy: 0.8554942607879639\n",
      "Step: 3800  \tValid loss: 0.3377385437488556\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.85573614\n",
      "Precision: 0.8772401\n",
      "Recall: 0.9681088\n",
      "F1 score: 0.89787436\n",
      "AUC: 0.67444986\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0  0.855736    0.87724  0.968109  0.897874  0.67445  0.329745      0.855645   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.336897       0.855592     0.3335      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3858.0  \n",
      "35\n",
      "(20155, 8)\n",
      "(20155, 1)\n",
      "(11120, 8)\n",
      "(11120, 1)\n",
      "(9035, 8)\n",
      "(9035, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4982854425907135\n",
      "Step: 100  \tTraining accuracy: 0.8303646445274353\n",
      "Step: 100  \tValid loss: 0.49746936559677124\n",
      "Step: 200  \tTraining loss: 0.33318042755126953\n",
      "Step: 200  \tTraining accuracy: 0.8458116054534912\n",
      "Step: 200  \tValid loss: 0.33675727248191833\n",
      "Step: 300  \tTraining loss: 0.29691267013549805\n",
      "Step: 300  \tTraining accuracy: 0.856174647808075\n",
      "Step: 300  \tValid loss: 0.3026110827922821\n",
      "Step: 400  \tTraining loss: 0.28177112340927124\n",
      "Step: 400  \tTraining accuracy: 0.861282229423523\n",
      "Step: 400  \tValid loss: 0.28745415806770325\n",
      "Step: 500  \tTraining loss: 0.2689104378223419\n",
      "Step: 500  \tTraining accuracy: 0.8651672005653381\n",
      "Step: 500  \tValid loss: 0.2741907238960266\n",
      "Step: 600  \tTraining loss: 0.25220367312431335\n",
      "Step: 600  \tTraining accuracy: 0.8694210648536682\n",
      "Step: 600  \tValid loss: 0.2571429908275604\n",
      "Step: 700  \tTraining loss: 0.24039146304130554\n",
      "Step: 700  \tTraining accuracy: 0.8741102814674377\n",
      "Step: 700  \tValid loss: 0.2450912594795227\n",
      "Step: 800  \tTraining loss: 0.2350357472896576\n",
      "Step: 800  \tTraining accuracy: 0.8785313963890076\n",
      "Step: 800  \tValid loss: 0.2397792488336563\n",
      "Step: 900  \tTraining loss: 0.2315824031829834\n",
      "Step: 900  \tTraining accuracy: 0.8821632266044617\n",
      "Step: 900  \tValid loss: 0.23480086028575897\n",
      "Step: 1000  \tTraining loss: 0.23010431230068207\n",
      "Step: 1000  \tTraining accuracy: 0.8851271271705627\n",
      "Step: 1000  \tValid loss: 0.23347119987010956\n",
      "Step: 1100  \tTraining loss: 0.2294662892818451\n",
      "Step: 1100  \tTraining accuracy: 0.8875453472137451\n",
      "Step: 1100  \tValid loss: 0.2329106330871582\n",
      "Step: 1200  \tTraining loss: 0.22908659279346466\n",
      "Step: 1200  \tTraining accuracy: 0.8895127773284912\n",
      "Step: 1200  \tValid loss: 0.23257111012935638\n",
      "Step: 1300  \tTraining loss: 0.22876062989234924\n",
      "Step: 1300  \tTraining accuracy: 0.8911515474319458\n",
      "Step: 1300  \tValid loss: 0.23234738409519196\n",
      "Step: 1400  \tTraining loss: 0.2284819483757019\n",
      "Step: 1400  \tTraining accuracy: 0.8925512433052063\n",
      "Step: 1400  \tValid loss: 0.23206652700901031\n",
      "Step: 1500  \tTraining loss: 0.22812439501285553\n",
      "Step: 1500  \tTraining accuracy: 0.8937458992004395\n",
      "Step: 1500  \tValid loss: 0.23172888159751892\n",
      "Step: 1600  \tTraining loss: 0.22720184922218323\n",
      "Step: 1600  \tTraining accuracy: 0.8948631882667542\n",
      "Step: 1600  \tValid loss: 0.23157791793346405\n",
      "Step: 1700  \tTraining loss: 0.22661523520946503\n",
      "Step: 1700  \tTraining accuracy: 0.8958736658096313\n",
      "Step: 1700  \tValid loss: 0.2311381995677948\n",
      "Step: 1800  \tTraining loss: 0.22576245665550232\n",
      "Step: 1800  \tTraining accuracy: 0.8967530131340027\n",
      "Step: 1800  \tValid loss: 0.22959250211715698\n",
      "Step: 1900  \tTraining loss: 0.2250850349664688\n",
      "Step: 1900  \tTraining accuracy: 0.8975158929824829\n",
      "Step: 1900  \tValid loss: 0.2287745177745819\n",
      "Step: 2000  \tTraining loss: 0.2247089147567749\n",
      "Step: 2000  \tTraining accuracy: 0.8982348442077637\n",
      "Step: 2000  \tValid loss: 0.22836071252822876\n",
      "Step: 2100  \tTraining loss: 0.22436603903770447\n",
      "Step: 2100  \tTraining accuracy: 0.8988933563232422\n",
      "Step: 2100  \tValid loss: 0.22788463532924652\n",
      "Step: 2200  \tTraining loss: 0.22401200234889984\n",
      "Step: 2200  \tTraining accuracy: 0.8994824886322021\n",
      "Step: 2200  \tValid loss: 0.2271994650363922\n",
      "Step: 2300  \tTraining loss: 0.2236427515745163\n",
      "Step: 2300  \tTraining accuracy: 0.9000093936920166\n",
      "Step: 2300  \tValid loss: 0.22645846009254456\n",
      "Step: 2400  \tTraining loss: 0.2232816219329834\n",
      "Step: 2400  \tTraining accuracy: 0.9004924893379211\n",
      "Step: 2400  \tValid loss: 0.22580619156360626\n",
      "Step: 2500  \tTraining loss: 0.22293046116828918\n",
      "Step: 2500  \tTraining accuracy: 0.9009300470352173\n",
      "Step: 2500  \tValid loss: 0.22522608935832977\n",
      "Step: 2600  \tTraining loss: 0.2225426286458969\n",
      "Step: 2600  \tTraining accuracy: 0.9013323187828064\n",
      "Step: 2600  \tValid loss: 0.22451242804527283\n",
      "Step: 2700  \tTraining loss: 0.2221640646457672\n",
      "Step: 2700  \tTraining accuracy: 0.901700496673584\n",
      "Step: 2700  \tValid loss: 0.2239328920841217\n",
      "Step: 2800  \tTraining loss: 0.2218122035264969\n",
      "Step: 2800  \tTraining accuracy: 0.902033805847168\n",
      "Step: 2800  \tValid loss: 0.22343304753303528\n",
      "Step: 2900  \tTraining loss: 0.22146646678447723\n",
      "Step: 2900  \tTraining accuracy: 0.9023401737213135\n",
      "Step: 2900  \tValid loss: 0.22294364869594574\n",
      "Step: 3000  \tTraining loss: 0.22107812762260437\n",
      "Step: 3000  \tTraining accuracy: 0.9026241302490234\n",
      "Step: 3000  \tValid loss: 0.22244244813919067\n",
      "Step: 3100  \tTraining loss: 0.22070710361003876\n",
      "Step: 3100  \tTraining accuracy: 0.9028943777084351\n",
      "Step: 3100  \tValid loss: 0.2220148742198944\n",
      "Step: 3200  \tTraining loss: 0.22036691009998322\n",
      "Step: 3200  \tTraining accuracy: 0.9031544923782349\n",
      "Step: 3200  \tValid loss: 0.221597820520401\n",
      "Step: 3300  \tTraining loss: 0.22002069652080536\n",
      "Step: 3300  \tTraining accuracy: 0.9033948183059692\n",
      "Step: 3300  \tValid loss: 0.22122053802013397\n",
      "Step: 3400  \tTraining loss: 0.21960218250751495\n",
      "Step: 3400  \tTraining accuracy: 0.903628945350647\n",
      "Step: 3400  \tValid loss: 0.2209063470363617\n",
      "Step: 3500  \tTraining loss: 0.21918517351150513\n",
      "Step: 3500  \tTraining accuracy: 0.903857409954071\n",
      "Step: 3500  \tValid loss: 0.22053945064544678\n",
      "Step: 3600  \tTraining loss: 0.21879036724567413\n",
      "Step: 3600  \tTraining accuracy: 0.90409255027771\n",
      "Step: 3600  \tValid loss: 0.22024619579315186\n",
      "Step: 3700  \tTraining loss: 0.21837812662124634\n",
      "Step: 3700  \tTraining accuracy: 0.9043121337890625\n",
      "Step: 3700  \tValid loss: 0.22008715569972992\n",
      "Step: 3800  \tTraining loss: 0.21802061796188354\n",
      "Step: 3800  \tTraining accuracy: 0.9045279026031494\n",
      "Step: 3800  \tValid loss: 0.2198476642370224\n",
      "Step: 3900  \tTraining loss: 0.217509925365448\n",
      "Step: 3900  \tTraining accuracy: 0.9047395586967468\n",
      "Step: 3900  \tValid loss: 0.21964828670024872\n",
      "Step: 4000  \tTraining loss: 0.21662646532058716\n",
      "Step: 4000  \tTraining accuracy: 0.9049524664878845\n",
      "Step: 4000  \tValid loss: 0.2194581776857376\n",
      "Step: 4100  \tTraining loss: 0.21624746918678284\n",
      "Step: 4100  \tTraining accuracy: 0.905154824256897\n",
      "Step: 4100  \tValid loss: 0.21901409327983856\n",
      "Step: 4200  \tTraining loss: 0.21600763499736786\n",
      "Step: 4200  \tTraining accuracy: 0.9053438305854797\n",
      "Step: 4200  \tValid loss: 0.21877700090408325\n",
      "Step: 4300  \tTraining loss: 0.21581821143627167\n",
      "Step: 4300  \tTraining accuracy: 0.9055251479148865\n",
      "Step: 4300  \tValid loss: 0.21868373453617096\n",
      "Step: 4400  \tTraining loss: 0.21565113961696625\n",
      "Step: 4400  \tTraining accuracy: 0.9056826829910278\n",
      "Step: 4400  \tValid loss: 0.2187010645866394\n",
      "Step: 4500  \tTraining loss: 0.21550066769123077\n",
      "Step: 4500  \tTraining accuracy: 0.9058231115341187\n",
      "Step: 4500  \tValid loss: 0.21871738135814667\n",
      "Step: 4600  \tTraining loss: 0.21535727381706238\n",
      "Step: 4600  \tTraining accuracy: 0.9059677720069885\n",
      "Step: 4600  \tValid loss: 0.21877476572990417\n",
      "Step: 4700  \tTraining loss: 0.21522928774356842\n",
      "Step: 4700  \tTraining accuracy: 0.906112015247345\n",
      "Step: 4700  \tValid loss: 0.21886108815670013\n",
      "Step: 4800  \tTraining loss: 0.21511009335517883\n",
      "Step: 4800  \tTraining accuracy: 0.9062538743019104\n",
      "Step: 4800  \tValid loss: 0.21891938149929047\n",
      "Step: 4900  \tTraining loss: 0.21498934924602509\n",
      "Step: 4900  \tTraining accuracy: 0.9063868522644043\n",
      "Step: 4900  \tValid loss: 0.2188999056816101\n",
      "Step: 5000  \tTraining loss: 0.21487846970558167\n",
      "Step: 5000  \tTraining accuracy: 0.906520426273346\n",
      "Step: 5000  \tValid loss: 0.21887363493442535\n",
      "Step: 5100  \tTraining loss: 0.21477174758911133\n",
      "Step: 5100  \tTraining accuracy: 0.906650185585022\n",
      "Step: 5100  \tValid loss: 0.21887296438217163\n",
      "Step: 5200  \tTraining loss: 0.21463370323181152\n",
      "Step: 5200  \tTraining accuracy: 0.9067744612693787\n",
      "Step: 5200  \tValid loss: 0.2188761979341507\n",
      "Step: 5300  \tTraining loss: 0.21450094878673553\n",
      "Step: 5300  \tTraining accuracy: 0.9068944454193115\n",
      "Step: 5300  \tValid loss: 0.21884901821613312\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.90700716\n",
      "Precision: 0.8749846\n",
      "Recall: 0.90797544\n",
      "F1 score: 0.92378986\n",
      "AUC: 0.91283125\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.907007   0.874985  0.907975   0.92379  0.912831  0.214501      0.906942   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.218676       0.906896   0.228993      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  5299.0  \n",
      "36\n",
      "(7105, 8)\n",
      "(7105, 1)\n",
      "(3920, 8)\n",
      "(3920, 1)\n",
      "(3185, 8)\n",
      "(3185, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.45310094952583313\n",
      "Step: 100  \tTraining accuracy: 0.838564395904541\n",
      "Step: 100  \tValid loss: 0.46851807832717896\n",
      "Step: 200  \tTraining loss: 0.4233787953853607\n",
      "Step: 200  \tTraining accuracy: 0.8418015241622925\n",
      "Step: 200  \tValid loss: 0.4368431866168976\n",
      "Step: 300  \tTraining loss: 0.3768516778945923\n",
      "Step: 300  \tTraining accuracy: 0.842674195766449\n",
      "Step: 300  \tValid loss: 0.38448503613471985\n",
      "Step: 400  \tTraining loss: 0.34297266602516174\n",
      "Step: 400  \tTraining accuracy: 0.8449180722236633\n",
      "Step: 400  \tValid loss: 0.34679433703422546\n",
      "Step: 500  \tTraining loss: 0.3192957937717438\n",
      "Step: 500  \tTraining accuracy: 0.8491985201835632\n",
      "Step: 500  \tValid loss: 0.32016271352767944\n",
      "Step: 600  \tTraining loss: 0.3033483624458313\n",
      "Step: 600  \tTraining accuracy: 0.853675365447998\n",
      "Step: 600  \tValid loss: 0.3029850125312805\n",
      "Step: 700  \tTraining loss: 0.2940323054790497\n",
      "Step: 700  \tTraining accuracy: 0.8572078347206116\n",
      "Step: 700  \tValid loss: 0.29351845383644104\n",
      "Step: 800  \tTraining loss: 0.28927525877952576\n",
      "Step: 800  \tTraining accuracy: 0.8596293926239014\n",
      "Step: 800  \tValid loss: 0.2889920473098755\n",
      "Step: 900  \tTraining loss: 0.2866647243499756\n",
      "Step: 900  \tTraining accuracy: 0.8613900542259216\n",
      "Step: 900  \tValid loss: 0.2865838408470154\n",
      "Step: 1000  \tTraining loss: 0.2848993241786957\n",
      "Step: 1000  \tTraining accuracy: 0.8627504706382751\n",
      "Step: 1000  \tValid loss: 0.2848440408706665\n",
      "Step: 1100  \tTraining loss: 0.2832307815551758\n",
      "Step: 1100  \tTraining accuracy: 0.863831639289856\n",
      "Step: 1100  \tValid loss: 0.2829437851905823\n",
      "Step: 1200  \tTraining loss: 0.2814551889896393\n",
      "Step: 1200  \tTraining accuracy: 0.8648226857185364\n",
      "Step: 1200  \tValid loss: 0.2808486521244049\n",
      "Step: 1300  \tTraining loss: 0.27994558215141296\n",
      "Step: 1300  \tTraining accuracy: 0.8657283782958984\n",
      "Step: 1300  \tValid loss: 0.2792681157588959\n",
      "Step: 1400  \tTraining loss: 0.27859607338905334\n",
      "Step: 1400  \tTraining accuracy: 0.8665311336517334\n",
      "Step: 1400  \tValid loss: 0.2779005467891693\n",
      "Step: 1500  \tTraining loss: 0.27733680605888367\n",
      "Step: 1500  \tTraining accuracy: 0.867315411567688\n",
      "Step: 1500  \tValid loss: 0.2766485810279846\n",
      "Step: 1600  \tTraining loss: 0.27619272470474243\n",
      "Step: 1600  \tTraining accuracy: 0.8679893612861633\n",
      "Step: 1600  \tValid loss: 0.27550938725471497\n",
      "Step: 1700  \tTraining loss: 0.2751878798007965\n",
      "Step: 1700  \tTraining accuracy: 0.8686328530311584\n",
      "Step: 1700  \tValid loss: 0.27452510595321655\n",
      "Step: 1800  \tTraining loss: 0.2743256390094757\n",
      "Step: 1800  \tTraining accuracy: 0.8692188858985901\n",
      "Step: 1800  \tValid loss: 0.2736913561820984\n",
      "Step: 1900  \tTraining loss: 0.2735849916934967\n",
      "Step: 1900  \tTraining accuracy: 0.8697453141212463\n",
      "Step: 1900  \tValid loss: 0.2729872763156891\n",
      "Step: 2000  \tTraining loss: 0.272932231426239\n",
      "Step: 2000  \tTraining accuracy: 0.8702574968338013\n",
      "Step: 2000  \tValid loss: 0.2723814845085144\n",
      "Step: 2100  \tTraining loss: 0.2723318338394165\n",
      "Step: 2100  \tTraining accuracy: 0.8707471489906311\n",
      "Step: 2100  \tValid loss: 0.2718440592288971\n",
      "Step: 2200  \tTraining loss: 0.2717565596103668\n",
      "Step: 2200  \tTraining accuracy: 0.8711716532707214\n",
      "Step: 2200  \tValid loss: 0.27134910225868225\n",
      "Step: 2300  \tTraining loss: 0.27119892835617065\n",
      "Step: 2300  \tTraining accuracy: 0.8715834021568298\n",
      "Step: 2300  \tValid loss: 0.2708856165409088\n",
      "Step: 2400  \tTraining loss: 0.2706700265407562\n",
      "Step: 2400  \tTraining accuracy: 0.8719720840454102\n",
      "Step: 2400  \tValid loss: 0.27046796679496765\n",
      "Step: 2500  \tTraining loss: 0.2701835334300995\n",
      "Step: 2500  \tTraining accuracy: 0.8723204135894775\n",
      "Step: 2500  \tValid loss: 0.27010777592658997\n",
      "Step: 2600  \tTraining loss: 0.2697409689426422\n",
      "Step: 2600  \tTraining accuracy: 0.8726359605789185\n",
      "Step: 2600  \tValid loss: 0.2698022425174713\n",
      "Step: 2700  \tTraining loss: 0.2693333327770233\n",
      "Step: 2700  \tTraining accuracy: 0.8729408979415894\n",
      "Step: 2700  \tValid loss: 0.2695447504520416\n",
      "Step: 2800  \tTraining loss: 0.26894813776016235\n",
      "Step: 2800  \tTraining accuracy: 0.8732390999794006\n",
      "Step: 2800  \tValid loss: 0.26932278275489807\n",
      "Step: 2900  \tTraining loss: 0.26857325434684753\n",
      "Step: 2900  \tTraining accuracy: 0.8735064268112183\n",
      "Step: 2900  \tValid loss: 0.269122451543808\n",
      "Step: 3000  \tTraining loss: 0.26819705963134766\n",
      "Step: 3000  \tTraining accuracy: 0.8737389445304871\n",
      "Step: 3000  \tValid loss: 0.26892948150634766\n",
      "Step: 3100  \tTraining loss: 0.26781004667282104\n",
      "Step: 3100  \tTraining accuracy: 0.8739492893218994\n",
      "Step: 3100  \tValid loss: 0.2687314748764038\n",
      "Step: 3200  \tTraining loss: 0.2674034833908081\n",
      "Step: 3200  \tTraining accuracy: 0.8741418123245239\n",
      "Step: 3200  \tValid loss: 0.26851850748062134\n",
      "Step: 3300  \tTraining loss: 0.26697245240211487\n",
      "Step: 3300  \tTraining accuracy: 0.874320387840271\n",
      "Step: 3300  \tValid loss: 0.2682850658893585\n",
      "Step: 3400  \tTraining loss: 0.26651647686958313\n",
      "Step: 3400  \tTraining accuracy: 0.8744840025901794\n",
      "Step: 3400  \tValid loss: 0.2680312693119049\n",
      "Step: 3500  \tTraining loss: 0.26603925228118896\n",
      "Step: 3500  \tTraining accuracy: 0.8746443390846252\n",
      "Step: 3500  \tValid loss: 0.26776108145713806\n",
      "Step: 3600  \tTraining loss: 0.2655438780784607\n",
      "Step: 3600  \tTraining accuracy: 0.8747995495796204\n",
      "Step: 3600  \tValid loss: 0.26747891306877136\n",
      "Step: 3700  \tTraining loss: 0.2650338113307953\n",
      "Step: 3700  \tTraining accuracy: 0.8749558925628662\n",
      "Step: 3700  \tValid loss: 0.26718953251838684\n",
      "Step: 3800  \tTraining loss: 0.264512300491333\n",
      "Step: 3800  \tTraining accuracy: 0.8751226663589478\n",
      "Step: 3800  \tValid loss: 0.2668974697589874\n",
      "Step: 3900  \tTraining loss: 0.2639845311641693\n",
      "Step: 3900  \tTraining accuracy: 0.8752789497375488\n",
      "Step: 3900  \tValid loss: 0.26660704612731934\n",
      "Step: 4000  \tTraining loss: 0.2634568512439728\n",
      "Step: 4000  \tTraining accuracy: 0.875423789024353\n",
      "Step: 4000  \tValid loss: 0.2663218677043915\n",
      "Step: 4100  \tTraining loss: 0.26293644309043884\n",
      "Step: 4100  \tTraining accuracy: 0.8755614757537842\n",
      "Step: 4100  \tValid loss: 0.26604264974594116\n",
      "Step: 4200  \tTraining loss: 0.2624298334121704\n",
      "Step: 4200  \tTraining accuracy: 0.8756958842277527\n",
      "Step: 4200  \tValid loss: 0.2657741606235504\n",
      "Step: 4300  \tTraining loss: 0.26194319128990173\n",
      "Step: 4300  \tTraining accuracy: 0.8758273124694824\n",
      "Step: 4300  \tValid loss: 0.2655193507671356\n",
      "Step: 4400  \tTraining loss: 0.2614808678627014\n",
      "Step: 4400  \tTraining accuracy: 0.875954270362854\n",
      "Step: 4400  \tValid loss: 0.26527896523475647\n",
      "Step: 4500  \tTraining loss: 0.2610451579093933\n",
      "Step: 4500  \tTraining accuracy: 0.8760771155357361\n",
      "Step: 4500  \tValid loss: 0.26505613327026367\n",
      "Step: 4600  \tTraining loss: 0.2606368362903595\n",
      "Step: 4600  \tTraining accuracy: 0.876200795173645\n",
      "Step: 4600  \tValid loss: 0.26484987139701843\n",
      "Step: 4700  \tTraining loss: 0.26025548577308655\n",
      "Step: 4700  \tTraining accuracy: 0.87632817029953\n",
      "Step: 4700  \tValid loss: 0.26466038823127747\n",
      "Step: 4800  \tTraining loss: 0.2598990797996521\n",
      "Step: 4800  \tTraining accuracy: 0.8764517307281494\n",
      "Step: 4800  \tValid loss: 0.2644832730293274\n",
      "Step: 4900  \tTraining loss: 0.2595652937889099\n",
      "Step: 4900  \tTraining accuracy: 0.8765672445297241\n",
      "Step: 4900  \tValid loss: 0.26432162523269653\n",
      "Step: 5000  \tTraining loss: 0.2592512369155884\n",
      "Step: 5000  \tTraining accuracy: 0.8766752481460571\n",
      "Step: 5000  \tValid loss: 0.26416948437690735\n",
      "Step: 5100  \tTraining loss: 0.25895383954048157\n",
      "Step: 5100  \tTraining accuracy: 0.8767887353897095\n",
      "Step: 5100  \tValid loss: 0.26402702927589417\n",
      "Step: 5200  \tTraining loss: 0.2586696147918701\n",
      "Step: 5200  \tTraining accuracy: 0.8769032955169678\n",
      "Step: 5200  \tValid loss: 0.26388758420944214\n",
      "Step: 5300  \tTraining loss: 0.2583896517753601\n",
      "Step: 5300  \tTraining accuracy: 0.8770188689231873\n",
      "Step: 5300  \tValid loss: 0.2637470066547394\n",
      "Step: 5400  \tTraining loss: 0.2581067681312561\n",
      "Step: 5400  \tTraining accuracy: 0.8771366477012634\n",
      "Step: 5400  \tValid loss: 0.2636011838912964\n",
      "Step: 5500  \tTraining loss: 0.2578120827674866\n",
      "Step: 5500  \tTraining accuracy: 0.8772475719451904\n",
      "Step: 5500  \tValid loss: 0.2634526491165161\n",
      "Step: 5600  \tTraining loss: 0.25718066096305847\n",
      "Step: 5600  \tTraining accuracy: 0.8773684501647949\n",
      "Step: 5600  \tValid loss: 0.2631688416004181\n",
      "Step: 5700  \tTraining loss: 0.25671321153640747\n",
      "Step: 5700  \tTraining accuracy: 0.87750244140625\n",
      "Step: 5700  \tValid loss: 0.26288196444511414\n",
      "Step: 5800  \tTraining loss: 0.25633350014686584\n",
      "Step: 5800  \tTraining accuracy: 0.87763911485672\n",
      "Step: 5800  \tValid loss: 0.26267269253730774\n",
      "Step: 5900  \tTraining loss: 0.25599005818367004\n",
      "Step: 5900  \tTraining accuracy: 0.8777747750282288\n",
      "Step: 5900  \tValid loss: 0.2625007927417755\n",
      "Step: 6000  \tTraining loss: 0.25566938519477844\n",
      "Step: 6000  \tTraining accuracy: 0.8779058456420898\n",
      "Step: 6000  \tValid loss: 0.26233363151550293\n",
      "Step: 6100  \tTraining loss: 0.2553652226924896\n",
      "Step: 6100  \tTraining accuracy: 0.8780255913734436\n",
      "Step: 6100  \tValid loss: 0.26219427585601807\n",
      "Step: 6200  \tTraining loss: 0.25507843494415283\n",
      "Step: 6200  \tTraining accuracy: 0.8781517744064331\n",
      "Step: 6200  \tValid loss: 0.2620513439178467\n",
      "Step: 6300  \tTraining loss: 0.25479623675346375\n",
      "Step: 6300  \tTraining accuracy: 0.8782885074615479\n",
      "Step: 6300  \tValid loss: 0.2619327902793884\n",
      "Step: 6400  \tTraining loss: 0.25453126430511475\n",
      "Step: 6400  \tTraining accuracy: 0.8784298300743103\n",
      "Step: 6400  \tValid loss: 0.2618121802806854\n",
      "Step: 6500  \tTraining loss: 0.25427234172821045\n",
      "Step: 6500  \tTraining accuracy: 0.8785645961761475\n",
      "Step: 6500  \tValid loss: 0.2617022395133972\n",
      "Step: 6600  \tTraining loss: 0.25402599573135376\n",
      "Step: 6600  \tTraining accuracy: 0.8786952495574951\n",
      "Step: 6600  \tValid loss: 0.26159682869911194\n",
      "Step: 6700  \tTraining loss: 0.25378894805908203\n",
      "Step: 6700  \tTraining accuracy: 0.8788230419158936\n",
      "Step: 6700  \tValid loss: 0.26150718331336975\n",
      "Step: 6800  \tTraining loss: 0.2535575330257416\n",
      "Step: 6800  \tTraining accuracy: 0.8789491057395935\n",
      "Step: 6800  \tValid loss: 0.26141357421875\n",
      "Step: 6900  \tTraining loss: 0.2533353269100189\n",
      "Step: 6900  \tTraining accuracy: 0.879085898399353\n",
      "Step: 6900  \tValid loss: 0.26132887601852417\n",
      "Step: 7000  \tTraining loss: 0.2531203031539917\n",
      "Step: 7000  \tTraining accuracy: 0.8792186975479126\n",
      "Step: 7000  \tValid loss: 0.261252760887146\n",
      "Step: 7100  \tTraining loss: 0.25291284918785095\n",
      "Step: 7100  \tTraining accuracy: 0.8793447613716125\n",
      "Step: 7100  \tValid loss: 0.2611706852912903\n",
      "Step: 7200  \tTraining loss: 0.25270673632621765\n",
      "Step: 7200  \tTraining accuracy: 0.8794702887535095\n",
      "Step: 7200  \tValid loss: 0.2611100673675537\n",
      "Step: 7300  \tTraining loss: 0.25250738859176636\n",
      "Step: 7300  \tTraining accuracy: 0.8795894384384155\n",
      "Step: 7300  \tValid loss: 0.2610405683517456\n",
      "Step: 7400  \tTraining loss: 0.25231730937957764\n",
      "Step: 7400  \tTraining accuracy: 0.87969571352005\n",
      "Step: 7400  \tValid loss: 0.2609773576259613\n",
      "Step: 7500  \tTraining loss: 0.25212687253952026\n",
      "Step: 7500  \tTraining accuracy: 0.8797991871833801\n",
      "Step: 7500  \tValid loss: 0.2609354853630066\n",
      "Step: 7600  \tTraining loss: 0.25194600224494934\n",
      "Step: 7600  \tTraining accuracy: 0.8798971176147461\n",
      "Step: 7600  \tValid loss: 0.26089218258857727\n",
      "Step: 7700  \tTraining loss: 0.2517702579498291\n",
      "Step: 7700  \tTraining accuracy: 0.8799906373023987\n",
      "Step: 7700  \tValid loss: 0.2608654797077179\n",
      "Step: 7800  \tTraining loss: 0.25159913301467896\n",
      "Step: 7800  \tTraining accuracy: 0.8800817131996155\n",
      "Step: 7800  \tValid loss: 0.26082298159599304\n",
      "Step: 7900  \tTraining loss: 0.2514316737651825\n",
      "Step: 7900  \tTraining accuracy: 0.8801678419113159\n",
      "Step: 7900  \tValid loss: 0.26077818870544434\n",
      "Step: 8000  \tTraining loss: 0.2512649893760681\n",
      "Step: 8000  \tTraining accuracy: 0.880245566368103\n",
      "Step: 8000  \tValid loss: 0.2607579827308655\n",
      "Step: 8100  \tTraining loss: 0.25110456347465515\n",
      "Step: 8100  \tTraining accuracy: 0.8803266286849976\n",
      "Step: 8100  \tValid loss: 0.26072973012924194\n",
      "Step: 8200  \tTraining loss: 0.2509450316429138\n",
      "Step: 8200  \tTraining accuracy: 0.8804064989089966\n",
      "Step: 8200  \tValid loss: 0.26069360971450806\n",
      "Step: 8300  \tTraining loss: 0.2507888078689575\n",
      "Step: 8300  \tTraining accuracy: 0.8804827928543091\n",
      "Step: 8300  \tValid loss: 0.26066794991493225\n",
      "Step: 8400  \tTraining loss: 0.2506374716758728\n",
      "Step: 8400  \tTraining accuracy: 0.8805513381958008\n",
      "Step: 8400  \tValid loss: 0.2606472373008728\n",
      "Step: 8500  \tTraining loss: 0.2504885494709015\n",
      "Step: 8500  \tTraining accuracy: 0.8806132674217224\n",
      "Step: 8500  \tValid loss: 0.26061493158340454\n",
      "Step: 8600  \tTraining loss: 0.2503373622894287\n",
      "Step: 8600  \tTraining accuracy: 0.8806679844856262\n",
      "Step: 8600  \tValid loss: 0.26058509945869446\n",
      "Step: 8700  \tTraining loss: 0.25019028782844543\n",
      "Step: 8700  \tTraining accuracy: 0.8807141184806824\n",
      "Step: 8700  \tValid loss: 0.2605486214160919\n",
      "Step: 8800  \tTraining loss: 0.25004228949546814\n",
      "Step: 8800  \tTraining accuracy: 0.8807680606842041\n",
      "Step: 8800  \tValid loss: 0.2605336308479309\n",
      "Step: 8900  \tTraining loss: 0.24989446997642517\n",
      "Step: 8900  \tTraining accuracy: 0.8808255791664124\n",
      "Step: 8900  \tValid loss: 0.2604864537715912\n",
      "Step: 9000  \tTraining loss: 0.24974672496318817\n",
      "Step: 9000  \tTraining accuracy: 0.8808809518814087\n",
      "Step: 9000  \tValid loss: 0.26046547293663025\n",
      "Step: 9100  \tTraining loss: 0.24960052967071533\n",
      "Step: 9100  \tTraining accuracy: 0.8809374570846558\n",
      "Step: 9100  \tValid loss: 0.26040950417518616\n",
      "Step: 9200  \tTraining loss: 0.2494499385356903\n",
      "Step: 9200  \tTraining accuracy: 0.880996584892273\n",
      "Step: 9200  \tValid loss: 0.26037344336509705\n",
      "Step: 9300  \tTraining loss: 0.24930211901664734\n",
      "Step: 9300  \tTraining accuracy: 0.8810605406761169\n",
      "Step: 9300  \tValid loss: 0.2603498697280884\n",
      "Step: 9400  \tTraining loss: 0.24915359914302826\n",
      "Step: 9400  \tTraining accuracy: 0.8811246156692505\n",
      "Step: 9400  \tValid loss: 0.2602894604206085\n",
      "Step: 9500  \tTraining loss: 0.24900278449058533\n",
      "Step: 9500  \tTraining accuracy: 0.8811866044998169\n",
      "Step: 9500  \tValid loss: 0.2602390646934509\n",
      "Step: 9600  \tTraining loss: 0.24885722994804382\n",
      "Step: 9600  \tTraining accuracy: 0.8812472820281982\n",
      "Step: 9600  \tValid loss: 0.26019832491874695\n",
      "Step: 9700  \tTraining loss: 0.24867984652519226\n",
      "Step: 9700  \tTraining accuracy: 0.8813110589981079\n",
      "Step: 9700  \tValid loss: 0.2601756453514099\n",
      "Step: 9800  \tTraining loss: 0.24852058291435242\n",
      "Step: 9800  \tTraining accuracy: 0.8813764452934265\n",
      "Step: 9800  \tValid loss: 0.26010680198669434\n",
      "Step: 9900  \tTraining loss: 0.24836482107639313\n",
      "Step: 9900  \tTraining accuracy: 0.8814404606819153\n",
      "Step: 9900  \tValid loss: 0.26002752780914307\n",
      "Step: 10000  \tTraining loss: 0.24821068346500397\n",
      "Step: 10000  \tTraining accuracy: 0.8815039396286011\n",
      "Step: 10000  \tValid loss: 0.2599603235721588\n",
      "Step: 10100  \tTraining loss: 0.24805504083633423\n",
      "Step: 10100  \tTraining accuracy: 0.8815675377845764\n",
      "Step: 10100  \tValid loss: 0.2598840296268463\n",
      "Step: 10200  \tTraining loss: 0.24790114164352417\n",
      "Step: 10200  \tTraining accuracy: 0.8816319704055786\n",
      "Step: 10200  \tValid loss: 0.25979867577552795\n",
      "Step: 10300  \tTraining loss: 0.24774929881095886\n",
      "Step: 10300  \tTraining accuracy: 0.8816971778869629\n",
      "Step: 10300  \tValid loss: 0.25971874594688416\n",
      "Step: 10400  \tTraining loss: 0.24760058522224426\n",
      "Step: 10400  \tTraining accuracy: 0.8817625045776367\n",
      "Step: 10400  \tValid loss: 0.25963273644447327\n",
      "Step: 10500  \tTraining loss: 0.24745118618011475\n",
      "Step: 10500  \tTraining accuracy: 0.8818265795707703\n",
      "Step: 10500  \tValid loss: 0.25954651832580566\n",
      "Step: 10600  \tTraining loss: 0.24730771780014038\n",
      "Step: 10600  \tTraining accuracy: 0.8818907737731934\n",
      "Step: 10600  \tValid loss: 0.25946348905563354\n",
      "Step: 10700  \tTraining loss: 0.24715982377529144\n",
      "Step: 10700  \tTraining accuracy: 0.8819518089294434\n",
      "Step: 10700  \tValid loss: 0.2593771815299988\n",
      "Step: 10800  \tTraining loss: 0.24701999127864838\n",
      "Step: 10800  \tTraining accuracy: 0.8820058107376099\n",
      "Step: 10800  \tValid loss: 0.2593000531196594\n",
      "Step: 10900  \tTraining loss: 0.2468799501657486\n",
      "Step: 10900  \tTraining accuracy: 0.8820568323135376\n",
      "Step: 10900  \tValid loss: 0.25921347737312317\n",
      "Step: 11000  \tTraining loss: 0.24674297869205475\n",
      "Step: 11000  \tTraining accuracy: 0.882108211517334\n",
      "Step: 11000  \tValid loss: 0.2591346502304077\n",
      "Step: 11100  \tTraining loss: 0.24660898745059967\n",
      "Step: 11100  \tTraining accuracy: 0.8821593523025513\n",
      "Step: 11100  \tValid loss: 0.25905072689056396\n",
      "Step: 11200  \tTraining loss: 0.24647505581378937\n",
      "Step: 11200  \tTraining accuracy: 0.8822095394134521\n",
      "Step: 11200  \tValid loss: 0.2589777410030365\n",
      "Step: 11300  \tTraining loss: 0.24634802341461182\n",
      "Step: 11300  \tTraining accuracy: 0.8822594285011292\n",
      "Step: 11300  \tValid loss: 0.25890299677848816\n",
      "Step: 11400  \tTraining loss: 0.2462208867073059\n",
      "Step: 11400  \tTraining accuracy: 0.882307231426239\n",
      "Step: 11400  \tValid loss: 0.2588273286819458\n",
      "Step: 11500  \tTraining loss: 0.2460983246564865\n",
      "Step: 11500  \tTraining accuracy: 0.882352352142334\n",
      "Step: 11500  \tValid loss: 0.2587509751319885\n",
      "Step: 11600  \tTraining loss: 0.24597953259944916\n",
      "Step: 11600  \tTraining accuracy: 0.8823961019515991\n",
      "Step: 11600  \tValid loss: 0.2586847245693207\n",
      "Step: 11700  \tTraining loss: 0.24586254358291626\n",
      "Step: 11700  \tTraining accuracy: 0.8824360370635986\n",
      "Step: 11700  \tValid loss: 0.25862374901771545\n",
      "Step: 11800  \tTraining loss: 0.24574890732765198\n",
      "Step: 11800  \tTraining accuracy: 0.8824771046638489\n",
      "Step: 11800  \tValid loss: 0.2585541009902954\n",
      "Step: 11900  \tTraining loss: 0.24563787877559662\n",
      "Step: 11900  \tTraining accuracy: 0.8825210928916931\n",
      "Step: 11900  \tValid loss: 0.25848886370658875\n",
      "Step: 12000  \tTraining loss: 0.24553079903125763\n",
      "Step: 12000  \tTraining accuracy: 0.8825637102127075\n",
      "Step: 12000  \tValid loss: 0.2584390640258789\n",
      "Step: 12100  \tTraining loss: 0.24542145431041718\n",
      "Step: 12100  \tTraining accuracy: 0.8826050162315369\n",
      "Step: 12100  \tValid loss: 0.25837263464927673\n",
      "Step: 12200  \tTraining loss: 0.2453163117170334\n",
      "Step: 12200  \tTraining accuracy: 0.8826462626457214\n",
      "Step: 12200  \tValid loss: 0.2583072781562805\n",
      "Step: 12300  \tTraining loss: 0.24521435797214508\n",
      "Step: 12300  \tTraining accuracy: 0.8826828002929688\n",
      "Step: 12300  \tValid loss: 0.2582564949989319\n",
      "Step: 12400  \tTraining loss: 0.2451169490814209\n",
      "Step: 12400  \tTraining accuracy: 0.8827181458473206\n",
      "Step: 12400  \tValid loss: 0.258211612701416\n",
      "Step: 12500  \tTraining loss: 0.24501816928386688\n",
      "Step: 12500  \tTraining accuracy: 0.8827529549598694\n",
      "Step: 12500  \tValid loss: 0.2581472098827362\n",
      "Step: 12600  \tTraining loss: 0.2449249029159546\n",
      "Step: 12600  \tTraining accuracy: 0.8827872276306152\n",
      "Step: 12600  \tValid loss: 0.25809866189956665\n",
      "Step: 12700  \tTraining loss: 0.24482978880405426\n",
      "Step: 12700  \tTraining accuracy: 0.8828209042549133\n",
      "Step: 12700  \tValid loss: 0.2580466866493225\n",
      "Step: 12800  \tTraining loss: 0.24473783373832703\n",
      "Step: 12800  \tTraining accuracy: 0.8828541040420532\n",
      "Step: 12800  \tValid loss: 0.257997065782547\n",
      "Step: 12900  \tTraining loss: 0.24464687705039978\n",
      "Step: 12900  \tTraining accuracy: 0.8828867673873901\n",
      "Step: 12900  \tValid loss: 0.2579433023929596\n",
      "Step: 13000  \tTraining loss: 0.24455846846103668\n",
      "Step: 13000  \tTraining accuracy: 0.8829227089881897\n",
      "Step: 13000  \tValid loss: 0.25789904594421387\n",
      "Step: 13100  \tTraining loss: 0.24447108805179596\n",
      "Step: 13100  \tTraining accuracy: 0.88295978307724\n",
      "Step: 13100  \tValid loss: 0.2578471004962921\n",
      "Step: 13200  \tTraining loss: 0.24438516795635223\n",
      "Step: 13200  \tTraining accuracy: 0.8829962015151978\n",
      "Step: 13200  \tValid loss: 0.25780028104782104\n",
      "Step: 13300  \tTraining loss: 0.2443043291568756\n",
      "Step: 13300  \tTraining accuracy: 0.8830305337905884\n",
      "Step: 13300  \tValid loss: 0.25775426626205444\n",
      "Step: 13400  \tTraining loss: 0.24422034621238708\n",
      "Step: 13400  \tTraining accuracy: 0.8830633163452148\n",
      "Step: 13400  \tValid loss: 0.25770553946495056\n",
      "Step: 13500  \tTraining loss: 0.24413807690143585\n",
      "Step: 13500  \tTraining accuracy: 0.8830945491790771\n",
      "Step: 13500  \tValid loss: 0.2576572299003601\n",
      "Step: 13600  \tTraining loss: 0.2440650910139084\n",
      "Step: 13600  \tTraining accuracy: 0.8831226825714111\n",
      "Step: 13600  \tValid loss: 0.25761088728904724\n",
      "Step: 13700  \tTraining loss: 0.24398182332515717\n",
      "Step: 13700  \tTraining accuracy: 0.8831489086151123\n",
      "Step: 13700  \tValid loss: 0.25756993889808655\n",
      "Step: 13800  \tTraining loss: 0.243903249502182\n",
      "Step: 13800  \tTraining accuracy: 0.8831721544265747\n",
      "Step: 13800  \tValid loss: 0.25752201676368713\n",
      "Step: 13900  \tTraining loss: 0.24382761120796204\n",
      "Step: 13900  \tTraining accuracy: 0.8831925392150879\n",
      "Step: 13900  \tValid loss: 0.25748181343078613\n",
      "Step: 14000  \tTraining loss: 0.2437543123960495\n",
      "Step: 14000  \tTraining accuracy: 0.8832126259803772\n",
      "Step: 14000  \tValid loss: 0.2574371099472046\n",
      "Step: 14100  \tTraining loss: 0.24367955327033997\n",
      "Step: 14100  \tTraining accuracy: 0.8832324743270874\n",
      "Step: 14100  \tValid loss: 0.25739702582359314\n",
      "Step: 14200  \tTraining loss: 0.243608295917511\n",
      "Step: 14200  \tTraining accuracy: 0.8832529783248901\n",
      "Step: 14200  \tValid loss: 0.2573498487472534\n",
      "Step: 14300  \tTraining loss: 0.24354341626167297\n",
      "Step: 14300  \tTraining accuracy: 0.8832746744155884\n",
      "Step: 14300  \tValid loss: 0.25730812549591064\n",
      "Step: 14400  \tTraining loss: 0.24346524477005005\n",
      "Step: 14400  \tTraining accuracy: 0.8832961320877075\n",
      "Step: 14400  \tValid loss: 0.257272332906723\n",
      "Step: 14500  \tTraining loss: 0.24339650571346283\n",
      "Step: 14500  \tTraining accuracy: 0.883320152759552\n",
      "Step: 14500  \tValid loss: 0.2572351098060608\n",
      "Step: 14600  \tTraining loss: 0.2433284968137741\n",
      "Step: 14600  \tTraining accuracy: 0.8833457827568054\n",
      "Step: 14600  \tValid loss: 0.25718945264816284\n",
      "Step: 14700  \tTraining loss: 0.24326488375663757\n",
      "Step: 14700  \tTraining accuracy: 0.8833710551261902\n",
      "Step: 14700  \tValid loss: 0.25715890526771545\n",
      "Step: 14800  \tTraining loss: 0.24319563806056976\n",
      "Step: 14800  \tTraining accuracy: 0.8833950757980347\n",
      "Step: 14800  \tValid loss: 0.2571098804473877\n",
      "Step: 14900  \tTraining loss: 0.24312996864318848\n",
      "Step: 14900  \tTraining accuracy: 0.8834173083305359\n",
      "Step: 14900  \tValid loss: 0.2570752501487732\n",
      "Step: 15000  \tTraining loss: 0.24306757748126984\n",
      "Step: 15000  \tTraining accuracy: 0.883439302444458\n",
      "Step: 15000  \tValid loss: 0.25703784823417664\n",
      "Step: 15100  \tTraining loss: 0.24299775063991547\n",
      "Step: 15100  \tTraining accuracy: 0.8834609389305115\n",
      "Step: 15100  \tValid loss: 0.2569977641105652\n",
      "Step: 15200  \tTraining loss: 0.24293644726276398\n",
      "Step: 15200  \tTraining accuracy: 0.8834841847419739\n",
      "Step: 15200  \tValid loss: 0.2569517493247986\n",
      "Step: 15300  \tTraining loss: 0.24287524819374084\n",
      "Step: 15300  \tTraining accuracy: 0.883508026599884\n",
      "Step: 15300  \tValid loss: 0.25693297386169434\n",
      "Step: 15400  \tTraining loss: 0.24281126260757446\n",
      "Step: 15400  \tTraining accuracy: 0.8835329413414001\n",
      "Step: 15400  \tValid loss: 0.2568855285644531\n",
      "Step: 15500  \tTraining loss: 0.24274784326553345\n",
      "Step: 15500  \tTraining accuracy: 0.883560299873352\n",
      "Step: 15500  \tValid loss: 0.25684472918510437\n",
      "Step: 15600  \tTraining loss: 0.2426878660917282\n",
      "Step: 15600  \tTraining accuracy: 0.8835872411727905\n",
      "Step: 15600  \tValid loss: 0.25681209564208984\n",
      "Step: 15700  \tTraining loss: 0.2426285296678543\n",
      "Step: 15700  \tTraining accuracy: 0.8836138844490051\n",
      "Step: 15700  \tValid loss: 0.2567734718322754\n",
      "Step: 15800  \tTraining loss: 0.24256788194179535\n",
      "Step: 15800  \tTraining accuracy: 0.8836401700973511\n",
      "Step: 15800  \tValid loss: 0.2567465603351593\n",
      "Step: 15900  \tTraining loss: 0.24251559376716614\n",
      "Step: 15900  \tTraining accuracy: 0.8836661577224731\n",
      "Step: 15900  \tValid loss: 0.2567136585712433\n",
      "Step: 16000  \tTraining loss: 0.24245136976242065\n",
      "Step: 16000  \tTraining accuracy: 0.8836917877197266\n",
      "Step: 16000  \tValid loss: 0.256673127412796\n",
      "Step: 16100  \tTraining loss: 0.24239258468151093\n",
      "Step: 16100  \tTraining accuracy: 0.8837171196937561\n",
      "Step: 16100  \tValid loss: 0.25664153695106506\n",
      "Step: 16200  \tTraining loss: 0.24233587086200714\n",
      "Step: 16200  \tTraining accuracy: 0.883743405342102\n",
      "Step: 16200  \tValid loss: 0.256602942943573\n",
      "Step: 16300  \tTraining loss: 0.2422781139612198\n",
      "Step: 16300  \tTraining accuracy: 0.8837702870368958\n",
      "Step: 16300  \tValid loss: 0.25656986236572266\n",
      "Step: 16400  \tTraining loss: 0.2422199696302414\n",
      "Step: 16400  \tTraining accuracy: 0.8837950825691223\n",
      "Step: 16400  \tValid loss: 0.25653234124183655\n",
      "Step: 16500  \tTraining loss: 0.24216395616531372\n",
      "Step: 16500  \tTraining accuracy: 0.8838170170783997\n",
      "Step: 16500  \tValid loss: 0.2564992308616638\n",
      "Step: 16600  \tTraining loss: 0.24210785329341888\n",
      "Step: 16600  \tTraining accuracy: 0.8838386535644531\n",
      "Step: 16600  \tValid loss: 0.25646907091140747\n",
      "Step: 16700  \tTraining loss: 0.24205073714256287\n",
      "Step: 16700  \tTraining accuracy: 0.8838583827018738\n",
      "Step: 16700  \tValid loss: 0.2564427852630615\n",
      "Step: 16800  \tTraining loss: 0.24199487268924713\n",
      "Step: 16800  \tTraining accuracy: 0.8838770389556885\n",
      "Step: 16800  \tValid loss: 0.2563968598842621\n",
      "Step: 16900  \tTraining loss: 0.24194075167179108\n",
      "Step: 16900  \tTraining accuracy: 0.8838937878608704\n",
      "Step: 16900  \tValid loss: 0.25636976957321167\n",
      "Step: 17000  \tTraining loss: 0.24188679456710815\n",
      "Step: 17000  \tTraining accuracy: 0.8839103579521179\n",
      "Step: 17000  \tValid loss: 0.25633883476257324\n",
      "Step: 17100  \tTraining loss: 0.2418300211429596\n",
      "Step: 17100  \tTraining accuracy: 0.8839266896247864\n",
      "Step: 17100  \tValid loss: 0.2563078701496124\n",
      "Step: 17200  \tTraining loss: 0.24177680909633636\n",
      "Step: 17200  \tTraining accuracy: 0.8839428424835205\n",
      "Step: 17200  \tValid loss: 0.2562774419784546\n",
      "Step: 17300  \tTraining loss: 0.24171848595142365\n",
      "Step: 17300  \tTraining accuracy: 0.8839580416679382\n",
      "Step: 17300  \tValid loss: 0.25623658299446106\n",
      "Step: 17400  \tTraining loss: 0.24166564643383026\n",
      "Step: 17400  \tTraining accuracy: 0.8839718103408813\n",
      "Step: 17400  \tValid loss: 0.2562078535556793\n",
      "Step: 17500  \tTraining loss: 0.24161207675933838\n",
      "Step: 17500  \tTraining accuracy: 0.8839854001998901\n",
      "Step: 17500  \tValid loss: 0.25616785883903503\n",
      "Step: 17600  \tTraining loss: 0.24155929684638977\n",
      "Step: 17600  \tTraining accuracy: 0.8839988708496094\n",
      "Step: 17600  \tValid loss: 0.25613126158714294\n",
      "Step: 17700  \tTraining loss: 0.24150191247463226\n",
      "Step: 17700  \tTraining accuracy: 0.8840121626853943\n",
      "Step: 17700  \tValid loss: 0.2561030685901642\n",
      "Step: 17800  \tTraining loss: 0.24144938588142395\n",
      "Step: 17800  \tTraining accuracy: 0.8840253353118896\n",
      "Step: 17800  \tValid loss: 0.2560584843158722\n",
      "Step: 17900  \tTraining loss: 0.2413991093635559\n",
      "Step: 17900  \tTraining accuracy: 0.8840391039848328\n",
      "Step: 17900  \tValid loss: 0.25602471828460693\n",
      "Step: 18000  \tTraining loss: 0.24134141206741333\n",
      "Step: 18000  \tTraining accuracy: 0.8840519785881042\n",
      "Step: 18000  \tValid loss: 0.2560032606124878\n",
      "Step: 18100  \tTraining loss: 0.24128755927085876\n",
      "Step: 18100  \tTraining accuracy: 0.8840647339820862\n",
      "Step: 18100  \tValid loss: 0.25596797466278076\n",
      "Step: 18200  \tTraining loss: 0.24123485386371613\n",
      "Step: 18200  \tTraining accuracy: 0.8840773105621338\n",
      "Step: 18200  \tValid loss: 0.2559257745742798\n",
      "Step: 18300  \tTraining loss: 0.2411869466304779\n",
      "Step: 18300  \tTraining accuracy: 0.8840909004211426\n",
      "Step: 18300  \tValid loss: 0.2558899521827698\n",
      "Step: 18400  \tTraining loss: 0.24112772941589355\n",
      "Step: 18400  \tTraining accuracy: 0.8841055035591125\n",
      "Step: 18400  \tValid loss: 0.25585711002349854\n",
      "Step: 18500  \tTraining loss: 0.24107615649700165\n",
      "Step: 18500  \tTraining accuracy: 0.8841195702552795\n",
      "Step: 18500  \tValid loss: 0.2558259665966034\n",
      "Step: 18600  \tTraining loss: 0.2410295009613037\n",
      "Step: 18600  \tTraining accuracy: 0.8841334581375122\n",
      "Step: 18600  \tValid loss: 0.25579169392585754\n",
      "Step: 18700  \tTraining loss: 0.2409711331129074\n",
      "Step: 18700  \tTraining accuracy: 0.8841472268104553\n",
      "Step: 18700  \tValid loss: 0.25575679540634155\n",
      "Step: 18800  \tTraining loss: 0.24091769754886627\n",
      "Step: 18800  \tTraining accuracy: 0.8841608166694641\n",
      "Step: 18800  \tValid loss: 0.25572535395622253\n",
      "Step: 18900  \tTraining loss: 0.24086351692676544\n",
      "Step: 18900  \tTraining accuracy: 0.8841742873191833\n",
      "Step: 18900  \tValid loss: 0.2556878328323364\n",
      "Step: 19000  \tTraining loss: 0.2408141791820526\n",
      "Step: 19000  \tTraining accuracy: 0.884187638759613\n",
      "Step: 19000  \tValid loss: 0.25564834475517273\n",
      "Step: 19100  \tTraining loss: 0.2407625913619995\n",
      "Step: 19100  \tTraining accuracy: 0.8842004537582397\n",
      "Step: 19100  \tValid loss: 0.2556246817111969\n",
      "Step: 19200  \tTraining loss: 0.24071134626865387\n",
      "Step: 19200  \tTraining accuracy: 0.8842127323150635\n",
      "Step: 19200  \tValid loss: 0.2555948495864868\n",
      "Step: 19300  \tTraining loss: 0.24066121876239777\n",
      "Step: 19300  \tTraining accuracy: 0.8842245936393738\n",
      "Step: 19300  \tValid loss: 0.255553275346756\n",
      "Step: 19400  \tTraining loss: 0.24060314893722534\n",
      "Step: 19400  \tTraining accuracy: 0.8842369914054871\n",
      "Step: 19400  \tValid loss: 0.25552183389663696\n",
      "Step: 19500  \tTraining loss: 0.24055297672748566\n",
      "Step: 19500  \tTraining accuracy: 0.8842496871948242\n",
      "Step: 19500  \tValid loss: 0.25548768043518066\n",
      "Step: 19600  \tTraining loss: 0.2405003309249878\n",
      "Step: 19600  \tTraining accuracy: 0.8842636346817017\n",
      "Step: 19600  \tValid loss: 0.25545358657836914\n",
      "Step: 19700  \tTraining loss: 0.24044997990131378\n",
      "Step: 19700  \tTraining accuracy: 0.8842778205871582\n",
      "Step: 19700  \tValid loss: 0.25542372465133667\n",
      "Step: 19800  \tTraining loss: 0.24039700627326965\n",
      "Step: 19800  \tTraining accuracy: 0.8842918872833252\n",
      "Step: 19800  \tValid loss: 0.2553889751434326\n",
      "Step: 19900  \tTraining loss: 0.24034632742404938\n",
      "Step: 19900  \tTraining accuracy: 0.8843072056770325\n",
      "Step: 19900  \tValid loss: 0.2553565800189972\n",
      "Step: 20000  \tTraining loss: 0.24029649794101715\n",
      "Step: 20000  \tTraining accuracy: 0.8843227028846741\n",
      "Step: 20000  \tValid loss: 0.255323588848114\n",
      "Step: 20100  \tTraining loss: 0.24024444818496704\n",
      "Step: 20100  \tTraining accuracy: 0.8843373656272888\n",
      "Step: 20100  \tValid loss: 0.2552964687347412\n",
      "Step: 20200  \tTraining loss: 0.24019230902194977\n",
      "Step: 20200  \tTraining accuracy: 0.8843525648117065\n",
      "Step: 20200  \tValid loss: 0.2552589774131775\n",
      "Step: 20300  \tTraining loss: 0.24014264345169067\n",
      "Step: 20300  \tTraining accuracy: 0.8843676447868347\n",
      "Step: 20300  \tValid loss: 0.2552192509174347\n",
      "Step: 20400  \tTraining loss: 0.2400917112827301\n",
      "Step: 20400  \tTraining accuracy: 0.8843825459480286\n",
      "Step: 20400  \tValid loss: 0.255191832780838\n",
      "Step: 20500  \tTraining loss: 0.2400413304567337\n",
      "Step: 20500  \tTraining accuracy: 0.8843973278999329\n",
      "Step: 20500  \tValid loss: 0.25515997409820557\n",
      "Step: 20600  \tTraining loss: 0.23999391496181488\n",
      "Step: 20600  \tTraining accuracy: 0.8844109177589417\n",
      "Step: 20600  \tValid loss: 0.25512632727622986\n",
      "Step: 20700  \tTraining loss: 0.2399407923221588\n",
      "Step: 20700  \tTraining accuracy: 0.8844247460365295\n",
      "Step: 20700  \tValid loss: 0.2550883889198303\n",
      "Step: 20800  \tTraining loss: 0.2398904263973236\n",
      "Step: 20800  \tTraining accuracy: 0.8844391107559204\n",
      "Step: 20800  \tValid loss: 0.25505760312080383\n",
      "Step: 20900  \tTraining loss: 0.23984436690807343\n",
      "Step: 20900  \tTraining accuracy: 0.8844519853591919\n",
      "Step: 20900  \tValid loss: 0.2550223767757416\n",
      "Step: 21000  \tTraining loss: 0.2397913783788681\n",
      "Step: 21000  \tTraining accuracy: 0.8844653964042664\n",
      "Step: 21000  \tValid loss: 0.2549948990345001\n",
      "Step: 21100  \tTraining loss: 0.23974472284317017\n",
      "Step: 21100  \tTraining accuracy: 0.8844783306121826\n",
      "Step: 21100  \tValid loss: 0.25496166944503784\n",
      "Step: 21200  \tTraining loss: 0.2396938055753708\n",
      "Step: 21200  \tTraining accuracy: 0.8844908475875854\n",
      "Step: 21200  \tValid loss: 0.25493165850639343\n",
      "Step: 21300  \tTraining loss: 0.239644393324852\n",
      "Step: 21300  \tTraining accuracy: 0.8845028877258301\n",
      "Step: 21300  \tValid loss: 0.2548949420452118\n",
      "Step: 21400  \tTraining loss: 0.23959502577781677\n",
      "Step: 21400  \tTraining accuracy: 0.8845148086547852\n",
      "Step: 21400  \tValid loss: 0.2548592984676361\n",
      "Step: 21500  \tTraining loss: 0.23955178260803223\n",
      "Step: 21500  \tTraining accuracy: 0.8845266103744507\n",
      "Step: 21500  \tValid loss: 0.2548395097255707\n",
      "Step: 21600  \tTraining loss: 0.239499032497406\n",
      "Step: 21600  \tTraining accuracy: 0.8845376968383789\n",
      "Step: 21600  \tValid loss: 0.2547931671142578\n",
      "Step: 21700  \tTraining loss: 0.2394525408744812\n",
      "Step: 21700  \tTraining accuracy: 0.8845493197441101\n",
      "Step: 21700  \tValid loss: 0.25476163625717163\n",
      "Step: 21800  \tTraining loss: 0.23940305411815643\n",
      "Step: 21800  \tTraining accuracy: 0.8845617771148682\n",
      "Step: 21800  \tValid loss: 0.25472867488861084\n",
      "Step: 21900  \tTraining loss: 0.23935283720493317\n",
      "Step: 21900  \tTraining accuracy: 0.8845747709274292\n",
      "Step: 21900  \tValid loss: 0.254699170589447\n",
      "Step: 22000  \tTraining loss: 0.23931092023849487\n",
      "Step: 22000  \tTraining accuracy: 0.8845877051353455\n",
      "Step: 22000  \tValid loss: 0.2546673119068146\n",
      "Step: 22100  \tTraining loss: 0.2392563670873642\n",
      "Step: 22100  \tTraining accuracy: 0.8846004605293274\n",
      "Step: 22100  \tValid loss: 0.2546256482601166\n",
      "Step: 22200  \tTraining loss: 0.2392103374004364\n",
      "Step: 22200  \tTraining accuracy: 0.8846130967140198\n",
      "Step: 22200  \tValid loss: 0.2545921802520752\n",
      "Step: 22300  \tTraining loss: 0.2391660511493683\n",
      "Step: 22300  \tTraining accuracy: 0.884624719619751\n",
      "Step: 22300  \tValid loss: 0.2545471489429474\n",
      "Step: 22400  \tTraining loss: 0.2391158938407898\n",
      "Step: 22400  \tTraining accuracy: 0.8846365213394165\n",
      "Step: 22400  \tValid loss: 0.25452595949172974\n",
      "Step: 22500  \tTraining loss: 0.23906826972961426\n",
      "Step: 22500  \tTraining accuracy: 0.8846487998962402\n",
      "Step: 22500  \tValid loss: 0.2544909715652466\n",
      "Step: 22600  \tTraining loss: 0.23902477324008942\n",
      "Step: 22600  \tTraining accuracy: 0.8846625685691833\n",
      "Step: 22600  \tValid loss: 0.254454642534256\n",
      "Step: 22700  \tTraining loss: 0.23898112773895264\n",
      "Step: 22700  \tTraining accuracy: 0.8846753239631653\n",
      "Step: 22700  \tValid loss: 0.2544042766094208\n",
      "Step: 22800  \tTraining loss: 0.23893503844738007\n",
      "Step: 22800  \tTraining accuracy: 0.8846873044967651\n",
      "Step: 22800  \tValid loss: 0.25438225269317627\n",
      "Step: 22900  \tTraining loss: 0.23888333141803741\n",
      "Step: 22900  \tTraining accuracy: 0.8847001194953918\n",
      "Step: 22900  \tValid loss: 0.25434619188308716\n",
      "Step: 23000  \tTraining loss: 0.23883800208568573\n",
      "Step: 23000  \tTraining accuracy: 0.8847134113311768\n",
      "Step: 23000  \tValid loss: 0.2543107867240906\n",
      "Step: 23100  \tTraining loss: 0.23879368603229523\n",
      "Step: 23100  \tTraining accuracy: 0.8847266435623169\n",
      "Step: 23100  \tValid loss: 0.2542753517627716\n",
      "Step: 23200  \tTraining loss: 0.23874832689762115\n",
      "Step: 23200  \tTraining accuracy: 0.8847405910491943\n",
      "Step: 23200  \tValid loss: 0.2542352080345154\n",
      "Step: 23300  \tTraining loss: 0.23870331048965454\n",
      "Step: 23300  \tTraining accuracy: 0.8847550749778748\n",
      "Step: 23300  \tValid loss: 0.2542036175727844\n",
      "Step: 23400  \tTraining loss: 0.23865780234336853\n",
      "Step: 23400  \tTraining accuracy: 0.8847700357437134\n",
      "Step: 23400  \tValid loss: 0.2541638910770416\n",
      "Step: 23500  \tTraining loss: 0.23861826956272125\n",
      "Step: 23500  \tTraining accuracy: 0.8847857713699341\n",
      "Step: 23500  \tValid loss: 0.2541241943836212\n",
      "Step: 23600  \tTraining loss: 0.23857106268405914\n",
      "Step: 23600  \tTraining accuracy: 0.8848016858100891\n",
      "Step: 23600  \tValid loss: 0.254099577665329\n",
      "Step: 23700  \tTraining loss: 0.23852968215942383\n",
      "Step: 23700  \tTraining accuracy: 0.8848183155059814\n",
      "Step: 23700  \tValid loss: 0.25405484437942505\n",
      "Step: 23800  \tTraining loss: 0.23848150670528412\n",
      "Step: 23800  \tTraining accuracy: 0.8848336338996887\n",
      "Step: 23800  \tValid loss: 0.2540200352668762\n",
      "Step: 23900  \tTraining loss: 0.238438680768013\n",
      "Step: 23900  \tTraining accuracy: 0.8848482966423035\n",
      "Step: 23900  \tValid loss: 0.2539868652820587\n",
      "Step: 24000  \tTraining loss: 0.23839494585990906\n",
      "Step: 24000  \tTraining accuracy: 0.8848627805709839\n",
      "Step: 24000  \tValid loss: 0.25394901633262634\n",
      "Step: 24100  \tTraining loss: 0.23835662007331848\n",
      "Step: 24100  \tTraining accuracy: 0.8848776817321777\n",
      "Step: 24100  \tValid loss: 0.2539154887199402\n",
      "Step: 24200  \tTraining loss: 0.23830828070640564\n",
      "Step: 24200  \tTraining accuracy: 0.8848925232887268\n",
      "Step: 24200  \tValid loss: 0.25387898087501526\n",
      "Step: 24300  \tTraining loss: 0.23826485872268677\n",
      "Step: 24300  \tTraining accuracy: 0.8849072456359863\n",
      "Step: 24300  \tValid loss: 0.25383976101875305\n",
      "Step: 24400  \tTraining loss: 0.2382219135761261\n",
      "Step: 24400  \tTraining accuracy: 0.8849218487739563\n",
      "Step: 24400  \tValid loss: 0.25380781292915344\n",
      "Step: 24500  \tTraining loss: 0.23818039894104004\n",
      "Step: 24500  \tTraining accuracy: 0.8849362730979919\n",
      "Step: 24500  \tValid loss: 0.25376832485198975\n",
      "Step: 24600  \tTraining loss: 0.2381378710269928\n",
      "Step: 24600  \tTraining accuracy: 0.8849506378173828\n",
      "Step: 24600  \tValid loss: 0.2537381052970886\n",
      "Step: 24700  \tTraining loss: 0.2380925714969635\n",
      "Step: 24700  \tTraining accuracy: 0.8849648833274841\n",
      "Step: 24700  \tValid loss: 0.25370195508003235\n",
      "Step: 24800  \tTraining loss: 0.23805050551891327\n",
      "Step: 24800  \tTraining accuracy: 0.8849790096282959\n",
      "Step: 24800  \tValid loss: 0.25366249680519104\n",
      "Step: 24900  \tTraining loss: 0.2380097508430481\n",
      "Step: 24900  \tTraining accuracy: 0.8849930167198181\n",
      "Step: 24900  \tValid loss: 0.2536330223083496\n",
      "Step: 25000  \tTraining loss: 0.23796460032463074\n",
      "Step: 25000  \tTraining accuracy: 0.885006308555603\n",
      "Step: 25000  \tValid loss: 0.2535947859287262\n",
      "Step: 25100  \tTraining loss: 0.23792050778865814\n",
      "Step: 25100  \tTraining accuracy: 0.8850187063217163\n",
      "Step: 25100  \tValid loss: 0.2535631060600281\n",
      "Step: 25200  \tTraining loss: 0.23787753283977509\n",
      "Step: 25200  \tTraining accuracy: 0.88503098487854\n",
      "Step: 25200  \tValid loss: 0.25352638959884644\n",
      "Step: 25300  \tTraining loss: 0.23783420026302338\n",
      "Step: 25300  \tTraining accuracy: 0.8850431442260742\n",
      "Step: 25300  \tValid loss: 0.2534927725791931\n",
      "Step: 25400  \tTraining loss: 0.23779094219207764\n",
      "Step: 25400  \tTraining accuracy: 0.8850560784339905\n",
      "Step: 25400  \tValid loss: 0.2534593939781189\n",
      "Step: 25500  \tTraining loss: 0.23774729669094086\n",
      "Step: 25500  \tTraining accuracy: 0.8850705623626709\n",
      "Step: 25500  \tValid loss: 0.25342467427253723\n",
      "Step: 25600  \tTraining loss: 0.2377028912305832\n",
      "Step: 25600  \tTraining accuracy: 0.8850859999656677\n",
      "Step: 25600  \tValid loss: 0.2533915042877197\n",
      "Step: 25700  \tTraining loss: 0.23765836656093597\n",
      "Step: 25700  \tTraining accuracy: 0.8851019144058228\n",
      "Step: 25700  \tValid loss: 0.2533513903617859\n",
      "Step: 25800  \tTraining loss: 0.2376003861427307\n",
      "Step: 25800  \tTraining accuracy: 0.8851177096366882\n",
      "Step: 25800  \tValid loss: 0.25328654050827026\n",
      "Step: 25900  \tTraining loss: 0.23752576112747192\n",
      "Step: 25900  \tTraining accuracy: 0.8851327896118164\n",
      "Step: 25900  \tValid loss: 0.25318998098373413\n",
      "Step: 26000  \tTraining loss: 0.23712053894996643\n",
      "Step: 26000  \tTraining accuracy: 0.8851488828659058\n",
      "Step: 26000  \tValid loss: 0.25343993306159973\n",
      "Step: 26100  \tTraining loss: 0.2370249480009079\n",
      "Step: 26100  \tTraining accuracy: 0.8851672410964966\n",
      "Step: 26100  \tValid loss: 0.2534157335758209\n",
      "Step: 26200  \tTraining loss: 0.2369573712348938\n",
      "Step: 26200  \tTraining accuracy: 0.885185182094574\n",
      "Step: 26200  \tValid loss: 0.2533600330352783\n",
      "Step: 26300  \tTraining loss: 0.2368950992822647\n",
      "Step: 26300  \tTraining accuracy: 0.8852030634880066\n",
      "Step: 26300  \tValid loss: 0.2533057630062103\n",
      "Step: 26400  \tTraining loss: 0.2368341088294983\n",
      "Step: 26400  \tTraining accuracy: 0.8852207660675049\n",
      "Step: 26400  \tValid loss: 0.2532651722431183\n",
      "Step: 26500  \tTraining loss: 0.23677785694599152\n",
      "Step: 26500  \tTraining accuracy: 0.8852385878562927\n",
      "Step: 26500  \tValid loss: 0.2532123029232025\n",
      "Step: 26600  \tTraining loss: 0.23672015964984894\n",
      "Step: 26600  \tTraining accuracy: 0.8852573037147522\n",
      "Step: 26600  \tValid loss: 0.25318121910095215\n",
      "Step: 26700  \tTraining loss: 0.23666468262672424\n",
      "Step: 26700  \tTraining accuracy: 0.8852759599685669\n",
      "Step: 26700  \tValid loss: 0.2531460225582123\n",
      "Step: 26800  \tTraining loss: 0.2366122305393219\n",
      "Step: 26800  \tTraining accuracy: 0.8852944374084473\n",
      "Step: 26800  \tValid loss: 0.253106027841568\n",
      "Step: 26900  \tTraining loss: 0.23655493557453156\n",
      "Step: 26900  \tTraining accuracy: 0.8853127360343933\n",
      "Step: 26900  \tValid loss: 0.2530732750892639\n",
      "Step: 27000  \tTraining loss: 0.2364981472492218\n",
      "Step: 27000  \tTraining accuracy: 0.8853309750556946\n",
      "Step: 27000  \tValid loss: 0.25304079055786133\n",
      "Step: 27100  \tTraining loss: 0.23644670844078064\n",
      "Step: 27100  \tTraining accuracy: 0.8853490352630615\n",
      "Step: 27100  \tValid loss: 0.2530044615268707\n",
      "Step: 27200  \tTraining loss: 0.23638705909252167\n",
      "Step: 27200  \tTraining accuracy: 0.885367214679718\n",
      "Step: 27200  \tValid loss: 0.25297674536705017\n",
      "Step: 27300  \tTraining loss: 0.23633216321468353\n",
      "Step: 27300  \tTraining accuracy: 0.8853862881660461\n",
      "Step: 27300  \tValid loss: 0.2529403865337372\n",
      "Step: 27400  \tTraining loss: 0.23627519607543945\n",
      "Step: 27400  \tTraining accuracy: 0.8854052424430847\n",
      "Step: 27400  \tValid loss: 0.25290828943252563\n",
      "Step: 27500  \tTraining loss: 0.23622190952301025\n",
      "Step: 27500  \tTraining accuracy: 0.8854240775108337\n",
      "Step: 27500  \tValid loss: 0.25287064909935\n",
      "Step: 27600  \tTraining loss: 0.23616603016853333\n",
      "Step: 27600  \tTraining accuracy: 0.8854427337646484\n",
      "Step: 27600  \tValid loss: 0.25283244252204895\n",
      "Step: 27700  \tTraining loss: 0.23610815405845642\n",
      "Step: 27700  \tTraining accuracy: 0.8854612708091736\n",
      "Step: 27700  \tValid loss: 0.2528057098388672\n",
      "Step: 27800  \tTraining loss: 0.2360520213842392\n",
      "Step: 27800  \tTraining accuracy: 0.8854796886444092\n",
      "Step: 27800  \tValid loss: 0.25276777148246765\n",
      "Step: 27900  \tTraining loss: 0.23599711060523987\n",
      "Step: 27900  \tTraining accuracy: 0.8854979872703552\n",
      "Step: 27900  \tValid loss: 0.2527356743812561\n",
      "Step: 28000  \tTraining loss: 0.2359393835067749\n",
      "Step: 28000  \tTraining accuracy: 0.8855158686637878\n",
      "Step: 28000  \tValid loss: 0.2526998519897461\n",
      "Step: 28100  \tTraining loss: 0.23589172959327698\n",
      "Step: 28100  \tTraining accuracy: 0.8855319023132324\n",
      "Step: 28100  \tValid loss: 0.2526572644710541\n",
      "Step: 28200  \tTraining loss: 0.23582744598388672\n",
      "Step: 28200  \tTraining accuracy: 0.8855478167533875\n",
      "Step: 28200  \tValid loss: 0.25262847542762756\n",
      "Step: 28300  \tTraining loss: 0.23577149212360382\n",
      "Step: 28300  \tTraining accuracy: 0.885563850402832\n",
      "Step: 28300  \tValid loss: 0.2525978684425354\n",
      "Step: 28400  \tTraining loss: 0.23571568727493286\n",
      "Step: 28400  \tTraining accuracy: 0.8855802416801453\n",
      "Step: 28400  \tValid loss: 0.2525651156902313\n",
      "Step: 28500  \tTraining loss: 0.235662043094635\n",
      "Step: 28500  \tTraining accuracy: 0.8855957984924316\n",
      "Step: 28500  \tValid loss: 0.2525264024734497\n",
      "Step: 28600  \tTraining loss: 0.23561206459999084\n",
      "Step: 28600  \tTraining accuracy: 0.88560950756073\n",
      "Step: 28600  \tValid loss: 0.2525002360343933\n",
      "Step: 28700  \tTraining loss: 0.23555561900138855\n",
      "Step: 28700  \tTraining accuracy: 0.8856224417686462\n",
      "Step: 28700  \tValid loss: 0.2524578869342804\n",
      "Step: 28800  \tTraining loss: 0.2355058193206787\n",
      "Step: 28800  \tTraining accuracy: 0.88563472032547\n",
      "Step: 28800  \tValid loss: 0.25243619084358215\n",
      "Step: 28900  \tTraining loss: 0.23545338213443756\n",
      "Step: 28900  \tTraining accuracy: 0.885647714138031\n",
      "Step: 28900  \tValid loss: 0.25240376591682434\n",
      "Step: 29000  \tTraining loss: 0.235401451587677\n",
      "Step: 29000  \tTraining accuracy: 0.8856603503227234\n",
      "Step: 29000  \tValid loss: 0.2523646652698517\n",
      "Step: 29100  \tTraining loss: 0.23534990847110748\n",
      "Step: 29100  \tTraining accuracy: 0.885672390460968\n",
      "Step: 29100  \tValid loss: 0.2523358166217804\n",
      "Step: 29200  \tTraining loss: 0.23529981076717377\n",
      "Step: 29200  \tTraining accuracy: 0.8856850862503052\n",
      "Step: 29200  \tValid loss: 0.2523152828216553\n",
      "Step: 29300  \tTraining loss: 0.23525196313858032\n",
      "Step: 29300  \tTraining accuracy: 0.885698139667511\n",
      "Step: 29300  \tValid loss: 0.25228583812713623\n",
      "Step: 29400  \tTraining loss: 0.23520532250404358\n",
      "Step: 29400  \tTraining accuracy: 0.8857097029685974\n",
      "Step: 29400  \tValid loss: 0.25224751234054565\n",
      "Step: 29500  \tTraining loss: 0.23515398800373077\n",
      "Step: 29500  \tTraining accuracy: 0.8857202529907227\n",
      "Step: 29500  \tValid loss: 0.25222477316856384\n",
      "Step: 29600  \tTraining loss: 0.23510749638080597\n",
      "Step: 29600  \tTraining accuracy: 0.8857307434082031\n",
      "Step: 29600  \tValid loss: 0.252203106880188\n",
      "Step: 29700  \tTraining loss: 0.23506274819374084\n",
      "Step: 29700  \tTraining accuracy: 0.885741114616394\n",
      "Step: 29700  \tValid loss: 0.25216054916381836\n",
      "Step: 29800  \tTraining loss: 0.23501698672771454\n",
      "Step: 29800  \tTraining accuracy: 0.8857514262199402\n",
      "Step: 29800  \tValid loss: 0.25214099884033203\n",
      "Step: 29900  \tTraining loss: 0.2349756807088852\n",
      "Step: 29900  \tTraining accuracy: 0.8857621550559998\n",
      "Step: 29900  \tValid loss: 0.25209906697273254\n",
      "Step: 30000  \tTraining loss: 0.23492729663848877\n",
      "Step: 30000  \tTraining accuracy: 0.8857734799385071\n",
      "Step: 30000  \tValid loss: 0.2520982325077057\n",
      "Step: 30100  \tTraining loss: 0.23488390445709229\n",
      "Step: 30100  \tTraining accuracy: 0.8857840895652771\n",
      "Step: 30100  \tValid loss: 0.2520751953125\n",
      "Step: 30200  \tTraining loss: 0.23483972251415253\n",
      "Step: 30200  \tTraining accuracy: 0.8857945799827576\n",
      "Step: 30200  \tValid loss: 0.2520497143268585\n",
      "Step: 30300  \tTraining loss: 0.23479868471622467\n",
      "Step: 30300  \tTraining accuracy: 0.8858068585395813\n",
      "Step: 30300  \tValid loss: 0.25203001499176025\n",
      "Step: 30400  \tTraining loss: 0.2347540408372879\n",
      "Step: 30400  \tTraining accuracy: 0.8858190774917603\n",
      "Step: 30400  \tValid loss: 0.2520046532154083\n",
      "Step: 30500  \tTraining loss: 0.23471182584762573\n",
      "Step: 30500  \tTraining accuracy: 0.885831892490387\n",
      "Step: 30500  \tValid loss: 0.25198498368263245\n",
      "Step: 30600  \tTraining loss: 0.23467206954956055\n",
      "Step: 30600  \tTraining accuracy: 0.8858451247215271\n",
      "Step: 30600  \tValid loss: 0.25196951627731323\n",
      "Step: 30700  \tTraining loss: 0.23463213443756104\n",
      "Step: 30700  \tTraining accuracy: 0.8858582377433777\n",
      "Step: 30700  \tValid loss: 0.25192996859550476\n",
      "Step: 30800  \tTraining loss: 0.23459003865718842\n",
      "Step: 30800  \tTraining accuracy: 0.8858729004859924\n",
      "Step: 30800  \tValid loss: 0.25192534923553467\n",
      "Step: 30900  \tTraining loss: 0.23455025255680084\n",
      "Step: 30900  \tTraining accuracy: 0.8858880996704102\n",
      "Step: 30900  \tValid loss: 0.25190818309783936\n",
      "Step: 31000  \tTraining loss: 0.23451226949691772\n",
      "Step: 31000  \tTraining accuracy: 0.8859032392501831\n",
      "Step: 31000  \tValid loss: 0.251883864402771\n",
      "Step: 31100  \tTraining loss: 0.23447291553020477\n",
      "Step: 31100  \tTraining accuracy: 0.8859182596206665\n",
      "Step: 31100  \tValid loss: 0.25186190009117126\n",
      "Step: 31200  \tTraining loss: 0.23443593084812164\n",
      "Step: 31200  \tTraining accuracy: 0.8859332203865051\n",
      "Step: 31200  \tValid loss: 0.2518387734889984\n",
      "Step: 31300  \tTraining loss: 0.2343963086605072\n",
      "Step: 31300  \tTraining accuracy: 0.8859484791755676\n",
      "Step: 31300  \tValid loss: 0.25182458758354187\n",
      "Step: 31400  \tTraining loss: 0.23435871303081512\n",
      "Step: 31400  \tTraining accuracy: 0.8859643340110779\n",
      "Step: 31400  \tValid loss: 0.251797080039978\n",
      "Step: 31500  \tTraining loss: 0.2343215048313141\n",
      "Step: 31500  \tTraining accuracy: 0.8859801292419434\n",
      "Step: 31500  \tValid loss: 0.25177696347236633\n",
      "Step: 31600  \tTraining loss: 0.23428359627723694\n",
      "Step: 31600  \tTraining accuracy: 0.8859958052635193\n",
      "Step: 31600  \tValid loss: 0.25175124406814575\n",
      "Step: 31700  \tTraining loss: 0.23424828052520752\n",
      "Step: 31700  \tTraining accuracy: 0.8860113620758057\n",
      "Step: 31700  \tValid loss: 0.2517297565937042\n",
      "Step: 31800  \tTraining loss: 0.23421013355255127\n",
      "Step: 31800  \tTraining accuracy: 0.8860267996788025\n",
      "Step: 31800  \tValid loss: 0.2517034709453583\n",
      "Step: 31900  \tTraining loss: 0.23417611420154572\n",
      "Step: 31900  \tTraining accuracy: 0.8860421776771545\n",
      "Step: 31900  \tValid loss: 0.2516842484474182\n",
      "Step: 32000  \tTraining loss: 0.2341383546590805\n",
      "Step: 32000  \tTraining accuracy: 0.886057436466217\n",
      "Step: 32000  \tValid loss: 0.25166037678718567\n",
      "Step: 32100  \tTraining loss: 0.23410265147686005\n",
      "Step: 32100  \tTraining accuracy: 0.8860726356506348\n",
      "Step: 32100  \tValid loss: 0.25163692235946655\n",
      "Step: 32200  \tTraining loss: 0.2340676635503769\n",
      "Step: 32200  \tTraining accuracy: 0.8860877156257629\n",
      "Step: 32200  \tValid loss: 0.2516131103038788\n",
      "Step: 32300  \tTraining loss: 0.23403207957744598\n",
      "Step: 32300  \tTraining accuracy: 0.8861024975776672\n",
      "Step: 32300  \tValid loss: 0.2515929341316223\n",
      "Step: 32400  \tTraining loss: 0.2339937537908554\n",
      "Step: 32400  \tTraining accuracy: 0.8861165046691895\n",
      "Step: 32400  \tValid loss: 0.2515218257904053\n",
      "Step: 32500  \tTraining loss: 0.23395682871341705\n",
      "Step: 32500  \tTraining accuracy: 0.8861311078071594\n",
      "Step: 32500  \tValid loss: 0.25139501690864563\n",
      "Step: 32600  \tTraining loss: 0.23391999304294586\n",
      "Step: 32600  \tTraining accuracy: 0.886145830154419\n",
      "Step: 32600  \tValid loss: 0.251343309879303\n",
      "Step: 32700  \tTraining loss: 0.23388060927391052\n",
      "Step: 32700  \tTraining accuracy: 0.8861604332923889\n",
      "Step: 32700  \tValid loss: 0.25129181146621704\n",
      "Step: 32800  \tTraining loss: 0.23384341597557068\n",
      "Step: 32800  \tTraining accuracy: 0.8861749768257141\n",
      "Step: 32800  \tValid loss: 0.25125619769096375\n",
      "Step: 32900  \tTraining loss: 0.23380808532238007\n",
      "Step: 32900  \tTraining accuracy: 0.8861894607543945\n",
      "Step: 32900  \tValid loss: 0.2512282431125641\n",
      "Step: 33000  \tTraining loss: 0.23377591371536255\n",
      "Step: 33000  \tTraining accuracy: 0.8862038254737854\n",
      "Step: 33000  \tValid loss: 0.25120893120765686\n",
      "Step: 33100  \tTraining loss: 0.23373883962631226\n",
      "Step: 33100  \tTraining accuracy: 0.8862180709838867\n",
      "Step: 33100  \tValid loss: 0.2511789798736572\n",
      "Step: 33200  \tTraining loss: 0.2337050884962082\n",
      "Step: 33200  \tTraining accuracy: 0.8862316012382507\n",
      "Step: 33200  \tValid loss: 0.25116589665412903\n",
      "Step: 33300  \tTraining loss: 0.23366814851760864\n",
      "Step: 33300  \tTraining accuracy: 0.8862446546554565\n",
      "Step: 33300  \tValid loss: 0.25113943219184875\n",
      "Step: 33400  \tTraining loss: 0.23363681137561798\n",
      "Step: 33400  \tTraining accuracy: 0.8862576484680176\n",
      "Step: 33400  \tValid loss: 0.25113290548324585\n",
      "Step: 33500  \tTraining loss: 0.23360057175159454\n",
      "Step: 33500  \tTraining accuracy: 0.8862713575363159\n",
      "Step: 33500  \tValid loss: 0.25110679864883423\n",
      "Step: 33600  \tTraining loss: 0.23356619477272034\n",
      "Step: 33600  \tTraining accuracy: 0.8862860798835754\n",
      "Step: 33600  \tValid loss: 0.25108808279037476\n",
      "Step: 33700  \tTraining loss: 0.23353591561317444\n",
      "Step: 33700  \tTraining accuracy: 0.8863009214401245\n",
      "Step: 33700  \tValid loss: 0.25108498334884644\n",
      "Step: 33800  \tTraining loss: 0.23350152373313904\n",
      "Step: 33800  \tTraining accuracy: 0.886315643787384\n",
      "Step: 33800  \tValid loss: 0.25106891989707947\n",
      "Step: 33900  \tTraining loss: 0.23346757888793945\n",
      "Step: 33900  \tTraining accuracy: 0.8863303065299988\n",
      "Step: 33900  \tValid loss: 0.25104621052742004\n",
      "Step: 34000  \tTraining loss: 0.2334325611591339\n",
      "Step: 34000  \tTraining accuracy: 0.8863444328308105\n",
      "Step: 34000  \tValid loss: 0.2510294020175934\n",
      "Step: 34100  \tTraining loss: 0.23340021073818207\n",
      "Step: 34100  \tTraining accuracy: 0.886357843875885\n",
      "Step: 34100  \tValid loss: 0.2510170638561249\n",
      "Step: 34200  \tTraining loss: 0.23336711525917053\n",
      "Step: 34200  \tTraining accuracy: 0.8863712549209595\n",
      "Step: 34200  \tValid loss: 0.2509923577308655\n",
      "Step: 34300  \tTraining loss: 0.2333361953496933\n",
      "Step: 34300  \tTraining accuracy: 0.8863845467567444\n",
      "Step: 34300  \tValid loss: 0.2509738504886627\n",
      "Step: 34400  \tTraining loss: 0.23330706357955933\n",
      "Step: 34400  \tTraining accuracy: 0.8863979578018188\n",
      "Step: 34400  \tValid loss: 0.25096964836120605\n",
      "Step: 34500  \tTraining loss: 0.23327133059501648\n",
      "Step: 34500  \tTraining accuracy: 0.8864120841026306\n",
      "Step: 34500  \tValid loss: 0.25094544887542725\n",
      "Step: 34600  \tTraining loss: 0.2332390546798706\n",
      "Step: 34600  \tTraining accuracy: 0.8864263892173767\n",
      "Step: 34600  \tValid loss: 0.25093117356300354\n",
      "Step: 34700  \tTraining loss: 0.23320676386356354\n",
      "Step: 34700  \tTraining accuracy: 0.8864403367042542\n",
      "Step: 34700  \tValid loss: 0.2509171962738037\n",
      "Step: 34800  \tTraining loss: 0.2331758737564087\n",
      "Step: 34800  \tTraining accuracy: 0.8864542841911316\n",
      "Step: 34800  \tValid loss: 0.2508985996246338\n",
      "Step: 34900  \tTraining loss: 0.23314198851585388\n",
      "Step: 34900  \tTraining accuracy: 0.8864681124687195\n",
      "Step: 34900  \tValid loss: 0.2508707642555237\n",
      "Step: 35000  \tTraining loss: 0.2331124246120453\n",
      "Step: 35000  \tTraining accuracy: 0.8864818215370178\n",
      "Step: 35000  \tValid loss: 0.25085851550102234\n",
      "Step: 35100  \tTraining loss: 0.23308564722537994\n",
      "Step: 35100  \tTraining accuracy: 0.8864955306053162\n",
      "Step: 35100  \tValid loss: 0.25083303451538086\n",
      "Step: 35200  \tTraining loss: 0.23305247724056244\n",
      "Step: 35200  \tTraining accuracy: 0.886509120464325\n",
      "Step: 35200  \tValid loss: 0.25081032514572144\n",
      "Step: 35300  \tTraining loss: 0.23301692306995392\n",
      "Step: 35300  \tTraining accuracy: 0.886522650718689\n",
      "Step: 35300  \tValid loss: 0.2507942318916321\n",
      "Step: 35400  \tTraining loss: 0.23298685252666473\n",
      "Step: 35400  \tTraining accuracy: 0.8865350484848022\n",
      "Step: 35400  \tValid loss: 0.25076520442962646\n",
      "Step: 35500  \tTraining loss: 0.23295655846595764\n",
      "Step: 35500  \tTraining accuracy: 0.8865474462509155\n",
      "Step: 35500  \tValid loss: 0.2507488429546356\n",
      "Step: 35600  \tTraining loss: 0.23292502760887146\n",
      "Step: 35600  \tTraining accuracy: 0.8865597248077393\n",
      "Step: 35600  \tValid loss: 0.25072333216667175\n",
      "Step: 35700  \tTraining loss: 0.23289602994918823\n",
      "Step: 35700  \tTraining accuracy: 0.886572003364563\n",
      "Step: 35700  \tValid loss: 0.25071075558662415\n",
      "Step: 35800  \tTraining loss: 0.23286448419094086\n",
      "Step: 35800  \tTraining accuracy: 0.8865841627120972\n",
      "Step: 35800  \tValid loss: 0.2506934404373169\n",
      "Step: 35900  \tTraining loss: 0.23283520340919495\n",
      "Step: 35900  \tTraining accuracy: 0.8865966200828552\n",
      "Step: 35900  \tValid loss: 0.25068312883377075\n",
      "Step: 36000  \tTraining loss: 0.23280248045921326\n",
      "Step: 36000  \tTraining accuracy: 0.8866092562675476\n",
      "Step: 36000  \tValid loss: 0.2506504952907562\n",
      "Step: 36100  \tTraining loss: 0.23277205228805542\n",
      "Step: 36100  \tTraining accuracy: 0.8866215944290161\n",
      "Step: 36100  \tValid loss: 0.2506368160247803\n",
      "Step: 36200  \tTraining loss: 0.23274411261081696\n",
      "Step: 36200  \tTraining accuracy: 0.8866344690322876\n",
      "Step: 36200  \tValid loss: 0.250618577003479\n",
      "Step: 36300  \tTraining loss: 0.23271284997463226\n",
      "Step: 36300  \tTraining accuracy: 0.8866472840309143\n",
      "Step: 36300  \tValid loss: 0.2505888044834137\n",
      "Step: 36400  \tTraining loss: 0.2326861321926117\n",
      "Step: 36400  \tTraining accuracy: 0.8866600394248962\n",
      "Step: 36400  \tValid loss: 0.2505720853805542\n",
      "Step: 36500  \tTraining loss: 0.23265795409679413\n",
      "Step: 36500  \tTraining accuracy: 0.8866726756095886\n",
      "Step: 36500  \tValid loss: 0.2505512535572052\n",
      "Step: 36600  \tTraining loss: 0.23262546956539154\n",
      "Step: 36600  \tTraining accuracy: 0.8866852521896362\n",
      "Step: 36600  \tValid loss: 0.2505287230014801\n",
      "Step: 36700  \tTraining loss: 0.2325974702835083\n",
      "Step: 36700  \tTraining accuracy: 0.8866977691650391\n",
      "Step: 36700  \tValid loss: 0.2505069971084595\n",
      "Step: 36800  \tTraining loss: 0.23257409036159515\n",
      "Step: 36800  \tTraining accuracy: 0.8867102265357971\n",
      "Step: 36800  \tValid loss: 0.2504887282848358\n",
      "Step: 36900  \tTraining loss: 0.23253998160362244\n",
      "Step: 36900  \tTraining accuracy: 0.8867218494415283\n",
      "Step: 36900  \tValid loss: 0.25046607851982117\n",
      "Step: 37000  \tTraining loss: 0.2325088381767273\n",
      "Step: 37000  \tTraining accuracy: 0.8867330551147461\n",
      "Step: 37000  \tValid loss: 0.25044357776641846\n",
      "Step: 37100  \tTraining loss: 0.23248599469661713\n",
      "Step: 37100  \tTraining accuracy: 0.886744499206543\n",
      "Step: 37100  \tValid loss: 0.25042739510536194\n",
      "Step: 37200  \tTraining loss: 0.23245231807231903\n",
      "Step: 37200  \tTraining accuracy: 0.8867552280426025\n",
      "Step: 37200  \tValid loss: 0.2503981292247772\n",
      "Step: 37300  \tTraining loss: 0.23242388665676117\n",
      "Step: 37300  \tTraining accuracy: 0.8867663741111755\n",
      "Step: 37300  \tValid loss: 0.2503870725631714\n",
      "Step: 37400  \tTraining loss: 0.23239611089229584\n",
      "Step: 37400  \tTraining accuracy: 0.8867775201797485\n",
      "Step: 37400  \tValid loss: 0.25036540627479553\n",
      "Step: 37500  \tTraining loss: 0.23237082362174988\n",
      "Step: 37500  \tTraining accuracy: 0.8867889642715454\n",
      "Step: 37500  \tValid loss: 0.2503534257411957\n",
      "Step: 37600  \tTraining loss: 0.23233914375305176\n",
      "Step: 37600  \tTraining accuracy: 0.8868008852005005\n",
      "Step: 37600  \tValid loss: 0.25032633543014526\n",
      "Step: 37700  \tTraining loss: 0.23231147229671478\n",
      "Step: 37700  \tTraining accuracy: 0.8868128061294556\n",
      "Step: 37700  \tValid loss: 0.25030699372291565\n",
      "Step: 37800  \tTraining loss: 0.23228789865970612\n",
      "Step: 37800  \tTraining accuracy: 0.8868246078491211\n",
      "Step: 37800  \tValid loss: 0.2502923905849457\n",
      "Step: 37900  \tTraining loss: 0.23225587606430054\n",
      "Step: 37900  \tTraining accuracy: 0.8868363499641418\n",
      "Step: 37900  \tValid loss: 0.2502734065055847\n",
      "Step: 38000  \tTraining loss: 0.23222880065441132\n",
      "Step: 38000  \tTraining accuracy: 0.8868474960327148\n",
      "Step: 38000  \tValid loss: 0.2502463459968567\n",
      "Step: 38100  \tTraining loss: 0.23220078647136688\n",
      "Step: 38100  \tTraining accuracy: 0.886857807636261\n",
      "Step: 38100  \tValid loss: 0.25023216009140015\n",
      "Step: 38200  \tTraining loss: 0.23217423260211945\n",
      "Step: 38200  \tTraining accuracy: 0.8868686556816101\n",
      "Step: 38200  \tValid loss: 0.2502140402793884\n",
      "Step: 38300  \tTraining loss: 0.23214852809906006\n",
      "Step: 38300  \tTraining accuracy: 0.8868796229362488\n",
      "Step: 38300  \tValid loss: 0.2501975893974304\n",
      "Step: 38400  \tTraining loss: 0.23211921751499176\n",
      "Step: 38400  \tTraining accuracy: 0.886890172958374\n",
      "Step: 38400  \tValid loss: 0.2501794397830963\n",
      "Step: 38500  \tTraining loss: 0.23209351301193237\n",
      "Step: 38500  \tTraining accuracy: 0.8869006633758545\n",
      "Step: 38500  \tValid loss: 0.25016042590141296\n",
      "Step: 38600  \tTraining loss: 0.23206917941570282\n",
      "Step: 38600  \tTraining accuracy: 0.8869114518165588\n",
      "Step: 38600  \tValid loss: 0.2501455247402191\n",
      "Step: 38700  \tTraining loss: 0.23203694820404053\n",
      "Step: 38700  \tTraining accuracy: 0.8869227170944214\n",
      "Step: 38700  \tValid loss: 0.25010740756988525\n",
      "Step: 38800  \tTraining loss: 0.23201227188110352\n",
      "Step: 38800  \tTraining accuracy: 0.8869336247444153\n",
      "Step: 38800  \tValid loss: 0.25010842084884644\n",
      "Step: 38900  \tTraining loss: 0.2319875806570053\n",
      "Step: 38900  \tTraining accuracy: 0.886944591999054\n",
      "Step: 38900  \tValid loss: 0.2500801086425781\n",
      "Step: 39000  \tTraining loss: 0.23195746541023254\n",
      "Step: 39000  \tTraining accuracy: 0.8869566321372986\n",
      "Step: 39000  \tValid loss: 0.25006482005119324\n",
      "Step: 39100  \tTraining loss: 0.23193210363388062\n",
      "Step: 39100  \tTraining accuracy: 0.8869685530662537\n",
      "Step: 39100  \tValid loss: 0.250059574842453\n",
      "Step: 39200  \tTraining loss: 0.23190662264823914\n",
      "Step: 39200  \tTraining accuracy: 0.8869804739952087\n",
      "Step: 39200  \tValid loss: 0.2500344514846802\n",
      "Step: 39300  \tTraining loss: 0.2318795621395111\n",
      "Step: 39300  \tTraining accuracy: 0.8869922757148743\n",
      "Step: 39300  \tValid loss: 0.25002148747444153\n",
      "Step: 39400  \tTraining loss: 0.23185203969478607\n",
      "Step: 39400  \tTraining accuracy: 0.8870042562484741\n",
      "Step: 39400  \tValid loss: 0.24999623000621796\n",
      "Step: 39500  \tTraining loss: 0.23182827234268188\n",
      "Step: 39500  \tTraining accuracy: 0.8870152235031128\n",
      "Step: 39500  \tValid loss: 0.24999043345451355\n",
      "Step: 39600  \tTraining loss: 0.23180009424686432\n",
      "Step: 39600  \tTraining accuracy: 0.8870260119438171\n",
      "Step: 39600  \tValid loss: 0.2499769777059555\n",
      "Step: 39700  \tTraining loss: 0.23177580535411835\n",
      "Step: 39700  \tTraining accuracy: 0.8870367407798767\n",
      "Step: 39700  \tValid loss: 0.24996325373649597\n",
      "Step: 39800  \tTraining loss: 0.23174993693828583\n",
      "Step: 39800  \tTraining accuracy: 0.8870481252670288\n",
      "Step: 39800  \tValid loss: 0.24993634223937988\n",
      "Step: 39900  \tTraining loss: 0.2317226231098175\n",
      "Step: 39900  \tTraining accuracy: 0.8870595693588257\n",
      "Step: 39900  \tValid loss: 0.24992351233959198\n",
      "Step: 40000  \tTraining loss: 0.2316986471414566\n",
      "Step: 40000  \tTraining accuracy: 0.8870708346366882\n",
      "Step: 40000  \tValid loss: 0.24990694224834442\n",
      "Step: 40100  \tTraining loss: 0.23167121410369873\n",
      "Step: 40100  \tTraining accuracy: 0.887082040309906\n",
      "Step: 40100  \tValid loss: 0.24988985061645508\n",
      "Step: 40200  \tTraining loss: 0.23164653778076172\n",
      "Step: 40200  \tTraining accuracy: 0.8870940804481506\n",
      "Step: 40200  \tValid loss: 0.24989356100559235\n",
      "Step: 40300  \tTraining loss: 0.23161670565605164\n",
      "Step: 40300  \tTraining accuracy: 0.8871051073074341\n",
      "Step: 40300  \tValid loss: 0.24984358251094818\n",
      "Step: 40400  \tTraining loss: 0.23159168660640717\n",
      "Step: 40400  \tTraining accuracy: 0.887115478515625\n",
      "Step: 40400  \tValid loss: 0.24983228743076324\n",
      "Step: 40500  \tTraining loss: 0.2315649390220642\n",
      "Step: 40500  \tTraining accuracy: 0.8871257305145264\n",
      "Step: 40500  \tValid loss: 0.24979646503925323\n",
      "Step: 40600  \tTraining loss: 0.2315395176410675\n",
      "Step: 40600  \tTraining accuracy: 0.8871359825134277\n",
      "Step: 40600  \tValid loss: 0.24978812038898468\n",
      "Step: 40700  \tTraining loss: 0.23151348531246185\n",
      "Step: 40700  \tTraining accuracy: 0.8871461749076843\n",
      "Step: 40700  \tValid loss: 0.24976763129234314\n",
      "Step: 40800  \tTraining loss: 0.23148825764656067\n",
      "Step: 40800  \tTraining accuracy: 0.8871563076972961\n",
      "Step: 40800  \tValid loss: 0.24974951148033142\n",
      "Step: 40900  \tTraining loss: 0.23146331310272217\n",
      "Step: 40900  \tTraining accuracy: 0.8871663808822632\n",
      "Step: 40900  \tValid loss: 0.24973563849925995\n",
      "Step: 41000  \tTraining loss: 0.23143857717514038\n",
      "Step: 41000  \tTraining accuracy: 0.8871763944625854\n",
      "Step: 41000  \tValid loss: 0.24972082674503326\n",
      "Step: 41100  \tTraining loss: 0.23141562938690186\n",
      "Step: 41100  \tTraining accuracy: 0.8871870636940002\n",
      "Step: 41100  \tValid loss: 0.24971067905426025\n",
      "Step: 41200  \tTraining loss: 0.23139061033725739\n",
      "Step: 41200  \tTraining accuracy: 0.8871978521347046\n",
      "Step: 41200  \tValid loss: 0.24969513714313507\n",
      "Step: 41300  \tTraining loss: 0.23136816918849945\n",
      "Step: 41300  \tTraining accuracy: 0.8872091174125671\n",
      "Step: 41300  \tValid loss: 0.24968747794628143\n",
      "Step: 41400  \tTraining loss: 0.231339231133461\n",
      "Step: 41400  \tTraining accuracy: 0.8872206211090088\n",
      "Step: 41400  \tValid loss: 0.24964937567710876\n",
      "Step: 41500  \tTraining loss: 0.23131485283374786\n",
      "Step: 41500  \tTraining accuracy: 0.8872321248054504\n",
      "Step: 41500  \tValid loss: 0.2496432214975357\n",
      "Step: 41600  \tTraining loss: 0.23129194974899292\n",
      "Step: 41600  \tTraining accuracy: 0.8872435092926025\n",
      "Step: 41600  \tValid loss: 0.2496403008699417\n",
      "Step: 41700  \tTraining loss: 0.23126673698425293\n",
      "Step: 41700  \tTraining accuracy: 0.8872548937797546\n",
      "Step: 41700  \tValid loss: 0.24962006509304047\n",
      "Step: 41800  \tTraining loss: 0.23124194145202637\n",
      "Step: 41800  \tTraining accuracy: 0.887266218662262\n",
      "Step: 41800  \tValid loss: 0.2496107816696167\n",
      "Step: 41900  \tTraining loss: 0.2312214970588684\n",
      "Step: 41900  \tTraining accuracy: 0.8872771263122559\n",
      "Step: 41900  \tValid loss: 0.24960854649543762\n",
      "Step: 42000  \tTraining loss: 0.23119451105594635\n",
      "Step: 42000  \tTraining accuracy: 0.8872874975204468\n",
      "Step: 42000  \tValid loss: 0.24957475066184998\n",
      "Step: 42100  \tTraining loss: 0.2311689406633377\n",
      "Step: 42100  \tTraining accuracy: 0.8872978091239929\n",
      "Step: 42100  \tValid loss: 0.24956771731376648\n",
      "Step: 42200  \tTraining loss: 0.23114512860774994\n",
      "Step: 42200  \tTraining accuracy: 0.8873080611228943\n",
      "Step: 42200  \tValid loss: 0.24955587089061737\n",
      "Step: 42300  \tTraining loss: 0.23112298548221588\n",
      "Step: 42300  \tTraining accuracy: 0.8873183131217957\n",
      "Step: 42300  \tValid loss: 0.2495458871126175\n",
      "Step: 42400  \tTraining loss: 0.23109674453735352\n",
      "Step: 42400  \tTraining accuracy: 0.8873284459114075\n",
      "Step: 42400  \tValid loss: 0.2495221197605133\n",
      "Step: 42500  \tTraining loss: 0.23107369244098663\n",
      "Step: 42500  \tTraining accuracy: 0.8873385787010193\n",
      "Step: 42500  \tValid loss: 0.24949979782104492\n",
      "Step: 42600  \tTraining loss: 0.23105038702487946\n",
      "Step: 42600  \tTraining accuracy: 0.8873486518859863\n",
      "Step: 42600  \tValid loss: 0.24949291348457336\n",
      "Step: 42700  \tTraining loss: 0.23102685809135437\n",
      "Step: 42700  \tTraining accuracy: 0.8873586654663086\n",
      "Step: 42700  \tValid loss: 0.24947690963745117\n",
      "Step: 42800  \tTraining loss: 0.23100215196609497\n",
      "Step: 42800  \tTraining accuracy: 0.8873686790466309\n",
      "Step: 42800  \tValid loss: 0.24946917593479156\n",
      "Step: 42900  \tTraining loss: 0.23097829520702362\n",
      "Step: 42900  \tTraining accuracy: 0.8873785734176636\n",
      "Step: 42900  \tValid loss: 0.24945245683193207\n",
      "Step: 43000  \tTraining loss: 0.23095488548278809\n",
      "Step: 43000  \tTraining accuracy: 0.8873884677886963\n",
      "Step: 43000  \tValid loss: 0.24943554401397705\n",
      "Step: 43100  \tTraining loss: 0.2309320569038391\n",
      "Step: 43100  \tTraining accuracy: 0.887398362159729\n",
      "Step: 43100  \tValid loss: 0.2494271993637085\n",
      "Step: 43200  \tTraining loss: 0.2309076488018036\n",
      "Step: 43200  \tTraining accuracy: 0.8874086141586304\n",
      "Step: 43200  \tValid loss: 0.24941329658031464\n",
      "Step: 43300  \tTraining loss: 0.23088383674621582\n",
      "Step: 43300  \tTraining accuracy: 0.8874191641807556\n",
      "Step: 43300  \tValid loss: 0.24940228462219238\n",
      "Step: 43400  \tTraining loss: 0.23086091876029968\n",
      "Step: 43400  \tTraining accuracy: 0.8874297142028809\n",
      "Step: 43400  \tValid loss: 0.24939647316932678\n",
      "Step: 43500  \tTraining loss: 0.230839803814888\n",
      "Step: 43500  \tTraining accuracy: 0.8874405026435852\n",
      "Step: 43500  \tValid loss: 0.2493923306465149\n",
      "Step: 43600  \tTraining loss: 0.2308148294687271\n",
      "Step: 43600  \tTraining accuracy: 0.8874514102935791\n",
      "Step: 43600  \tValid loss: 0.2493746280670166\n",
      "Step: 43700  \tTraining loss: 0.23079168796539307\n",
      "Step: 43700  \tTraining accuracy: 0.88746178150177\n",
      "Step: 43700  \tValid loss: 0.2493618279695511\n",
      "Step: 43800  \tTraining loss: 0.23076778650283813\n",
      "Step: 43800  \tTraining accuracy: 0.8874720931053162\n",
      "Step: 43800  \tValid loss: 0.2493407279253006\n",
      "Step: 43900  \tTraining loss: 0.23074522614479065\n",
      "Step: 43900  \tTraining accuracy: 0.8874823451042175\n",
      "Step: 43900  \tValid loss: 0.24933482706546783\n",
      "Step: 44000  \tTraining loss: 0.23072227835655212\n",
      "Step: 44000  \tTraining accuracy: 0.8874928951263428\n",
      "Step: 44000  \tValid loss: 0.24932274222373962\n",
      "Step: 44100  \tTraining loss: 0.23069974780082703\n",
      "Step: 44100  \tTraining accuracy: 0.8875038623809814\n",
      "Step: 44100  \tValid loss: 0.2493199110031128\n",
      "Step: 44200  \tTraining loss: 0.230676531791687\n",
      "Step: 44200  \tTraining accuracy: 0.8875144720077515\n",
      "Step: 44200  \tValid loss: 0.24929939210414886\n",
      "Step: 44300  \tTraining loss: 0.23065316677093506\n",
      "Step: 44300  \tTraining accuracy: 0.8875249028205872\n",
      "Step: 44300  \tValid loss: 0.24928468465805054\n",
      "Step: 44400  \tTraining loss: 0.23063072562217712\n",
      "Step: 44400  \tTraining accuracy: 0.8875352740287781\n",
      "Step: 44400  \tValid loss: 0.24927838146686554\n",
      "Step: 44500  \tTraining loss: 0.23060733079910278\n",
      "Step: 44500  \tTraining accuracy: 0.8875452280044556\n",
      "Step: 44500  \tValid loss: 0.2492574006319046\n",
      "Step: 44600  \tTraining loss: 0.23058496415615082\n",
      "Step: 44600  \tTraining accuracy: 0.8875551819801331\n",
      "Step: 44600  \tValid loss: 0.24924591183662415\n",
      "Step: 44700  \tTraining loss: 0.23056155443191528\n",
      "Step: 44700  \tTraining accuracy: 0.8875650763511658\n",
      "Step: 44700  \tValid loss: 0.24923625588417053\n",
      "Step: 44800  \tTraining loss: 0.23053990304470062\n",
      "Step: 44800  \tTraining accuracy: 0.8875749707221985\n",
      "Step: 44800  \tValid loss: 0.24923622608184814\n",
      "Step: 44900  \tTraining loss: 0.2305169552564621\n",
      "Step: 44900  \tTraining accuracy: 0.8875848054885864\n",
      "Step: 44900  \tValid loss: 0.24921011924743652\n",
      "Step: 45000  \tTraining loss: 0.23049521446228027\n",
      "Step: 45000  \tTraining accuracy: 0.8875945806503296\n",
      "Step: 45000  \tValid loss: 0.24920940399169922\n",
      "Step: 45100  \tTraining loss: 0.2304702252149582\n",
      "Step: 45100  \tTraining accuracy: 0.887604296207428\n",
      "Step: 45100  \tValid loss: 0.2491958737373352\n",
      "Step: 45200  \tTraining loss: 0.23044727742671967\n",
      "Step: 45200  \tTraining accuracy: 0.8876139521598816\n",
      "Step: 45200  \tValid loss: 0.24917560815811157\n",
      "Step: 45300  \tTraining loss: 0.23042596876621246\n",
      "Step: 45300  \tTraining accuracy: 0.8876236081123352\n",
      "Step: 45300  \tValid loss: 0.24916677176952362\n",
      "Step: 45400  \tTraining loss: 0.2304014414548874\n",
      "Step: 45400  \tTraining accuracy: 0.887633204460144\n",
      "Step: 45400  \tValid loss: 0.24914661049842834\n",
      "Step: 45500  \tTraining loss: 0.230378657579422\n",
      "Step: 45500  \tTraining accuracy: 0.8876427412033081\n",
      "Step: 45500  \tValid loss: 0.2491465061903\n",
      "Step: 45600  \tTraining loss: 0.23035559058189392\n",
      "Step: 45600  \tTraining accuracy: 0.8876522779464722\n",
      "Step: 45600  \tValid loss: 0.24912284314632416\n",
      "Step: 45700  \tTraining loss: 0.23033304512500763\n",
      "Step: 45700  \tTraining accuracy: 0.8876617550849915\n",
      "Step: 45700  \tValid loss: 0.2491113692522049\n",
      "Step: 45800  \tTraining loss: 0.23030951619148254\n",
      "Step: 45800  \tTraining accuracy: 0.887671172618866\n",
      "Step: 45800  \tValid loss: 0.24909336864948273\n",
      "Step: 45900  \tTraining loss: 0.23028725385665894\n",
      "Step: 45900  \tTraining accuracy: 0.8876805901527405\n",
      "Step: 45900  \tValid loss: 0.24909432232379913\n",
      "Step: 46000  \tTraining loss: 0.23026415705680847\n",
      "Step: 46000  \tTraining accuracy: 0.8876900672912598\n",
      "Step: 46000  \tValid loss: 0.24907030165195465\n",
      "Step: 46100  \tTraining loss: 0.23024137318134308\n",
      "Step: 46100  \tTraining accuracy: 0.8877001404762268\n",
      "Step: 46100  \tValid loss: 0.24906288087368011\n",
      "Step: 46200  \tTraining loss: 0.23021987080574036\n",
      "Step: 46200  \tTraining accuracy: 0.8877102136611938\n",
      "Step: 46200  \tValid loss: 0.24905775487422943\n",
      "Step: 46300  \tTraining loss: 0.2301969826221466\n",
      "Step: 46300  \tTraining accuracy: 0.8877197504043579\n",
      "Step: 46300  \tValid loss: 0.24905353784561157\n",
      "Step: 46400  \tTraining loss: 0.2301744520664215\n",
      "Step: 46400  \tTraining accuracy: 0.8877289295196533\n",
      "Step: 46400  \tValid loss: 0.24902203679084778\n",
      "Step: 46500  \tTraining loss: 0.2301526516675949\n",
      "Step: 46500  \tTraining accuracy: 0.8877377510070801\n",
      "Step: 46500  \tValid loss: 0.24902606010437012\n",
      "Step: 46600  \tTraining loss: 0.2301289141178131\n",
      "Step: 46600  \tTraining accuracy: 0.8877460956573486\n",
      "Step: 46600  \tValid loss: 0.24900035560131073\n",
      "Step: 46700  \tTraining loss: 0.23010554909706116\n",
      "Step: 46700  \tTraining accuracy: 0.8877544403076172\n",
      "Step: 46700  \tValid loss: 0.2489929348230362\n",
      "Step: 46800  \tTraining loss: 0.23008444905281067\n",
      "Step: 46800  \tTraining accuracy: 0.887762725353241\n",
      "Step: 46800  \tValid loss: 0.24898819625377655\n",
      "Step: 46900  \tTraining loss: 0.23006565868854523\n",
      "Step: 46900  \tTraining accuracy: 0.8877714276313782\n",
      "Step: 46900  \tValid loss: 0.24898262321949005\n",
      "Step: 47000  \tTraining loss: 0.23003792762756348\n",
      "Step: 47000  \tTraining accuracy: 0.8877798914909363\n",
      "Step: 47000  \tValid loss: 0.24894274771213531\n",
      "Step: 47100  \tTraining loss: 0.23001901805400848\n",
      "Step: 47100  \tTraining accuracy: 0.8877885341644287\n",
      "Step: 47100  \tValid loss: 0.24893085658550262\n",
      "Step: 47200  \tTraining loss: 0.22999124228954315\n",
      "Step: 47200  \tTraining accuracy: 0.8877974152565002\n",
      "Step: 47200  \tValid loss: 0.24887408316135406\n",
      "Step: 47300  \tTraining loss: 0.22996699810028076\n",
      "Step: 47300  \tTraining accuracy: 0.887806236743927\n",
      "Step: 47300  \tValid loss: 0.248822420835495\n",
      "Step: 47400  \tTraining loss: 0.22994443774223328\n",
      "Step: 47400  \tTraining accuracy: 0.8878150582313538\n",
      "Step: 47400  \tValid loss: 0.24879449605941772\n",
      "Step: 47500  \tTraining loss: 0.2299218475818634\n",
      "Step: 47500  \tTraining accuracy: 0.8878239989280701\n",
      "Step: 47500  \tValid loss: 0.24875696003437042\n",
      "Step: 47600  \tTraining loss: 0.22989816963672638\n",
      "Step: 47600  \tTraining accuracy: 0.8878331780433655\n",
      "Step: 47600  \tValid loss: 0.24875187873840332\n",
      "Step: 47700  \tTraining loss: 0.22987522184848785\n",
      "Step: 47700  \tTraining accuracy: 0.8878424763679504\n",
      "Step: 47700  \tValid loss: 0.2487342357635498\n",
      "Step: 47800  \tTraining loss: 0.22985245287418365\n",
      "Step: 47800  \tTraining accuracy: 0.8878511190414429\n",
      "Step: 47800  \tValid loss: 0.24872417747974396\n",
      "Step: 47900  \tTraining loss: 0.2298303097486496\n",
      "Step: 47900  \tTraining accuracy: 0.8878597617149353\n",
      "Step: 47900  \tValid loss: 0.24870623648166656\n",
      "Step: 48000  \tTraining loss: 0.22980818152427673\n",
      "Step: 48000  \tTraining accuracy: 0.887868344783783\n",
      "Step: 48000  \tValid loss: 0.24868328869342804\n",
      "Step: 48100  \tTraining loss: 0.22978509962558746\n",
      "Step: 48100  \tTraining accuracy: 0.8878768682479858\n",
      "Step: 48100  \tValid loss: 0.2486840933561325\n",
      "Step: 48200  \tTraining loss: 0.2297627180814743\n",
      "Step: 48200  \tTraining accuracy: 0.8878853917121887\n",
      "Step: 48200  \tValid loss: 0.2486686110496521\n",
      "Step: 48300  \tTraining loss: 0.22974000871181488\n",
      "Step: 48300  \tTraining accuracy: 0.8878938555717468\n",
      "Step: 48300  \tValid loss: 0.24865399301052094\n",
      "Step: 48400  \tTraining loss: 0.2297195941209793\n",
      "Step: 48400  \tTraining accuracy: 0.8879023194313049\n",
      "Step: 48400  \tValid loss: 0.24866099655628204\n",
      "Step: 48500  \tTraining loss: 0.22969560325145721\n",
      "Step: 48500  \tTraining accuracy: 0.8879106044769287\n",
      "Step: 48500  \tValid loss: 0.24861961603164673\n",
      "Step: 48600  \tTraining loss: 0.22967278957366943\n",
      "Step: 48600  \tTraining accuracy: 0.8879189491271973\n",
      "Step: 48600  \tValid loss: 0.24861621856689453\n",
      "Step: 48700  \tTraining loss: 0.2296501100063324\n",
      "Step: 48700  \tTraining accuracy: 0.8879272937774658\n",
      "Step: 48700  \tValid loss: 0.24861720204353333\n",
      "Step: 48800  \tTraining loss: 0.22962750494480133\n",
      "Step: 48800  \tTraining accuracy: 0.8879356384277344\n",
      "Step: 48800  \tValid loss: 0.2485998123884201\n",
      "Step: 48900  \tTraining loss: 0.2296052724123001\n",
      "Step: 48900  \tTraining accuracy: 0.8879441618919373\n",
      "Step: 48900  \tValid loss: 0.2485833615064621\n",
      "Step: 49000  \tTraining loss: 0.22958369553089142\n",
      "Step: 49000  \tTraining accuracy: 0.8879531621932983\n",
      "Step: 49000  \tValid loss: 0.2485908716917038\n",
      "Step: 49100  \tTraining loss: 0.2295590192079544\n",
      "Step: 49100  \tTraining accuracy: 0.8879620432853699\n",
      "Step: 49100  \tValid loss: 0.2485625296831131\n",
      "Step: 49200  \tTraining loss: 0.22953662276268005\n",
      "Step: 49200  \tTraining accuracy: 0.8879709243774414\n",
      "Step: 49200  \tValid loss: 0.24855810403823853\n",
      "Step: 49300  \tTraining loss: 0.22951433062553406\n",
      "Step: 49300  \tTraining accuracy: 0.8879798054695129\n",
      "Step: 49300  \tValid loss: 0.24855513870716095\n",
      "Step: 49400  \tTraining loss: 0.22949087619781494\n",
      "Step: 49400  \tTraining accuracy: 0.8879886269569397\n",
      "Step: 49400  \tValid loss: 0.24853166937828064\n",
      "Step: 49500  \tTraining loss: 0.22947171330451965\n",
      "Step: 49500  \tTraining accuracy: 0.8879973888397217\n",
      "Step: 49500  \tValid loss: 0.24853521585464478\n",
      "Step: 49600  \tTraining loss: 0.22944635152816772\n",
      "Step: 49600  \tTraining accuracy: 0.8880061507225037\n",
      "Step: 49600  \tValid loss: 0.24851927161216736\n",
      "Step: 49700  \tTraining loss: 0.22942590713500977\n",
      "Step: 49700  \tTraining accuracy: 0.8880148530006409\n",
      "Step: 49700  \tValid loss: 0.24849829077720642\n",
      "Step: 49800  \tTraining loss: 0.2294021099805832\n",
      "Step: 49800  \tTraining accuracy: 0.8880234956741333\n",
      "Step: 49800  \tValid loss: 0.24849072098731995\n",
      "Step: 49900  \tTraining loss: 0.22938184440135956\n",
      "Step: 49900  \tTraining accuracy: 0.8880321383476257\n",
      "Step: 49900  \tValid loss: 0.24849560856819153\n",
      "Step: 50000  \tTraining loss: 0.2293584793806076\n",
      "Step: 50000  \tTraining accuracy: 0.8880407214164734\n",
      "Step: 50000  \tValid loss: 0.24848049879074097\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8880493\n",
      "Precision: 0.9186419\n",
      "Recall: 0.96273917\n",
      "F1 score: 0.9088641\n",
      "AUC: 0.7599224\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.888049   0.918642  0.962739  0.908864  0.759922  0.229358       0.88804   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.248462       0.888045   0.235741      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  49999.0  \n"
     ]
    }
   ],
   "source": [
    "neurons = 8\n",
    "\n",
    "\n",
    "for num, subj_file_path in enumerate(subj_files_list):\n",
    "    print(num)\n",
    "# for subj_file_path in [subj_files_list[0]]:\n",
    "    \n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "                \n",
    "#     file_path = file_path + \"/OddEvenPlays\"\n",
    "    file_path = file_path + \"/OddEvenPlays/RandomizedPlays10\"\n",
    "\n",
    "    train_data_df= pd.read_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df = pd.read_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df = pd.read_csv(file_path+\"/val_data.csv\")\n",
    "\n",
    " \n",
    "    train_X, train_y, test_X, test_y,val_X,val_y = data_split_odd_even(train_data_df,test_data_df,val_data_df)\n",
    "\n",
    "    pretraining = False;\n",
    "    metric_out_df, prob_train, prob_test, prob_val = train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y)\n",
    "    \n",
    "    print(metric_out_df)\n",
    "    \n",
    "    metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    " \n",
    "    prob_train_df = pd.DataFrame(prob_train.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_test_df = pd.DataFrame(prob_test.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_val_df = pd.DataFrame(prob_val.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "\n",
    "\n",
    "# ################################\n",
    "    prob_train_df.to_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_test_df.to_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_val_df.to_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "# #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
