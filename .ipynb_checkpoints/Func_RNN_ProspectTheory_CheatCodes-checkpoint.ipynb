{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as sc_stats\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "onehot_encoder=OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "time_steps = 1\n",
    "inputs = 8\n",
    "outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_releveant_features(task_df):\n",
    "\n",
    "\n",
    "    task_df['PrevOutcome']=task_df['Outcome'].shift(1)\n",
    "    task_df.loc[1,'PrevOutcome']= 0\n",
    "\n",
    "    task_df['PrevChoice']=task_df['Choice'].shift(1)\n",
    "    task_df.loc[1,'PrevChoice']= 0\n",
    "\n",
    "    task_df['PrevSafe']=task_df['Safe'].shift(1)\n",
    "    task_df.loc[1,'PrevSafe']= 0\n",
    "\n",
    "    task_df['PrevBigRisky']=task_df['BigRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevBigRisky']= 0\n",
    "\n",
    "    task_df['PrevSmallRisky']=task_df['SmallRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevSmallRisky']= 0\n",
    "    \n",
    "#     task_df['PrevRT']=task_df['RT'].shift(1)\n",
    "#     task_df.loc[1,'PrevRT']= N\n",
    "    \n",
    "    \n",
    "    \n",
    "    return task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=150\n",
    "num_batches=1000\n",
    "seq_len=3\n",
    "\n",
    "def data_split(task_df,dopa_task_df):\n",
    "#     stop = 200\n",
    "\n",
    "    stop = 150\n",
    "\n",
    "#     stop=300\n",
    "\n",
    "#     stop = 750\n",
    "\n",
    "#     stop=1500\n",
    "\n",
    "\n",
    "    print(task_df.shape)\n",
    " \n",
    "\n",
    "    ##----------------- UNCOMMENT BELOW\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "    \n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "####### Prev O + C+ R + CurrO--------------------\n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    #### PLACEHOLDER VAL_X\n",
    "    val_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    val_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "#### - Prev RT+C+R+O + Curr O----------------------\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "    ###### when splitting data into train and validation\n",
    "\n",
    "    # train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.35, random_state=1)\n",
    "\n",
    "    \n",
    "#     train_X, val_X, train_y, val_y = train_X[:stop], train_X[stop:], train_y[:stop], train_y[stop:]\n",
    "\n",
    "\n",
    "####################\n",
    " ##### splitting data into train test valid from the same dataset ###############\n",
    "    \n",
    "#     train_X, val_X, test_X, train_y, val_y, test_y = train_X[:stop], train_X[stop:stop+int(stop/2)], train_X[stop+int(stop/2):], train_y[:stop], train_y[stop:stop+int(stop/2)], train_y[stop+int(stop/2):],\n",
    "\n",
    "#### switching the order for test and val\n",
    "#     half = 1\n",
    "\n",
    "#     if half==1:\n",
    "#         train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int(stop/2)], train_X[stop+int(stop/2):], train_y[:stop], train_y[stop:stop+int(stop/2)], train_y[stop+int(stop/2):],\n",
    "    \n",
    "#     else:\n",
    "    \n",
    "#         train_X, test_X, val_X, train_y, test_y, val_y= train_X[stop-1:], train_X[:int(stop/2)], train_X[int(stop/2):stop-1], train_y[stop-1:], train_y[:int(stop/2)], train_y[int(stop/2):stop-1]\n",
    "\n",
    "#     ##############\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(train_X)\n",
    "    \n",
    "    \n",
    "    ## Combining PLAC + LDOPA datasets\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = np.concatenate(train_X[:stop],test_X[:stop]), np.concatenate(train_X[stop:stop+int(stop/2)])#,test_X[stop:stop+int(stop/2)]),np.concatenate(train_X[stop+int(stop/2):],test_X[stop+int(stop/2):]),np.concatenate(train_y[:stop],test_y[:stop]), np.concatenate(train_y[stop:stop+int(stop/2)],test_y[stop:stop+int(stop/2)]), np.concatenate(train_y[stop+int(stop/2):],test_y[stop+int(stop/2):])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X, test_X,val_X = np.concatenate((train_X[:stop],test_X[:stop]),axis=0), np.concatenate((train_X[stop:stop+int(stop/2)] ,test_X[stop:stop+int(stop/2)]),axis=0), np.concatenate((train_X[stop+int(stop/2):],test_X[stop+int(stop/2):]),axis=0)\n",
    "#     train_y, test_y,val_y = np.concatenate((train_y[:stop],test_y[:stop]),axis=0), np.concatenate((train_y[stop:stop+int(stop/2)] ,test_y[stop:stop+int(stop/2)]),axis=0), np.concatenate((train_y[stop+int(stop/2):],test_y[stop+int(stop/2):]),axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#     ## blocking data \n",
    "#     train_X_aside, train_y_aside, test_X_aside, test_y_aside  = train_X, train_y, test_X, test_y \n",
    "    \n",
    "#     train_X= np.concatenate((build_dataset_train(train_X_aside),build_dataset_train(test_X_aside)), axis=0)\n",
    "#     train_y= np.concatenate((build_dataset_train(train_y_aside),build_dataset_train(test_y_aside)), axis=0)\n",
    "\n",
    "    \n",
    "#     val_X= np.concatenate((build_dataset_valid(train_X_aside),build_dataset_valid(test_X_aside)), axis=0)\n",
    "#     val_y= np.concatenate((build_dataset_valid(train_y_aside),build_dataset_valid(test_y_aside)), axis=0)\n",
    "\n",
    "    \n",
    "#     test_X= np.concatenate((build_dataset_test(train_X_aside),build_dataset_test(test_X_aside)), axis=0)\n",
    "#     test_y= np.concatenate((build_dataset_test(train_y_aside),build_dataset_test(test_y_aside)), axis=0)\n",
    "    \n",
    "\n",
    "        ### CHUNK SPLITTING\n",
    "\n",
    "        \n",
    "        \n",
    "    train_X_aside, train_y_aside, test_X_aside, test_y_aside  = train_X, train_y, test_X, test_y \n",
    "    \n",
    "    train_X= np.concatenate((chunk_split_data(train_X_aside,0,6),chunk_split_data(test_X_aside,0,6)), axis=0)\n",
    "    train_y= np.concatenate((chunk_split_data(train_y_aside,0,6),chunk_split_data(test_y_aside,0,6)), axis=0)\n",
    "\n",
    "    \n",
    "    val_X= np.concatenate((chunk_split_data(train_X_aside,8,10),chunk_split_data(test_X_aside,8,10)), axis=0)\n",
    "    val_y= np.concatenate((chunk_split_data(train_y_aside,8,10),chunk_split_data(test_y_aside,8,10)), axis=0)\n",
    "\n",
    "    test_X= np.concatenate((chunk_split_data(train_X_aside,6,8),chunk_split_data(test_X_aside,6,8)), axis=0)\n",
    "    test_y= np.concatenate((chunk_split_data(train_y_aside,6,8),chunk_split_data(test_y_aside,6,8)), axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     ##### FURTHER TRAINING WITH SUBSEQUENCES WITH REPLACEMENT\n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#     for k in range(num_batches):\n",
    "#             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#             train_X = np.concatenate((train_X,X_seq), axis=0)\n",
    "# #             print(train_X.shape)\n",
    "#             train_y = np.concatenate((train_y, y_seq),axis=0)\n",
    "\n",
    "\n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#     train_X, train_y = X_seq, y_seq\n",
    "#     for k in range(num_batches-1):\n",
    "#             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#             train_X = np.concatenate((train_X,X_seq), axis=0)\n",
    "# #             print(train_X.shape)\n",
    "#             train_y = np.concatenate((train_y, y_seq),axis=0)\n",
    "# # # # # ##########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### PRE TRAINING\n",
    "#     stop = int(0.7*len(train_X))\n",
    "#     print(stop)\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int((len(train_X)-stop)/2)], train_X[stop+int((len(train_X)-stop)/2):],train_y[:stop], train_y[stop:stop+int((len(train_X)-stop)/2)], train_y[stop+int((len(train_X)-stop)/2):]\n",
    "    \n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = train_X, test_X, test_X, train_y, test_y, test_y\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(val_X.shape)\n",
    "    print(val_y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "    # # center and scale\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.fit_transform(test_X)\n",
    "    val_X = scaler.fit_transform(val_X)\n",
    "\n",
    "\n",
    "    train_X = train_X[:,None,:]\n",
    "    val_X = val_X[:,None,:]\n",
    "    test_X = test_X[:,None,:]\n",
    "\n",
    "\n",
    "    # # one-hot encode the outputs\n",
    "\n",
    "    onehot_encoder = OneHotEncoder()\n",
    "    encode_categorical = train_y.reshape(len(train_y), 1)\n",
    "    encode_categorical_test = test_y.reshape(len(test_y), 1)\n",
    "    encode_categorical_val = val_y.reshape(len(val_y),1)\n",
    "\n",
    "\n",
    "    train_y = onehot_encoder.fit_transform(encode_categorical).toarray()\n",
    "    test_y = onehot_encoder.fit_transform(encode_categorical_test).toarray()\n",
    "    val_y = onehot_encoder.fit_transform(encode_categorical_val).toarray()\n",
    "\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, val_X,val_y\n",
    "#     return train_X, test_X, val_X#, test_X, test_y, val_X,val_y\n",
    "\n",
    "def build_dataset_train(data):\n",
    "    \n",
    "    return np.concatenate((data[:int(stop/3)],data[2*int(stop/3):int(stop)], data[stop+int(stop/3):stop+2*int(stop/3)]), axis=0)\n",
    "    \n",
    "\n",
    "def build_dataset_test(data):\n",
    "    \n",
    "    return np.concatenate((data[int(stop/3):int(stop/3)+int(stop/6)], data[int(stop):int(stop)+int(stop/6)], data[stop+2*int(stop/3): stop+2*int(stop/3) + int(stop/6) ]), axis=0)\n",
    "\n",
    "def build_dataset_valid(data):\n",
    "    \n",
    "    return np.concatenate((data[int(stop/3)+int(stop/6):int(stop/3)+2*int(stop/6)], data[int(stop)+int(stop/6):int(stop)+2*int(stop/6)], data[stop+2*int(stop/3) + int(stop/6) : ]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_split_data(data,start_chunk,end_chunk):\n",
    "    \n",
    "    a=[k for k in range(start_chunk,end_chunk)]\n",
    "    out=[]\n",
    "\n",
    "    for d in range(0,data.shape[0],10):\n",
    "\n",
    "        c= [c+d for c in a]\n",
    "        out = out+c\n",
    "\n",
    "    while out[-1]>=data.shape[0]-1:\n",
    "        out.pop()\n",
    "#     return out\n",
    "    return data[out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_split_data(train_X,0,6)\n",
    "# chunk_split_data(train_X,6,8)\n",
    "# chunk_split_data(train_X,8,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y): \n",
    "    reset_graph()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50000\n",
    "    batch_size = int(train_X.shape[0]/2)\n",
    "    # batch_size = 100\n",
    "    length = train_X.shape[0]\n",
    "    display = 100\n",
    "    neurons = neurons\n",
    "\n",
    "    num_batches = 100\n",
    "    seq_len = 10\n",
    "\n",
    "    percent_above_PT = 1\n",
    "\n",
    "    train_threshold = PT_R2 + percent_above_PT\n",
    "\n",
    "\n",
    "    save_step = 100\n",
    "\n",
    "\n",
    "    best_loss_val = np.infty\n",
    "    checks_since_last_progress = 0\n",
    "    max_checks_without_progress = 1000\n",
    "\n",
    "\n",
    "    # clear graph (if any) before running\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, time_steps, inputs])\n",
    "\n",
    "    y = tf.placeholder(tf.float32, [None, outputs])\n",
    "\n",
    "    # LSTM Cell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=neurons, activation=tf.nn.relu)\n",
    "    cell_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "    # pass into Dense layer\n",
    "    stacked_outputs = tf.reshape(cell_outputs, [-1, neurons])\n",
    "    out = tf.layers.dense(inputs=stacked_outputs, units=outputs)\n",
    "\n",
    "    probability = tf.nn.softmax(out)\n",
    "\n",
    "    # squared error loss or cost function for linear regression\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=y, logits=out))\n",
    "\n",
    "    # optimizer to minimize cost\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels =  tf.argmax(y, 1),\n",
    "                          predictions = tf.argmax(out, 1),\n",
    "                          name = \"accuracy\")\n",
    "    precision = tf.metrics.precision(labels=tf.argmax(y, 1),\n",
    "                                 predictions=tf.argmax(out, 1),\n",
    "                                 name=\"precision\")\n",
    "    recall = tf.metrics.recall(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"recall\")\n",
    "    f1 = 2 * accuracy[1] * recall[1] / ( precision[1] + recall[1] )\n",
    "\n",
    "    acc_up,acc_val = accuracy\n",
    "    auc = tf.metrics.auc(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"auc\")\n",
    "    \n",
    "    valid_store = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #######################\n",
    "#         saver.restore(sess, \"./checkpts/Original_RNN_LSTM_8features_v2.ckpt\")\n",
    "#         saver.restore(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "        saver.restore(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        #######################\n",
    "        \n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "\n",
    "        # Train the model\n",
    "        for steps in range(epochs):\n",
    "            mini_batch = zip(range(0, length, batch_size),\n",
    "                       range(batch_size, length+1, batch_size))\n",
    "\n",
    "            # train data in mini-batches\n",
    "            for (start, end) in mini_batch:\n",
    "    #             print(start,end)\n",
    "                sess.run(training_op, feed_dict = {X: train_X[start:end,:,:],\n",
    "                                                   y: train_y[start:end,:]}) \n",
    "\n",
    "            ## train data in batches of length subsequence\n",
    "\n",
    "    #         for k in range(num_batches):\n",
    "    #             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "\n",
    "    #             sess.run(training_op, feed_dict = {X:X_seq,y:y_seq}) \n",
    "            loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "            loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "            # print training performance \n",
    "            if (steps+1) % display == 0:\n",
    "                # evaluate loss function on training set\n",
    "\n",
    "\n",
    "                loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining loss: {}'.format((steps+1), loss_fn))\n",
    "\n",
    "                acc_train = acc_val.eval(feed_dict={X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining accuracy: {}'.format((steps+1), acc_train))\n",
    "\n",
    "\n",
    "                acc_test = acc_val.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest accuracy: {}'.format((steps+1), acc_test))\n",
    "\n",
    "                loss_test = loss.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest loss: {}'.format((steps+1), loss_test))\n",
    "\n",
    "                accu_val = acc_val.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "                loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "                print('Step: {}  \\tValid loss: {}'.format((steps+1), loss_val))\n",
    "\n",
    "                valid_store.append(loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (1 + loss_fn/np.log(0.5)) > train_threshold:\n",
    "                    print(\"Threshold achieved, quit training\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            if loss_val < best_loss_val:\n",
    "\n",
    "                        best_loss_val = loss_val\n",
    "                        checks_since_last_progress = 0\n",
    "            else:\n",
    "                            checks_since_last_progress += 1\n",
    "\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if checks_since_last_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "\n",
    "            if (steps+1) % save_step ==0:\n",
    "                                save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#                 save_path = saver.save(sess, \"./checkpts/RNN_Internet_LSTM_model_5features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     evaluate model accuracy\n",
    "        acc, prec, recall, f1, AUC = sess.run([accuracy, precision, recall, f1,auc],\n",
    "                                         feed_dict = {X: train_X, y: train_y})\n",
    "        prob_train = probability.eval(feed_dict = {X: train_X, y: train_y})\n",
    "        prob_test = probability.eval(feed_dict = {X: test_X, y: test_y})\n",
    "        prob_valid = probability.eval(feed_dict = {X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nEvaluation  on training set')\n",
    "        print('Accuracy:', acc[1])\n",
    "        print('Precision:', prec[1])\n",
    "        print('Recall:', recall[1])\n",
    "        print('F1 score:', f1)\n",
    "        print('AUC:', AUC[1])\n",
    "        \n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "        save_path = saver.save(sess, \"./checkpts/Later_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/LaterDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "    metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,AUC[1],loss_fn,accu_val,best_loss_val,acc_test,loss_test,neurons,learning_rate,epochs,steps]).reshape(-1,14),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\",\"loss\",\"accuracy_val\",\"loss_val\",\"accuracy_test\",\"loss_test\",\"neurons\",\"learning_rate\",\"n_epochs\",\"steps\"])\n",
    "    return metric_out_df, prob_train, prob_test, prob_valid\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def random_subsequence(X,y,seq_len):\n",
    "    rnd  = random.randint(0,len(X)-seq_len)\n",
    "    X_seq, y_seq = X[rnd:rnd+seq_len,:], y[rnd:rnd+seq_len,:]\n",
    "    return X_seq, y_seq\n",
    "\n",
    "    print(y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/\"\n",
    "subj_num =11\n",
    "file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_subj29_params.csv\"\n",
    "\n",
    "file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1/generateddata300mu1params.csv\"\n",
    "\n",
    "# file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5/generateddata300mu0_5params.csv\"\n",
    "\n",
    "### ACTUAL DATA\n",
    "file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "task_df = pd.read_csv(file_name)\n",
    "dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "task_df = add_releveant_features(task_df)\n",
    "dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "\n",
    "\n",
    "task_df.head(10)\n",
    "\n",
    "\n",
    "# train_data = np.concatenate((build_dataset_train(task_df.loc[task_df.TrialNum>1]),build_dataset_train(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "# train_data_df = pd.DataFrame(train_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "# test_data = np.concatenate((build_dataset_test(task_df.loc[task_df.TrialNum>1]),build_dataset_test(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "# test_data_df = pd.DataFrame(test_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "\n",
    "\n",
    "train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n",
    "\n",
    "\n",
    "# train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v6chunks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject28\n",
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n",
      "Subject41\n"
     ]
    }
   ],
   "source": [
    "# for subj_num in range(11,27):\n",
    "for subj_num in range(28,42):\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v2/subject_num_\"+str(subj_num)\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v6chunks/subject_num_\"+str(subj_num)\n",
    "\n",
    "\n",
    "    os.mkdir(file_path)\n",
    "\n",
    "    \n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    \n",
    "    ## blocking ####\n",
    "#     train_data = np.concatenate((build_dataset_train(task_df.loc[task_df.TrialNum>1]),build_dataset_train(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "#     train_data_df = pd.DataFrame(train_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "#     test_data = np.concatenate((build_dataset_test(task_df.loc[task_df.TrialNum>1]),build_dataset_test(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "#     test_data_df = pd.DataFrame(test_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "    \n",
    "#     val_data = np.concatenate((build_dataset_valid(task_df.loc[task_df.TrialNum>1]),build_dataset_valid(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "#     val_data_df = pd.DataFrame(val_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "#     ##########\n",
    "\n",
    "    ## CHUNKING\n",
    "    train_X_aside = task_df.loc[task_df.TrialNum>1].values; test_X_aside= dopa_task_df.loc[task_df.TrialNum>1].values;\n",
    "    \n",
    "    train_data= np.concatenate((chunk_split_data(train_X_aside,0,6),chunk_split_data(test_X_aside,0,6)), axis=0)\n",
    "    train_data_df = pd.DataFrame(train_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "    \n",
    "    val_data= np.concatenate((chunk_split_data(train_X_aside,8,10),chunk_split_data(test_X_aside,8,10)), axis=0)\n",
    "    val_data_df = pd.DataFrame(val_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "    test_data= np.concatenate((chunk_split_data(train_X_aside,6,8),chunk_split_data(test_X_aside,6,8)), axis=0)\n",
    "    test_data_df = pd.DataFrame(test_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_data_df.to_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df.to_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df.to_csv(file_path+\"/val_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/\"\n",
    "# os.mkdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject28\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.5644280910491943\n",
      "Step: 100  \tTraining accuracy: 0.7277777791023254\n",
      "Step: 100  \tValid loss: 0.627092719078064\n",
      "Step: 200  \tTraining loss: 0.5277586579322815\n",
      "Step: 200  \tTraining accuracy: 0.7290794849395752\n",
      "Step: 200  \tValid loss: 0.6147720217704773\n",
      "Step: 300  \tTraining loss: 0.5016141533851624\n",
      "Step: 300  \tTraining accuracy: 0.7287371158599854\n",
      "Step: 300  \tValid loss: 0.6020849347114563\n",
      "Step: 400  \tTraining loss: 0.4834318459033966\n",
      "Step: 400  \tTraining accuracy: 0.7304469347000122\n",
      "Step: 400  \tValid loss: 0.5963056087493896\n",
      "Step: 500  \tTraining loss: 0.47059303522109985\n",
      "Step: 500  \tTraining accuracy: 0.7317784428596497\n",
      "Step: 500  \tValid loss: 0.5908421874046326\n",
      "Step: 600  \tTraining loss: 0.46030136942863464\n",
      "Step: 600  \tTraining accuracy: 0.7332335114479065\n",
      "Step: 600  \tValid loss: 0.5833248496055603\n",
      "Step: 700  \tTraining loss: 0.45150434970855713\n",
      "Step: 700  \tTraining accuracy: 0.7355182766914368\n",
      "Step: 700  \tValid loss: 0.5755020380020142\n",
      "Step: 800  \tTraining loss: 0.4433607757091522\n",
      "Step: 800  \tTraining accuracy: 0.7380847334861755\n",
      "Step: 800  \tValid loss: 0.5687652826309204\n",
      "Step: 900  \tTraining loss: 0.4356718361377716\n",
      "Step: 900  \tTraining accuracy: 0.739664614200592\n",
      "Step: 900  \tValid loss: 0.5619744658470154\n",
      "Step: 1000  \tTraining loss: 0.42855513095855713\n",
      "Step: 1000  \tTraining accuracy: 0.7416142821311951\n",
      "Step: 1000  \tValid loss: 0.5557070970535278\n",
      "Step: 1100  \tTraining loss: 0.422011137008667\n",
      "Step: 1100  \tTraining accuracy: 0.7430379986763\n",
      "Step: 1100  \tValid loss: 0.5503959655761719\n",
      "Step: 1200  \tTraining loss: 0.4159509837627411\n",
      "Step: 1200  \tTraining accuracy: 0.7446500658988953\n",
      "Step: 1200  \tValid loss: 0.5448923110961914\n",
      "Step: 1300  \tTraining loss: 0.4103679358959198\n",
      "Step: 1300  \tTraining accuracy: 0.7468051314353943\n",
      "Step: 1300  \tValid loss: 0.540610671043396\n",
      "Step: 1400  \tTraining loss: 0.4052525460720062\n",
      "Step: 1400  \tTraining accuracy: 0.7487666606903076\n",
      "Step: 1400  \tValid loss: 0.5365941524505615\n",
      "Step: 1500  \tTraining loss: 0.4006388485431671\n",
      "Step: 1500  \tTraining accuracy: 0.7504595518112183\n",
      "Step: 1500  \tValid loss: 0.5328208208084106\n",
      "Step: 1600  \tTraining loss: 0.3964483439922333\n",
      "Step: 1600  \tTraining accuracy: 0.7521505355834961\n",
      "Step: 1600  \tValid loss: 0.5303438305854797\n",
      "Step: 1700  \tTraining loss: 0.3925122022628784\n",
      "Step: 1700  \tTraining accuracy: 0.753839910030365\n",
      "Step: 1700  \tValid loss: 0.5281124711036682\n",
      "Step: 1800  \tTraining loss: 0.3889389932155609\n",
      "Step: 1800  \tTraining accuracy: 0.7554327249526978\n",
      "Step: 1800  \tValid loss: 0.526512622833252\n",
      "Step: 1900  \tTraining loss: 0.3856215178966522\n",
      "Step: 1900  \tTraining accuracy: 0.7570346593856812\n",
      "Step: 1900  \tValid loss: 0.5248401761054993\n",
      "Step: 2000  \tTraining loss: 0.38258519768714905\n",
      "Step: 2000  \tTraining accuracy: 0.7585586905479431\n",
      "Step: 2000  \tValid loss: 0.5234834551811218\n",
      "Step: 2100  \tTraining loss: 0.37978222966194153\n",
      "Step: 2100  \tTraining accuracy: 0.7599348425865173\n",
      "Step: 2100  \tValid loss: 0.5221420526504517\n",
      "Step: 2200  \tTraining loss: 0.3772106468677521\n",
      "Step: 2200  \tTraining accuracy: 0.7611836194992065\n",
      "Step: 2200  \tValid loss: 0.521034836769104\n",
      "Step: 2300  \tTraining loss: 0.3748345375061035\n",
      "Step: 2300  \tTraining accuracy: 0.7623960971832275\n",
      "Step: 2300  \tValid loss: 0.5200934410095215\n",
      "Step: 2400  \tTraining loss: 0.37266260385513306\n",
      "Step: 2400  \tTraining accuracy: 0.7635769248008728\n",
      "Step: 2400  \tValid loss: 0.5195683836936951\n",
      "Step: 2500  \tTraining loss: 0.370681494474411\n",
      "Step: 2500  \tTraining accuracy: 0.7646617293357849\n",
      "Step: 2500  \tValid loss: 0.5192052125930786\n",
      "Step: 2600  \tTraining loss: 0.36886849999427795\n",
      "Step: 2600  \tTraining accuracy: 0.765596330165863\n",
      "Step: 2600  \tValid loss: 0.5189554691314697\n",
      "Step: 2700  \tTraining loss: 0.3672023415565491\n",
      "Step: 2700  \tTraining accuracy: 0.766460657119751\n",
      "Step: 2700  \tValid loss: 0.5188441872596741\n",
      "Step: 2800  \tTraining loss: 0.3656809628009796\n",
      "Step: 2800  \tTraining accuracy: 0.7672015428543091\n",
      "Step: 2800  \tValid loss: 0.5189827084541321\n",
      "Step: 2900  \tTraining loss: 0.36426782608032227\n",
      "Step: 2900  \tTraining accuracy: 0.7679493427276611\n",
      "Step: 2900  \tValid loss: 0.5192247033119202\n",
      "Step: 3000  \tTraining loss: 0.36294394731521606\n",
      "Step: 3000  \tTraining accuracy: 0.7687599062919617\n",
      "Step: 3000  \tValid loss: 0.5194504261016846\n",
      "Step: 3100  \tTraining loss: 0.36169180274009705\n",
      "Step: 3100  \tTraining accuracy: 0.7696272134780884\n",
      "Step: 3100  \tValid loss: 0.5197226405143738\n",
      "Step: 3200  \tTraining loss: 0.3605033755302429\n",
      "Step: 3200  \tTraining accuracy: 0.770492672920227\n",
      "Step: 3200  \tValid loss: 0.5200349688529968\n",
      "Step: 3300  \tTraining loss: 0.3593711853027344\n",
      "Step: 3300  \tTraining accuracy: 0.7713050842285156\n",
      "Step: 3300  \tValid loss: 0.5204638838768005\n",
      "Step: 3400  \tTraining loss: 0.3582843244075775\n",
      "Step: 3400  \tTraining accuracy: 0.7721190452575684\n",
      "Step: 3400  \tValid loss: 0.5209813714027405\n",
      "Step: 3500  \tTraining loss: 0.3572365641593933\n",
      "Step: 3500  \tTraining accuracy: 0.7728859782218933\n",
      "Step: 3500  \tValid loss: 0.5215620398521423\n",
      "Step: 3600  \tTraining loss: 0.356218546628952\n",
      "Step: 3600  \tTraining accuracy: 0.7736569046974182\n",
      "Step: 3600  \tValid loss: 0.5222342014312744\n",
      "Step: 3700  \tTraining loss: 0.3552286922931671\n",
      "Step: 3700  \tTraining accuracy: 0.7743857502937317\n",
      "Step: 3700  \tValid loss: 0.5228671431541443\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7751205\n",
      "Precision: 0.7118644\n",
      "Recall: 0.7304348\n",
      "F1 score: 0.78510064\n",
      "AUC: 0.79582965\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0   0.77512   0.711864  0.730435  0.785101  0.79583  0.354818      0.774397   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0   0.51877       0.774435   0.563277      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3741.0  \n",
      "Subject29\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.5948523283004761\n",
      "Step: 100  \tTraining accuracy: 0.699999988079071\n",
      "Step: 100  \tValid loss: 0.620421290397644\n",
      "Step: 200  \tTraining loss: 0.48421308398246765\n",
      "Step: 200  \tTraining accuracy: 0.7353556752204895\n",
      "Step: 200  \tValid loss: 0.51369708776474\n",
      "Step: 300  \tTraining loss: 0.4468732476234436\n",
      "Step: 300  \tTraining accuracy: 0.7564433217048645\n",
      "Step: 300  \tValid loss: 0.46755528450012207\n",
      "Step: 400  \tTraining loss: 0.4173022508621216\n",
      "Step: 400  \tTraining accuracy: 0.7756052017211914\n",
      "Step: 400  \tValid loss: 0.4322548508644104\n",
      "Step: 500  \tTraining loss: 0.385611891746521\n",
      "Step: 500  \tTraining accuracy: 0.7889941930770874\n",
      "Step: 500  \tValid loss: 0.394029825925827\n",
      "Step: 600  \tTraining loss: 0.3519531190395355\n",
      "Step: 600  \tTraining accuracy: 0.7997006177902222\n",
      "Step: 600  \tValid loss: 0.35669785737991333\n",
      "Step: 700  \tTraining loss: 0.31700271368026733\n",
      "Step: 700  \tTraining accuracy: 0.8086889982223511\n",
      "Step: 700  \tValid loss: 0.3190667927265167\n",
      "Step: 800  \tTraining loss: 0.27899208664894104\n",
      "Step: 800  \tTraining accuracy: 0.8157546520233154\n",
      "Step: 800  \tValid loss: 0.28030890226364136\n",
      "Step: 900  \tTraining loss: 0.2465720921754837\n",
      "Step: 900  \tTraining accuracy: 0.8241029381752014\n",
      "Step: 900  \tValid loss: 0.24987082183361053\n",
      "Step: 1000  \tTraining loss: 0.22121520340442657\n",
      "Step: 1000  \tTraining accuracy: 0.8336827158927917\n",
      "Step: 1000  \tValid loss: 0.2281872034072876\n",
      "Step: 1100  \tTraining loss: 0.2012660950422287\n",
      "Step: 1100  \tTraining accuracy: 0.8416139483451843\n",
      "Step: 1100  \tValid loss: 0.212142676115036\n",
      "Step: 1200  \tTraining loss: 0.1853383332490921\n",
      "Step: 1200  \tTraining accuracy: 0.8487564921379089\n",
      "Step: 1200  \tValid loss: 0.20105119049549103\n",
      "Step: 1300  \tTraining loss: 0.17195028066635132\n",
      "Step: 1300  \tTraining accuracy: 0.8547657132148743\n",
      "Step: 1300  \tValid loss: 0.1930857002735138\n",
      "Step: 1400  \tTraining loss: 0.161017507314682\n",
      "Step: 1400  \tTraining accuracy: 0.8607547879219055\n",
      "Step: 1400  \tValid loss: 0.18819652497768402\n",
      "Step: 1500  \tTraining loss: 0.1518646478652954\n",
      "Step: 1500  \tTraining accuracy: 0.8655790686607361\n",
      "Step: 1500  \tValid loss: 0.18556658923625946\n",
      "Step: 1600  \tTraining loss: 0.14391778409481049\n",
      "Step: 1600  \tTraining accuracy: 0.8696774244308472\n",
      "Step: 1600  \tValid loss: 0.18415667116641998\n",
      "Step: 1700  \tTraining loss: 0.13708369433879852\n",
      "Step: 1700  \tTraining accuracy: 0.8731811046600342\n",
      "Step: 1700  \tValid loss: 0.18315467238426208\n",
      "Step: 1800  \tTraining loss: 0.13112980127334595\n",
      "Step: 1800  \tTraining accuracy: 0.8761913776397705\n",
      "Step: 1800  \tValid loss: 0.1824316829442978\n",
      "Step: 1900  \tTraining loss: 0.12587310373783112\n",
      "Step: 1900  \tTraining accuracy: 0.8787878751754761\n",
      "Step: 1900  \tValid loss: 0.1818702667951584\n",
      "Step: 2000  \tTraining loss: 0.1211899146437645\n",
      "Step: 2000  \tTraining accuracy: 0.8811194896697998\n",
      "Step: 2000  \tValid loss: 0.18137340247631073\n",
      "Step: 2100  \tTraining loss: 0.11699424684047699\n",
      "Step: 2100  \tTraining accuracy: 0.8832247853279114\n",
      "Step: 2100  \tValid loss: 0.18113815784454346\n",
      "Step: 2200  \tTraining loss: 0.11320729553699493\n",
      "Step: 2200  \tTraining accuracy: 0.8852904438972473\n",
      "Step: 2200  \tValid loss: 0.18102245032787323\n",
      "Step: 2300  \tTraining loss: 0.10976990312337875\n",
      "Step: 2300  \tTraining accuracy: 0.8871734142303467\n",
      "Step: 2300  \tValid loss: 0.18123209476470947\n",
      "Step: 2400  \tTraining loss: 0.10663079470396042\n",
      "Step: 2400  \tTraining accuracy: 0.8889678716659546\n",
      "Step: 2400  \tValid loss: 0.1814427524805069\n",
      "Step: 2500  \tTraining loss: 0.10375595092773438\n",
      "Step: 2500  \tTraining accuracy: 0.8905482888221741\n",
      "Step: 2500  \tValid loss: 0.18185201287269592\n",
      "Step: 2600  \tTraining loss: 0.10111859440803528\n",
      "Step: 2600  \tTraining accuracy: 0.8920052647590637\n",
      "Step: 2600  \tValid loss: 0.1822836697101593\n",
      "Step: 2700  \tTraining loss: 0.09870317578315735\n",
      "Step: 2700  \tTraining accuracy: 0.8933526873588562\n",
      "Step: 2700  \tValid loss: 0.18303360044956207\n",
      "Step: 2800  \tTraining loss: 0.09649822115898132\n",
      "Step: 2800  \tTraining accuracy: 0.8946024775505066\n",
      "Step: 2800  \tValid loss: 0.18385587632656097\n",
      "Step: 2900  \tTraining loss: 0.09448215365409851\n",
      "Step: 2900  \tTraining accuracy: 0.8958821892738342\n",
      "Step: 2900  \tValid loss: 0.1850261390209198\n",
      "Step: 3000  \tTraining loss: 0.09262991696596146\n",
      "Step: 3000  \tTraining accuracy: 0.8970754742622375\n",
      "Step: 3000  \tValid loss: 0.186146542429924\n",
      "Step: 3100  \tTraining loss: 0.09092311561107635\n",
      "Step: 3100  \tTraining accuracy: 0.8981907963752747\n",
      "Step: 3100  \tValid loss: 0.18755684792995453\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8992355\n",
      "Precision: 0.98058254\n",
      "Recall: 0.9758454\n",
      "F1 score: 0.8970582\n",
      "AUC: 0.9748508\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.899235   0.980583  0.975845  0.897058  0.974851  0.089771      0.897759   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.181003       0.897604   0.439307      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3171.0  \n",
      "Subject30\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.586138904094696\n",
      "Step: 100  \tTraining accuracy: 0.605555534362793\n",
      "Step: 100  \tValid loss: 0.5843035578727722\n",
      "Step: 200  \tTraining loss: 0.460068941116333\n",
      "Step: 200  \tTraining accuracy: 0.6569037437438965\n",
      "Step: 200  \tValid loss: 0.4425883889198303\n",
      "Step: 300  \tTraining loss: 0.41973528265953064\n",
      "Step: 300  \tTraining accuracy: 0.6990979313850403\n",
      "Step: 300  \tValid loss: 0.4095824062824249\n",
      "Step: 400  \tTraining loss: 0.4012603759765625\n",
      "Step: 400  \tTraining accuracy: 0.723929226398468\n",
      "Step: 400  \tValid loss: 0.40282684564590454\n",
      "Step: 500  \tTraining loss: 0.3922218680381775\n",
      "Step: 500  \tTraining accuracy: 0.7408891916275024\n",
      "Step: 500  \tValid loss: 0.4006139934062958\n",
      "Step: 600  \tTraining loss: 0.3856658935546875\n",
      "Step: 600  \tTraining accuracy: 0.7517964243888855\n",
      "Step: 600  \tValid loss: 0.3949277102947235\n",
      "Step: 700  \tTraining loss: 0.3789221942424774\n",
      "Step: 700  \tTraining accuracy: 0.7583841681480408\n",
      "Step: 700  \tValid loss: 0.3862862288951874\n",
      "Step: 800  \tTraining loss: 0.37403368949890137\n",
      "Step: 800  \tTraining accuracy: 0.7643424272537231\n",
      "Step: 800  \tValid loss: 0.37925851345062256\n",
      "Step: 900  \tTraining loss: 0.36969587206840515\n",
      "Step: 900  \tTraining accuracy: 0.7685257196426392\n",
      "Step: 900  \tValid loss: 0.3725251257419586\n",
      "Step: 1000  \tTraining loss: 0.3663015067577362\n",
      "Step: 1000  \tTraining accuracy: 0.7723619937896729\n",
      "Step: 1000  \tValid loss: 0.3675577640533447\n",
      "Step: 1100  \tTraining loss: 0.36342212557792664\n",
      "Step: 1100  \tTraining accuracy: 0.7753164768218994\n",
      "Step: 1100  \tValid loss: 0.363448828458786\n",
      "Step: 1200  \tTraining loss: 0.36102020740509033\n",
      "Step: 1200  \tTraining accuracy: 0.7780508995056152\n",
      "Step: 1200  \tValid loss: 0.36064520478248596\n",
      "Step: 1300  \tTraining loss: 0.35891976952552795\n",
      "Step: 1300  \tTraining accuracy: 0.780351459980011\n",
      "Step: 1300  \tValid loss: 0.3586024045944214\n",
      "Step: 1400  \tTraining loss: 0.3570748269557953\n",
      "Step: 1400  \tTraining accuracy: 0.7824370861053467\n",
      "Step: 1400  \tValid loss: 0.3570213317871094\n",
      "Step: 1500  \tTraining loss: 0.35543292760849\n",
      "Step: 1500  \tTraining accuracy: 0.7844669222831726\n",
      "Step: 1500  \tValid loss: 0.35590922832489014\n",
      "Step: 1600  \tTraining loss: 0.3539291024208069\n",
      "Step: 1600  \tTraining accuracy: 0.786236584186554\n",
      "Step: 1600  \tValid loss: 0.3551870882511139\n",
      "Step: 1700  \tTraining loss: 0.3525075614452362\n",
      "Step: 1700  \tTraining accuracy: 0.7876920104026794\n",
      "Step: 1700  \tValid loss: 0.3546704351902008\n",
      "Step: 1800  \tTraining loss: 0.35114848613739014\n",
      "Step: 1800  \tTraining accuracy: 0.788982093334198\n",
      "Step: 1800  \tValid loss: 0.35418471693992615\n",
      "Step: 1900  \tTraining loss: 0.34983184933662415\n",
      "Step: 1900  \tTraining accuracy: 0.7902236580848694\n",
      "Step: 1900  \tValid loss: 0.35370224714279175\n",
      "Step: 2000  \tTraining loss: 0.3485322594642639\n",
      "Step: 2000  \tTraining accuracy: 0.7913385629653931\n",
      "Step: 2000  \tValid loss: 0.3534351885318756\n",
      "Step: 2100  \tTraining loss: 0.34722477197647095\n",
      "Step: 2100  \tTraining accuracy: 0.7925896048545837\n",
      "Step: 2100  \tValid loss: 0.353280246257782\n",
      "Step: 2200  \tTraining loss: 0.3458804488182068\n",
      "Step: 2200  \tTraining accuracy: 0.7935694456100464\n",
      "Step: 2200  \tValid loss: 0.353240966796875\n",
      "Step: 2300  \tTraining loss: 0.34447985887527466\n",
      "Step: 2300  \tTraining accuracy: 0.7944625616073608\n",
      "Step: 2300  \tValid loss: 0.3533369302749634\n",
      "Step: 2400  \tTraining loss: 0.3430020213127136\n",
      "Step: 2400  \tTraining accuracy: 0.7952800393104553\n",
      "Step: 2400  \tValid loss: 0.35350897908210754\n",
      "Step: 2500  \tTraining loss: 0.34142330288887024\n",
      "Step: 2500  \tTraining accuracy: 0.7961674928665161\n",
      "Step: 2500  \tValid loss: 0.3538588881492615\n",
      "Step: 2600  \tTraining loss: 0.3397241532802582\n",
      "Step: 2600  \tTraining accuracy: 0.7969200611114502\n",
      "Step: 2600  \tValid loss: 0.3543882369995117\n",
      "Step: 2700  \tTraining loss: 0.33790460228919983\n",
      "Step: 2700  \tTraining accuracy: 0.7978052496910095\n",
      "Step: 2700  \tValid loss: 0.35507795214653015\n",
      "Step: 2800  \tTraining loss: 0.33596399426460266\n",
      "Step: 2800  \tTraining accuracy: 0.7985655069351196\n",
      "Step: 2800  \tValid loss: 0.35604700446128845\n",
      "Step: 2900  \tTraining loss: 0.3339241147041321\n",
      "Step: 2900  \tTraining accuracy: 0.7993899583816528\n",
      "Step: 2900  \tValid loss: 0.35718148946762085\n",
      "Step: 3000  \tTraining loss: 0.3318018317222595\n",
      "Step: 3000  \tTraining accuracy: 0.8002153635025024\n",
      "Step: 3000  \tValid loss: 0.35860782861709595\n",
      "Step: 3100  \tTraining loss: 0.32963258028030396\n",
      "Step: 3100  \tTraining accuracy: 0.8009868264198303\n",
      "Step: 3100  \tValid loss: 0.3603907525539398\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8017626\n",
      "Precision: 0.8786408\n",
      "Recall: 0.8418605\n",
      "F1 score: 0.7846228\n",
      "AUC: 0.83472335\n",
      "   accuracy  precision   recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.801763   0.878641  0.84186  0.784623  0.834723  0.328655      0.801093   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.353235       0.800817   0.611572      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3144.0  \n",
      "Subject31\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.1794838309288025\n",
      "Step: 100  \tTraining accuracy: 0.9611111283302307\n",
      "Step: 100  \tValid loss: 0.23754625022411346\n",
      "Step: 200  \tTraining loss: 0.16582417488098145\n",
      "Step: 200  \tTraining accuracy: 0.9612970948219299\n",
      "Step: 200  \tValid loss: 0.24529443681240082\n",
      "Step: 300  \tTraining loss: 0.16361859440803528\n",
      "Step: 300  \tTraining accuracy: 0.9613401889801025\n",
      "Step: 300  \tValid loss: 0.2443084716796875\n",
      "Step: 400  \tTraining loss: 0.1617574840784073\n",
      "Step: 400  \tTraining accuracy: 0.9613593816757202\n",
      "Step: 400  \tValid loss: 0.24301667511463165\n",
      "Step: 500  \tTraining loss: 0.16027569770812988\n",
      "Step: 500  \tTraining accuracy: 0.9613702893257141\n",
      "Step: 500  \tValid loss: 0.2421851009130478\n",
      "Step: 600  \tTraining loss: 0.1591435819864273\n",
      "Step: 600  \tTraining accuracy: 0.9613772630691528\n",
      "Step: 600  \tValid loss: 0.24173472821712494\n",
      "Step: 700  \tTraining loss: 0.1582956314086914\n",
      "Step: 700  \tTraining accuracy: 0.9613820910453796\n",
      "Step: 700  \tValid loss: 0.24153739213943481\n",
      "Step: 800  \tTraining loss: 0.1576528251171112\n",
      "Step: 800  \tTraining accuracy: 0.9613857269287109\n",
      "Step: 800  \tValid loss: 0.24145661294460297\n",
      "Step: 900  \tTraining loss: 0.15711838006973267\n",
      "Step: 900  \tTraining accuracy: 0.9613884687423706\n",
      "Step: 900  \tValid loss: 0.24133557081222534\n",
      "Step: 1000  \tTraining loss: 0.1565336436033249\n",
      "Step: 1000  \tTraining accuracy: 0.9613906145095825\n",
      "Step: 1000  \tValid loss: 0.24102289974689484\n",
      "Step: 1100  \tTraining loss: 0.15590769052505493\n",
      "Step: 1100  \tTraining accuracy: 0.9613924026489258\n",
      "Step: 1100  \tValid loss: 0.2407727688550949\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.9613939\n",
      "Precision: 0.9611111\n",
      "Recall: 1.0\n",
      "F1 score: 0.9804584\n",
      "AUC: 0.49999997\n",
      "   accuracy  precision  recall  f1_score  auc      loss  accuracy_val  \\\n",
      "0  0.961394   0.961111     1.0  0.980458  0.5  0.155872      0.961409   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.237289       0.961957   0.126279      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  1105.0  \n",
      "Subject32\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.5676628947257996\n",
      "Step: 100  \tTraining accuracy: 0.7111111283302307\n",
      "Step: 100  \tValid loss: 0.5453969240188599\n",
      "Step: 200  \tTraining loss: 0.5249466896057129\n",
      "Step: 200  \tTraining accuracy: 0.7426778078079224\n",
      "Step: 200  \tValid loss: 0.4818840026855469\n",
      "Step: 300  \tTraining loss: 0.4999833405017853\n",
      "Step: 300  \tTraining accuracy: 0.7661082744598389\n",
      "Step: 300  \tValid loss: 0.45589715242385864\n",
      "Step: 400  \tTraining loss: 0.4764103293418884\n",
      "Step: 400  \tTraining accuracy: 0.7797951698303223\n",
      "Step: 400  \tValid loss: 0.433569073677063\n",
      "Step: 500  \tTraining loss: 0.44823747873306274\n",
      "Step: 500  \tTraining accuracy: 0.7893586158752441\n",
      "Step: 500  \tValid loss: 0.4091675579547882\n",
      "Step: 600  \tTraining loss: 0.4129650592803955\n",
      "Step: 600  \tTraining accuracy: 0.7973054051399231\n",
      "Step: 600  \tValid loss: 0.38904479146003723\n",
      "Step: 700  \tTraining loss: 0.379594087600708\n",
      "Step: 700  \tTraining accuracy: 0.8046239614486694\n",
      "Step: 700  \tValid loss: 0.36863282322883606\n",
      "Step: 800  \tTraining loss: 0.3504244387149811\n",
      "Step: 800  \tTraining accuracy: 0.8106796145439148\n",
      "Step: 800  \tValid loss: 0.3509559631347656\n",
      "Step: 900  \tTraining loss: 0.32828009128570557\n",
      "Step: 900  \tTraining accuracy: 0.8155226111412048\n",
      "Step: 900  \tValid loss: 0.33918634057044983\n",
      "Step: 1000  \tTraining loss: 0.3123633563518524\n",
      "Step: 1000  \tTraining accuracy: 0.8200559020042419\n",
      "Step: 1000  \tValid loss: 0.33309245109558105\n",
      "Step: 1100  \tTraining loss: 0.30113485455513\n",
      "Step: 1100  \tTraining accuracy: 0.8245252966880798\n",
      "Step: 1100  \tValid loss: 0.3286733627319336\n",
      "Step: 1200  \tTraining loss: 0.2931534945964813\n",
      "Step: 1200  \tTraining accuracy: 0.8279352188110352\n",
      "Step: 1200  \tValid loss: 0.3279954195022583\n",
      "Step: 1300  \tTraining loss: 0.2871595025062561\n",
      "Step: 1300  \tTraining accuracy: 0.8313365578651428\n",
      "Step: 1300  \tValid loss: 0.32891419529914856\n",
      "Step: 1400  \tTraining loss: 0.2823898494243622\n",
      "Step: 1400  \tTraining accuracy: 0.8346077799797058\n",
      "Step: 1400  \tValid loss: 0.3305734097957611\n",
      "Step: 1500  \tTraining loss: 0.27854982018470764\n",
      "Step: 1500  \tTraining accuracy: 0.8374310731887817\n",
      "Step: 1500  \tValid loss: 0.33247047662734985\n",
      "Step: 1600  \tTraining loss: 0.27551954984664917\n",
      "Step: 1600  \tTraining accuracy: 0.8398924469947815\n",
      "Step: 1600  \tValid loss: 0.33359864354133606\n",
      "Step: 1700  \tTraining loss: 0.273071825504303\n",
      "Step: 1700  \tTraining accuracy: 0.8419563174247742\n",
      "Step: 1700  \tValid loss: 0.33516108989715576\n",
      "Step: 1800  \tTraining loss: 0.2710314095020294\n",
      "Step: 1800  \tTraining accuracy: 0.8437857627868652\n",
      "Step: 1800  \tValid loss: 0.3369349539279938\n",
      "Step: 1900  \tTraining loss: 0.2691923677921295\n",
      "Step: 1900  \tTraining accuracy: 0.8451479077339172\n",
      "Step: 1900  \tValid loss: 0.3388773202896118\n",
      "Step: 2000  \tTraining loss: 0.2672962546348572\n",
      "Step: 2000  \tTraining accuracy: 0.8464567065238953\n",
      "Step: 2000  \tValid loss: 0.3414328992366791\n",
      "Step: 2100  \tTraining loss: 0.2644585072994232\n",
      "Step: 2100  \tTraining accuracy: 0.8477198481559753\n",
      "Step: 2100  \tValid loss: 0.34644463658332825\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.84902143\n",
      "Precision: 0.90612245\n",
      "Recall: 0.9288703\n",
      "F1 score: 0.8595465\n",
      "AUC: 0.86939377\n",
      "   accuracy  precision   recall  f1_score       auc     loss  accuracy_val  \\\n",
      "0  0.849021   0.906122  0.92887  0.859546  0.869394  0.26268      0.847875   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0   0.32767       0.848226    0.29152      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  2186.0  \n",
      "Subject33\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.578799843788147\n",
      "Step: 100  \tTraining accuracy: 0.7166666388511658\n",
      "Step: 100  \tValid loss: 0.6363555788993835\n",
      "Step: 200  \tTraining loss: 0.5349095463752747\n",
      "Step: 200  \tTraining accuracy: 0.7133890986442566\n",
      "Step: 200  \tValid loss: 0.6056904792785645\n",
      "Step: 300  \tTraining loss: 0.5020952820777893\n",
      "Step: 300  \tTraining accuracy: 0.7261598110198975\n",
      "Step: 300  \tValid loss: 0.5761403441429138\n",
      "Step: 400  \tTraining loss: 0.47435909509658813\n",
      "Step: 400  \tTraining accuracy: 0.7364990711212158\n",
      "Step: 400  \tValid loss: 0.547425389289856\n",
      "Step: 500  \tTraining loss: 0.45643487572669983\n",
      "Step: 500  \tTraining accuracy: 0.7474489808082581\n",
      "Step: 500  \tValid loss: 0.5274446606636047\n",
      "Step: 600  \tTraining loss: 0.4459269940853119\n",
      "Step: 600  \tTraining accuracy: 0.7559880018234253\n",
      "Step: 600  \tValid loss: 0.5194739699363708\n",
      "Step: 700  \tTraining loss: 0.43858015537261963\n",
      "Step: 700  \tTraining accuracy: 0.7624492049217224\n",
      "Step: 700  \tValid loss: 0.5151904821395874\n",
      "Step: 800  \tTraining loss: 0.4327373504638672\n",
      "Step: 800  \tTraining accuracy: 0.7674316167831421\n",
      "Step: 800  \tValid loss: 0.5126671195030212\n",
      "Step: 900  \tTraining loss: 0.4274044334888458\n",
      "Step: 900  \tTraining accuracy: 0.7714508771896362\n",
      "Step: 900  \tValid loss: 0.5097182393074036\n",
      "Step: 1000  \tTraining loss: 0.4223915934562683\n",
      "Step: 1000  \tTraining accuracy: 0.7751572132110596\n",
      "Step: 1000  \tValid loss: 0.5056058764457703\n",
      "Step: 1100  \tTraining loss: 0.4175105094909668\n",
      "Step: 1100  \tTraining accuracy: 0.7783227562904358\n",
      "Step: 1100  \tValid loss: 0.5004634261131287\n",
      "Step: 1200  \tTraining loss: 0.41258642077445984\n",
      "Step: 1200  \tTraining accuracy: 0.7809427380561829\n",
      "Step: 1200  \tValid loss: 0.49484124779701233\n",
      "Step: 1300  \tTraining loss: 0.40758153796195984\n",
      "Step: 1300  \tTraining accuracy: 0.7834132313728333\n",
      "Step: 1300  \tValid loss: 0.4902578890323639\n",
      "Step: 1400  \tTraining loss: 0.4025358557701111\n",
      "Step: 1400  \tTraining accuracy: 0.785767138004303\n",
      "Step: 1400  \tValid loss: 0.48819035291671753\n",
      "Step: 1500  \tTraining loss: 0.39756545424461365\n",
      "Step: 1500  \tTraining accuracy: 0.7875689268112183\n",
      "Step: 1500  \tValid loss: 0.48763853311538696\n",
      "Step: 1600  \tTraining loss: 0.39298608899116516\n",
      "Step: 1600  \tTraining accuracy: 0.7891398072242737\n",
      "Step: 1600  \tValid loss: 0.4875030517578125\n",
      "Step: 1700  \tTraining loss: 0.38848140835762024\n",
      "Step: 1700  \tTraining accuracy: 0.7905214428901672\n",
      "Step: 1700  \tValid loss: 0.4872611463069916\n",
      "Step: 1800  \tTraining loss: 0.38437169790267944\n",
      "Step: 1800  \tTraining accuracy: 0.7918413877487183\n",
      "Step: 1800  \tValid loss: 0.4891097843647003\n",
      "Step: 1900  \tTraining loss: 0.3809363842010498\n",
      "Step: 1900  \tTraining accuracy: 0.793109655380249\n",
      "Step: 1900  \tValid loss: 0.4914965033531189\n",
      "Step: 2000  \tTraining loss: 0.3779517412185669\n",
      "Step: 2000  \tTraining accuracy: 0.7942485213279724\n",
      "Step: 2000  \tValid loss: 0.49433258175849915\n",
      "Step: 2100  \tTraining loss: 0.3751992881298065\n",
      "Step: 2100  \tTraining accuracy: 0.7952768802642822\n",
      "Step: 2100  \tValid loss: 0.49649325013160706\n",
      "Step: 2200  \tTraining loss: 0.37253761291503906\n",
      "Step: 2200  \tTraining accuracy: 0.796209990978241\n",
      "Step: 2200  \tValid loss: 0.49808838963508606\n",
      "Step: 2300  \tTraining loss: 0.36987197399139404\n",
      "Step: 2300  \tTraining accuracy: 0.7971348166465759\n",
      "Step: 2300  \tValid loss: 0.49893805384635925\n",
      "Step: 2400  \tTraining loss: 0.36719173192977905\n",
      "Step: 2400  \tTraining accuracy: 0.7981234192848206\n",
      "Step: 2400  \tValid loss: 0.5000108480453491\n",
      "Step: 2500  \tTraining loss: 0.3644871115684509\n",
      "Step: 2500  \tTraining accuracy: 0.7988952398300171\n",
      "Step: 2500  \tValid loss: 0.5013651251792908\n",
      "Step: 2600  \tTraining loss: 0.3617967963218689\n",
      "Step: 2600  \tTraining accuracy: 0.7996723651885986\n",
      "Step: 2600  \tValid loss: 0.5029658079147339\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8004541\n",
      "Precision: 0.8088235\n",
      "Recall: 0.7746479\n",
      "F1 score: 0.7831781\n",
      "AUC: 0.82769096\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.800454   0.808824  0.774648  0.783178  0.827691  0.359381      0.799561   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.487131       0.799675   0.444899      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  2691.0  \n",
      "Subject34\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.5862481594085693\n",
      "Step: 100  \tTraining accuracy: 0.7166666388511658\n",
      "Step: 100  \tValid loss: 0.6382973790168762\n",
      "Step: 200  \tTraining loss: 0.539129376411438\n",
      "Step: 200  \tTraining accuracy: 0.7071129679679871\n",
      "Step: 200  \tValid loss: 0.5919049978256226\n",
      "Step: 300  \tTraining loss: 0.4742799401283264\n",
      "Step: 300  \tTraining accuracy: 0.7048969268798828\n",
      "Step: 300  \tValid loss: 0.5337940454483032\n",
      "Step: 400  \tTraining loss: 0.40985387563705444\n",
      "Step: 400  \tTraining accuracy: 0.7206704020500183\n",
      "Step: 400  \tValid loss: 0.4783951938152313\n",
      "Step: 500  \tTraining loss: 0.36293309926986694\n",
      "Step: 500  \tTraining accuracy: 0.7434402108192444\n",
      "Step: 500  \tValid loss: 0.4327833354473114\n",
      "Step: 600  \tTraining loss: 0.3345664441585541\n",
      "Step: 600  \tTraining accuracy: 0.7625748515129089\n",
      "Step: 600  \tValid loss: 0.4041639268398285\n",
      "Step: 700  \tTraining loss: 0.3169509172439575\n",
      "Step: 700  \tTraining accuracy: 0.7779471278190613\n",
      "Step: 700  \tValid loss: 0.3858877420425415\n",
      "Step: 800  \tTraining loss: 0.30525073409080505\n",
      "Step: 800  \tTraining accuracy: 0.790820837020874\n",
      "Step: 800  \tValid loss: 0.37390780448913574\n",
      "Step: 900  \tTraining loss: 0.29680487513542175\n",
      "Step: 900  \tTraining accuracy: 0.8010920286178589\n",
      "Step: 900  \tValid loss: 0.3659551441669464\n",
      "Step: 1000  \tTraining loss: 0.2904566824436188\n",
      "Step: 1000  \tTraining accuracy: 0.8097484111785889\n",
      "Step: 1000  \tValid loss: 0.3605688810348511\n",
      "Step: 1100  \tTraining loss: 0.28563106060028076\n",
      "Step: 1100  \tTraining accuracy: 0.81693035364151\n",
      "Step: 1100  \tValid loss: 0.3567662537097931\n",
      "Step: 1200  \tTraining loss: 0.28168079257011414\n",
      "Step: 1200  \tTraining accuracy: 0.8230190873146057\n",
      "Step: 1200  \tValid loss: 0.35451540350914\n",
      "Step: 1300  \tTraining loss: 0.27861982583999634\n",
      "Step: 1300  \tTraining accuracy: 0.8280085325241089\n",
      "Step: 1300  \tValid loss: 0.35240834951400757\n",
      "Step: 1400  \tTraining loss: 0.2761811912059784\n",
      "Step: 1400  \tTraining accuracy: 0.8325111269950867\n",
      "Step: 1400  \tValid loss: 0.35092034935951233\n",
      "Step: 1500  \tTraining loss: 0.274230033159256\n",
      "Step: 1500  \tTraining accuracy: 0.8362821936607361\n",
      "Step: 1500  \tValid loss: 0.3501705229282379\n",
      "Step: 1600  \tTraining loss: 0.27266761660575867\n",
      "Step: 1600  \tTraining accuracy: 0.8395698666572571\n",
      "Step: 1600  \tValid loss: 0.34981676936149597\n",
      "Step: 1700  \tTraining loss: 0.27137649059295654\n",
      "Step: 1700  \tTraining accuracy: 0.8424615859985352\n",
      "Step: 1700  \tValid loss: 0.3499148190021515\n",
      "Step: 1800  \tTraining loss: 0.27029797434806824\n",
      "Step: 1800  \tTraining accuracy: 0.8450247645378113\n",
      "Step: 1800  \tValid loss: 0.35017985105514526\n",
      "Step: 1900  \tTraining loss: 0.2693754732608795\n",
      "Step: 1900  \tTraining accuracy: 0.8474025726318359\n",
      "Step: 1900  \tValid loss: 0.3507727384567261\n",
      "Step: 2000  \tTraining loss: 0.2685851454734802\n",
      "Step: 2000  \tTraining accuracy: 0.8495378494262695\n",
      "Step: 2000  \tValid loss: 0.3514305055141449\n",
      "Step: 2100  \tTraining loss: 0.2679295837879181\n",
      "Step: 2100  \tTraining accuracy: 0.8514658212661743\n",
      "Step: 2100  \tValid loss: 0.3517930805683136\n",
      "Step: 2200  \tTraining loss: 0.2673535943031311\n",
      "Step: 2200  \tTraining accuracy: 0.8532152771949768\n",
      "Step: 2200  \tValid loss: 0.3524147570133209\n",
      "Step: 2300  \tTraining loss: 0.2668522000312805\n",
      "Step: 2300  \tTraining accuracy: 0.8548842072486877\n",
      "Step: 2300  \tValid loss: 0.35281306505203247\n",
      "Step: 2400  \tTraining loss: 0.26641324162483215\n",
      "Step: 2400  \tTraining accuracy: 0.8564116954803467\n",
      "Step: 2400  \tValid loss: 0.35307419300079346\n",
      "Step: 2500  \tTraining loss: 0.26601460576057434\n",
      "Step: 2500  \tTraining accuracy: 0.8577468395233154\n",
      "Step: 2500  \tValid loss: 0.3533845543861389\n",
      "Step: 2600  \tTraining loss: 0.2656480669975281\n",
      "Step: 2600  \tTraining accuracy: 0.858977735042572\n",
      "Step: 2600  \tValid loss: 0.3537185490131378\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.86011606\n",
      "Precision: 0.9233717\n",
      "Recall: 0.93410856\n",
      "F1 score: 0.8650878\n",
      "AUC: 0.86901504\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.860116   0.923372  0.934109  0.865088  0.869015  0.265512      0.859254   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.349757       0.859103   0.380179      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  2638.0  \n",
      "Subject35\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.6071968078613281\n",
      "Step: 100  \tTraining accuracy: 0.675000011920929\n",
      "Step: 100  \tValid loss: 0.6366688013076782\n",
      "Step: 200  \tTraining loss: 0.5354129672050476\n",
      "Step: 200  \tTraining accuracy: 0.7018828392028809\n",
      "Step: 200  \tValid loss: 0.5730108022689819\n",
      "Step: 300  \tTraining loss: 0.46252551674842834\n",
      "Step: 300  \tTraining accuracy: 0.7242268323898315\n",
      "Step: 300  \tValid loss: 0.49421969056129456\n",
      "Step: 400  \tTraining loss: 0.38788264989852905\n",
      "Step: 400  \tTraining accuracy: 0.75698322057724\n",
      "Step: 400  \tValid loss: 0.42578965425491333\n",
      "Step: 500  \tTraining loss: 0.3248918056488037\n",
      "Step: 500  \tTraining accuracy: 0.7827988266944885\n",
      "Step: 500  \tValid loss: 0.3883979022502899\n",
      "Step: 600  \tTraining loss: 0.28233349323272705\n",
      "Step: 600  \tTraining accuracy: 0.8047904372215271\n",
      "Step: 600  \tValid loss: 0.37472569942474365\n",
      "Step: 700  \tTraining loss: 0.25321605801582336\n",
      "Step: 700  \tTraining accuracy: 0.8206300735473633\n",
      "Step: 700  \tValid loss: 0.3706687092781067\n",
      "Step: 800  \tTraining loss: 0.23309366405010223\n",
      "Step: 800  \tTraining accuracy: 0.8334068655967712\n",
      "Step: 800  \tValid loss: 0.3741953670978546\n",
      "Step: 900  \tTraining loss: 0.21838319301605225\n",
      "Step: 900  \tTraining accuracy: 0.8436037302017212\n",
      "Step: 900  \tValid loss: 0.38181039690971375\n",
      "Step: 1000  \tTraining loss: 0.2070247232913971\n",
      "Step: 1000  \tTraining accuracy: 0.8520265817642212\n",
      "Step: 1000  \tValid loss: 0.3918987214565277\n",
      "Step: 1100  \tTraining loss: 0.19804351031780243\n",
      "Step: 1100  \tTraining accuracy: 0.8590189814567566\n",
      "Step: 1100  \tValid loss: 0.4008539915084839\n",
      "Step: 1200  \tTraining loss: 0.19040068984031677\n",
      "Step: 1200  \tTraining accuracy: 0.8649508357048035\n",
      "Step: 1200  \tValid loss: 0.4089493453502655\n",
      "Step: 1300  \tTraining loss: 0.18412543833255768\n",
      "Step: 1300  \tTraining accuracy: 0.870207667350769\n",
      "Step: 1300  \tValid loss: 0.41658756136894226\n",
      "Step: 1400  \tTraining loss: 0.17874319851398468\n",
      "Step: 1400  \tTraining accuracy: 0.8748149871826172\n",
      "Step: 1400  \tValid loss: 0.4254007339477539\n",
      "Step: 1500  \tTraining loss: 0.1740073561668396\n",
      "Step: 1500  \tTraining accuracy: 0.87890625\n",
      "Step: 1500  \tValid loss: 0.43202680349349976\n",
      "Step: 1600  \tTraining loss: 0.16969949007034302\n",
      "Step: 1600  \tTraining accuracy: 0.8825806379318237\n",
      "Step: 1600  \tValid loss: 0.4383666515350342\n",
      "Step: 1700  \tTraining loss: 0.16574686765670776\n",
      "Step: 1700  \tTraining accuracy: 0.8858124613761902\n",
      "Step: 1700  \tValid loss: 0.4456760883331299\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.88867706\n",
      "Precision: 0.9476744\n",
      "Recall: 0.97604793\n",
      "F1 score: 0.9017844\n",
      "AUC: 0.964708\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.888677   0.947674  0.976048  0.901784  0.964708  0.165708      0.886005   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.370669       0.886382   0.158263      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  1700.0  \n",
      "Subject36\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.6126810312271118\n",
      "Step: 100  \tTraining accuracy: 0.6166666746139526\n",
      "Step: 100  \tValid loss: 0.6550641655921936\n",
      "Step: 200  \tTraining loss: 0.5311688780784607\n",
      "Step: 200  \tTraining accuracy: 0.6443514823913574\n",
      "Step: 200  \tValid loss: 0.5763233304023743\n",
      "Step: 300  \tTraining loss: 0.49339988827705383\n",
      "Step: 300  \tTraining accuracy: 0.6804123520851135\n",
      "Step: 300  \tValid loss: 0.555217981338501\n",
      "Step: 400  \tTraining loss: 0.4554187059402466\n",
      "Step: 400  \tTraining accuracy: 0.697858452796936\n",
      "Step: 400  \tValid loss: 0.5248146653175354\n",
      "Step: 500  \tTraining loss: 0.41757866740226746\n",
      "Step: 500  \tTraining accuracy: 0.7120991349220276\n",
      "Step: 500  \tValid loss: 0.4879811704158783\n",
      "Step: 600  \tTraining loss: 0.383341908454895\n",
      "Step: 600  \tTraining accuracy: 0.727544903755188\n",
      "Step: 600  \tValid loss: 0.45206528902053833\n",
      "Step: 700  \tTraining loss: 0.3563407361507416\n",
      "Step: 700  \tTraining accuracy: 0.7413617968559265\n",
      "Step: 700  \tValid loss: 0.4233149290084839\n",
      "Step: 800  \tTraining loss: 0.3362569510936737\n",
      "Step: 800  \tTraining accuracy: 0.7535304427146912\n",
      "Step: 800  \tValid loss: 0.40273934602737427\n",
      "Step: 900  \tTraining loss: 0.32108837366104126\n",
      "Step: 900  \tTraining accuracy: 0.7652106285095215\n",
      "Step: 900  \tValid loss: 0.3872964382171631\n",
      "Step: 1000  \tTraining loss: 0.30980345606803894\n",
      "Step: 1000  \tTraining accuracy: 0.7742837071418762\n",
      "Step: 1000  \tValid loss: 0.37570032477378845\n",
      "Step: 1100  \tTraining loss: 0.3012446463108063\n",
      "Step: 1100  \tTraining accuracy: 0.7821202278137207\n",
      "Step: 1100  \tValid loss: 0.3676029443740845\n",
      "Step: 1200  \tTraining loss: 0.2946045398712158\n",
      "Step: 1200  \tTraining accuracy: 0.7893291115760803\n",
      "Step: 1200  \tValid loss: 0.36181485652923584\n",
      "Step: 1300  \tTraining loss: 0.2893717586994171\n",
      "Step: 1300  \tTraining accuracy: 0.7955271601676941\n",
      "Step: 1300  \tValid loss: 0.3577628433704376\n",
      "Step: 1400  \tTraining loss: 0.2851712107658386\n",
      "Step: 1400  \tTraining accuracy: 0.8011839985847473\n",
      "Step: 1400  \tValid loss: 0.35469844937324524\n",
      "Step: 1500  \tTraining loss: 0.2817326784133911\n",
      "Step: 1500  \tTraining accuracy: 0.8059512972831726\n",
      "Step: 1500  \tValid loss: 0.3523947596549988\n",
      "Step: 1600  \tTraining loss: 0.27887365221977234\n",
      "Step: 1600  \tTraining accuracy: 0.8103225827217102\n",
      "Step: 1600  \tValid loss: 0.3507595658302307\n",
      "Step: 1700  \tTraining loss: 0.2764640748500824\n",
      "Step: 1700  \tTraining accuracy: 0.814167320728302\n",
      "Step: 1700  \tValid loss: 0.3496364653110504\n",
      "Step: 1800  \tTraining loss: 0.2744062840938568\n",
      "Step: 1800  \tTraining accuracy: 0.8176705837249756\n",
      "Step: 1800  \tValid loss: 0.3489483594894409\n",
      "Step: 1900  \tTraining loss: 0.27263081073760986\n",
      "Step: 1900  \tTraining accuracy: 0.8207972645759583\n",
      "Step: 1900  \tValid loss: 0.3485705852508545\n",
      "Step: 2000  \tTraining loss: 0.2710787355899811\n",
      "Step: 2000  \tTraining accuracy: 0.823604941368103\n",
      "Step: 2000  \tValid loss: 0.348444402217865\n",
      "Step: 2100  \tTraining loss: 0.2697038948535919\n",
      "Step: 2100  \tTraining accuracy: 0.8261400461196899\n",
      "Step: 2100  \tValid loss: 0.3485318422317505\n",
      "Step: 2200  \tTraining loss: 0.2684708535671234\n",
      "Step: 2200  \tTraining accuracy: 0.8284404873847961\n",
      "Step: 2200  \tValid loss: 0.3488190174102783\n",
      "Step: 2300  \tTraining loss: 0.26735034584999084\n",
      "Step: 2300  \tTraining accuracy: 0.830463171005249\n",
      "Step: 2300  \tValid loss: 0.34928107261657715\n",
      "Step: 2400  \tTraining loss: 0.266316682100296\n",
      "Step: 2400  \tTraining accuracy: 0.8321723341941833\n",
      "Step: 2400  \tValid loss: 0.34989455342292786\n",
      "Step: 2500  \tTraining loss: 0.26534852385520935\n",
      "Step: 2500  \tTraining accuracy: 0.8337424993515015\n",
      "Step: 2500  \tValid loss: 0.35077759623527527\n",
      "Step: 2600  \tTraining loss: 0.26442044973373413\n",
      "Step: 2600  \tTraining accuracy: 0.8352555632591248\n",
      "Step: 2600  \tValid loss: 0.3518158495426178\n",
      "Step: 2700  \tTraining loss: 0.26349109411239624\n",
      "Step: 2700  \tTraining accuracy: 0.8365918397903442\n",
      "Step: 2700  \tValid loss: 0.3530585467815399\n",
      "Step: 2800  \tTraining loss: 0.26251107454299927\n",
      "Step: 2800  \tTraining accuracy: 0.8378312587738037\n",
      "Step: 2800  \tValid loss: 0.3545852303504944\n",
      "Step: 2900  \tTraining loss: 0.2614552080631256\n",
      "Step: 2900  \tTraining accuracy: 0.8389840722084045\n",
      "Step: 2900  \tValid loss: 0.35650476813316345\n",
      "Step: 3000  \tTraining loss: 0.26039010286331177\n",
      "Step: 3000  \tTraining accuracy: 0.8400589227676392\n",
      "Step: 3000  \tValid loss: 0.3588108420372009\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8410636\n",
      "Precision: 0.86567163\n",
      "Recall: 0.8405797\n",
      "F1 score: 0.828695\n",
      "AUC: 0.8797493\n",
      "   accuracy  precision   recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.841064   0.865672  0.84058  0.828695  0.879749  0.260277      0.840101   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.348443       0.840239   0.282951      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3010.0  \n",
      "Subject37\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.5743165612220764\n",
      "Step: 100  \tTraining accuracy: 0.7583333253860474\n",
      "Step: 100  \tValid loss: 0.5614184141159058\n",
      "Step: 200  \tTraining loss: 0.47311002016067505\n",
      "Step: 200  \tTraining accuracy: 0.776150643825531\n",
      "Step: 200  \tValid loss: 0.44840526580810547\n",
      "Step: 300  \tTraining loss: 0.43979212641716003\n",
      "Step: 300  \tTraining accuracy: 0.781572163105011\n",
      "Step: 300  \tValid loss: 0.4114534258842468\n",
      "Step: 400  \tTraining loss: 0.4114586412906647\n",
      "Step: 400  \tTraining accuracy: 0.7849162220954895\n",
      "Step: 400  \tValid loss: 0.3888949751853943\n",
      "Step: 500  \tTraining loss: 0.3804764747619629\n",
      "Step: 500  \tTraining accuracy: 0.7897230386734009\n",
      "Step: 500  \tValid loss: 0.3707333207130432\n",
      "Step: 600  \tTraining loss: 0.3513677716255188\n",
      "Step: 600  \tTraining accuracy: 0.7982035875320435\n",
      "Step: 600  \tValid loss: 0.35278695821762085\n",
      "Step: 700  \tTraining loss: 0.32553794980049133\n",
      "Step: 700  \tTraining accuracy: 0.8066564798355103\n",
      "Step: 700  \tValid loss: 0.3360750377178192\n",
      "Step: 800  \tTraining loss: 0.30217692255973816\n",
      "Step: 800  \tTraining accuracy: 0.8139894008636475\n",
      "Step: 800  \tValid loss: 0.32173946499824524\n",
      "Step: 900  \tTraining loss: 0.28066486120224\n",
      "Step: 900  \tTraining accuracy: 0.8198127746582031\n",
      "Step: 900  \tValid loss: 0.30596259236335754\n",
      "Step: 1000  \tTraining loss: 0.2599788010120392\n",
      "Step: 1000  \tTraining accuracy: 0.8252969980239868\n",
      "Step: 1000  \tValid loss: 0.28936511278152466\n",
      "Step: 1100  \tTraining loss: 0.2403932362794876\n",
      "Step: 1100  \tTraining accuracy: 0.8303797245025635\n",
      "Step: 1100  \tValid loss: 0.274008572101593\n",
      "Step: 1200  \tTraining loss: 0.2234422117471695\n",
      "Step: 1200  \tTraining accuracy: 0.8355985879898071\n",
      "Step: 1200  \tValid loss: 0.26004931330680847\n",
      "Step: 1300  \tTraining loss: 0.20974117517471313\n",
      "Step: 1300  \tTraining accuracy: 0.8406549692153931\n",
      "Step: 1300  \tValid loss: 0.25013500452041626\n",
      "Step: 1400  \tTraining loss: 0.19902916252613068\n",
      "Step: 1400  \tTraining accuracy: 0.8453379273414612\n",
      "Step: 1400  \tValid loss: 0.2417377233505249\n",
      "Step: 1500  \tTraining loss: 0.1901746541261673\n",
      "Step: 1500  \tTraining accuracy: 0.8498391509056091\n",
      "Step: 1500  \tValid loss: 0.2351708561182022\n",
      "Step: 1600  \tTraining loss: 0.1825968474149704\n",
      "Step: 1600  \tTraining accuracy: 0.8538709878921509\n",
      "Step: 1600  \tValid loss: 0.22976252436637878\n",
      "Step: 1700  \tTraining loss: 0.1759684681892395\n",
      "Step: 1700  \tTraining accuracy: 0.8573160767555237\n",
      "Step: 1700  \tValid loss: 0.22561709582805634\n",
      "Step: 1800  \tTraining loss: 0.17014560103416443\n",
      "Step: 1800  \tTraining accuracy: 0.8604651093482971\n",
      "Step: 1800  \tValid loss: 0.22195614874362946\n",
      "Step: 1900  \tTraining loss: 0.16497617959976196\n",
      "Step: 1900  \tTraining accuracy: 0.8634560108184814\n",
      "Step: 1900  \tValid loss: 0.2190464437007904\n",
      "Step: 2000  \tTraining loss: 0.16037656366825104\n",
      "Step: 2000  \tTraining accuracy: 0.8663129210472107\n",
      "Step: 2000  \tValid loss: 0.21703563630580902\n",
      "Step: 2100  \tTraining loss: 0.15626198053359985\n",
      "Step: 2100  \tTraining accuracy: 0.8690553903579712\n",
      "Step: 2100  \tValid loss: 0.2158821076154709\n",
      "Step: 2200  \tTraining loss: 0.15253190696239471\n",
      "Step: 2200  \tTraining accuracy: 0.8716216087341309\n",
      "Step: 2200  \tValid loss: 0.2150656282901764\n",
      "Step: 2300  \tTraining loss: 0.1491226851940155\n",
      "Step: 2300  \tTraining accuracy: 0.8741834759712219\n",
      "Step: 2300  \tValid loss: 0.21443261206150055\n",
      "Step: 2400  \tTraining loss: 0.14599484205245972\n",
      "Step: 2400  \tTraining accuracy: 0.876528263092041\n",
      "Step: 2400  \tValid loss: 0.2139909714460373\n",
      "Step: 2500  \tTraining loss: 0.14312760531902313\n",
      "Step: 2500  \tTraining accuracy: 0.8786824941635132\n",
      "Step: 2500  \tValid loss: 0.2137293815612793\n",
      "Step: 2600  \tTraining loss: 0.1404990255832672\n",
      "Step: 2600  \tTraining accuracy: 0.8806684017181396\n",
      "Step: 2600  \tValid loss: 0.21368204057216644\n",
      "Step: 2700  \tTraining loss: 0.13808467984199524\n",
      "Step: 2700  \tTraining accuracy: 0.8824419975280762\n",
      "Step: 2700  \tValid loss: 0.21385425329208374\n",
      "Step: 2800  \tTraining loss: 0.13583877682685852\n",
      "Step: 2800  \tTraining accuracy: 0.8840870261192322\n",
      "Step: 2800  \tValid loss: 0.21381673216819763\n",
      "Step: 2900  \tTraining loss: 0.1337854117155075\n",
      "Step: 2900  \tTraining accuracy: 0.8856170773506165\n",
      "Step: 2900  \tValid loss: 0.21461845934391022\n",
      "Step: 3000  \tTraining loss: 0.13190588355064392\n",
      "Step: 3000  \tTraining accuracy: 0.8870437741279602\n",
      "Step: 3000  \tValid loss: 0.2157638669013977\n",
      "Step: 3100  \tTraining loss: 0.13017259538173676\n",
      "Step: 3100  \tTraining accuracy: 0.8883771896362305\n",
      "Step: 3100  \tValid loss: 0.21686537563800812\n",
      "Step: 3200  \tTraining loss: 0.12855002284049988\n",
      "Step: 3200  \tTraining accuracy: 0.8895200490951538\n",
      "Step: 3200  \tValid loss: 0.21788781881332397\n",
      "Step: 3300  \tTraining loss: 0.12700457870960236\n",
      "Step: 3300  \tTraining accuracy: 0.8905928134918213\n",
      "Step: 3300  \tValid loss: 0.21894283592700958\n",
      "Step: 3400  \tTraining loss: 0.12551654875278473\n",
      "Step: 3400  \tTraining accuracy: 0.8917016386985779\n",
      "Step: 3400  \tValid loss: 0.2199280709028244\n",
      "Step: 3500  \tTraining loss: 0.12404198199510574\n",
      "Step: 3500  \tTraining accuracy: 0.8927463293075562\n",
      "Step: 3500  \tValid loss: 0.22076940536499023\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8936852\n",
      "Precision: 0.9357143\n",
      "Recall: 0.95620435\n",
      "F1 score: 0.9033641\n",
      "AUC: 0.9579228\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.893685   0.935714  0.956204  0.903364  0.957923  0.123447      0.892569   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.213654       0.892547   0.639532      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3539.0  \n",
      "Subject38\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.5555012822151184\n",
      "Step: 100  \tTraining accuracy: 0.7083333134651184\n",
      "Step: 100  \tValid loss: 0.518554151058197\n",
      "Step: 200  \tTraining loss: 0.49671512842178345\n",
      "Step: 200  \tTraining accuracy: 0.7447698712348938\n",
      "Step: 200  \tValid loss: 0.43372660875320435\n",
      "Step: 300  \tTraining loss: 0.4526170492172241\n",
      "Step: 300  \tTraining accuracy: 0.7641752362251282\n",
      "Step: 300  \tValid loss: 0.3821660280227661\n",
      "Step: 400  \tTraining loss: 0.41388002038002014\n",
      "Step: 400  \tTraining accuracy: 0.7783985137939453\n",
      "Step: 400  \tValid loss: 0.33890578150749207\n",
      "Step: 500  \tTraining loss: 0.3846780061721802\n",
      "Step: 500  \tTraining accuracy: 0.7875364422798157\n",
      "Step: 500  \tValid loss: 0.3048776388168335\n",
      "Step: 600  \tTraining loss: 0.36455392837524414\n",
      "Step: 600  \tTraining accuracy: 0.7952095866203308\n",
      "Step: 600  \tValid loss: 0.27910393476486206\n",
      "Step: 700  \tTraining loss: 0.35054445266723633\n",
      "Step: 700  \tTraining accuracy: 0.8028455376625061\n",
      "Step: 700  \tValid loss: 0.25975099205970764\n",
      "Step: 800  \tTraining loss: 0.34058114886283875\n",
      "Step: 800  \tTraining accuracy: 0.8095763325691223\n",
      "Step: 800  \tValid loss: 0.24450789391994476\n",
      "Step: 900  \tTraining loss: 0.33319512009620667\n",
      "Step: 900  \tTraining accuracy: 0.8155226111412048\n",
      "Step: 900  \tValid loss: 0.23234285414218903\n",
      "Step: 1000  \tTraining loss: 0.32748833298683167\n",
      "Step: 1000  \tTraining accuracy: 0.8212788105010986\n",
      "Step: 1000  \tValid loss: 0.22257429361343384\n",
      "Step: 1100  \tTraining loss: 0.32288116216659546\n",
      "Step: 1100  \tTraining accuracy: 0.8267405033111572\n",
      "Step: 1100  \tValid loss: 0.21455396711826324\n",
      "Step: 1200  \tTraining loss: 0.31911978125572205\n",
      "Step: 1200  \tTraining accuracy: 0.8315500020980835\n",
      "Step: 1200  \tValid loss: 0.207767054438591\n",
      "Step: 1300  \tTraining loss: 0.31573566794395447\n",
      "Step: 1300  \tTraining accuracy: 0.8359957337379456\n",
      "Step: 1300  \tValid loss: 0.20231258869171143\n",
      "Step: 1400  \tTraining loss: 0.3126961588859558\n",
      "Step: 1400  \tTraining accuracy: 0.8395411968231201\n",
      "Step: 1400  \tValid loss: 0.19805783033370972\n",
      "Step: 1500  \tTraining loss: 0.3077593445777893\n",
      "Step: 1500  \tTraining accuracy: 0.8427159786224365\n",
      "Step: 1500  \tValid loss: 0.19604922831058502\n",
      "Step: 1600  \tTraining loss: 0.3027561902999878\n",
      "Step: 1600  \tTraining accuracy: 0.8455914258956909\n",
      "Step: 1600  \tValid loss: 0.19428442418575287\n",
      "Step: 1700  \tTraining loss: 0.2998206317424774\n",
      "Step: 1700  \tTraining accuracy: 0.848221480846405\n",
      "Step: 1700  \tValid loss: 0.19000835716724396\n",
      "Step: 1800  \tTraining loss: 0.2972068786621094\n",
      "Step: 1800  \tTraining accuracy: 0.8505527973175049\n",
      "Step: 1800  \tValid loss: 0.18538150191307068\n",
      "Step: 1900  \tTraining loss: 0.2950079143047333\n",
      "Step: 1900  \tTraining accuracy: 0.8526334762573242\n",
      "Step: 1900  \tValid loss: 0.18206869065761566\n",
      "Step: 2000  \tTraining loss: 0.29288050532341003\n",
      "Step: 2000  \tTraining accuracy: 0.8545874953269958\n",
      "Step: 2000  \tValid loss: 0.17895664274692535\n",
      "Step: 2100  \tTraining loss: 0.2907465696334839\n",
      "Step: 2100  \tTraining accuracy: 0.8565146327018738\n",
      "Step: 2100  \tValid loss: 0.17621001601219177\n",
      "Step: 2200  \tTraining loss: 0.28877460956573486\n",
      "Step: 2200  \tTraining accuracy: 0.8583410978317261\n",
      "Step: 2200  \tValid loss: 0.17393670976161957\n",
      "Step: 2300  \tTraining loss: 0.2868175804615021\n",
      "Step: 2300  \tTraining accuracy: 0.8600059151649475\n",
      "Step: 2300  \tValid loss: 0.17183557152748108\n",
      "Step: 2400  \tTraining loss: 0.284860223531723\n",
      "Step: 2400  \tTraining accuracy: 0.8617429733276367\n",
      "Step: 2400  \tValid loss: 0.16999846696853638\n",
      "Step: 2500  \tTraining loss: 0.28290706872940063\n",
      "Step: 2500  \tTraining accuracy: 0.8634069561958313\n",
      "Step: 2500  \tValid loss: 0.1682068407535553\n",
      "Step: 2600  \tTraining loss: 0.2809542417526245\n",
      "Step: 2600  \tTraining accuracy: 0.8649410009384155\n",
      "Step: 2600  \tValid loss: 0.16662126779556274\n",
      "Step: 2700  \tTraining loss: 0.27901214361190796\n",
      "Step: 2700  \tTraining accuracy: 0.8663597106933594\n",
      "Step: 2700  \tValid loss: 0.16515113413333893\n",
      "Step: 2800  \tTraining loss: 0.27709531784057617\n",
      "Step: 2800  \tTraining accuracy: 0.8676756620407104\n",
      "Step: 2800  \tValid loss: 0.16380244493484497\n",
      "Step: 2900  \tTraining loss: 0.2752220630645752\n",
      "Step: 2900  \tTraining accuracy: 0.8690168857574463\n",
      "Step: 2900  \tValid loss: 0.16263321042060852\n",
      "Step: 3000  \tTraining loss: 0.2733955681324005\n",
      "Step: 3000  \tTraining accuracy: 0.8703808784484863\n",
      "Step: 3000  \tValid loss: 0.16154752671718597\n",
      "Step: 3100  \tTraining loss: 0.27161234617233276\n",
      "Step: 3100  \tTraining accuracy: 0.8716557025909424\n",
      "Step: 3100  \tValid loss: 0.16054682433605194\n",
      "Step: 3200  \tTraining loss: 0.26986023783683777\n",
      "Step: 3200  \tTraining accuracy: 0.8728498816490173\n",
      "Step: 3200  \tValid loss: 0.15959826111793518\n",
      "Step: 3300  \tTraining loss: 0.26813042163848877\n",
      "Step: 3300  \tTraining accuracy: 0.8740222454071045\n",
      "Step: 3300  \tValid loss: 0.15890200436115265\n",
      "Step: 3400  \tTraining loss: 0.26639875769615173\n",
      "Step: 3400  \tTraining accuracy: 0.8750749230384827\n",
      "Step: 3400  \tValid loss: 0.1582697182893753\n",
      "Step: 3500  \tTraining loss: 0.2646311819553375\n",
      "Step: 3500  \tTraining accuracy: 0.8760667443275452\n",
      "Step: 3500  \tValid loss: 0.15757259726524353\n",
      "Step: 3600  \tTraining loss: 0.2627078890800476\n",
      "Step: 3600  \tTraining accuracy: 0.87704998254776\n",
      "Step: 3600  \tValid loss: 0.15726405382156372\n",
      "Step: 3700  \tTraining loss: 0.2605139911174774\n",
      "Step: 3700  \tTraining accuracy: 0.8778877854347229\n",
      "Step: 3700  \tValid loss: 0.15732182562351227\n",
      "Step: 3800  \tTraining loss: 0.25811198353767395\n",
      "Step: 3800  \tTraining accuracy: 0.8785918354988098\n",
      "Step: 3800  \tValid loss: 0.15788352489471436\n",
      "Step: 3900  \tTraining loss: 0.25566366314888\n",
      "Step: 3900  \tTraining accuracy: 0.8792158961296082\n",
      "Step: 3900  \tValid loss: 0.15882571041584015\n",
      "Step: 4000  \tTraining loss: 0.2532718777656555\n",
      "Step: 4000  \tTraining accuracy: 0.8798508644104004\n",
      "Step: 4000  \tValid loss: 0.15986455976963043\n",
      "Step: 4100  \tTraining loss: 0.25072845816612244\n",
      "Step: 4100  \tTraining accuracy: 0.8804545402526855\n",
      "Step: 4100  \tValid loss: 0.16129997372627258\n",
      "Step: 4200  \tTraining loss: 0.24786679446697235\n",
      "Step: 4200  \tTraining accuracy: 0.881069540977478\n",
      "Step: 4200  \tValid loss: 0.1643005758523941\n",
      "Step: 4300  \tTraining loss: 0.2455463409423828\n",
      "Step: 4300  \tTraining accuracy: 0.8816162347793579\n",
      "Step: 4300  \tValid loss: 0.16553905606269836\n",
      "Step: 4400  \tTraining loss: 0.24347025156021118\n",
      "Step: 4400  \tTraining accuracy: 0.8821378946304321\n",
      "Step: 4400  \tValid loss: 0.1660400629043579\n",
      "Step: 4500  \tTraining loss: 0.2415316253900528\n",
      "Step: 4500  \tTraining accuracy: 0.8826361894607544\n",
      "Step: 4500  \tValid loss: 0.166252002120018\n",
      "Step: 4600  \tTraining loss: 0.23969073593616486\n",
      "Step: 4600  \tTraining accuracy: 0.8830757737159729\n",
      "Step: 4600  \tValid loss: 0.16640561819076538\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8835325\n",
      "Precision: 0.9264706\n",
      "Recall: 0.9174757\n",
      "F1 score: 0.8792226\n",
      "AUC: 0.91003656\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.883533   0.926471  0.917476  0.879223  0.910037  0.239224       0.88317   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.157147       0.882857   0.315482      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  4625.0  \n",
      "Subject39\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.6258677244186401\n",
      "Step: 100  \tTraining accuracy: 0.644444465637207\n",
      "Step: 100  \tValid loss: 0.5774765014648438\n",
      "Step: 200  \tTraining loss: 0.5898944139480591\n",
      "Step: 200  \tTraining accuracy: 0.6956067085266113\n",
      "Step: 200  \tValid loss: 0.5532135963439941\n",
      "Step: 300  \tTraining loss: 0.5546212792396545\n",
      "Step: 300  \tTraining accuracy: 0.7222937941551208\n",
      "Step: 300  \tValid loss: 0.5287563800811768\n",
      "Step: 400  \tTraining loss: 0.5112688541412354\n",
      "Step: 400  \tTraining accuracy: 0.7383612394332886\n",
      "Step: 400  \tValid loss: 0.4886765778064728\n",
      "Step: 500  \tTraining loss: 0.4676242470741272\n",
      "Step: 500  \tTraining accuracy: 0.7496355772018433\n",
      "Step: 500  \tValid loss: 0.45236238837242126\n",
      "Step: 600  \tTraining loss: 0.43964093923568726\n",
      "Step: 600  \tTraining accuracy: 0.7580838203430176\n",
      "Step: 600  \tValid loss: 0.43863382935523987\n",
      "Step: 700  \tTraining loss: 0.42444637417793274\n",
      "Step: 700  \tTraining accuracy: 0.7649898529052734\n",
      "Step: 700  \tValid loss: 0.4368394613265991\n",
      "Step: 800  \tTraining loss: 0.41355106234550476\n",
      "Step: 800  \tTraining accuracy: 0.7705207467079163\n",
      "Step: 800  \tValid loss: 0.44131845235824585\n",
      "Step: 900  \tTraining loss: 0.4066928029060364\n",
      "Step: 900  \tTraining accuracy: 0.7751560211181641\n",
      "Step: 900  \tValid loss: 0.4472487270832062\n",
      "Step: 1000  \tTraining loss: 0.4012658894062042\n",
      "Step: 1000  \tTraining accuracy: 0.7784765958786011\n",
      "Step: 1000  \tValid loss: 0.45324358344078064\n",
      "Step: 1100  \tTraining loss: 0.3967479169368744\n",
      "Step: 1100  \tTraining accuracy: 0.7806962132453918\n",
      "Step: 1100  \tValid loss: 0.45810213685035706\n",
      "Step: 1200  \tTraining loss: 0.39318257570266724\n",
      "Step: 1200  \tTraining accuracy: 0.7826778292655945\n",
      "Step: 1200  \tValid loss: 0.4629627466201782\n",
      "Step: 1300  \tTraining loss: 0.3903048634529114\n",
      "Step: 1300  \tTraining accuracy: 0.7838125824928284\n",
      "Step: 1300  \tValid loss: 0.4670383930206299\n",
      "Step: 1400  \tTraining loss: 0.38775843381881714\n",
      "Step: 1400  \tTraining accuracy: 0.7850271463394165\n",
      "Step: 1400  \tValid loss: 0.47017648816108704\n",
      "Step: 1500  \tTraining loss: 0.3854016959667206\n",
      "Step: 1500  \tTraining accuracy: 0.7858455777168274\n",
      "Step: 1500  \tValid loss: 0.4726482033729553\n",
      "Step: 1600  \tTraining loss: 0.38331833481788635\n",
      "Step: 1600  \tTraining accuracy: 0.7865591645240784\n",
      "Step: 1600  \tValid loss: 0.4746913015842438\n",
      "Step: 1700  \tTraining loss: 0.3813163638114929\n",
      "Step: 1700  \tTraining accuracy: 0.7872877717018127\n",
      "Step: 1700  \tValid loss: 0.476036936044693\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.78812426\n",
      "Precision: 0.82520324\n",
      "Recall: 0.8942731\n",
      "F1 score: 0.81978256\n",
      "AUC: 0.78548247\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.788124   0.825203  0.894273  0.819783  0.785482  0.381088      0.787209   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.436696       0.787141   0.430219      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  1709.0  \n",
      "Subject40\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.49370110034942627\n",
      "Step: 100  \tTraining accuracy: 0.8055555820465088\n",
      "Step: 100  \tValid loss: 0.4822569489479065\n",
      "Step: 200  \tTraining loss: 0.38961854577064514\n",
      "Step: 200  \tTraining accuracy: 0.8085774183273315\n",
      "Step: 200  \tValid loss: 0.43134090304374695\n",
      "Step: 300  \tTraining loss: 0.3496854901313782\n",
      "Step: 300  \tTraining accuracy: 0.8182989954948425\n",
      "Step: 300  \tValid loss: 0.432799369096756\n",
      "Step: 400  \tTraining loss: 0.334370881319046\n",
      "Step: 400  \tTraining accuracy: 0.8258845210075378\n",
      "Step: 400  \tValid loss: 0.4369054138660431\n",
      "Step: 500  \tTraining loss: 0.32420527935028076\n",
      "Step: 500  \tTraining accuracy: 0.830539345741272\n",
      "Step: 500  \tValid loss: 0.4396436810493469\n",
      "Step: 600  \tTraining loss: 0.31622231006622314\n",
      "Step: 600  \tTraining accuracy: 0.8326347470283508\n",
      "Step: 600  \tValid loss: 0.44233158230781555\n",
      "Step: 700  \tTraining loss: 0.3093900978565216\n",
      "Step: 700  \tTraining accuracy: 0.8338414430618286\n",
      "Step: 700  \tValid loss: 0.44479939341545105\n",
      "Step: 800  \tTraining loss: 0.30318599939346313\n",
      "Step: 800  \tTraining accuracy: 0.8347308039665222\n",
      "Step: 800  \tValid loss: 0.44668295979499817\n",
      "Step: 900  \tTraining loss: 0.2973425090312958\n",
      "Step: 900  \tTraining accuracy: 0.8350234031677246\n",
      "Step: 900  \tValid loss: 0.44785192608833313\n",
      "Step: 1000  \tTraining loss: 0.29175323247909546\n",
      "Step: 1000  \tTraining accuracy: 0.8359538912773132\n",
      "Step: 1000  \tValid loss: 0.4484074115753174\n",
      "Step: 1100  \tTraining loss: 0.28640419244766235\n",
      "Step: 1100  \tTraining accuracy: 0.8370253443717957\n",
      "Step: 1100  \tValid loss: 0.44861215353012085\n",
      "Step: 1200  \tTraining loss: 0.28132325410842896\n",
      "Step: 1200  \tTraining accuracy: 0.8382012844085693\n",
      "Step: 1200  \tValid loss: 0.44870495796203613\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8391906\n",
      "Precision: 0.7254902\n",
      "Recall: 0.5285714\n",
      "F1 score: 0.70741683\n",
      "AUC: 0.7401477\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.839191    0.72549  0.528571  0.707417  0.740148  0.279857      0.837668   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.429454       0.838545   0.325155      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  1229.0  \n",
      "Subject41\n",
      "(301, 15)\n",
      "(360, 8)\n",
      "(360, 1)\n",
      "(116, 8)\n",
      "(116, 1)\n",
      "(120, 8)\n",
      "(120, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.6634929180145264\n",
      "Step: 100  \tTraining accuracy: 0.6305555701255798\n",
      "Step: 100  \tValid loss: 0.6591928601264954\n",
      "Step: 200  \tTraining loss: 0.5968325138092041\n",
      "Step: 200  \tTraining accuracy: 0.6548117399215698\n",
      "Step: 200  \tValid loss: 0.5983301401138306\n",
      "Step: 300  \tTraining loss: 0.5724014639854431\n",
      "Step: 300  \tTraining accuracy: 0.6675257682800293\n",
      "Step: 300  \tValid loss: 0.5812005400657654\n",
      "Step: 400  \tTraining loss: 0.5567231774330139\n",
      "Step: 400  \tTraining accuracy: 0.6769087314605713\n",
      "Step: 400  \tValid loss: 0.5684295892715454\n",
      "Step: 500  \tTraining loss: 0.5424636602401733\n",
      "Step: 500  \tTraining accuracy: 0.6836734414100647\n",
      "Step: 500  \tValid loss: 0.55646812915802\n",
      "Step: 600  \tTraining loss: 0.5287242531776428\n",
      "Step: 600  \tTraining accuracy: 0.6874251365661621\n",
      "Step: 600  \tValid loss: 0.5453572869300842\n",
      "Step: 700  \tTraining loss: 0.5153282880783081\n",
      "Step: 700  \tTraining accuracy: 0.692581295967102\n",
      "Step: 700  \tValid loss: 0.535011887550354\n",
      "Step: 800  \tTraining loss: 0.5022245049476624\n",
      "Step: 800  \tTraining accuracy: 0.697043240070343\n",
      "Step: 800  \tValid loss: 0.5254276394844055\n",
      "Step: 900  \tTraining loss: 0.4895004332065582\n",
      "Step: 900  \tTraining accuracy: 0.7018330693244934\n",
      "Step: 900  \tValid loss: 0.5166359543800354\n",
      "Step: 1000  \tTraining loss: 0.4773009419441223\n",
      "Step: 1000  \tTraining accuracy: 0.7058001160621643\n",
      "Step: 1000  \tValid loss: 0.5086775422096252\n",
      "Step: 1100  \tTraining loss: 0.4657812714576721\n",
      "Step: 1100  \tTraining accuracy: 0.7096518874168396\n",
      "Step: 1100  \tValid loss: 0.501602292060852\n",
      "Step: 1200  \tTraining loss: 0.4550907611846924\n",
      "Step: 1200  \tTraining accuracy: 0.7129843831062317\n",
      "Step: 1200  \tValid loss: 0.4954790771007538\n",
      "Step: 1300  \tTraining loss: 0.4453413188457489\n",
      "Step: 1300  \tTraining accuracy: 0.7163205742835999\n",
      "Step: 1300  \tValid loss: 0.49032270908355713\n",
      "Step: 1400  \tTraining loss: 0.4365685284137726\n",
      "Step: 1400  \tTraining accuracy: 0.7190428972244263\n",
      "Step: 1400  \tValid loss: 0.4859742820262909\n",
      "Step: 1500  \tTraining loss: 0.4287464916706085\n",
      "Step: 1500  \tTraining accuracy: 0.7209329009056091\n",
      "Step: 1500  \tValid loss: 0.4824473261833191\n",
      "Step: 1600  \tTraining loss: 0.4217997193336487\n",
      "Step: 1600  \tTraining accuracy: 0.7233333587646484\n",
      "Step: 1600  \tValid loss: 0.4796088635921478\n",
      "Step: 1700  \tTraining loss: 0.4156181216239929\n",
      "Step: 1700  \tTraining accuracy: 0.7260509133338928\n",
      "Step: 1700  \tValid loss: 0.4772825539112091\n",
      "Step: 1800  \tTraining loss: 0.4101172685623169\n",
      "Step: 1800  \tTraining accuracy: 0.7285550832748413\n",
      "Step: 1800  \tValid loss: 0.4754371643066406\n",
      "Step: 1900  \tTraining loss: 0.40525850653648376\n",
      "Step: 1900  \tTraining accuracy: 0.7307900190353394\n",
      "Step: 1900  \tValid loss: 0.47397562861442566\n",
      "Step: 2000  \tTraining loss: 0.4009256958961487\n",
      "Step: 2000  \tTraining accuracy: 0.7325402498245239\n",
      "Step: 2000  \tValid loss: 0.4727942943572998\n",
      "Step: 2100  \tTraining loss: 0.3970608115196228\n",
      "Step: 2100  \tTraining accuracy: 0.7345277070999146\n",
      "Step: 2100  \tValid loss: 0.471968412399292\n",
      "Step: 2200  \tTraining loss: 0.39360547065734863\n",
      "Step: 2200  \tTraining accuracy: 0.7364864945411682\n",
      "Step: 2200  \tValid loss: 0.4713810682296753\n",
      "Step: 2300  \tTraining loss: 0.3905065357685089\n",
      "Step: 2300  \tTraining accuracy: 0.738271951675415\n",
      "Step: 2300  \tValid loss: 0.4709239602088928\n",
      "Step: 2400  \tTraining loss: 0.3877102732658386\n",
      "Step: 2400  \tTraining accuracy: 0.7401193976402283\n",
      "Step: 2400  \tValid loss: 0.47057074308395386\n",
      "Step: 2500  \tTraining loss: 0.3851768374443054\n",
      "Step: 2500  \tTraining accuracy: 0.7416803240776062\n",
      "Step: 2500  \tValid loss: 0.4703100919723511\n",
      "Step: 2600  \tTraining loss: 0.3828810453414917\n",
      "Step: 2600  \tTraining accuracy: 0.7431848049163818\n",
      "Step: 2600  \tValid loss: 0.47015631198883057\n",
      "Step: 2700  \tTraining loss: 0.3807182312011719\n",
      "Step: 2700  \tTraining accuracy: 0.7445761561393738\n",
      "Step: 2700  \tValid loss: 0.46992141008377075\n",
      "Step: 2800  \tTraining loss: 0.37872493267059326\n",
      "Step: 2800  \tTraining accuracy: 0.7459275722503662\n",
      "Step: 2800  \tValid loss: 0.46980902552604675\n",
      "Step: 2900  \tTraining loss: 0.3768867552280426\n",
      "Step: 2900  \tTraining accuracy: 0.7471257448196411\n",
      "Step: 2900  \tValid loss: 0.4697927236557007\n",
      "Step: 3000  \tTraining loss: 0.3752257525920868\n",
      "Step: 3000  \tTraining accuracy: 0.7482430338859558\n",
      "Step: 3000  \tValid loss: 0.4699733555316925\n",
      "Step: 3100  \tTraining loss: 0.37369418144226074\n",
      "Step: 3100  \tTraining accuracy: 0.7493420839309692\n",
      "Step: 3100  \tValid loss: 0.47023332118988037\n",
      "Step: 3200  \tTraining loss: 0.3722744286060333\n",
      "Step: 3200  \tTraining accuracy: 0.7502654194831848\n",
      "Step: 3200  \tValid loss: 0.470490425825119\n",
      "Step: 3300  \tTraining loss: 0.370954304933548\n",
      "Step: 3300  \tTraining accuracy: 0.7511321306228638\n",
      "Step: 3300  \tValid loss: 0.4707523286342621\n",
      "Step: 3400  \tTraining loss: 0.3693218529224396\n",
      "Step: 3400  \tTraining accuracy: 0.7519972324371338\n",
      "Step: 3400  \tValid loss: 0.4701346755027771\n",
      "Step: 3500  \tTraining loss: 0.3675174415111542\n",
      "Step: 3500  \tTraining accuracy: 0.7528122663497925\n",
      "Step: 3500  \tValid loss: 0.47009986639022827\n",
      "Step: 3600  \tTraining loss: 0.3661339581012726\n",
      "Step: 3600  \tTraining accuracy: 0.7536757588386536\n",
      "Step: 3600  \tValid loss: 0.47111016511917114\n",
      "Step: 3700  \tTraining loss: 0.36484405398368835\n",
      "Step: 3700  \tTraining accuracy: 0.7544921040534973\n",
      "Step: 3700  \tValid loss: 0.4718184173107147\n",
      "Step: 3800  \tTraining loss: 0.3634082078933716\n",
      "Step: 3800  \tTraining accuracy: 0.7553542852401733\n",
      "Step: 3800  \tValid loss: 0.47138723731040955\n",
      "Step: 3900  \tTraining loss: 0.36217179894447327\n",
      "Step: 3900  \tTraining accuracy: 0.7561283111572266\n",
      "Step: 3900  \tValid loss: 0.47228601574897766\n",
      "Step: 4000  \tTraining loss: 0.361070841550827\n",
      "Step: 4000  \tTraining accuracy: 0.7568208575248718\n",
      "Step: 4000  \tValid loss: 0.4735136330127716\n",
      "Step: 4100  \tTraining loss: 0.36004510521888733\n",
      "Step: 4100  \tTraining accuracy: 0.757561981678009\n",
      "Step: 4100  \tValid loss: 0.4751032590866089\n",
      "Step: 4200  \tTraining loss: 0.35908591747283936\n",
      "Step: 4200  \tTraining accuracy: 0.7582674622535706\n",
      "Step: 4200  \tValid loss: 0.4763621389865875\n",
      "Step: 4300  \tTraining loss: 0.3581889271736145\n",
      "Step: 4300  \tTraining accuracy: 0.7589792013168335\n",
      "Step: 4300  \tValid loss: 0.4775463044643402\n",
      "Step: 4400  \tTraining loss: 0.35735461115837097\n",
      "Step: 4400  \tTraining accuracy: 0.7596582770347595\n",
      "Step: 4400  \tValid loss: 0.47876331210136414\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7603822\n",
      "Precision: 0.78571427\n",
      "Recall: 0.84615386\n",
      "F1 score: 0.7885445\n",
      "AUC: 0.83484167\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.760382   0.785714  0.846154  0.788544  0.834842  0.357041      0.759381   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.469735       0.759576   0.537317      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  4438.0  \n"
     ]
    }
   ],
   "source": [
    "neurons = 8\n",
    "# for subj_num in range(11,27):\n",
    "for subj_num in range(28,42):\n",
    "\n",
    "# for subj_num in [20]:##[15, 16, 17]:# 19, 20, 23, 24, 25, 26, 29, 36, 37, 40]:\n",
    "\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    PT_file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/\"\n",
    "    PT_file_name = PT_file_path  + \"PT_loss_updated.csv\"\n",
    "    PT_metrics = pd.read_csv(PT_file_name)\n",
    "    PT_metrics = PT_metrics[PT_metrics.PT_loss !=0]\n",
    "    PT_R2= PT_metrics[PT_metrics.Subject_number ==subj_num].PT_pseudoR2.iloc[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for generated data ##\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_subj29_params.csv\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_1500_subj29_params.csv\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_3000_subj29_params.csv\"\n",
    "\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1\"\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5\"\n",
    "\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1/generateddata1500mu1params.csv\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5/generateddata300mu0_5params.csv\"\n",
    "\n",
    "\n",
    "\n",
    "############# ORIGINAL ##############\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5/generateddata3000mu0_5params.csv\"\n",
    "\n",
    "\n",
    "# ############ ACTUAL ###############\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5/generateddata600mu0_5params.csv\"\n",
    "\n",
    "\n",
    "#     task_df=pd.read_csv(file_name)\n",
    "# #     task_df.TrialNum = task_df.TrialNum-1\n",
    "\n",
    "#     task_df = add_releveant_features(task_df)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### ORIGINAL\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining\"\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining_v2\"\n",
    "\n",
    "\n",
    "#     task_df = task_net_df; dopa_task_df = task_net_df;\n",
    "    \n",
    "    #############################\n",
    "#     task_df = task_net_train_df\n",
    "#     dopa_task_df  = task_net_valid_df\n",
    "\n",
    "    \n",
    "    \n",
    "     \n",
    "    ### ACTUAL DATA PER SUBJECT\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/subject_num_\"+str(subj_num)+\"/first_half_train\"\n",
    "\n",
    "    \n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v6chunks/subject_num_\"+str(subj_num)\n",
    "\n",
    "    \n",
    "    train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X,train_y= data_split(task_df,dopa_task_df)\n",
    "\n",
    "    \n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,10)\n",
    "\n",
    "    metric_out_df, prob_train, prob_test, prob_val = train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y)\n",
    "    \n",
    "    print(metric_out_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     os.mkdir(file_path + \"50_splits_combined_1sthalf\")\n",
    "#     os.mkdir(file_path + \"50_splits_combined_2ndhalf\")\n",
    "\n",
    "\n",
    "    \n",
    "#     os.mkdir(file_path + \"combined_1sthalf\")\n",
    "#     os.mkdir(file_path + \"combined_2ndhalf\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     os.mkdir(file_path + \"300_sub29\")\n",
    "#     os.mkdir(file_path + \"1500_sub29\")\n",
    "#     os.mkdir(file_path + \"3000_sub29\")\n",
    "\n",
    "#     os.mkdir(file_path + \"/300\")\n",
    "#     os.mkdir(file_path + \"/1500\")\n",
    "#     os.mkdir(file_path + \"/3000\")\n",
    "#     os.mkdir(file_path + \"/600\")\n",
    "\n",
    "\n",
    "\n",
    "#     os.mkdir(file_path)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_curr_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_v2_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "########################################\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#######################################\n",
    "    \n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_RT_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_CrossvalTESTinsess1sthalf_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_CrossvalTESTinsess2ndhalf_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"combined_1sthalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"combined_2ndhalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"50_splits_combined_1sthalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"50_splits_combined_2ndhalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "##### generated data ######\n",
    "#     metric_out_df.to_csv(file_path+\"300_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"1500_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"3000_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"3000_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/300/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"/1500/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/3000/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/3000/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\"_v2.csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\"_v2.csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "    metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "    \n",
    "    prob_train_df = pd.DataFrame(prob_train.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_test_df = pd.DataFrame(prob_test.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_val_df = pd.DataFrame(prob_val.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"prob_train_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"prob_test_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"prob_val_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"prob_train_currentopts_prev_outchoicevv2_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"prob_test_currentopts_prev_outchoicev2_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"prob_val_currentopts_prev_outchoicev2_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "################################\n",
    "    prob_train_df.to_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_test_df.to_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_val_df.to_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#############################\n",
    "\n",
    "    \n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "    \n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"2ndhalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"2ndhalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"2ndhalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"combined_1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"combined_1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"combined_2ndhalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"combined_2ndhalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"combined_2ndhalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"50_splits_combined_1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"50_splits_combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"50_splits_combined_1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### Generated data #######\n",
    "#     prob_train_df.to_csv(file_path + \"300_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"300_sub29_combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"300_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"1500_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"1500_sub29/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"1500_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"3000_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"3000_sub29/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"3000_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/300/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/300/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/300/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/1500/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/1500/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/1500/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/3000/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/3000/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/3000/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/600/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/600/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/600/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>loss_test</th>\n",
       "      <th>neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.760382</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.788544</td>\n",
       "      <td>0.834842</td>\n",
       "      <td>0.357041</td>\n",
       "      <td>0.759381</td>\n",
       "      <td>0.469735</td>\n",
       "      <td>0.759576</td>\n",
       "      <td>0.537317</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>4438.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
       "0  0.760382   0.785714  0.846154  0.788544  0.834842  0.357041      0.759381   \n",
       "\n",
       "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
       "0  0.469735       0.759576   0.537317      8.0          0.001   50000.0   \n",
       "\n",
       "    steps  \n",
       "0  4438.0  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.177761\n",
       "Name: loss_test, dtype: float64"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+metric_out_df.loss_test/(np.log(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 1, 8)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_manual(file_path):\n",
    "    \n",
    "\n",
    "    # subj_num=12\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    # file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "\n",
    "    prob_train_df = pd.read_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_test_df = pd.read_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_val_df =  pd.read_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    \n",
    "    train_data_df = pd.read_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df = pd.read_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df = pd.read_csv(file_path+\"/val_data.csv\")\n",
    "\n",
    "\n",
    "    prob_train =prob_train_df.values[:,1:]\n",
    "    prob_test =prob_test_df.values[:,1:]\n",
    "    prob_val =prob_val_df.values[:,1:]\n",
    "\n",
    "    \n",
    "\n",
    "    train_yy = train_data_df.Choice.values\n",
    "    test_yy = test_data_df.Choice.values\n",
    "    val_yy = val_data_df.Choice.values\n",
    "\n",
    "\n",
    "    encode_categorical = train_yy.reshape(len(train_yy), 1)\n",
    "    train_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "\n",
    "    encode_categorical = test_yy.reshape(len(test_yy), 1)\n",
    "    test_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "    \n",
    "    encode_categorical = val_yy.reshape(len(val_yy), 1)\n",
    "    val_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    loss_train = -(np.dot(train_yy[:,0],np.log(prob_train[:,0])) + np.dot(train_yy[:,1],np.log(prob_train[:,1]) )) / train_yy.shape[0]\n",
    "    acc_train = (np.dot(train_yy[:,0],prob_train[:,0]) + np.dot(train_yy[:,1],prob_train[:,1]))/train_yy.shape[0]\n",
    "    pseudo_R2_train = 1 + loss_train/np.log(0.5)\n",
    "\n",
    "    loss_test = -(np.dot(test_yy[:,0],np.log(prob_test[:,0])) + np.dot(test_yy[:,1],np.log(prob_test[:,1]) )) /test_yy.shape[0]\n",
    "    acc_test = (np.dot(test_yy[:,0],prob_test[:,0]) + np.dot(test_yy[:,1],prob_test[:,1]))/150\n",
    "    pseudo_R2_test  = 1 + loss_test/np.log(0.5)\n",
    "    \n",
    "    \n",
    "    loss_val = -(np.dot(val_yy[:,0],np.log(prob_val[:,0])) + np.dot(val_yy[:,1],np.log(prob_val[:,1]) )) / val_yy.shape[0]\n",
    "    acc_val = (np.dot(val_yy[:,0],prob_val[:,0]) + np.dot(val_yy[:,1],prob_val[:,1]))/val_yy.shape[0]\n",
    "    pseudo_R2_val = 1 + loss_val/np.log(0.5)\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    # pseudoR2_test  = 1 + (-((np.dot(test_yy[:,0],np.log(prob_test[:,0])) + np.dot(test_yy[:,1],np.log(prob_test[:,1]))))/150)/np.log(0.5)\n",
    "\n",
    "    # print(acc_test)\n",
    "    # print(pseudoR2_test)\n",
    "    \n",
    "    metric_manual_df= pd.DataFrame(np.array([subj_num,acc_train,loss_train,pseudo_R2_train, acc_val,loss_val,pseudo_R2_val,acc_test,loss_test,pseudo_R2_test,neurons]).reshape(-1,11),columns =[\"Subject_number\",\"accuracy_train\",\"loss_train\",\"pseudoR2_train\", \"accuracy_val\",\"loss_val\",\"pseudoR2_val\",\"accuracy_test\",\"loss_test\",\"pseudo_R2_test\",\"neurons\"])\n",
    "\n",
    "    metric_manual_df.to_csv(file_path+\"/metric_manual.csv\")\n",
    "    \n",
    "    return metric_manual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# for subj_num in range(11,27):\n",
    "for subj_num in range(28,42):\n",
    "#     file_path =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "    file_path =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v6chunks/subject_num_\"+str(subj_num)\n",
    "\n",
    "\n",
    "    metrics_manual_df = metrics_manual(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     onehot_encoder = OneHotEncoder(sparse=False)\n",
    "#     # file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "\n",
    "#     prob_train_df = pd.read_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df = pd.read_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df =  pd.read_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    \n",
    "#     train_data_df = pd.read_csv(file_path+\"/train_data.csv\")\n",
    "#     test_data_df = pd.read_csv(file_path+\"/test_data.csv\")\n",
    "#     val_data_df = pd.read_csv(file_path+\"/val_data.csv\")\n",
    "\n",
    "\n",
    "#     prob_train =prob_train_df.values[:,1:]\n",
    "#     prob_test =prob_test_df.values[:,1:]\n",
    "#     prob_val =prob_val_df.values[:,1:]\n",
    "\n",
    "    \n",
    "\n",
    "#     train_yy = train_data_df.Choice.values\n",
    "#     test_yy = test_data_df.Choice.values\n",
    "#     val_yy = val_data_df.Choice.values\n",
    "\n",
    "\n",
    "#     encode_categorical = train_yy.reshape(len(train_yy), 1)\n",
    "#     train_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "\n",
    "#     encode_categorical = test_yy.reshape(len(test_yy), 1)\n",
    "#     test_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "    \n",
    "#     encode_categorical = val_yy.reshape(len(val_yy), 1)\n",
    "#     val_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "\n",
    "#     ############################################\n",
    "# #     loss_train = -(np.dot(train_yy[:,0],np.log(prob_train[:,0])) + np.dot(train_yy[:,1],np.log(prob_train[:,1]) )) / train_yy.shape[0]\n",
    "# #     acc_train = (np.dot(train_yy[:,0],prob_train[:,0]) + np.dot(train_yy[:,1],prob_train[:,1]))/train_yy.shape[0]\n",
    "# #     pseudo_R2_train = 1 + loss_train/np.log(0.5)\n",
    "\n",
    "#     loss_test = -(np.dot(test_yy[:,0],np.log(prob_test[:,0])) + np.dot(test_yy[:,1],np.log(prob_test[:,1]) )) /test_yy.shape[0]\n",
    "#     acc_test = (np.dot(test_yy[:,0],prob_test[:,0]) + np.dot(test_yy[:,1],prob_test[:,1]))/150\n",
    "#     pseudo_R2_test  = 1 + loss_test/np.log(0.5)\n",
    "    \n",
    "    \n",
    "#     loss_val = -(np.dot(val_yy[:,0],np.log(prob_val[:,0])) + np.dot(val_yy[:,1],np.log(prob_val[:,1]) )) / val_yy.shape[0]\n",
    "#     acc_val = (np.dot(val_yy[:,0],prob_val[:,0]) + np.dot(val_yy[:,1],prob_val[:,1]))/val_yy.shape[0]\n",
    "#     pseudo_R2_val = 1 + loss_val/np.log(0.5)\n",
    "\n",
    "#     ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4544890380098299"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.9888845198603855"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "metrics_manual_df = metrics_manual(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_number</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>pseudoR2_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>pseudoR2_val</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>loss_test</th>\n",
       "      <th>pseudo_R2_test</th>\n",
       "      <th>neurons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.752035</td>\n",
       "      <td>0.357041</td>\n",
       "      <td>0.484899</td>\n",
       "      <td>0.702774</td>\n",
       "      <td>0.479203</td>\n",
       "      <td>0.308656</td>\n",
       "      <td>0.564067</td>\n",
       "      <td>0.537597</td>\n",
       "      <td>0.224412</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject_number  accuracy_train  loss_train  pseudoR2_train  accuracy_val  \\\n",
       "0            41.0        0.752035    0.357041        0.484899      0.702774   \n",
       "\n",
       "   loss_val  pseudoR2_val  accuracy_test  loss_test  pseudo_R2_test  neurons  \n",
       "0  0.479203      0.308656       0.564067   0.537597        0.224412      8.0  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_manual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v6chunks/subject_num_41'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Subject_number</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>pseudoR2_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>pseudoR2_val</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>loss_test</th>\n",
       "      <th>pseudo_R2_test</th>\n",
       "      <th>neurons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.752035</td>\n",
       "      <td>0.357041</td>\n",
       "      <td>0.484899</td>\n",
       "      <td>0.702774</td>\n",
       "      <td>0.479203</td>\n",
       "      <td>0.308656</td>\n",
       "      <td>0.564067</td>\n",
       "      <td>0.537597</td>\n",
       "      <td>0.224412</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Subject_number  accuracy_train  loss_train  pseudoR2_train  \\\n",
       "0           0            41.0        0.752035    0.357041        0.484899   \n",
       "\n",
       "   accuracy_val  loss_val  pseudoR2_val  accuracy_test  loss_test  \\\n",
       "0      0.702774  0.479203      0.308656       0.564067   0.537597   \n",
       "\n",
       "   pseudo_R2_test  neurons  \n",
       "0        0.224412      8.0  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(file_path+\"/metric_manual.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4284486017238011\n"
     ]
    }
   ],
   "source": [
    "loss_train= - ((np.dot(train_y[:,0],np.log(prob_train[:,0])) + np.dot(train_y[:,1],np.log(prob_train[:,1]))))/300\n",
    "print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9024422042568525"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train = ((np.dot(train_y[:,0],prob_train[:,0]) + np.dot(train_y[:,1],prob_train[:,1])))/300\n",
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43007743664011816\n"
     ]
    }
   ],
   "source": [
    "loss_test= - ((np.dot(test_y[:,0],np.log(prob_test[:,0])) + np.dot(test_y[:,1],np.log(prob_test[:,1]))))/150\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.564066525879316"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test = ((np.dot(test_y[:,0],prob_test[:,0]) + np.dot(test_y[:,1],prob_test[:,1])))/150\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rechecking below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path\n",
    "train_data_df = pd.read_csv(file_path+\"/train_data.csv\")\n",
    "test_data_df = pd.read_csv(file_path+\"/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_yy = train_data_df.Choice.values\n",
    "# # print(train_yy.shape)\n",
    "\n",
    "# encode_categorical = train_yy.reshape(len(train_yy), 1)\n",
    "\n",
    "# train_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "# ((np.dot(train_yy[:,0],prob_train_1[:,0]) + np.dot(train_yy[:,1],prob_train_1[:,1])))/300\n",
    "\n",
    "\n",
    "# # train_data_df.Choice.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6847332539657752"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prob_test_df = pd.read_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "prob_test_1 =prob_test_df.values[:,1:]\n",
    "test_yy = test_data_df.Choice.values\n",
    "\n",
    "\n",
    "encode_categorical = test_yy.reshape(len(test_yy), 1)\n",
    "\n",
    "test_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "((np.dot(test_yy[:,0],prob_test[:,0]) + np.dot(test_yy[:,1],prob_test[:,1])))/150\n",
    "\n",
    "# ((np.dot(test_yy[:,0],prob_test_1[:,0]) + np.dot(test_yy[:,1],prob_test_1[:,1])))/150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the above is correct. Below is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36571429],\n",
       "       [0.82857143],\n",
       "       [0.73142857],\n",
       "       [1.        ],\n",
       "       [0.6       ],\n",
       "       [0.81714286],\n",
       "       [0.85714286],\n",
       "       [1.        ],\n",
       "       [1.        ],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0:10,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject28\n",
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n"
     ]
    }
   ],
   "source": [
    "### create a composite dataset comprising all subject's actual data\n",
    "\n",
    "# task_mega_df = pd.DataFrame(); dopa_task_mega_df = pd.DataFrame();\n",
    "\n",
    "for subj_num in range(28,41):\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    \n",
    "    task_mega_df = task_mega_df.append(task_df)\n",
    "    dopa_task_mega_df = dopa_task_mega_df.append(dopa_task_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17458, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_df=pd.concat([task_mega_df,dopa_task_mega_df])\n",
    "task_net_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17458, 15)\n",
      "12139\n",
      "(12139, 8)\n",
      "(12139, 1)\n",
      "(2602, 8)\n",
      "(2602, 1)\n",
      "(2601, 8)\n",
      "(2601, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "task_df = task_net_df; dopa_task_df = task_net_df;\n",
    "train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5       , 0.25714286, 0.82857143, ..., 0.        ,\n",
       "         0.        , 0.36571429]],\n",
       "\n",
       "       [[0.28571429, 0.        , 0.73142857, ..., 0.5       ,\n",
       "         0.25714286, 0.82857143]],\n",
       "\n",
       "       [[0.92857143, 0.61714286, 1.        , ..., 0.28571429,\n",
       "         0.        , 0.73142857]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.        , 0.63428571, 1.        , ..., 0.92857143,\n",
       "         0.28571429, 1.        ]],\n",
       "\n",
       "       [[0.28571429, 0.        , 0.76      , ..., 1.        ,\n",
       "         0.63428571, 1.        ]],\n",
       "\n",
       "       [[0.5       , 0.37142857, 0.62857143, ..., 0.28571429,\n",
       "         0.        , 0.76      ]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject28\n",
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n",
      "Subject41\n"
     ]
    }
   ],
   "source": [
    "## create a composite dataset comprising all subject's actual data\n",
    "\n",
    "\n",
    "# task_net_train_df = pd.DataFrame(); task_net_valid_df = pd.DataFrame();\n",
    "\n",
    "\n",
    "# for subj_num in range(11,27):\n",
    "for subj_num in range(28,42):\n",
    "\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    task_net_train_df = task_net_train_df.append(pd.concat([task_df.loc[(task_df.TrialNum<=241) & (task_df.TrialNum>1)],dopa_task_df.loc[(dopa_task_df.TrialNum<=241) & (dopa_task_df.TrialNum>1)]]))\n",
    "    task_net_valid_df = task_net_valid_df.append(pd.concat([task_df.loc[(task_df.TrialNum>241) ],dopa_task_df.loc[(dopa_task_df.TrialNum>241) ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14400, 15)\n",
      "(14400, 8)\n",
      "(14400, 1)\n",
      "(3540, 8)\n",
      "(3540, 1)\n",
      "(3540, 8)\n",
      "(3540, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "task_df = task_net_train_df\n",
    "dopa_task_df  = task_net_valid_df\n",
    "\n",
    "train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_train_df.shape[0]/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_valid_df.shape[0]/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dopa_task_mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
