{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as sc_stats\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "onehot_encoder=OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "time_steps = 1\n",
    "inputs = 8\n",
    "outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_releveant_features(task_df):\n",
    "\n",
    "\n",
    "    task_df['PrevOutcome']=task_df['Outcome'].shift(1)\n",
    "    task_df.loc[1,'PrevOutcome']= 0\n",
    "\n",
    "    task_df['PrevChoice']=task_df['Choice'].shift(1)\n",
    "    task_df.loc[1,'PrevChoice']= 0\n",
    "\n",
    "    task_df['PrevSafe']=task_df['Safe'].shift(1)\n",
    "    task_df.loc[1,'PrevSafe']= 0\n",
    "\n",
    "    task_df['PrevBigRisky']=task_df['BigRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevBigRisky']= 0\n",
    "\n",
    "    task_df['PrevSmallRisky']=task_df['SmallRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevSmallRisky']= 0\n",
    "    \n",
    "#     task_df['PrevRT']=task_df['RT'].shift(1)\n",
    "#     task_df.loc[1,'PrevRT']= N\n",
    "    \n",
    "    \n",
    "    \n",
    "    return task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=150\n",
    "num_batches=1000\n",
    "seq_len=10\n",
    "\n",
    "def data_split(task_df,dopa_task_df):\n",
    "#     stop = 200\n",
    "\n",
    "    stop = 150\n",
    "\n",
    "#     stop=300\n",
    "\n",
    "#     stop = 750\n",
    "\n",
    "#     stop=1500\n",
    "\n",
    "\n",
    "    print(task_df.shape)\n",
    " \n",
    "\n",
    "    ##----------------- UNCOMMENT BELOW\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "    \n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "####### Prev O + C+ R + CurrO--------------------\n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "#### - Prev RT+C+R+O + Curr O----------------------\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "    ###### when splitting data into train and validation\n",
    "\n",
    "    # train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.35, random_state=1)\n",
    "\n",
    "    \n",
    "#     train_X, val_X, train_y, val_y = train_X[:stop], train_X[stop:], train_y[:stop], train_y[stop:]\n",
    "\n",
    "\n",
    "####################\n",
    " ##### splitting data into train test valid from the same dataset ###############\n",
    "    \n",
    "#     train_X, val_X, test_X, train_y, val_y, test_y = train_X[:stop], train_X[stop:stop+int(stop/2)], train_X[stop+int(stop/2):], train_y[:stop], train_y[stop:stop+int(stop/2)], train_y[stop+int(stop/2):],\n",
    "\n",
    "#### switching the order for test and val\n",
    "#     half = 1\n",
    "\n",
    "#     if half==1:\n",
    "#         train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int(stop/2)], train_X[stop+int(stop/2):], train_y[:stop], train_y[stop:stop+int(stop/2)], train_y[stop+int(stop/2):],\n",
    "    \n",
    "#     else:\n",
    "    \n",
    "#         train_X, test_X, val_X, train_y, test_y, val_y= train_X[stop-1:], train_X[:int(stop/2)], train_X[int(stop/2):stop-1], train_y[stop-1:], train_y[:int(stop/2)], train_y[int(stop/2):stop-1]\n",
    "\n",
    "#     ##############\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(train_X)\n",
    "    \n",
    "    \n",
    "    ## Combining PLAC + LDOPA datasets\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = np.concatenate(train_X[:stop],test_X[:stop]), np.concatenate(train_X[stop:stop+int(stop/2)])#,test_X[stop:stop+int(stop/2)]),np.concatenate(train_X[stop+int(stop/2):],test_X[stop+int(stop/2):]),np.concatenate(train_y[:stop],test_y[:stop]), np.concatenate(train_y[stop:stop+int(stop/2)],test_y[stop:stop+int(stop/2)]), np.concatenate(train_y[stop+int(stop/2):],test_y[stop+int(stop/2):])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X, test_X,val_X = np.concatenate((train_X[:stop],test_X[:stop]),axis=0), np.concatenate((train_X[stop:stop+int(stop/2)] ,test_X[stop:stop+int(stop/2)]),axis=0), np.concatenate((train_X[stop+int(stop/2):],test_X[stop+int(stop/2):]),axis=0)\n",
    "#     train_y, test_y,val_y = np.concatenate((train_y[:stop],test_y[:stop]),axis=0), np.concatenate((train_y[stop:stop+int(stop/2)] ,test_y[stop:stop+int(stop/2)]),axis=0), np.concatenate((train_y[stop+int(stop/2):],test_y[stop+int(stop/2):]),axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    ## blocking data \n",
    "    train_X_aside, train_y_aside, test_X_aside, test_y_aside  = train_X, train_y, test_X, test_y \n",
    "    \n",
    "    train_X= np.concatenate((build_dataset_train(train_X_aside),build_dataset_train(test_X_aside)), axis=0)\n",
    "    train_y= np.concatenate((build_dataset_train(train_y_aside),build_dataset_train(test_y_aside)), axis=0)\n",
    "\n",
    "    \n",
    "    val_X= np.concatenate((build_dataset_valid(train_X_aside),build_dataset_valid(test_X_aside)), axis=0)\n",
    "    val_y= np.concatenate((build_dataset_valid(train_y_aside),build_dataset_valid(test_y_aside)), axis=0)\n",
    "\n",
    "    \n",
    "    test_X= np.concatenate((build_dataset_test(train_X_aside),build_dataset_test(test_X_aside)), axis=0)\n",
    "    test_y= np.concatenate((build_dataset_test(train_y_aside),build_dataset_test(test_y_aside)), axis=0)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    ##### FURTHER TRAINING WITH SUBSEQUENCES WITH REPLACEMENT\n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#     for k in range(num_batches):\n",
    "#             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#             train_X = np.concatenate((train_X,X_seq), axis=0)\n",
    "# #             print(train_X.shape)\n",
    "#             train_y = np.concatenate((train_y, y_seq),axis=0)\n",
    "\n",
    "\n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#     train_X, train_y = X_seq, y_seq\n",
    "#     for k in range(num_batches-1):\n",
    "#             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#             train_X = np.concatenate((train_X,X_seq), axis=0)\n",
    "# #             print(train_X.shape)\n",
    "#             train_y = np.concatenate((train_y, y_seq),axis=0)\n",
    "# # # # ##########################\n",
    "\n",
    "\n",
    "\n",
    "    #### PRE TRAINING\n",
    "#     stop = int(0.7*len(train_X))\n",
    "#     print(stop)\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int((len(train_X)-stop)/2)], train_X[stop+int((len(train_X)-stop)/2):],train_y[:stop], train_y[stop:stop+int((len(train_X)-stop)/2)], train_y[stop+int((len(train_X)-stop)/2):]\n",
    "    \n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = train_X, test_X, test_X, train_y, test_y, test_y\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(val_X.shape)\n",
    "    print(val_y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "    # # center and scale\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.fit_transform(test_X)\n",
    "    val_X = scaler.fit_transform(val_X)\n",
    "\n",
    "\n",
    "    train_X = train_X[:,None,:]\n",
    "    val_X = val_X[:,None,:]\n",
    "    test_X = test_X[:,None,:]\n",
    "\n",
    "\n",
    "    # # one-hot encode the outputs\n",
    "\n",
    "    onehot_encoder = OneHotEncoder()\n",
    "    encode_categorical = train_y.reshape(len(train_y), 1)\n",
    "    encode_categorical_test = test_y.reshape(len(test_y), 1)\n",
    "    encode_categorical_val = val_y.reshape(len(val_y),1)\n",
    "\n",
    "\n",
    "    train_y = onehot_encoder.fit_transform(encode_categorical).toarray()\n",
    "    test_y = onehot_encoder.fit_transform(encode_categorical_test).toarray()\n",
    "    val_y = onehot_encoder.fit_transform(encode_categorical_val).toarray()\n",
    "\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, val_X,val_y\n",
    "#     return train_X, test_X, val_X#, test_X, test_y, val_X,val_y\n",
    "\n",
    "def build_dataset_train(data):\n",
    "    \n",
    "    return np.concatenate((data[:int(stop/3)],data[2*int(stop/3):int(stop)], data[stop+int(stop/3):stop+2*int(stop/3)]), axis=0)\n",
    "    \n",
    "\n",
    "def build_dataset_test(data):\n",
    "    \n",
    "    return np.concatenate((data[int(stop/3):int(stop/3)+int(stop/6)], data[int(stop):int(stop)+int(stop/6)], data[stop+2*int(stop/3): stop+2*int(stop/3) + int(stop/6) ]), axis=0)\n",
    "\n",
    "def build_dataset_valid(data):\n",
    "    \n",
    "    return np.concatenate((data[int(stop/3)+int(stop/6):int(stop/3)+2*int(stop/6)], data[int(stop)+int(stop/6):int(stop)+2*int(stop/6)], data[stop+2*int(stop/3) + int(stop/6) : ]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y): \n",
    "    reset_graph()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50000\n",
    "    batch_size = int(train_X.shape[0]/2)\n",
    "    # batch_size = 100\n",
    "    length = train_X.shape[0]\n",
    "    display = 100\n",
    "    neurons = neurons\n",
    "\n",
    "    num_batches = 100\n",
    "    seq_len = 10\n",
    "\n",
    "    percent_above_PT = 1\n",
    "\n",
    "    train_threshold = PT_R2 + percent_above_PT\n",
    "\n",
    "\n",
    "    save_step = 100\n",
    "\n",
    "\n",
    "    best_loss_val = np.infty\n",
    "    checks_since_last_progress = 0\n",
    "    max_checks_without_progress = 1000\n",
    "\n",
    "\n",
    "    # clear graph (if any) before running\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, time_steps, inputs])\n",
    "\n",
    "    y = tf.placeholder(tf.float32, [None, outputs])\n",
    "\n",
    "    # LSTM Cell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=neurons, activation=tf.nn.relu)\n",
    "    cell_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "    # pass into Dense layer\n",
    "    stacked_outputs = tf.reshape(cell_outputs, [-1, neurons])\n",
    "    out = tf.layers.dense(inputs=stacked_outputs, units=outputs)\n",
    "\n",
    "    probability = tf.nn.softmax(out)\n",
    "\n",
    "    # squared error loss or cost function for linear regression\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=y, logits=out))\n",
    "\n",
    "    # optimizer to minimize cost\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels =  tf.argmax(y, 1),\n",
    "                          predictions = tf.argmax(out, 1),\n",
    "                          name = \"accuracy\")\n",
    "    precision = tf.metrics.precision(labels=tf.argmax(y, 1),\n",
    "                                 predictions=tf.argmax(out, 1),\n",
    "                                 name=\"precision\")\n",
    "    recall = tf.metrics.recall(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"recall\")\n",
    "    f1 = 2 * accuracy[1] * recall[1] / ( precision[1] + recall[1] )\n",
    "\n",
    "    acc_up,acc_val = accuracy\n",
    "    auc = tf.metrics.auc(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"auc\")\n",
    "    \n",
    "    valid_store = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #######################\n",
    "#         saver.restore(sess, \"./checkpts/Original_RNN_LSTM_8features_v2.ckpt\")\n",
    "#         saver.restore(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "        saver.restore(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        #######################\n",
    "        \n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "\n",
    "        # Train the model\n",
    "        for steps in range(epochs):\n",
    "            mini_batch = zip(range(0, length, batch_size),\n",
    "                       range(batch_size, length+1, batch_size))\n",
    "\n",
    "            # train data in mini-batches\n",
    "            for (start, end) in mini_batch:\n",
    "    #             print(start,end)\n",
    "                sess.run(training_op, feed_dict = {X: train_X[start:end,:,:],\n",
    "                                                   y: train_y[start:end,:]}) \n",
    "\n",
    "            ## train data in batches of length subsequence\n",
    "\n",
    "    #         for k in range(num_batches):\n",
    "    #             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "\n",
    "    #             sess.run(training_op, feed_dict = {X:X_seq,y:y_seq}) \n",
    "            loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "            loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "            # print training performance \n",
    "            if (steps+1) % display == 0:\n",
    "                # evaluate loss function on training set\n",
    "\n",
    "\n",
    "                loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining loss: {}'.format((steps+1), loss_fn))\n",
    "\n",
    "                acc_train = acc_val.eval(feed_dict={X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining accuracy: {}'.format((steps+1), acc_train))\n",
    "\n",
    "\n",
    "                acc_test = acc_val.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest accuracy: {}'.format((steps+1), acc_test))\n",
    "\n",
    "                loss_test = loss.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest loss: {}'.format((steps+1), loss_test))\n",
    "\n",
    "                accu_val = acc_val.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "                loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "                print('Step: {}  \\tValid loss: {}'.format((steps+1), loss_val))\n",
    "\n",
    "                valid_store.append(loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (1 + loss_fn/np.log(0.5)) > train_threshold:\n",
    "                    print(\"Threshold achieved, quit training\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            if loss_val < best_loss_val:\n",
    "\n",
    "                        best_loss_val = loss_val\n",
    "                        checks_since_last_progress = 0\n",
    "            else:\n",
    "                            checks_since_last_progress += 1\n",
    "\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if checks_since_last_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "\n",
    "            if (steps+1) % save_step ==0:\n",
    "                                save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#                 save_path = saver.save(sess, \"./checkpts/RNN_Internet_LSTM_model_5features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     evaluate model accuracy\n",
    "        acc, prec, recall, f1, AUC = sess.run([accuracy, precision, recall, f1,auc],\n",
    "                                         feed_dict = {X: train_X, y: train_y})\n",
    "        prob_train = probability.eval(feed_dict = {X: train_X, y: train_y})\n",
    "        prob_test = probability.eval(feed_dict = {X: test_X, y: test_y})\n",
    "        prob_valid = probability.eval(feed_dict = {X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nEvaluation  on training set')\n",
    "        print('Accuracy:', acc[1])\n",
    "        print('Precision:', prec[1])\n",
    "        print('Recall:', recall[1])\n",
    "        print('F1 score:', f1)\n",
    "        print('AUC:', AUC[1])\n",
    "        \n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "        save_path = saver.save(sess, \"./checkpts/Later_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/LaterDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "    metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,AUC[1],loss_fn,accu_val,best_loss_val,acc_test,loss_test,neurons,learning_rate,epochs,steps]).reshape(-1,14),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\",\"loss\",\"accuracy_val\",\"loss_val\",\"accuracy_test\",\"loss_test\",\"neurons\",\"learning_rate\",\"n_epochs\",\"steps\"])\n",
    "    return metric_out_df, prob_train, prob_test, prob_valid\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def random_subsequence(X,y,seq_len):\n",
    "    rnd  = random.randint(0,len(X)-seq_len)\n",
    "    X_seq, y_seq = X[rnd:rnd+seq_len], y[rnd:rnd+seq_len]\n",
    "    return X_seq, y_seq\n",
    "\n",
    "    print(y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subj_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-88950e9566c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m### ACTUAL DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubj_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/experiment_data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtask_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subj_num' is not defined"
     ]
    }
   ],
   "source": [
    "# file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/\"\n",
    "# file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_subj29_params.csv\"\n",
    "\n",
    "file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1/generateddata300mu1params.csv\"\n",
    "\n",
    "# file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5/generateddata300mu0_5params.csv\"\n",
    "\n",
    "### ACTUAL DATA\n",
    "file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "\n",
    "task_df=pd.read_csv(file_name)\n",
    "task_df.head(10)\n",
    "task_df = add_releveant_features(task_df)\n",
    "\n",
    "\n",
    "task_df.head(10)\n",
    "\n",
    "\n",
    "train_data = np.concatenate((build_dataset_train(task_df.loc[task_df.TrialNum>1]),build_dataset_train(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "train_data_df = pd.DataFrame(train_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "test_data = np.concatenate((build_dataset_test(task_df.loc[task_df.TrialNum>1]),build_dataset_test(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "test_data_df = pd.DataFrame(test_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "\n",
    "\n",
    "# train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n",
    "\n",
    "\n",
    "# train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n",
      "Subject41\n"
     ]
    }
   ],
   "source": [
    "for subj_num in range(29,42):\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v2/subject_num_\"+str(subj_num)\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "#     os.mkdir(file_path)\n",
    "\n",
    "    \n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    train_data = np.concatenate((build_dataset_train(task_df.loc[task_df.TrialNum>1]),build_dataset_train(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "    train_data_df = pd.DataFrame(train_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "    test_data = np.concatenate((build_dataset_test(task_df.loc[task_df.TrialNum>1]),build_dataset_test(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "    test_data_df = pd.DataFrame(test_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "    \n",
    "    train_data_df.to_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df.to_csv(file_path+\"/test_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/\"\n",
    "# os.mkdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject41\n",
      "(301, 15)\n",
      "(300, 8)\n",
      "(300, 1)\n",
      "(148, 8)\n",
      "(148, 1)\n",
      "(150, 8)\n",
      "(150, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.6357920169830322\n",
      "Step: 100  \tTraining accuracy: 0.70333331823349\n",
      "Step: 100  \tValid loss: 0.6754299998283386\n",
      "Step: 200  \tTraining loss: 0.5792643427848816\n",
      "Step: 200  \tTraining accuracy: 0.6770601272583008\n",
      "Step: 200  \tValid loss: 0.6351091265678406\n",
      "Step: 300  \tTraining loss: 0.5575792193412781\n",
      "Step: 300  \tTraining accuracy: 0.6871657967567444\n",
      "Step: 300  \tValid loss: 0.608918309211731\n",
      "Step: 400  \tTraining loss: 0.5418059229850769\n",
      "Step: 400  \tTraining accuracy: 0.6977077126502991\n",
      "Step: 400  \tValid loss: 0.5885292291641235\n",
      "Step: 500  \tTraining loss: 0.5270706415176392\n",
      "Step: 500  \tTraining accuracy: 0.7050520181655884\n",
      "Step: 500  \tValid loss: 0.570593535900116\n",
      "Step: 600  \tTraining loss: 0.5129058957099915\n",
      "Step: 600  \tTraining accuracy: 0.7112461924552917\n",
      "Step: 600  \tValid loss: 0.553838849067688\n",
      "Step: 700  \tTraining loss: 0.49947747588157654\n",
      "Step: 700  \tTraining accuracy: 0.7155349850654602\n",
      "Step: 700  \tValid loss: 0.538257896900177\n",
      "Step: 800  \tTraining loss: 0.48705923557281494\n",
      "Step: 800  \tTraining accuracy: 0.7195720076560974\n",
      "Step: 800  \tValid loss: 0.5238504409790039\n",
      "Step: 900  \tTraining loss: 0.4757569432258606\n",
      "Step: 900  \tTraining accuracy: 0.7236428260803223\n",
      "Step: 900  \tValid loss: 0.5103268027305603\n",
      "Step: 1000  \tTraining loss: 0.4655989110469818\n",
      "Step: 1000  \tTraining accuracy: 0.7270327210426331\n",
      "Step: 1000  \tValid loss: 0.498043417930603\n",
      "Step: 1100  \tTraining loss: 0.456667423248291\n",
      "Step: 1100  \tTraining accuracy: 0.7304140329360962\n",
      "Step: 1100  \tValid loss: 0.4869387745857239\n",
      "Step: 1200  \tTraining loss: 0.44882872700691223\n",
      "Step: 1200  \tTraining accuracy: 0.7330619096755981\n",
      "Step: 1200  \tValid loss: 0.47679761052131653\n",
      "Step: 1300  \tTraining loss: 0.4419785439968109\n",
      "Step: 1300  \tTraining accuracy: 0.7348849773406982\n",
      "Step: 1300  \tValid loss: 0.4678359031677246\n",
      "Step: 1400  \tTraining loss: 0.43600764870643616\n",
      "Step: 1400  \tTraining accuracy: 0.7363141179084778\n",
      "Step: 1400  \tValid loss: 0.45978865027427673\n",
      "Step: 1500  \tTraining loss: 0.43072256445884705\n",
      "Step: 1500  \tTraining accuracy: 0.7374308109283447\n",
      "Step: 1500  \tValid loss: 0.45332852005958557\n",
      "Step: 1600  \tTraining loss: 0.4261806607246399\n",
      "Step: 1600  \tTraining accuracy: 0.738511323928833\n",
      "Step: 1600  \tValid loss: 0.4472575783729553\n",
      "Step: 1700  \tTraining loss: 0.4222240447998047\n",
      "Step: 1700  \tTraining accuracy: 0.7392582297325134\n",
      "Step: 1700  \tValid loss: 0.4418906569480896\n",
      "Step: 1800  \tTraining loss: 0.41877755522727966\n",
      "Step: 1800  \tTraining accuracy: 0.7401108145713806\n",
      "Step: 1800  \tValid loss: 0.43718522787094116\n",
      "Step: 1900  \tTraining loss: 0.4157596230506897\n",
      "Step: 1900  \tTraining accuracy: 0.7409616708755493\n",
      "Step: 1900  \tValid loss: 0.43295204639434814\n",
      "Step: 2000  \tTraining loss: 0.4131084680557251\n",
      "Step: 2000  \tTraining accuracy: 0.7417252659797668\n",
      "Step: 2000  \tValid loss: 0.42914673686027527\n",
      "Step: 2100  \tTraining loss: 0.41075795888900757\n",
      "Step: 2100  \tTraining accuracy: 0.7423328161239624\n",
      "Step: 2100  \tValid loss: 0.4257798194885254\n",
      "Step: 2200  \tTraining loss: 0.40866219997406006\n",
      "Step: 2200  \tTraining accuracy: 0.7428838014602661\n",
      "Step: 2200  \tValid loss: 0.4227985143661499\n",
      "Step: 2300  \tTraining loss: 0.40678560733795166\n",
      "Step: 2300  \tTraining accuracy: 0.743534505367279\n",
      "Step: 2300  \tValid loss: 0.4200756549835205\n",
      "Step: 2400  \tTraining loss: 0.4050963222980499\n",
      "Step: 2400  \tTraining accuracy: 0.7441297769546509\n",
      "Step: 2400  \tValid loss: 0.41754424571990967\n",
      "Step: 2500  \tTraining loss: 0.40356558561325073\n",
      "Step: 2500  \tTraining accuracy: 0.7446764707565308\n",
      "Step: 2500  \tValid loss: 0.4152738153934479\n",
      "Step: 2600  \tTraining loss: 0.40217408537864685\n",
      "Step: 2600  \tTraining accuracy: 0.7451803088188171\n",
      "Step: 2600  \tValid loss: 0.41310080885887146\n",
      "Step: 2700  \tTraining loss: 0.40090352296829224\n",
      "Step: 2700  \tTraining accuracy: 0.745709240436554\n",
      "Step: 2700  \tValid loss: 0.41111451387405396\n",
      "Step: 2800  \tTraining loss: 0.39973127841949463\n",
      "Step: 2800  \tTraining accuracy: 0.7461996674537659\n",
      "Step: 2800  \tValid loss: 0.4092694818973541\n",
      "Step: 2900  \tTraining loss: 0.3986439108848572\n",
      "Step: 2900  \tTraining accuracy: 0.7466557025909424\n",
      "Step: 2900  \tValid loss: 0.4075504243373871\n",
      "Step: 3000  \tTraining loss: 0.39762866497039795\n",
      "Step: 3000  \tTraining accuracy: 0.7470241189002991\n",
      "Step: 3000  \tValid loss: 0.4059098958969116\n",
      "Step: 3100  \tTraining loss: 0.3966731131076813\n",
      "Step: 3100  \tTraining accuracy: 0.7473683953285217\n",
      "Step: 3100  \tValid loss: 0.4043431282043457\n",
      "Step: 3200  \tTraining loss: 0.39576759934425354\n",
      "Step: 3200  \tTraining accuracy: 0.7476377487182617\n",
      "Step: 3200  \tValid loss: 0.4028104245662689\n",
      "Step: 3300  \tTraining loss: 0.3949052691459656\n",
      "Step: 3300  \tTraining accuracy: 0.7478905320167542\n",
      "Step: 3300  \tValid loss: 0.4013293385505676\n",
      "Step: 3400  \tTraining loss: 0.3940790295600891\n",
      "Step: 3400  \tTraining accuracy: 0.7481781244277954\n",
      "Step: 3400  \tValid loss: 0.3998944163322449\n",
      "Step: 3500  \tTraining loss: 0.39328283071517944\n",
      "Step: 3500  \tTraining accuracy: 0.7484490275382996\n",
      "Step: 3500  \tValid loss: 0.39849650859832764\n",
      "Step: 3600  \tTraining loss: 0.3925112187862396\n",
      "Step: 3600  \tTraining accuracy: 0.7487517595291138\n",
      "Step: 3600  \tValid loss: 0.39713916182518005\n",
      "Step: 3700  \tTraining loss: 0.3917139172554016\n",
      "Step: 3700  \tTraining accuracy: 0.7489921450614929\n",
      "Step: 3700  \tValid loss: 0.3955601751804352\n",
      "Step: 3800  \tTraining loss: 0.3908253610134125\n",
      "Step: 3800  \tTraining accuracy: 0.7492196559906006\n",
      "Step: 3800  \tValid loss: 0.39465203881263733\n",
      "Step: 3900  \tTraining loss: 0.3900517225265503\n",
      "Step: 3900  \tTraining accuracy: 0.7494353652000427\n",
      "Step: 3900  \tValid loss: 0.39405402541160583\n",
      "Step: 4000  \tTraining loss: 0.38933396339416504\n",
      "Step: 4000  \tTraining accuracy: 0.7495978474617004\n",
      "Step: 4000  \tValid loss: 0.3927334249019623\n",
      "Step: 4100  \tTraining loss: 0.3886262774467468\n",
      "Step: 4100  \tTraining accuracy: 0.7498761415481567\n",
      "Step: 4100  \tValid loss: 0.3914698660373688\n",
      "Step: 4200  \tTraining loss: 0.3879251480102539\n",
      "Step: 4200  \tTraining accuracy: 0.7501410245895386\n",
      "Step: 4200  \tValid loss: 0.3902585804462433\n",
      "Step: 4300  \tTraining loss: 0.3872298300266266\n",
      "Step: 4300  \tTraining accuracy: 0.7503934502601624\n",
      "Step: 4300  \tValid loss: 0.3890199065208435\n",
      "Step: 4400  \tTraining loss: 0.38639023900032043\n",
      "Step: 4400  \tTraining accuracy: 0.7506342530250549\n",
      "Step: 4400  \tValid loss: 0.3884464204311371\n",
      "Step: 4500  \tTraining loss: 0.3856615424156189\n",
      "Step: 4500  \tTraining accuracy: 0.7509018778800964\n",
      "Step: 4500  \tValid loss: 0.3872683346271515\n",
      "Step: 4600  \tTraining loss: 0.38495418429374695\n",
      "Step: 4600  \tTraining accuracy: 0.7511576414108276\n",
      "Step: 4600  \tValid loss: 0.38589924573898315\n",
      "Step: 4700  \tTraining loss: 0.38425397872924805\n",
      "Step: 4700  \tTraining accuracy: 0.7514384388923645\n",
      "Step: 4700  \tValid loss: 0.38470524549484253\n",
      "Step: 4800  \tTraining loss: 0.3835611045360565\n",
      "Step: 4800  \tTraining accuracy: 0.7517426013946533\n",
      "Step: 4800  \tValid loss: 0.3836091160774231\n",
      "Step: 4900  \tTraining loss: 0.3828774690628052\n",
      "Step: 4900  \tTraining accuracy: 0.7521031498908997\n",
      "Step: 4900  \tValid loss: 0.38263842463493347\n",
      "Step: 5000  \tTraining loss: 0.38220295310020447\n",
      "Step: 5000  \tTraining accuracy: 0.7524829506874084\n",
      "Step: 5000  \tValid loss: 0.3817422091960907\n",
      "Step: 5100  \tTraining loss: 0.3815400004386902\n",
      "Step: 5100  \tTraining accuracy: 0.7528145909309387\n",
      "Step: 5100  \tValid loss: 0.38090062141418457\n",
      "Step: 5200  \tTraining loss: 0.3808911144733429\n",
      "Step: 5200  \tTraining accuracy: 0.7531657814979553\n",
      "Step: 5200  \tValid loss: 0.3801569938659668\n",
      "Step: 5300  \tTraining loss: 0.38025644421577454\n",
      "Step: 5300  \tTraining accuracy: 0.7534717917442322\n",
      "Step: 5300  \tValid loss: 0.3795149624347687\n",
      "Step: 5400  \tTraining loss: 0.3796394467353821\n",
      "Step: 5400  \tTraining accuracy: 0.7537975907325745\n",
      "Step: 5400  \tValid loss: 0.3789847791194916\n",
      "Step: 5500  \tTraining loss: 0.3790540099143982\n",
      "Step: 5500  \tTraining accuracy: 0.7540807723999023\n",
      "Step: 5500  \tValid loss: 0.37834081053733826\n",
      "Step: 5600  \tTraining loss: 0.37849190831184387\n",
      "Step: 5600  \tTraining accuracy: 0.7543838620185852\n",
      "Step: 5600  \tValid loss: 0.37773776054382324\n",
      "Step: 5700  \tTraining loss: 0.3779465854167938\n",
      "Step: 5700  \tTraining accuracy: 0.7546762228012085\n",
      "Step: 5700  \tValid loss: 0.3771666884422302\n",
      "Step: 5800  \tTraining loss: 0.3774169981479645\n",
      "Step: 5800  \tTraining accuracy: 0.7549874782562256\n",
      "Step: 5800  \tValid loss: 0.37665197253227234\n",
      "Step: 5900  \tTraining loss: 0.3769029378890991\n",
      "Step: 5900  \tTraining accuracy: 0.7552881240844727\n",
      "Step: 5900  \tValid loss: 0.37618982791900635\n",
      "Step: 6000  \tTraining loss: 0.3764030933380127\n",
      "Step: 6000  \tTraining accuracy: 0.7555786371231079\n",
      "Step: 6000  \tValid loss: 0.37576228380203247\n",
      "Step: 6100  \tTraining loss: 0.3759196102619171\n",
      "Step: 6100  \tTraining accuracy: 0.7558596134185791\n",
      "Step: 6100  \tValid loss: 0.3754304349422455\n",
      "Step: 6200  \tTraining loss: 0.3754490315914154\n",
      "Step: 6200  \tTraining accuracy: 0.7561314105987549\n",
      "Step: 6200  \tValid loss: 0.37514030933380127\n",
      "Step: 6300  \tTraining loss: 0.37499144673347473\n",
      "Step: 6300  \tTraining accuracy: 0.7564212083816528\n",
      "Step: 6300  \tValid loss: 0.3748690187931061\n",
      "Step: 6400  \tTraining loss: 0.37454551458358765\n",
      "Step: 6400  \tTraining accuracy: 0.7567282915115356\n",
      "Step: 6400  \tValid loss: 0.3746401369571686\n",
      "Step: 6500  \tTraining loss: 0.37411126494407654\n",
      "Step: 6500  \tTraining accuracy: 0.7570258378982544\n",
      "Step: 6500  \tValid loss: 0.37440791726112366\n",
      "Step: 6600  \tTraining loss: 0.37368643283843994\n",
      "Step: 6600  \tTraining accuracy: 0.7572887539863586\n",
      "Step: 6600  \tValid loss: 0.3742542862892151\n",
      "Step: 6700  \tTraining loss: 0.37327253818511963\n",
      "Step: 6700  \tTraining accuracy: 0.7575437426567078\n",
      "Step: 6700  \tValid loss: 0.37410643696784973\n",
      "Step: 6800  \tTraining loss: 0.37286829948425293\n",
      "Step: 6800  \tTraining accuracy: 0.7577912211418152\n",
      "Step: 6800  \tValid loss: 0.3739509880542755\n",
      "Step: 6900  \tTraining loss: 0.3724728226661682\n",
      "Step: 6900  \tTraining accuracy: 0.75803142786026\n",
      "Step: 6900  \tValid loss: 0.3738533854484558\n",
      "Step: 7000  \tTraining loss: 0.37208548188209534\n",
      "Step: 7000  \tTraining accuracy: 0.7582888007164001\n",
      "Step: 7000  \tValid loss: 0.37375199794769287\n",
      "Step: 7100  \tTraining loss: 0.3717057406902313\n",
      "Step: 7100  \tTraining accuracy: 0.7585626244544983\n",
      "Step: 7100  \tValid loss: 0.3736914098262787\n",
      "Step: 7200  \tTraining loss: 0.3713340163230896\n",
      "Step: 7200  \tTraining accuracy: 0.7588521242141724\n",
      "Step: 7200  \tValid loss: 0.3736327290534973\n",
      "Step: 7300  \tTraining loss: 0.37096959352493286\n",
      "Step: 7300  \tTraining accuracy: 0.7591336965560913\n",
      "Step: 7300  \tValid loss: 0.3735981583595276\n",
      "Step: 7400  \tTraining loss: 0.37061163783073425\n",
      "Step: 7400  \tTraining accuracy: 0.7594075798988342\n",
      "Step: 7400  \tValid loss: 0.3735899031162262\n",
      "Step: 7500  \tTraining loss: 0.37026000022888184\n",
      "Step: 7500  \tTraining accuracy: 0.7596965432167053\n",
      "Step: 7500  \tValid loss: 0.3735867142677307\n",
      "Step: 7600  \tTraining loss: 0.3699146509170532\n",
      "Step: 7600  \tTraining accuracy: 0.7599557042121887\n",
      "Step: 7600  \tValid loss: 0.3735906183719635\n",
      "Step: 7700  \tTraining loss: 0.36957696080207825\n",
      "Step: 7700  \tTraining accuracy: 0.7602299451828003\n",
      "Step: 7700  \tValid loss: 0.3736250400543213\n",
      "Step: 7800  \tTraining loss: 0.36924493312835693\n",
      "Step: 7800  \tTraining accuracy: 0.7604971528053284\n",
      "Step: 7800  \tValid loss: 0.3736763596534729\n",
      "Step: 7900  \tTraining loss: 0.3689180910587311\n",
      "Step: 7900  \tTraining accuracy: 0.7607361674308777\n",
      "Step: 7900  \tValid loss: 0.37374335527420044\n",
      "Step: 8000  \tTraining loss: 0.368595689535141\n",
      "Step: 8000  \tTraining accuracy: 0.7609692215919495\n",
      "Step: 8000  \tValid loss: 0.37382256984710693\n",
      "Step: 8100  \tTraining loss: 0.3682782053947449\n",
      "Step: 8100  \tTraining accuracy: 0.761196494102478\n",
      "Step: 8100  \tValid loss: 0.37394362688064575\n",
      "Step: 8200  \tTraining loss: 0.36796772480010986\n",
      "Step: 8200  \tTraining accuracy: 0.761397659778595\n",
      "Step: 8200  \tValid loss: 0.37407445907592773\n",
      "Step: 8300  \tTraining loss: 0.3676621913909912\n",
      "Step: 8300  \tTraining accuracy: 0.7615939974784851\n",
      "Step: 8300  \tValid loss: 0.374233216047287\n",
      "Step: 8400  \tTraining loss: 0.3673619031906128\n",
      "Step: 8400  \tTraining accuracy: 0.7617855668067932\n",
      "Step: 8400  \tValid loss: 0.37437835335731506\n",
      "Step: 8500  \tTraining loss: 0.36706602573394775\n",
      "Step: 8500  \tTraining accuracy: 0.7619726061820984\n",
      "Step: 8500  \tValid loss: 0.37451067566871643\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7621553\n",
      "Precision: 0.7777778\n",
      "Recall: 0.8206897\n",
      "F1 score: 0.78261584\n",
      "AUC: 0.8006674\n",
      "   accuracy  precision   recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.762155   0.777778  0.82069  0.782616  0.800667  0.366888      0.761932   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.373546       0.761927   0.488988      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  8560.0  \n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_41'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-aaf72d910029>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_41'"
     ]
    }
   ],
   "source": [
    "neurons = 8\n",
    "for subj_num in range(41,42):\n",
    "# for subj_num in range(28,42):\n",
    "\n",
    "# for subj_num in [20]:##[15, 16, 17]:# 19, 20, 23, 24, 25, 26, 29, 36, 37, 40]:\n",
    "\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    PT_file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/\"\n",
    "    PT_file_name = PT_file_path  + \"PT_loss_updated.csv\"\n",
    "    PT_metrics = pd.read_csv(PT_file_name)\n",
    "    PT_metrics = PT_metrics[PT_metrics.PT_loss !=0]\n",
    "    PT_R2= PT_metrics[PT_metrics.Subject_number ==subj_num].PT_pseudoR2.iloc[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for generated data ##\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_subj29_params.csv\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_1500_subj29_params.csv\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_3000_subj29_params.csv\"\n",
    "\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1\"\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5\"\n",
    "\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1/generateddata1500mu1params.csv\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5/generateddata300mu0_5params.csv\"\n",
    "\n",
    "\n",
    "\n",
    "############# ORIGINAL ##############\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5/generateddata3000mu0_5params.csv\"\n",
    "\n",
    "\n",
    "# ############ ACTUAL ###############\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5/generateddata600mu0_5params.csv\"\n",
    "\n",
    "\n",
    "#     task_df=pd.read_csv(file_name)\n",
    "# #     task_df.TrialNum = task_df.TrialNum-1\n",
    "\n",
    "#     task_df = add_releveant_features(task_df)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### ORIGINAL\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining\"\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining_v2\"\n",
    "\n",
    "\n",
    "#     task_df = task_net_df; dopa_task_df = task_net_df;\n",
    "    \n",
    "    #############################\n",
    "#     task_df = task_net_train_df\n",
    "#     dopa_task_df  = task_net_valid_df\n",
    "\n",
    "    \n",
    "    \n",
    "     \n",
    "    ### ACTUAL DATA PER SUBJECT\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/subject_num_\"+str(subj_num)+\"/first_half_train\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X,train_y= data_split(task_df,dopa_task_df)\n",
    "\n",
    "    \n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,10)\n",
    "\n",
    "    metric_out_df, prob_train, prob_test, prob_val = train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y)\n",
    "    \n",
    "    print(metric_out_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     os.mkdir(file_path + \"50_splits_combined_1sthalf\")\n",
    "#     os.mkdir(file_path + \"50_splits_combined_2ndhalf\")\n",
    "\n",
    "\n",
    "    \n",
    "#     os.mkdir(file_path + \"combined_1sthalf\")\n",
    "#     os.mkdir(file_path + \"combined_2ndhalf\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     os.mkdir(file_path + \"300_sub29\")\n",
    "#     os.mkdir(file_path + \"1500_sub29\")\n",
    "#     os.mkdir(file_path + \"3000_sub29\")\n",
    "\n",
    "#     os.mkdir(file_path + \"/300\")\n",
    "#     os.mkdir(file_path + \"/1500\")\n",
    "#     os.mkdir(file_path + \"/3000\")\n",
    "#     os.mkdir(file_path + \"/600\")\n",
    "\n",
    "\n",
    "\n",
    "#     os.mkdir(file_path)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_curr_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_v2_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "########################################\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#######################################\n",
    "    \n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_RT_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_CrossvalTESTinsess1sthalf_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_CrossvalTESTinsess2ndhalf_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"combined_1sthalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"combined_2ndhalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"50_splits_combined_1sthalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"50_splits_combined_2ndhalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "##### generated data ######\n",
    "#     metric_out_df.to_csv(file_path+\"300_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"1500_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"3000_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"3000_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/300/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"/1500/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/3000/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/3000/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\"_v2.csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\"_v2.csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "    metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "    \n",
    "    prob_train_df = pd.DataFrame(prob_train.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_test_df = pd.DataFrame(prob_test.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_val_df = pd.DataFrame(prob_val.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"prob_train_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"prob_test_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"prob_val_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"prob_train_currentopts_prev_outchoicevv2_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"prob_test_currentopts_prev_outchoicev2_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"prob_val_currentopts_prev_outchoicev2_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "################################\n",
    "    prob_train_df.to_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_test_df.to_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_val_df.to_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#############################\n",
    "\n",
    "    \n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "    \n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"2ndhalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"2ndhalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"2ndhalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"combined_1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"combined_1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"combined_2ndhalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"combined_2ndhalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"combined_2ndhalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"50_splits_combined_1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"50_splits_combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"50_splits_combined_1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### Generated data #######\n",
    "#     prob_train_df.to_csv(file_path + \"300_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"300_sub29_combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"300_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"1500_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"1500_sub29/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"1500_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"3000_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"3000_sub29/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"3000_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/300/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/300/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/300/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/1500/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/1500/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/1500/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/3000/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/3000/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/3000/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/600/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/600/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/600/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>loss_test</th>\n",
       "      <th>neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.773315</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.855172</td>\n",
       "      <td>0.791542</td>\n",
       "      <td>0.837264</td>\n",
       "      <td>0.334274</td>\n",
       "      <td>0.773034</td>\n",
       "      <td>0.389838</td>\n",
       "      <td>0.77301</td>\n",
       "      <td>0.563058</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>11321.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
       "0  0.773315   0.815789  0.855172  0.791542  0.837264  0.334274      0.773034   \n",
       "\n",
       "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
       "0  0.389838        0.77301   0.563058      8.0          0.001   50000.0   \n",
       "\n",
       "     steps  \n",
       "0  11321.0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.187679\n",
       "Name: loss_test, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+metric_out_df.loss_test/(np.log(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3326462289056345\n"
     ]
    }
   ],
   "source": [
    "loss_train= - ((np.dot(train_y[:,0],np.log(prob_train[:,0])) + np.dot(train_y[:,1],np.log(prob_train[:,1]))))/300\n",
    "print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7701054074366888"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train = ((np.dot(train_y[:,0],prob_train[:,0]) + np.dot(train_y[:,1],prob_train[:,1])))/300\n",
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4338135284109194\n"
     ]
    }
   ],
   "source": [
    "loss_test= - ((np.dot(test_y[:,0],np.log(prob_test[:,0])) + np.dot(test_y[:,1],np.log(prob_test[:,1]))))/150\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7385222125550112"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test = ((np.dot(test_y[:,0],prob_test[:,0]) + np.dot(test_y[:,1],prob_test[:,1])))/150\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rechecking below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path\n",
    "train_data_df = pd.read_csv(file_path+\"/train_data.csv\")\n",
    "test_data_df = pd.read_csv(file_path+\"/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_yy = train_data_df.Choice.values\n",
    "# # print(train_yy.shape)\n",
    "\n",
    "# encode_categorical = train_yy.reshape(len(train_yy), 1)\n",
    "\n",
    "# train_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "# ((np.dot(train_yy[:,0],prob_train_1[:,0]) + np.dot(train_yy[:,1],prob_train_1[:,1])))/300\n",
    "\n",
    "\n",
    "# # train_data_df.Choice.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7385222125550112"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prob_test_df = pd.read_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "prob_test_1 =prob_test_df.values[:,1:]\n",
    "test_yy = test_data_df.Choice.values\n",
    "\n",
    "\n",
    "encode_categorical = test_yy.reshape(len(test_yy), 1)\n",
    "\n",
    "test_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "((np.dot(test_yy[:,0],prob_test[:,0]) + np.dot(test_yy[:,1],prob_test[:,1])))/150\n",
    "\n",
    "# ((np.dot(test_yy[:,0],prob_test_1[:,0]) + np.dot(test_yy[:,1],prob_test_1[:,1])))/150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the above is correct. Below is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36571429],\n",
       "       [0.82857143],\n",
       "       [0.73142857],\n",
       "       [1.        ],\n",
       "       [0.6       ],\n",
       "       [0.81714286],\n",
       "       [0.85714286],\n",
       "       [1.        ],\n",
       "       [1.        ],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0:10,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject28\n",
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n"
     ]
    }
   ],
   "source": [
    "### create a composite dataset comprising all subject's actual data\n",
    "\n",
    "# task_mega_df = pd.DataFrame(); dopa_task_mega_df = pd.DataFrame();\n",
    "\n",
    "for subj_num in range(28,41):\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    \n",
    "    task_mega_df = task_mega_df.append(task_df)\n",
    "    dopa_task_mega_df = dopa_task_mega_df.append(dopa_task_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17458, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_df=pd.concat([task_mega_df,dopa_task_mega_df])\n",
    "task_net_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17458, 15)\n",
      "12139\n",
      "(12139, 8)\n",
      "(12139, 1)\n",
      "(2602, 8)\n",
      "(2602, 1)\n",
      "(2601, 8)\n",
      "(2601, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "task_df = task_net_df; dopa_task_df = task_net_df;\n",
    "train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5       , 0.25714286, 0.82857143, ..., 0.        ,\n",
       "         0.        , 0.36571429]],\n",
       "\n",
       "       [[0.28571429, 0.        , 0.73142857, ..., 0.5       ,\n",
       "         0.25714286, 0.82857143]],\n",
       "\n",
       "       [[0.92857143, 0.61714286, 1.        , ..., 0.28571429,\n",
       "         0.        , 0.73142857]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.        , 0.63428571, 1.        , ..., 0.92857143,\n",
       "         0.28571429, 1.        ]],\n",
       "\n",
       "       [[0.28571429, 0.        , 0.76      , ..., 1.        ,\n",
       "         0.63428571, 1.        ]],\n",
       "\n",
       "       [[0.5       , 0.37142857, 0.62857143, ..., 0.28571429,\n",
       "         0.        , 0.76      ]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject28\n",
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n",
      "Subject41\n"
     ]
    }
   ],
   "source": [
    "## create a composite dataset comprising all subject's actual data\n",
    "\n",
    "\n",
    "# task_net_train_df = pd.DataFrame(); task_net_valid_df = pd.DataFrame();\n",
    "\n",
    "\n",
    "# for subj_num in range(11,27):\n",
    "for subj_num in range(28,42):\n",
    "\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    task_net_train_df = task_net_train_df.append(pd.concat([task_df.loc[(task_df.TrialNum<=241) & (task_df.TrialNum>1)],dopa_task_df.loc[(dopa_task_df.TrialNum<=241) & (dopa_task_df.TrialNum>1)]]))\n",
    "    task_net_valid_df = task_net_valid_df.append(pd.concat([task_df.loc[(task_df.TrialNum>241) ],dopa_task_df.loc[(dopa_task_df.TrialNum>241) ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14400, 15)\n",
      "(14400, 8)\n",
      "(14400, 1)\n",
      "(3540, 8)\n",
      "(3540, 1)\n",
      "(3540, 8)\n",
      "(3540, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "task_df = task_net_train_df\n",
    "dopa_task_df  = task_net_valid_df\n",
    "\n",
    "train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_train_df.shape[0]/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_valid_df.shape[0]/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dopa_task_mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
