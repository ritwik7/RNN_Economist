{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as sc_stats\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "onehot_encoder=OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "time_steps = 1\n",
    "inputs = 3\n",
    "outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_releveant_features(task_df):\n",
    "\n",
    "\n",
    "    task_df['PrevOutcome']=task_df['Outcome'].shift(1)\n",
    "    task_df.loc[1,'PrevOutcome']= 0\n",
    "\n",
    "    task_df['PrevChoice']=task_df['Choice'].shift(1)\n",
    "    task_df.loc[1,'PrevChoice']= 0\n",
    "\n",
    "    task_df['PrevSafe']=task_df['Safe'].shift(1)\n",
    "    task_df.loc[1,'PrevSafe']= 0\n",
    "\n",
    "    task_df['PrevBigRisky']=task_df['BigRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevBigRisky']= 0\n",
    "\n",
    "    task_df['PrevSmallRisky']=task_df['SmallRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevSmallRisky']= 0\n",
    "    \n",
    "#     task_df['PrevRT']=task_df['RT'].shift(1)\n",
    "#     task_df.loc[1,'PrevRT']= 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    return task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_split_data(data,start_chunk,end_chunk):\n",
    "    \n",
    "    a=[k for k in range(start_chunk,end_chunk)]\n",
    "    out=[]\n",
    "\n",
    "    for d in range(0,data.shape[0],20):\n",
    "\n",
    "        c= [c+d for c in a]\n",
    "        out = out+c\n",
    "\n",
    "    while out[-1]>=data.shape[0]-1:\n",
    "        out.pop()\n",
    "#     return out\n",
    "    return data[out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y): \n",
    "    reset_graph()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50000\n",
    "    batch_size = int(train_X.shape[0]/2)\n",
    "    # batch_size = 100\n",
    "    length = train_X.shape[0]\n",
    "    display = 100\n",
    "    neurons = neurons\n",
    "\n",
    "    num_batches = 100\n",
    "    seq_len = 10\n",
    "\n",
    "    percent_above_PT = 1\n",
    "\n",
    "    train_threshold = 1.5#PT_R2 + percent_above_PT\n",
    "\n",
    "\n",
    "    save_step = 100\n",
    "\n",
    "\n",
    "    best_loss_val = np.infty\n",
    "    checks_since_last_progress = 0\n",
    "    max_checks_without_progress = 1000\n",
    "\n",
    "\n",
    "    # clear graph (if any) before running\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, time_steps, inputs])\n",
    "\n",
    "    y = tf.placeholder(tf.float32, [None, outputs])\n",
    "\n",
    "    # LSTM Cell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=neurons, activation=tf.nn.relu)\n",
    "    cell_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "    # pass into Dense layer\n",
    "    stacked_outputs = tf.reshape(cell_outputs, [-1, neurons])\n",
    "    out = tf.layers.dense(inputs=stacked_outputs, units=outputs)\n",
    "\n",
    "    probability = tf.nn.softmax(out)\n",
    "\n",
    "    # squared error loss or cost function for linear regression\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=y, logits=out))\n",
    "\n",
    "    # optimizer to minimize cost\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels =  tf.argmax(y, 1),\n",
    "                          predictions = tf.argmax(out, 1),\n",
    "                          name = \"accuracy\")\n",
    "    precision = tf.metrics.precision(labels=tf.argmax(y, 1),\n",
    "                                 predictions=tf.argmax(out, 1),\n",
    "                                 name=\"precision\")\n",
    "    recall = tf.metrics.recall(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"recall\")\n",
    "    f1 = 2 * accuracy[1] * recall[1] / ( precision[1] + recall[1] )\n",
    "\n",
    "    acc_up,acc_val = accuracy\n",
    "    auc = tf.metrics.auc(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"auc\")\n",
    "    \n",
    "    valid_store = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #######################\n",
    "#         saver.restore(sess, \"./checkpts/Original_RNN_LSTM_8features_v2.ckpt\")\n",
    "#         saver.restore(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "        \n",
    "        if pretraining == True:\n",
    "\n",
    "            saver.restore(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        #######################\n",
    "        \n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "\n",
    "        # Train the model\n",
    "        for steps in range(epochs):\n",
    "            mini_batch = zip(range(0, length, batch_size),\n",
    "                       range(batch_size, length+1, batch_size))\n",
    "\n",
    "            # train data in mini-batches\n",
    "            for (start, end) in mini_batch:\n",
    "    #             print(start,end)\n",
    "                sess.run(training_op, feed_dict = {X: train_X[start:end,:,:],\n",
    "                                                   y: train_y[start:end,:]}) \n",
    "\n",
    "            ## train data in batches of length subsequence\n",
    "\n",
    "    #         for k in range(num_batches):\n",
    "    #             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "\n",
    "    #             sess.run(training_op, feed_dict = {X:X_seq,y:y_seq}) \n",
    "            loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "            loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "            # print training performance \n",
    "            if (steps+1) % display == 0:\n",
    "                # evaluate loss function on training set\n",
    "\n",
    "\n",
    "                loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining loss: {}'.format((steps+1), loss_fn))\n",
    "\n",
    "                acc_train = acc_val.eval(feed_dict={X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining accuracy: {}'.format((steps+1), acc_train))\n",
    "\n",
    "\n",
    "                acc_test = acc_val.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest accuracy: {}'.format((steps+1), acc_test))\n",
    "\n",
    "                loss_test = loss.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest loss: {}'.format((steps+1), loss_test))\n",
    "\n",
    "                accu_val = acc_val.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "                loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "                print('Step: {}  \\tValid loss: {}'.format((steps+1), loss_val))\n",
    "\n",
    "                valid_store.append(loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (1 + loss_fn/np.log(0.5)) > train_threshold:\n",
    "                    print(\"Threshold achieved, quit training\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            if loss_val < best_loss_val:\n",
    "\n",
    "                        best_loss_val = loss_val\n",
    "                        checks_since_last_progress = 0\n",
    "            else:\n",
    "                            checks_since_last_progress += 1\n",
    "\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if checks_since_last_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "\n",
    "            if (steps+1) % save_step ==0:\n",
    "                                save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#                 save_path = saver.save(sess, \"./checkpts/RNN_Internet_LSTM_model_5features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     evaluate model accuracy\n",
    "        acc, prec, recall, f1, AUC = sess.run([accuracy, precision, recall, f1,auc],\n",
    "                                         feed_dict = {X: train_X, y: train_y})\n",
    "        prob_train = probability.eval(feed_dict = {X: train_X, y: train_y})\n",
    "        prob_test = probability.eval(feed_dict = {X: test_X, y: test_y})\n",
    "        prob_valid = probability.eval(feed_dict = {X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nEvaluation  on training set')\n",
    "        print('Accuracy:', acc[1])\n",
    "        print('Precision:', prec[1])\n",
    "        print('Recall:', recall[1])\n",
    "        print('F1 score:', f1)\n",
    "        print('AUC:', AUC[1])\n",
    "        \n",
    "      \n",
    "    \n",
    "    \n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/LaterDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## APP DATA\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_APPDATA_RNN_LSTM_8features.ckpt\")\n",
    "        save_path = saver.save(sess, \"./checkpts/Later_v2_APPDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "    metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,AUC[1],loss_fn,accu_val,best_loss_val,acc_test,loss_test,neurons,learning_rate,epochs,steps]).reshape(-1,14),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\",\"loss\",\"accuracy_val\",\"loss_val\",\"accuracy_test\",\"loss_test\",\"neurons\",\"learning_rate\",\"n_epochs\",\"steps\"])\n",
    "    return metric_out_df, prob_train, prob_test, prob_valid\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def random_subsequence(X,y,seq_len):\n",
    "    rnd  = random.randint(0,len(X)-seq_len)\n",
    "    X_seq, y_seq = X[rnd:rnd+seq_len,:], y[rnd:rnd+seq_len,:]\n",
    "    return X_seq, y_seq\n",
    "\n",
    "    print(y_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odd plays train, even plays test and valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate subject files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\")\n",
    "dir_path =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"\n",
    "\n",
    "subj_files_list =[]; ## list of subject_files fullfilling a criteria\n",
    "\n",
    "dir_files = [i for i in os.listdir(dir_path) if i.startswith('sub')]\n",
    "\n",
    "for subj_file_path in dir_files:\n",
    "\n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "    mypath =file_path\n",
    "    \n",
    "    play_names = [i for i in os.listdir(mypath) if os.path.isfile(os.path.join(mypath,i)) and i.startswith('app')]   \n",
    "    \n",
    "    if len(play_names) >= 50: ## criteria\n",
    "        subj_files_list.append(subj_file_path)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subj_files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate train, valid and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "num_shuffles=1\n",
    "for num, subj_file_path in enumerate(subj_files_list):\n",
    "    print(num)\n",
    "# for subj_file_path in [subj_files_list[0]]:\n",
    "    \n",
    "#     train_data,test_data, val_data = np.empty((0,task_df.columns.shape[0])),  np.empty((0,task_df.columns.shape[0])), np.empty((0,task_df.columns.shape[0]))\n",
    "    train_data,test_data, val_data = np.empty((0,15)),  np.empty((0,15)), np.empty((0,15))\n",
    "\n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "    mypath =file_path\n",
    "        \n",
    "    comp_task_train_df = pd.DataFrame()\n",
    "\n",
    "    play_names = [i for i in os.listdir(mypath) if os.path.isfile(os.path.join(mypath,i)) and i.startswith('app')]    \n",
    "\n",
    "    for randomization_counter in range(0,num_shuffles):\n",
    "            randomized_play_names= random.sample(play_names,len(play_names))\n",
    "            \n",
    "            for play_num, play_name in enumerate(randomized_play_names):\n",
    "#         for play_num,play_name in enumerate(play_names):\n",
    "\n",
    "                file_name = file_path + \"/\" + play_name\n",
    "                task_df = pd.read_csv(file_name)\n",
    "                task_df = add_releveant_features(task_df)\n",
    "\n",
    "                if np.mod(play_num,2)==0: ## odd trials\n",
    "                    train_data = np.append(train_data,task_df[task_df.TrialNum>1].values, axis=0)\n",
    "\n",
    "                else:\n",
    "                    test_data =  np.append(test_data, task_df[task_df.TrialNum>1].values[0:16], axis=0)\n",
    "                    val_data =  np.append(val_data, task_df[task_df.TrialNum>1].values[16:], axis=0)\n",
    "\n",
    "\n",
    "    train_data_df= pd.DataFrame(train_data,columns=task_df.columns)\n",
    "    val_data_df = pd.DataFrame(test_data,columns=task_df.columns)\n",
    "    test_data_df= pd.DataFrame(val_data,columns=task_df.columns)\n",
    "\n",
    "#     file_path = file_path + \"/OddEvenPlays/\"\n",
    "    file_path = file_path + \"/OddEvenPlays/RandomizedPlays1\"\n",
    "\n",
    "#     os.mkdir(file_path)\n",
    "    train_data_df.to_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df.to_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df.to_csv(file_path+\"/val_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_trials(df):\n",
    "\n",
    "    locs= random.sample([a for a in range(0,df.shape[0])],df.shape[0])\n",
    "    # len(locs)\n",
    "    df = add_releveant_features(df.loc[locs])\n",
    "    \n",
    "    ### get rid of first index since it contains NaN for previous trials\n",
    "    df  = df.iloc[1:]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_odd_even(train_data_df,test_data_df,val_data_df):\n",
    "\n",
    "#     train_len = 29\n",
    "#     test_len = 14\n",
    "#     val_len = 15\n",
    "\n",
    "    ##----------------- UNCOMMENT BELOW\n",
    "    \n",
    "    if hist_flag==0: ## CURR OPTIONS ONLY\n",
    "        \n",
    "    \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        val_X = val_data_df[['Safe','BigRisky','SmallRisky']].values\n",
    "        val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "    elif hist_flag==1: ## CURR OPTIONS, PREV ACTIONS:\n",
    "        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevChoice']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevChoice']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevChoice']].values\n",
    "        val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "        \n",
    "        \n",
    "      \n",
    "    elif hist_flag==2: # CURR OPTIONS, PREV OUTCOME        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome']].values\n",
    "        val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif hist_flag==3: ## CURR OPTIONS, PREV ACTIONS, PREV OUTCOME\n",
    "        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevChoice','PrevOutcome']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevChoice','PrevOutcome']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevChoice','PrevOutcome']].values\n",
    "        val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "             \n",
    "        \n",
    "\n",
    "####### Prev O + C+ R + CurrO--------------------\n",
    "    elif hist_flag==4: # CURR OPTIONS, PREV ACTIONS, PREV OUTCOME, PREV OPTIONS\n",
    "        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "        val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ######## sampling \n",
    "    \n",
    "    \n",
    "#### - Prev RT+C+R+O + Curr O----------------------\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### PRE TRAINING\n",
    "#     stop = int(0.7*len(train_X))\n",
    "#     print(stop)\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int((len(train_X)-stop)/2)], train_X[stop+int((len(train_X)-stop)/2):],train_y[:stop], train_y[stop:stop+int((len(train_X)-stop)/2)], train_y[stop+int((len(train_X)-stop)/2):]\n",
    "    \n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = train_X, test_X, test_X, train_y, test_y, test_y\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(val_X.shape)\n",
    "    print(val_y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "    # # center and scale\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.fit_transform(test_X)\n",
    "    val_X = scaler.fit_transform(val_X)\n",
    "\n",
    "\n",
    "    train_X = train_X[:,None,:]\n",
    "    val_X = val_X[:,None,:]\n",
    "    test_X = test_X[:,None,:]\n",
    "\n",
    "\n",
    "    # # one-hot encode the outputs\n",
    "\n",
    "    onehot_encoder = OneHotEncoder()\n",
    "    encode_categorical = train_y.reshape(len(train_y), 1)\n",
    "    encode_categorical_test = test_y.reshape(len(test_y), 1)\n",
    "    encode_categorical_val = val_y.reshape(len(val_y),1)\n",
    "\n",
    "\n",
    "    train_y = onehot_encoder.fit_transform(encode_categorical).toarray()\n",
    "    test_y = onehot_encoder.fit_transform(encode_categorical_test).toarray()\n",
    "    val_y = onehot_encoder.fit_transform(encode_categorical_val).toarray()\n",
    "\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, val_X,val_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run main training and save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4262, 3)\n",
      "(4262, 1)\n",
      "(2351, 3)\n",
      "(2351, 1)\n",
      "(1910, 3)\n",
      "(1910, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5345728993415833\n",
      "Step: 100  \tTraining accuracy: 0.7595025897026062\n",
      "Step: 100  \tValid loss: 0.5500977039337158\n",
      "Step: 200  \tTraining loss: 0.5040456652641296\n",
      "Step: 200  \tTraining accuracy: 0.7525224685668945\n",
      "Step: 200  \tValid loss: 0.5226559638977051\n",
      "Step: 300  \tTraining loss: 0.4805479049682617\n",
      "Step: 300  \tTraining accuracy: 0.7511263489723206\n",
      "Step: 300  \tValid loss: 0.49625205993652344\n",
      "Step: 400  \tTraining loss: 0.45828476548194885\n",
      "Step: 400  \tTraining accuracy: 0.7520700097084045\n",
      "Step: 400  \tValid loss: 0.4712671637535095\n",
      "Step: 500  \tTraining loss: 0.44050678610801697\n",
      "Step: 500  \tTraining accuracy: 0.7547844052314758\n",
      "Step: 500  \tValid loss: 0.4512675106525421\n",
      "Step: 600  \tTraining loss: 0.4287789762020111\n",
      "Step: 600  \tTraining accuracy: 0.7580476403236389\n",
      "Step: 600  \tValid loss: 0.4377281367778778\n",
      "Step: 700  \tTraining loss: 0.42239806056022644\n",
      "Step: 700  \tTraining accuracy: 0.7620036005973816\n",
      "Step: 700  \tValid loss: 0.43015041947364807\n",
      "Step: 800  \tTraining loss: 0.41944196820259094\n",
      "Step: 800  \tTraining accuracy: 0.7650141716003418\n",
      "Step: 800  \tValid loss: 0.42631852626800537\n",
      "Step: 900  \tTraining loss: 0.41819196939468384\n",
      "Step: 900  \tTraining accuracy: 0.7678270936012268\n",
      "Step: 900  \tValid loss: 0.42452818155288696\n",
      "Step: 1000  \tTraining loss: 0.4176558554172516\n",
      "Step: 1000  \tTraining accuracy: 0.7704306840896606\n",
      "Step: 1000  \tValid loss: 0.4237392246723175\n",
      "Step: 1100  \tTraining loss: 0.4173761010169983\n",
      "Step: 1100  \tTraining accuracy: 0.7726389169692993\n",
      "Step: 1100  \tValid loss: 0.42328575253486633\n",
      "Step: 1200  \tTraining loss: 0.41713854670524597\n",
      "Step: 1200  \tTraining accuracy: 0.7744631171226501\n",
      "Step: 1200  \tValid loss: 0.42296311259269714\n",
      "Step: 1300  \tTraining loss: 0.4168812334537506\n",
      "Step: 1300  \tTraining accuracy: 0.7759954333305359\n",
      "Step: 1300  \tValid loss: 0.42267072200775146\n",
      "Step: 1400  \tTraining loss: 0.41659411787986755\n",
      "Step: 1400  \tTraining accuracy: 0.7773007154464722\n",
      "Step: 1400  \tValid loss: 0.42240482568740845\n",
      "Step: 1500  \tTraining loss: 0.41625988483428955\n",
      "Step: 1500  \tTraining accuracy: 0.7784259915351868\n",
      "Step: 1500  \tValid loss: 0.4220861494541168\n",
      "Step: 1600  \tTraining loss: 0.41587063670158386\n",
      "Step: 1600  \tTraining accuracy: 0.7794060707092285\n",
      "Step: 1600  \tValid loss: 0.42173489928245544\n",
      "Step: 1700  \tTraining loss: 0.41542744636535645\n",
      "Step: 1700  \tTraining accuracy: 0.7802673578262329\n",
      "Step: 1700  \tValid loss: 0.42135441303253174\n",
      "Step: 1800  \tTraining loss: 0.4149402379989624\n",
      "Step: 1800  \tTraining accuracy: 0.7810302376747131\n",
      "Step: 1800  \tValid loss: 0.4209558665752411\n",
      "Step: 1900  \tTraining loss: 0.41442397236824036\n",
      "Step: 1900  \tTraining accuracy: 0.7817359566688538\n",
      "Step: 1900  \tValid loss: 0.42055386304855347\n",
      "Step: 2000  \tTraining loss: 0.41389742493629456\n",
      "Step: 2000  \tTraining accuracy: 0.782387375831604\n",
      "Step: 2000  \tValid loss: 0.4201641082763672\n",
      "Step: 2100  \tTraining loss: 0.41337695717811584\n",
      "Step: 2100  \tTraining accuracy: 0.7829752564430237\n",
      "Step: 2100  \tValid loss: 0.4197962284088135\n",
      "Step: 2200  \tTraining loss: 0.4128737151622772\n",
      "Step: 2200  \tTraining accuracy: 0.7835084199905396\n",
      "Step: 2200  \tValid loss: 0.41945093870162964\n",
      "Step: 2300  \tTraining loss: 0.4123917520046234\n",
      "Step: 2300  \tTraining accuracy: 0.783994197845459\n",
      "Step: 2300  \tValid loss: 0.4191201329231262\n",
      "Step: 2400  \tTraining loss: 0.4119311571121216\n",
      "Step: 2400  \tTraining accuracy: 0.7844935655593872\n",
      "Step: 2400  \tValid loss: 0.418794721364975\n",
      "Step: 2500  \tTraining loss: 0.4114934206008911\n",
      "Step: 2500  \tTraining accuracy: 0.7850527167320251\n",
      "Step: 2500  \tValid loss: 0.4184727072715759\n",
      "Step: 2600  \tTraining loss: 0.4110810160636902\n",
      "Step: 2600  \tTraining accuracy: 0.7855404019355774\n",
      "Step: 2600  \tValid loss: 0.418157696723938\n",
      "Step: 2700  \tTraining loss: 0.4106970727443695\n",
      "Step: 2700  \tTraining accuracy: 0.7859780192375183\n",
      "Step: 2700  \tValid loss: 0.41785746812820435\n",
      "Step: 2800  \tTraining loss: 0.4103420078754425\n",
      "Step: 2800  \tTraining accuracy: 0.7863838076591492\n",
      "Step: 2800  \tValid loss: 0.41757744550704956\n",
      "Step: 2900  \tTraining loss: 0.41001495718955994\n",
      "Step: 2900  \tTraining accuracy: 0.7867611050605774\n",
      "Step: 2900  \tValid loss: 0.4173145890235901\n",
      "Step: 3000  \tTraining loss: 0.409713476896286\n",
      "Step: 3000  \tTraining accuracy: 0.7871128916740417\n",
      "Step: 3000  \tValid loss: 0.4170762300491333\n",
      "Step: 3100  \tTraining loss: 0.4094335436820984\n",
      "Step: 3100  \tTraining accuracy: 0.7874415516853333\n",
      "Step: 3100  \tValid loss: 0.4168577492237091\n",
      "Step: 3200  \tTraining loss: 0.4091719686985016\n",
      "Step: 3200  \tTraining accuracy: 0.7877493500709534\n",
      "Step: 3200  \tValid loss: 0.4166528284549713\n",
      "Step: 3300  \tTraining loss: 0.4089254140853882\n",
      "Step: 3300  \tTraining accuracy: 0.7880381941795349\n",
      "Step: 3300  \tValid loss: 0.41645678877830505\n",
      "Step: 3400  \tTraining loss: 0.4086915850639343\n",
      "Step: 3400  \tTraining accuracy: 0.7883098125457764\n",
      "Step: 3400  \tValid loss: 0.4162670373916626\n",
      "Step: 3500  \tTraining loss: 0.4084681570529938\n",
      "Step: 3500  \tTraining accuracy: 0.7885656356811523\n",
      "Step: 3500  \tValid loss: 0.4160802364349365\n",
      "Step: 3600  \tTraining loss: 0.40825238823890686\n",
      "Step: 3600  \tTraining accuracy: 0.7888070940971375\n",
      "Step: 3600  \tValid loss: 0.4158937335014343\n",
      "Step: 3700  \tTraining loss: 0.4080435335636139\n",
      "Step: 3700  \tTraining accuracy: 0.7890224456787109\n",
      "Step: 3700  \tValid loss: 0.41570591926574707\n",
      "Step: 3800  \tTraining loss: 0.40784093737602234\n",
      "Step: 3800  \tTraining accuracy: 0.7892075777053833\n",
      "Step: 3800  \tValid loss: 0.4155184328556061\n",
      "Step: 3900  \tTraining loss: 0.40764546394348145\n",
      "Step: 3900  \tTraining accuracy: 0.7894013524055481\n",
      "Step: 3900  \tValid loss: 0.41532883048057556\n",
      "Step: 4000  \tTraining loss: 0.4074562191963196\n",
      "Step: 4000  \tTraining accuracy: 0.789585292339325\n",
      "Step: 4000  \tValid loss: 0.41513869166374207\n",
      "Step: 4100  \tTraining loss: 0.4072747230529785\n",
      "Step: 4100  \tTraining accuracy: 0.7898065447807312\n",
      "Step: 4100  \tValid loss: 0.4149504601955414\n",
      "Step: 4200  \tTraining loss: 0.40710169076919556\n",
      "Step: 4200  \tTraining accuracy: 0.7900171279907227\n",
      "Step: 4200  \tValid loss: 0.4147641956806183\n",
      "Step: 4300  \tTraining loss: 0.4069375693798065\n",
      "Step: 4300  \tTraining accuracy: 0.7902177572250366\n",
      "Step: 4300  \tValid loss: 0.41458165645599365\n",
      "Step: 4400  \tTraining loss: 0.4067825973033905\n",
      "Step: 4400  \tTraining accuracy: 0.7904173135757446\n",
      "Step: 4400  \tValid loss: 0.4144040048122406\n",
      "Step: 4500  \tTraining loss: 0.4066374599933624\n",
      "Step: 4500  \tTraining accuracy: 0.7906210422515869\n",
      "Step: 4500  \tValid loss: 0.4142322242259979\n",
      "Step: 4600  \tTraining loss: 0.406502902507782\n",
      "Step: 4600  \tTraining accuracy: 0.7908158302307129\n",
      "Step: 4600  \tValid loss: 0.4140682518482208\n",
      "Step: 4700  \tTraining loss: 0.4063788652420044\n",
      "Step: 4700  \tTraining accuracy: 0.791024923324585\n",
      "Step: 4700  \tValid loss: 0.4139135479927063\n",
      "Step: 4800  \tTraining loss: 0.40626510977745056\n",
      "Step: 4800  \tTraining accuracy: 0.7912326455116272\n",
      "Step: 4800  \tValid loss: 0.4137682318687439\n",
      "Step: 4900  \tTraining loss: 0.40616196393966675\n",
      "Step: 4900  \tTraining accuracy: 0.7914317846298218\n",
      "Step: 4900  \tValid loss: 0.4136340320110321\n",
      "Step: 5000  \tTraining loss: 0.40606945753097534\n",
      "Step: 5000  \tTraining accuracy: 0.7916229367256165\n",
      "Step: 5000  \tValid loss: 0.41351157426834106\n",
      "Step: 5100  \tTraining loss: 0.4059866666793823\n",
      "Step: 5100  \tTraining accuracy: 0.7918064594268799\n",
      "Step: 5100  \tValid loss: 0.4134008586406708\n",
      "Step: 5200  \tTraining loss: 0.40591344237327576\n",
      "Step: 5200  \tTraining accuracy: 0.791982889175415\n",
      "Step: 5200  \tValid loss: 0.41330236196517944\n",
      "Step: 5300  \tTraining loss: 0.40584900975227356\n",
      "Step: 5300  \tTraining accuracy: 0.7921525835990906\n",
      "Step: 5300  \tValid loss: 0.41321656107902527\n",
      "Step: 5400  \tTraining loss: 0.40579214692115784\n",
      "Step: 5400  \tTraining accuracy: 0.7923159003257751\n",
      "Step: 5400  \tValid loss: 0.4131416380405426\n",
      "Step: 5500  \tTraining loss: 0.4057416617870331\n",
      "Step: 5500  \tTraining accuracy: 0.7924732565879822\n",
      "Step: 5500  \tValid loss: 0.41307714581489563\n",
      "Step: 5600  \tTraining loss: 0.4056980609893799\n",
      "Step: 5600  \tTraining accuracy: 0.7926249504089355\n",
      "Step: 5600  \tValid loss: 0.4130237102508545\n",
      "Step: 5700  \tTraining loss: 0.40565913915634155\n",
      "Step: 5700  \tTraining accuracy: 0.7927712798118591\n",
      "Step: 5700  \tValid loss: 0.4129786789417267\n",
      "Step: 5800  \tTraining loss: 0.4056236147880554\n",
      "Step: 5800  \tTraining accuracy: 0.792912483215332\n",
      "Step: 5800  \tValid loss: 0.41294026374816895\n",
      "Step: 5900  \tTraining loss: 0.4055919349193573\n",
      "Step: 5900  \tTraining accuracy: 0.7930488586425781\n",
      "Step: 5900  \tValid loss: 0.4129098057746887\n",
      "Step: 6000  \tTraining loss: 0.4055626392364502\n",
      "Step: 6000  \tTraining accuracy: 0.7931807041168213\n",
      "Step: 6000  \tValid loss: 0.4128826856613159\n",
      "Step: 6100  \tTraining loss: 0.4055355489253998\n",
      "Step: 6100  \tTraining accuracy: 0.7933081388473511\n",
      "Step: 6100  \tValid loss: 0.4128596782684326\n",
      "Step: 6200  \tTraining loss: 0.4055095613002777\n",
      "Step: 6200  \tTraining accuracy: 0.7934314608573914\n",
      "Step: 6200  \tValid loss: 0.4128391742706299\n",
      "Step: 6300  \tTraining loss: 0.40548455715179443\n",
      "Step: 6300  \tTraining accuracy: 0.7935508489608765\n",
      "Step: 6300  \tValid loss: 0.41282057762145996\n",
      "Step: 6400  \tTraining loss: 0.40546005964279175\n",
      "Step: 6400  \tTraining accuracy: 0.793666422367096\n",
      "Step: 6400  \tValid loss: 0.4128032326698303\n",
      "Step: 6500  \tTraining loss: 0.4054363965988159\n",
      "Step: 6500  \tTraining accuracy: 0.7937784194946289\n",
      "Step: 6500  \tValid loss: 0.4127871096134186\n",
      "Step: 6600  \tTraining loss: 0.40541353821754456\n",
      "Step: 6600  \tTraining accuracy: 0.7938870191574097\n",
      "Step: 6600  \tValid loss: 0.41277259588241577\n",
      "Step: 6700  \tTraining loss: 0.40539103746414185\n",
      "Step: 6700  \tTraining accuracy: 0.7939924001693726\n",
      "Step: 6700  \tValid loss: 0.4127586781978607\n",
      "Step: 6800  \tTraining loss: 0.4053685963153839\n",
      "Step: 6800  \tTraining accuracy: 0.7940945625305176\n",
      "Step: 6800  \tValid loss: 0.4127454161643982\n",
      "Step: 6900  \tTraining loss: 0.4053463637828827\n",
      "Step: 6900  \tTraining accuracy: 0.7941938042640686\n",
      "Step: 6900  \tValid loss: 0.41273269057273865\n",
      "Step: 7000  \tTraining loss: 0.4053249657154083\n",
      "Step: 7000  \tTraining accuracy: 0.7942901849746704\n",
      "Step: 7000  \tValid loss: 0.41272106766700745\n",
      "Step: 7100  \tTraining loss: 0.4053031802177429\n",
      "Step: 7100  \tTraining accuracy: 0.7943838238716125\n",
      "Step: 7100  \tValid loss: 0.4127093553543091\n",
      "Step: 7200  \tTraining loss: 0.4052816331386566\n",
      "Step: 7200  \tTraining accuracy: 0.7944748401641846\n",
      "Step: 7200  \tValid loss: 0.4126984775066376\n",
      "Step: 7300  \tTraining loss: 0.40526050329208374\n",
      "Step: 7300  \tTraining accuracy: 0.794563353061676\n",
      "Step: 7300  \tValid loss: 0.41268816590309143\n",
      "Step: 7400  \tTraining loss: 0.4052397906780243\n",
      "Step: 7400  \tTraining accuracy: 0.7946494817733765\n",
      "Step: 7400  \tValid loss: 0.4126785397529602\n",
      "Step: 7500  \tTraining loss: 0.4052193760871887\n",
      "Step: 7500  \tTraining accuracy: 0.7947332262992859\n",
      "Step: 7500  \tValid loss: 0.4126697778701782\n",
      "Step: 7600  \tTraining loss: 0.4051988422870636\n",
      "Step: 7600  \tTraining accuracy: 0.7948148250579834\n",
      "Step: 7600  \tValid loss: 0.41266101598739624\n",
      "Step: 7700  \tTraining loss: 0.40517914295196533\n",
      "Step: 7700  \tTraining accuracy: 0.794894278049469\n",
      "Step: 7700  \tValid loss: 0.4126533567905426\n",
      "Step: 7800  \tTraining loss: 0.40515926480293274\n",
      "Step: 7800  \tTraining accuracy: 0.7949716448783875\n",
      "Step: 7800  \tValid loss: 0.4126461148262024\n",
      "Step: 7900  \tTraining loss: 0.4051399827003479\n",
      "Step: 7900  \tTraining accuracy: 0.7950470447540283\n",
      "Step: 7900  \tValid loss: 0.41263988614082336\n",
      "Step: 8000  \tTraining loss: 0.4051207900047302\n",
      "Step: 8000  \tTraining accuracy: 0.7951205968856812\n",
      "Step: 8000  \tValid loss: 0.4126345217227936\n",
      "Step: 8100  \tTraining loss: 0.4050999879837036\n",
      "Step: 8100  \tTraining accuracy: 0.7951922416687012\n",
      "Step: 8100  \tValid loss: 0.4126454293727875\n",
      "Step: 8200  \tTraining loss: 0.4050803482532501\n",
      "Step: 8200  \tTraining accuracy: 0.7952622175216675\n",
      "Step: 8200  \tValid loss: 0.41265520453453064\n",
      "Step: 8300  \tTraining loss: 0.40506482124328613\n",
      "Step: 8300  \tTraining accuracy: 0.7953304052352905\n",
      "Step: 8300  \tValid loss: 0.4126613140106201\n",
      "Step: 8400  \tTraining loss: 0.4050499498844147\n",
      "Step: 8400  \tTraining accuracy: 0.7953970432281494\n",
      "Step: 8400  \tValid loss: 0.4126709997653961\n",
      "Step: 8500  \tTraining loss: 0.4050350785255432\n",
      "Step: 8500  \tTraining accuracy: 0.7954620718955994\n",
      "Step: 8500  \tValid loss: 0.41267046332359314\n",
      "Step: 8600  \tTraining loss: 0.4050202965736389\n",
      "Step: 8600  \tTraining accuracy: 0.7955255508422852\n",
      "Step: 8600  \tValid loss: 0.4126795828342438\n",
      "Step: 8700  \tTraining loss: 0.4050062596797943\n",
      "Step: 8700  \tTraining accuracy: 0.7955875992774963\n",
      "Step: 8700  \tValid loss: 0.4126874506473541\n",
      "Step: 8800  \tTraining loss: 0.40499234199523926\n",
      "Step: 8800  \tTraining accuracy: 0.7956616282463074\n",
      "Step: 8800  \tValid loss: 0.41269806027412415\n",
      "Step: 8900  \tTraining loss: 0.40497887134552\n",
      "Step: 8900  \tTraining accuracy: 0.7957260012626648\n",
      "Step: 8900  \tValid loss: 0.41269275546073914\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.795789\n",
      "Precision: 0.84684163\n",
      "Recall: 0.90701264\n",
      "F1 score: 0.8230908\n",
      "AUC: 0.69448197\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.795789   0.846842  0.907013  0.823091  0.694482  0.404965      0.795738   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.412634       0.795716   0.413293      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  8998.0  \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_86257/OddEvenPlays/RandomizedPlays1/RandomizeTrials/LSTM_updated_Crossval_currO_metricsneurons=8.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-94175d7e9f25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhist_flag\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mmetric_out_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/LSTM_updated_Crossval_currO_metricsneurons=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mprob_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/prob_train_currO_neurons=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mprob_test_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/prob_test_currO_neurons=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3018\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3019\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 3020\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m    156\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                                      compression=self.compression)\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# Python 3 and encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0;31m# Python 3 and no explicit encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_86257/OddEvenPlays/RandomizedPlays1/RandomizeTrials/LSTM_updated_Crossval_currO_metricsneurons=8.csv'"
     ]
    }
   ],
   "source": [
    "neurons = 8\n",
    "hist_flag=0\n",
    "randomize_trials_flag=True\n",
    "\n",
    "\n",
    "for num, subj_file_path in enumerate(subj_files_list[1:]):\n",
    "#     print(num)\n",
    "# for subj_file_path in [subj_files_list[0]]:\n",
    "    \n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "                \n",
    "#     file_path = file_path + \"/OddEvenPlays\"\n",
    "    file_path = file_path + \"/OddEvenPlays/RandomizedPlays1\"\n",
    "\n",
    "\n",
    "\n",
    "    train_data_df= pd.read_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df = pd.read_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df = pd.read_csv(file_path+\"/val_data.csv\")\n",
    "    \n",
    "\n",
    "    if randomize_trials_flag==True:\n",
    "        file_path = file_path + \"/RandomizeTrials\"\n",
    "        os.mkdir(file_path)\n",
    "        train_data_df = randomize_trials(train_data_df)\n",
    "        val_data_df = randomize_trials(val_data_df)\n",
    "        test_data_df = randomize_trials(test_data_df)\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    train_X, train_y, test_X, test_y,val_X,val_y = data_split_odd_even(train_data_df,test_data_df,val_data_df)\n",
    "\n",
    "    pretraining = False; \n",
    "    metric_out_df, prob_train, prob_test, prob_val = train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y)\n",
    "    \n",
    "    print(metric_out_df)\n",
    "    \n",
    "   \n",
    " \n",
    "    prob_train_df = pd.DataFrame(prob_train.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_test_df = pd.DataFrame(prob_test.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_val_df = pd.DataFrame(prob_val.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "\n",
    "    if hist_flag==0:\n",
    "        metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currO_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(file_path + \"/prob_train_currO_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(file_path + \"/prob_test_currO_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_val_df.to_csv(file_path + \"/prob_val_currO_neurons=\"+str(neurons)+\".csv\")\n",
    "    \n",
    "    \n",
    "    elif hist_flag==1:\n",
    "        metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currOprevC_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(file_path + \"/prob_train_currOprevC_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(file_path + \"/prob_test_currOprevC_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_val_df.to_csv(file_path + \"/prob_val_currOprevC_neurons=\"+str(neurons)+\".csv\")\n",
    "        \n",
    "        \n",
    "    elif hist_flag==2:\n",
    "        metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currOprevR_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(file_path + \"/prob_train_currOprevR_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(file_path + \"/prob_test_currOprevR_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_val_df.to_csv(file_path + \"/prob_val_currOprevR_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "    elif hist_flag==3:\n",
    "        metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currOprevRC_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(file_path + \"/prob_train_currOprevRC_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(file_path + \"/prob_test_currOprevRC_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_val_df.to_csv(file_path + \"/prob_val_currOprevRC_neurons=\"+str(neurons)+\".csv\")\n",
    "# ################################\n",
    "    elif hist_flag==4:\n",
    "        metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_val_df.to_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "# #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_out = randomize_trials(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TrialNum</th>\n",
       "      <th>SideOfScreen</th>\n",
       "      <th>Safe</th>\n",
       "      <th>BigRisky</th>\n",
       "      <th>SmallRisky</th>\n",
       "      <th>SideChosen</th>\n",
       "      <th>Choice</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>RT</th>\n",
       "      <th>Happiness</th>\n",
       "      <th>PrevOutcome</th>\n",
       "      <th>PrevChoice</th>\n",
       "      <th>PrevSafe</th>\n",
       "      <th>PrevBigRisky</th>\n",
       "      <th>PrevSmallRisky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-92.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>3.710</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>1.287</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.298</td>\n",
       "      <td>0.54</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>1.222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.238</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-79.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  TrialNum  SideOfScreen  Safe  BigRisky  SmallRisky  SideChosen  \\\n",
       "0           0       2.0           1.0 -35.0       0.0       -92.0         NaN   \n",
       "1           1       3.0           1.0 -35.0       0.0       -59.0         NaN   \n",
       "2           2       4.0           1.0  55.0     150.0         0.0         NaN   \n",
       "3           3       5.0           1.0  35.0      85.0         0.0         NaN   \n",
       "4           4       6.0           1.0 -55.0       0.0      -105.0         NaN   \n",
       "5           5       7.0           1.0  45.0      92.0         0.0         NaN   \n",
       "6           6       8.0           1.0  55.0      59.0         0.0         NaN   \n",
       "7           7       9.0           1.0  35.0     105.0         0.0         NaN   \n",
       "8           8      10.0           1.0 -45.0       0.0       -79.0         NaN   \n",
       "9           9      11.0           1.0   0.0      55.0       -72.0         NaN   \n",
       "\n",
       "   Choice  Outcome     RT  Happiness  PrevOutcome  PrevChoice  PrevSafe  \\\n",
       "0     1.0      0.0  3.660        NaN          0.0         0.0       0.0   \n",
       "1     1.0    -59.0  3.710       0.52          0.0         1.0     -35.0   \n",
       "2     1.0    150.0  1.183        NaN        -59.0         1.0     -35.0   \n",
       "3     1.0      0.0  1.367        NaN        150.0         1.0      55.0   \n",
       "4     1.0   -105.0  1.287       0.53          0.0         1.0      35.0   \n",
       "5     1.0      0.0  1.301        NaN       -105.0         1.0     -55.0   \n",
       "6     0.0     55.0  1.786        NaN          0.0         1.0      45.0   \n",
       "7     1.0      0.0  1.298       0.54         55.0         0.0      55.0   \n",
       "8     1.0    -79.0  1.222        NaN          0.0         1.0      35.0   \n",
       "9     1.0     55.0  1.238       0.54        -79.0         1.0     -45.0   \n",
       "\n",
       "   PrevBigRisky  PrevSmallRisky  \n",
       "0           0.0             0.0  \n",
       "1           0.0           -92.0  \n",
       "2           0.0           -59.0  \n",
       "3         150.0             0.0  \n",
       "4          85.0             0.0  \n",
       "5           0.0          -105.0  \n",
       "6          92.0             0.0  \n",
       "7          59.0             0.0  \n",
       "8         105.0             0.0  \n",
       "9           0.0           -79.0  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TrialNum</th>\n",
       "      <th>SideOfScreen</th>\n",
       "      <th>Safe</th>\n",
       "      <th>BigRisky</th>\n",
       "      <th>SmallRisky</th>\n",
       "      <th>SideChosen</th>\n",
       "      <th>Choice</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>RT</th>\n",
       "      <th>Happiness</th>\n",
       "      <th>PrevOutcome</th>\n",
       "      <th>PrevChoice</th>\n",
       "      <th>PrevSafe</th>\n",
       "      <th>PrevBigRisky</th>\n",
       "      <th>PrevSmallRisky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>548</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2.237</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-112.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>730</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>708</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-85.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-85.0</td>\n",
       "      <td>1.421</td>\n",
       "      <td>0.52</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>349</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.075</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-85.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>794</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>449</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>446</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>-54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>514</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>1093</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-111.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.293</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>211</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-111.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  TrialNum  SideOfScreen  Safe  BigRisky  SmallRisky  \\\n",
       "548          548      28.0           1.0  35.0      68.0         0.0   \n",
       "730          730       7.0           1.0  55.0     144.0         0.0   \n",
       "708          708      14.0           1.0 -35.0       0.0       -85.0   \n",
       "349          349       3.0           1.0  55.0      90.0         0.0   \n",
       "794          794      13.0           1.0 -55.0       0.0       -79.0   \n",
       "449          449      16.0           1.0   0.0      75.0       -54.0   \n",
       "446          446      13.0           1.0  55.0     118.0         0.0   \n",
       "514          514      23.0           1.0  55.0      79.0         0.0   \n",
       "1093        1093      22.0           1.0 -35.0       0.0      -111.0   \n",
       "211          211      10.0           1.0  55.0     118.0         0.0   \n",
       "\n",
       "      SideChosen  Choice  Outcome     RT  Happiness  PrevOutcome  PrevChoice  \\\n",
       "548          NaN     1.0     68.0  2.237       0.50       -112.0         1.0   \n",
       "730          NaN     1.0    144.0  1.350        NaN         68.0         1.0   \n",
       "708          NaN     1.0    -85.0  1.421       0.52        144.0         1.0   \n",
       "349          NaN     0.0     55.0  2.075       0.50        -85.0         1.0   \n",
       "794          NaN     1.0      0.0  1.321        NaN         55.0         0.0   \n",
       "449          NaN     1.0     75.0  1.622        NaN          0.0         1.0   \n",
       "446          NaN     1.0      0.0  1.471        NaN         75.0         1.0   \n",
       "514          NaN     1.0      0.0  1.219        NaN          0.0         1.0   \n",
       "1093         NaN     1.0      0.0  4.293       0.49          0.0         1.0   \n",
       "211          NaN     1.0      0.0  1.166        NaN          0.0         1.0   \n",
       "\n",
       "      PrevSafe  PrevBigRisky  PrevSmallRisky  \n",
       "548      -35.0           0.0          -112.0  \n",
       "730       35.0          68.0             0.0  \n",
       "708       55.0         144.0             0.0  \n",
       "349      -35.0           0.0           -85.0  \n",
       "794       55.0          90.0             0.0  \n",
       "449      -55.0           0.0           -79.0  \n",
       "446        0.0          75.0           -54.0  \n",
       "514       55.0         118.0             0.0  \n",
       "1093      55.0          79.0             0.0  \n",
       "211      -35.0           0.0          -111.0  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_out.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
