{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_27883/Play_by_play\n",
      "61.0\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_27883/Play_by_play/PT_generated\n",
      "(1769, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ritwik7/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "61/61 [==============================] - 0s 8ms/step\n",
      "20/20 [==============================] - 0s 410us/step\n",
      "Train: 0.708, Test: 0.653\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "# %matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as sc_stats\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "onehot_encoder=OneHotEncoder(sparse=False)\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import load_model\n",
    "from tensorflow.python.keras.metrics import Metric\n",
    "\n",
    "# Data Dimension\n",
    "inputs = 3        # number of input features\n",
    "timesteps = 29         # Timesteps\n",
    "outputs= 1         # Number of outputs\n",
    "\n",
    "\n",
    "def add_releveant_features(task_df):\n",
    "\n",
    "\n",
    "    task_df['PrevOutcome']=task_df['Outcome'].shift(1)\n",
    "    task_df.loc[1,'PrevOutcome']= 0\n",
    "\n",
    "    task_df['PrevChoice']=task_df['Choice'].shift(1)\n",
    "    task_df.loc[1,'PrevChoice']= 0\n",
    "\n",
    "    task_df['PrevSafe']=task_df['Safe'].shift(1)\n",
    "    task_df.loc[1,'PrevSafe']= 0\n",
    "\n",
    "    task_df['PrevBigRisky']=task_df['BigRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevBigRisky']= 0\n",
    "\n",
    "    task_df['PrevSmallRisky']=task_df['SmallRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevSmallRisky']= 0\n",
    "    \n",
    "#     task_df['PrevRT']=task_df['RT'].shift(1)\n",
    "#     task_df.loc[1,'PrevRT']= 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    return task_df\n",
    "\n",
    "\n",
    "\n",
    "def add_kback_features(task_df):\n",
    "\n",
    "\tfor k in range(1,11):\n",
    "\t        task_df[str(k)+'backOutcome']=task_df['Outcome'].shift(k)\n",
    "\t        task_df[str(k)+'backChoice']=task_df['Choice'].shift(k)\n",
    "\t        task_df[str(k)+'backSafe']=task_df['Safe'].shift(k)\n",
    "\t        task_df[str(k)+'backBigRisky']=task_df['BigRisky'].shift(k)\n",
    "\t        task_df[str(k)+'backSmallRisky']=task_df['SmallRisky'].shift(k)\n",
    "\n",
    "\treturn task_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compose_data(train_data_df,test_data_df,val_data_df):\n",
    "    \n",
    "    train_new_data_df = train_data_df.append(test_data_df.append(val_data_df))\n",
    "    \n",
    "    train_data = train_new_data_df.iloc[[ a for a in range(0, int(0.8*train_new_data_df.shape[0]))]]\n",
    "    test_data= train_new_data_df.iloc[[ a for a in range(int(0.8*train_new_data_df.shape[0]),train_new_data_df.shape[0])]]\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def randomize_trials(df):\n",
    "\n",
    "    locs= random.sample([a for a in range(0,df.shape[0])],df.shape[0])\n",
    "    # len(locs)\n",
    "    df = add_releveant_features(df.loc[locs])\n",
    "    \n",
    "    ### get rid of first index since it contains NaN for previous trials\n",
    "    df  = df.iloc[1:]\n",
    "    \n",
    "    return df , pd.DataFrame(locs)\n",
    "\n",
    "def generate_randomizations(train_data_df,test_data_df):\n",
    "        train_data_df,train_locs = randomize_trials(train_data_df)\n",
    "        test_data_df,test_locs = randomize_trials(test_data_df)\n",
    "       \n",
    "        \n",
    "        #         os.mkdir(file_path)\n",
    "        train_locs.to_csv(file_path+\"/train_locs.csv\")\n",
    "        test_locs.to_csv(file_path+\"/test_locs.csv\")\n",
    "                \n",
    "        return train_locs, test_locs\n",
    "\n",
    "def randomize_trials_play_by_play(df_in,play_len):\n",
    "    \n",
    "    locs_hat=[]; df=pd.DataFrame()\n",
    "    for ep in range(0, int(df_in.shape[0]/play_len)):\n",
    "        locs= random.sample([a for a in range(ep*play_len,(ep+1)*play_len)],play_len)\n",
    "        df_temp = add_releveant_features(df_in.loc[locs])\n",
    "        df_temp = add_kback_features(df_temp.loc[locs])\n",
    "        \n",
    "        df = pd.concat((df,df_temp),axis=0)\n",
    "        locs_hat.append(locs)\n",
    "\n",
    "    \n",
    "    ### get rid of first index since it contains NaN for previous trials\n",
    "#     df  = df.iloc[1:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df , pd.DataFrame(np.array(locs_hat).reshape(df_in.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "def generate_randomizations_play_by_play(train_data_df,test_data_df,play_len):\n",
    "        train_data_df,train_locs = randomize_trials_play_by_play(train_data_df,play_len)\n",
    "        test_data_df,test_locs = randomize_trials_play_by_play(test_data_df,play_len)\n",
    "       \n",
    "        \n",
    "#         #         os.mkdir(file_path)\n",
    "        train_locs.to_csv(Keras_file_path+\"/train_locs.csv\")\n",
    "        test_locs.to_csv(Keras_file_path+\"/test_locs.csv\")\n",
    "                \n",
    "        return train_locs, test_locs\n",
    "\n",
    "\n",
    "def get_shuffled_data_play_by_play(play_len):\n",
    "        train_locs = pd.read_csv(Keras_file_path+\"/train_locs.csv\")\n",
    "        test_locs = pd.read_csv(Keras_file_path+\"/test_locs.csv\")\n",
    "\n",
    "        train_data_random_df  = train_data_df.iloc[train_locs.iloc[:,1]]\n",
    "        test_data_random_df  = test_data_df.iloc[test_locs.iloc[:,1]]\n",
    "        \n",
    "#         \n",
    "        return train_data_random_df, test_data_random_df, train_locs, test_locs\n",
    "\n",
    "\n",
    "def read_randomized_data():\n",
    "    train_data_random_df = pd.read_csv(Keras_file_path + \"/train_data_random.csv\")\n",
    "    test_data_random_df = pd.read_csv(Keras_file_path + \"/test_data_random.csv\")\n",
    "    train_locs = pd.read_csv(Keras_file_path+\"/train_locs.csv\")\n",
    "    test_locs = pd.read_csv(Keras_file_path+\"/test_locs.csv\")\n",
    "    \n",
    "    return train_data_random_df, test_data_random_df, train_locs, test_locs  \n",
    "\n",
    "\n",
    "def get_shuffled_data():\n",
    "        train_locs = pd.read_csv(file_path+\"/train_locs.csv\")\n",
    "        test_locs = pd.read_csv(file_path+\"/test_locs.csv\")\n",
    "\n",
    "        train_data_random_df  = train_data_df.iloc[train_locs.iloc[:,1]]\n",
    "        test_data_random_df  = test_data_df.iloc[test_locs.iloc[:,1]]\n",
    "        \n",
    "        train_data_random_df = add_releveant_features(train_data_random_df)\n",
    "        train_data_random_df = add_kback_features(train_data_random_df)\n",
    "        test_data_random_df = add_releveant_features(test_data_random_df)\n",
    "        test_data_random_df = add_kback_features(test_data_random_df)\n",
    "        \n",
    "        return train_data_random_df, test_data_random_df, train_locs, test_locs\n",
    "\n",
    "\n",
    "def restore_input_feature(randomize_trials_flag,hist_flag):\n",
    "\n",
    "    if randomize_trials_flag==True:\n",
    "        if hist_flag==0: ## CURR OPTIONS ONLY\n",
    "            print(\"return as is\")\n",
    "            \n",
    "    \n",
    "        if hist_flag==1: ## CURR OPTIONS, PREV ACTIONS:\n",
    "            train_data_random_df.loc[train_locs.iloc[:,1],'PrevChoice'] = train_data_df.loc[train_locs.iloc[:,1],'PrevChoice']\n",
    "            test_data_random_df.loc[test_locs.iloc[:,1],'PrevChoice'] = test_data_df.loc[test_locs.iloc[:,1],'PrevChoice']\n",
    "           \n",
    "\n",
    "\n",
    "        elif hist_flag==2: # CURR OPTIONS, PREV OUTCOME        \n",
    "\n",
    "            train_data_random_df.loc[train_locs.iloc[:,1],'PrevOutcome'] = train_data_df.loc[train_locs.iloc[:,1],'PrevOutcome']\n",
    "            test_data_random_df.loc[test_locs.iloc[:,1],'PrevOutcome'] = test_data_df.loc[test_locs.iloc[:,1],'PrevOutcome']\n",
    "            \n",
    "\n",
    "        elif hist_flag==3: ## CURR OPTIONS, PREV ACTIONS, PREV OUTCOME\n",
    "\n",
    "            train_data_random_df.loc[train_locs.iloc[:,1],'PrevChoice','PrevOutcome'] = train_data_df.loc[train_locs.iloc[:,1],'PrevChoice','PrevOutcome']\n",
    "            test_data_random_df.loc[test_locs.iloc[:,1],'PrevChoice','PrevOutcome'] = test_data_df.loc[test_locs.iloc[:,1],'PrevChoice','PrevOutcome']\n",
    "            \n",
    "\n",
    "\n",
    "    ####### Prev O + C+ R + CurrO--------------------\n",
    "        elif hist_flag==4: # CURR OPTIONS, PREV ACTIONS, PREV OUTCOME, PREV OPTIONS\n",
    "            train_data_random_df.loc[train_locs.iloc[:,1],'PrevChoice','PrevOutcome','PrevSafe','PrevBigRisky','PrevSmallRisky'] = train_data_df.loc[train_locs.iloc[:,1],'PrevChoice','PrevOutcome','PrevSafe','PrevBigRisky','PrevSmallRisky']\n",
    "            test_data_random_df.loc[test_locs.iloc[:,1],'PrevChoice','PrevOutcome','PrevSafe','PrevBigRisky','PrevSmallRisky'] = test_data_df.loc[test_locs.iloc[:,1],'PrevChoice','PrevOutcome','PrevSafe','PrevBigRisky','PrevSmallRisky']\n",
    "                        \n",
    "   \n",
    "    return train_data_random_df, test_data_random_df\n",
    "\n",
    "\n",
    "def data_split_train_test(train_data_df,test_data_df):\n",
    "\n",
    "# def data_split_train_test(train_data_df,test_data_df,val_data_df):\n",
    "\n",
    "#     train_len = 29\n",
    "  \n",
    "    ##----------------- UNCOMMENT BELOW\n",
    "    \n",
    "    if hist_flag==0: ## CURR OPTIONS ONLY\n",
    "        \n",
    "    \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "#         val_X = val_data_df[['Safe','BigRisky','SmallRisky']].values\n",
    "#         val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "    elif hist_flag==1: ## CURR OPTIONS, PREV ACTIONS:\n",
    "        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevChoice']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevChoice']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "#         val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevChoice']].values\n",
    "#         val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "        \n",
    "        \n",
    "      \n",
    "    elif hist_flag==2: # CURR OPTIONS, PREV OUTCOME        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "#         val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome']].values\n",
    "#         val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif hist_flag==3: ## CURR OPTIONS, PREV ACTIONS, PREV OUTCOME\n",
    "        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevChoice','PrevOutcome']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevChoice','PrevOutcome']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "#         val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevChoice','PrevOutcome']].values\n",
    "#         val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "             \n",
    "        \n",
    "\n",
    "####### Prev O + C+ R + CurrO--------------------\n",
    "    elif hist_flag==4: # CURR OPTIONS, PREV ACTIONS, PREV OUTCOME, PREV OPTIONS\n",
    "        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "#         val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "#         val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    \n",
    "    elif hist_flag == 5: ## PREV ACTIONS\n",
    "    \n",
    "        train_X = train_data_df[['PrevChoice']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['PrevChoice']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        \n",
    "    elif hist_flag == 6: ## PREV REWARDS\n",
    "    \n",
    "        train_X = train_data_df[['PrevOutcome']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Outcome']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "    \n",
    "    elif hist_flag == 7: ## RPE\n",
    "    \n",
    "    \n",
    "        train_X = np.empty([train_data_df.shape[0],inputs]) \n",
    "        train_X[:,0:inputs-1] = train_data_df[['Safe','BigRisky','SmallRisky']]\n",
    "        ind, val  = train_data_df.loc[train_data_df.Choice==0].index, train_data_df.loc[train_data_df.Choice==0].PrevOutcome - train_data_df.loc[train_data_df.Choice==0].PrevSafe\n",
    "        train_X[ind,inputs-1:] = val[ind].values.reshape(-1,1)\n",
    "        ind, val = train_data_df.loc[train_data_df.Choice==1].index, train_data_df.loc[train_data_df.Choice==1].PrevOutcome - 0.5*(train_data_df.loc[train_data_df.Choice==1].PrevBigRisky + train_data_df.loc[train_data_df.Choice==1].PrevSmallRisky)\n",
    "        train_X[ind,inputs-1:] = val[ind].values.reshape(-1,1)\n",
    "\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "        \n",
    "        test_X = np.empty([test_data_df.shape[0],inputs]) \n",
    "        test_X[:,0:inputs-1] = test_data_df[['Safe','BigRisky','SmallRisky']]\n",
    "        ind, val  = test_data_df.loc[test_data_df.Choice==0].index, test_data_df.loc[test_data_df.Choice==0].PrevOutcome - test_data_df.loc[test_data_df.Choice==0].PrevSafe\n",
    "        test_X[ind,inputs-1:] = val[ind].values.reshape(-1,1)\n",
    "        ind, val = test_data_df.loc[test_data_df.Choice==1].index, test_data_df.loc[test_data_df.Choice==1].PrevOutcome - 0.5*(test_data_df.loc[test_data_df.Choice==1].PrevBigRisky + test_data_df.loc[test_data_df.Choice==1].PrevSmallRisky)\n",
    "        test_X[ind,inputs-1:] = val[ind].values.reshape(-1,1)\n",
    "\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "    \n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.fit_transform(test_X)\n",
    "\n",
    "    \n",
    "    train_X = train_X.reshape(-1,timesteps,inputs)\n",
    "    test_X = test_X.reshape(-1,timesteps,inputs)\n",
    "    \n",
    "    ### Add half the episodes from test_X into train_X\n",
    "#     train_X = np.concatenate((train_X, test_X[0:int(test_X.shape[0]/2),:,:]), axis=0)\n",
    "#     test_X = test_X[int(test_X.shape[0]/2):,:,:]\n",
    "#     print(train_X.shape)\n",
    "#     print(test_X.shape)\n",
    "    \n",
    "    \n",
    "    train_y = train_y.reshape(-1,timesteps,outputs)\n",
    "    test_y = test_y.reshape(-1,timesteps,outputs)\n",
    "    \n",
    "#     ### Add half the episodes from test_y into train_y\n",
    "#     train_y = np.concatenate((train_y, test_y[0:int(test_y.shape[0]/2),:,:]), axis=0)\n",
    "#     test_y = test_y[int(test_y.shape[0]/2):,:,:]\n",
    "#     print(train_y.shape)\n",
    "#     print(test_y.shape)\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y\n",
    "\n",
    "\n",
    "\n",
    "def train_RNN_play_by_play(neurons,train_X,train_y,test_X,test_y):\n",
    "\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=50)\n",
    "    # es = keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', min_delta=1)\n",
    "\n",
    "    mc =keras.callbacks.ModelCheckpoint(file_path+'/best_model.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True)\n",
    "  \n",
    "    # define problem properties\n",
    "    n_timesteps = 29\n",
    "    n_epochs = 5000\n",
    "    # define LSTM\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, input_shape=(n_timesteps, inputs), return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1, activation='sigmoid'))) ## dense + softmax activation \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=METRICS)\n",
    "\n",
    "\n",
    "    x,y = train_X,train_y\n",
    "    history=  model.fit(x, y, epochs=n_epochs, batch_size=32, verbose=0, validation_split = 0.2,callbacks=[es,mc])\n",
    "\n",
    "\n",
    "    # load the saved model\n",
    "    saved_model = load_model(file_path+'/best_model.h5')\n",
    "    # evaluate the model\n",
    "    train_loss, train_acc = saved_model.evaluate(train_X, train_y, verbose=1)\n",
    "    test_loss, test_acc = saved_model.evaluate(test_X, test_y, verbose=1)\n",
    "    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "    \n",
    "    pseudo_R2_train = 1 + train_loss/np.log(0.5)\n",
    "    pseudo_R2_test = 1 + test_loss/np.log(0.5)\n",
    "    \n",
    "    metric_out_df= pd.DataFrame(np.array([train_acc,train_loss,pseudo_R2_train, test_acc,test_loss,pseudo_R2_test,neurons,n_epochs]).reshape(-1,8),columns =[\"accuracy_train\",\"loss_train\",\"pseudoR2_train\",\"accuracy_test\",\"loss_test\",\"pseudoR2_test\",\"neurons\",\"epochs\"])\n",
    "        \n",
    "    prob_train =  model.predict_proba(train_X)\n",
    "    prob_test  =  model.predict_proba(test_X)\n",
    "    \n",
    "    \n",
    "    return metric_out_df, prob_train, prob_test\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dir_list = os.listdir(\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\")\n",
    "dir_path =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"\n",
    "\n",
    "subj_files_list =[]; ## list of subject_files fullfilling a criteria\n",
    "\n",
    "dir_files = [i for i in os.listdir(dir_path) if i.startswith('sub')]\n",
    "\n",
    "for subj_file_path in dir_files:\n",
    "\n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "    mypath =file_path\n",
    "    \n",
    "    play_names = [i for i in os.listdir(mypath) if os.path.isfile(os.path.join(mypath,i)) and i.startswith('app')]   \n",
    "    \n",
    "    if len(play_names) >= 50: ## criteria\n",
    "        subj_files_list.append(subj_file_path)\n",
    "        \n",
    "    \n",
    "neurons=8\n",
    "pretraining=False\n",
    "hist_flag=0\n",
    "\n",
    "####\n",
    "randomize_trials_flag = False\n",
    "\n",
    "for num, subj_file_path in enumerate(subj_files_list[0:1]):\n",
    "\n",
    "#\n",
    "# for num, subj_file_path in enumerate([subj_files_list[0]]):\n",
    "    print(num)\n",
    "    \n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "    Keras_file_path = file_path\n",
    "                \n",
    "#     file_path = file_path + \"/OddEvenPlays\"\n",
    "#     file_path = file_path + \"/OddEvenPlays/RandomizedPlays1\"\n",
    "\n",
    "#     train_data_df= pd.read_csv(file_path+\"/train_data.csv\")\n",
    "#     test_data_df = pd.read_csv(file_path+\"/val_test_data.csv\")\n",
    "# #     val_data_df = pd.read_csv(file_path+\"/val_data.csv\")\n",
    "    \n",
    "#     Keras_file_path = Keras_file_path + \"/Play_by_play\"\n",
    "#     print(Keras_file_path)\n",
    "#     os.mkdir(Keras_file_path)\n",
    "    \n",
    "    \n",
    "\n",
    "#### generate from PT and fit with RNN\n",
    "    Keras_file_path = Keras_file_path + \"/Play_by_play\"\n",
    "    print(Keras_file_path)\n",
    "\n",
    "\n",
    "    train_data_df= pd.read_csv(Keras_file_path+\"/PT_generated_train_data.csv\")\n",
    "    test_data_df = pd.read_csv(Keras_file_path+\"/PT_generated_test_data.csv\")\n",
    "    \n",
    "#     train_data_df= pd.read_csv(Keras_file_path+\"/train_data.csv\")\n",
    "#     test_data_df = pd.read_csv(Keras_file_path+\"/test_data.csv\")\n",
    "    \n",
    "    print(train_data_df.shape[0]/29)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Keras_file_path = Keras_file_path + \"/PT_generated\"\n",
    "    print(Keras_file_path)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###### EXTRA STEP TO COMPILE DATASET\n",
    "#     train_data_df_hat, test_data_df_hat = compose_data(train_data_df,test_data_df,val_data_df)\n",
    "#     train_data_df_hat.to_csv(file_path + \"/train_data.csv\")\n",
    "#     test_data_df_hat.to_csv(file_path + \"/test_data.csv\")\n",
    "# # #     ############################\n",
    "    \n",
    "#     print(train_data_df.shape)\n",
    "\n",
    "    if randomize_trials_flag==True:\n",
    "        Keras_file_path = Keras_file_path + \"/RandomizeTrials\"\n",
    "\n",
    "#         os.mkdir(Keras_file_path)\n",
    "\n",
    "        #### FOR GENERATING RANDOMIZATIONS\n",
    "#         train_locs, test_locs= generate_randomizations_play_by_play(train_data_df,test_data_df,play_len=29)\n",
    "        train_data_random_df, test_data_random_df, train_locs, test_locs = get_shuffled_data_play_by_play(play_len=29)\n",
    "\n",
    "# #         train_data_random_df, test_data_random_df, train_locs, test_locs  = read_randomized_data()\n",
    "#        \n",
    "        train_data_random_df, test_data_random_df = restore_input_feature(randomize_trials_flag,hist_flag)\n",
    "#       \n",
    "#         Keras_file_path = Keras_file_path + \"/UnrestoredFeatures\"\n",
    "        os.mkdir(Keras_file_path)\n",
    "        train_data_df , test_data_df = train_data_random_df, test_data_random_df\n",
    "        \n",
    "    \n",
    "     \n",
    "    print(train_data_df.shape)\n",
    "    train_X, train_y, test_X, test_y= data_split_train_test(train_data_df,test_data_df)\n",
    "    \n",
    "    metric_out_df, prob_train, prob_test = train_RNN_play_by_play(neurons,train_X,train_y,test_X,test_y)\n",
    "\n",
    "    \n",
    " \n",
    "    prob_train_df = pd.DataFrame(prob_train.reshape(prob_train.shape[0],prob_train.shape[1]))\n",
    "    prob_test_df = pd.DataFrame(prob_test.reshape(prob_test.shape[0],prob_test.shape[1]))\n",
    "\n",
    "    if hist_flag==0:\n",
    "        metric_out_df.to_csv(Keras_file_path+\"/LSTM_updated_Crossval_currO_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(Keras_file_path + \"/prob_train_currO_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(Keras_file_path + \"/prob_test_currO_neurons=\"+str(neurons)+\".csv\")\n",
    "    \n",
    "    \n",
    "    elif hist_flag==1:\n",
    "        metric_out_df.to_csv(Keras_file_path+\"/LSTM_updated_Crossval_currOprevC_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(Keras_file_path + \"/prob_train_currOprevC_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(Keras_file_path + \"/prob_test_currOprevC_neurons=\"+str(neurons)+\".csv\")\n",
    "        \n",
    "        \n",
    "    elif hist_flag==2:\n",
    "        metric_out_df.to_csv(Keras_file_path+\"/LSTM_updated_Crossval_currOprevR_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(Keras_file_path + \"/prob_train_currOprevR_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(Keras_file_path + \"/prob_test_currOprevR_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "    elif hist_flag==3:\n",
    "        metric_out_df.to_csv(Keras_file_path+\"/LSTM_updated_Crossval_currOprevRC_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(Keras_file_path + \"/prob_train_currOprevRC_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(Keras_file_path + \"/prob_test_currOprevRC_neurons=\"+str(neurons)+\".csv\")\n",
    "# ################################\n",
    "    elif hist_flag==4:\n",
    "        metric_out_df.to_csv(Keras_file_path+\"/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(Keras_file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(Keras_file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "        \n",
    "        \n",
    "    elif hist_flag==5:\n",
    "        metric_out_df.to_csv(Keras_file_path+\"/LSTM_updated_Crossval_prevC_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(Keras_file_path + \"/prob_train_prevC_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(Keras_file_path + \"/prob_test_prevC_neurons=\"+str(neurons)+\".csv\")\n",
    "        \n",
    "    elif hist_flag==6:\n",
    "        metric_out_df.to_csv(Keras_file_path+\"/LSTM_updated_Crossval_prevR_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(Keras_file_path + \"/prob_train_prevR_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(Keras_file_path + \"/prob_test_prevR_neurons=\"+str(neurons)+\".csv\")\n",
    "        \n",
    "    elif hist_flag==7: ## currO + RPE\n",
    "        metric_out_df.to_csv(Keras_file_path+\"/LSTM_updated_Crossval_currOprevRPE_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(Keras_file_path + \"/prob_train_currOprevRPE_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(Keras_file_path + \"/prob_test_currOprevRPE_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
