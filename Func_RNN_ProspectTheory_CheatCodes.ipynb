{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as sc_stats\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "onehot_encoder=OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "time_steps = 1\n",
    "inputs = 8\n",
    "outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_releveant_features(task_df):\n",
    "\n",
    "\n",
    "    task_df['PrevOutcome']=task_df['Outcome'].shift(1)\n",
    "    task_df.loc[1,'PrevOutcome']= 0\n",
    "\n",
    "    task_df['PrevChoice']=task_df['Choice'].shift(1)\n",
    "    task_df.loc[1,'PrevChoice']= 0\n",
    "\n",
    "    task_df['PrevSafe']=task_df['Safe'].shift(1)\n",
    "    task_df.loc[1,'PrevSafe']= 0\n",
    "\n",
    "    task_df['PrevBigRisky']=task_df['BigRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevBigRisky']= 0\n",
    "\n",
    "    task_df['PrevSmallRisky']=task_df['SmallRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevSmallRisky']= 0\n",
    "    \n",
    "#     task_df['PrevRT']=task_df['RT'].shift(1)\n",
    "#     task_df.loc[1,'PrevRT']= N\n",
    "    \n",
    "    \n",
    "    \n",
    "    return task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=150\n",
    "num_batches=1000\n",
    "seq_len=3\n",
    "\n",
    "def data_split(task_df,dopa_task_df):\n",
    "#     stop = 200\n",
    "\n",
    "    stop = 150\n",
    "\n",
    "#     stop=300\n",
    "\n",
    "#     stop = 750\n",
    "\n",
    "#     stop=1500\n",
    "\n",
    "\n",
    "    print(task_df.shape)\n",
    " \n",
    "\n",
    "    ##----------------- UNCOMMENT BELOW\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "    \n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "####### Prev O + C+ R + CurrO--------------------\n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    #### PLACEHOLDER VAL_X\n",
    "    val_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    val_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "#### - Prev RT+C+R+O + Curr O----------------------\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "    ###### when splitting data into train and validation\n",
    "\n",
    "    # train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.35, random_state=1)\n",
    "\n",
    "    \n",
    "#     train_X, val_X, train_y, val_y = train_X[:stop], train_X[stop:], train_y[:stop], train_y[stop:]\n",
    "\n",
    "\n",
    "####################\n",
    " ##### splitting data into train test valid from the same dataset ###############\n",
    "    \n",
    "#     train_X, val_X, test_X, train_y, val_y, test_y = train_X[:stop], train_X[stop:stop+int(stop/2)], train_X[stop+int(stop/2):], train_y[:stop], train_y[stop:stop+int(stop/2)], train_y[stop+int(stop/2):],\n",
    "\n",
    "#### switching the order for test and val\n",
    "#     half = 1\n",
    "\n",
    "#     if half==1:\n",
    "#         train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int(stop/2)], train_X[stop+int(stop/2):], train_y[:stop], train_y[stop:stop+int(stop/2)], train_y[stop+int(stop/2):],\n",
    "    \n",
    "#     else:\n",
    "    \n",
    "#         train_X, test_X, val_X, train_y, test_y, val_y= train_X[stop-1:], train_X[:int(stop/2)], train_X[int(stop/2):stop-1], train_y[stop-1:], train_y[:int(stop/2)], train_y[int(stop/2):stop-1]\n",
    "\n",
    "#     ##############\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(train_X)\n",
    "    \n",
    "    \n",
    "    ## Combining PLAC + LDOPA datasets\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = np.concatenate(train_X[:stop],test_X[:stop]), np.concatenate(train_X[stop:stop+int(stop/2)])#,test_X[stop:stop+int(stop/2)]),np.concatenate(train_X[stop+int(stop/2):],test_X[stop+int(stop/2):]),np.concatenate(train_y[:stop],test_y[:stop]), np.concatenate(train_y[stop:stop+int(stop/2)],test_y[stop:stop+int(stop/2)]), np.concatenate(train_y[stop+int(stop/2):],test_y[stop+int(stop/2):])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X, test_X,val_X = np.concatenate((train_X[:stop],test_X[:stop]),axis=0), np.concatenate((train_X[stop:stop+int(stop/2)] ,test_X[stop:stop+int(stop/2)]),axis=0), np.concatenate((train_X[stop+int(stop/2):],test_X[stop+int(stop/2):]),axis=0)\n",
    "#     train_y, test_y,val_y = np.concatenate((train_y[:stop],test_y[:stop]),axis=0), np.concatenate((train_y[stop:stop+int(stop/2)] ,test_y[stop:stop+int(stop/2)]),axis=0), np.concatenate((train_y[stop+int(stop/2):],test_y[stop+int(stop/2):]),axis=0)\n",
    "\n",
    "\n",
    "\n",
    "#     ## blocking data \n",
    "#     train_X_aside, train_y_aside, test_X_aside, test_y_aside  = train_X, train_y, test_X, test_y \n",
    "    \n",
    "#     train_X= np.concatenate((build_dataset_train(train_X_aside),build_dataset_train(test_X_aside)), axis=0)\n",
    "#     train_y= np.concatenate((build_dataset_train(train_y_aside),build_dataset_train(test_y_aside)), axis=0)\n",
    "\n",
    "    \n",
    "#     val_X= np.concatenate((build_dataset_valid(train_X_aside),build_dataset_valid(test_X_aside)), axis=0)\n",
    "#     val_y= np.concatenate((build_dataset_valid(train_y_aside),build_dataset_valid(test_y_aside)), axis=0)\n",
    "\n",
    "    \n",
    "#     test_X= np.concatenate((build_dataset_test(train_X_aside),build_dataset_test(test_X_aside)), axis=0)\n",
    "#     test_y= np.concatenate((build_dataset_test(train_y_aside),build_dataset_test(test_y_aside)), axis=0)\n",
    "    \n",
    "\n",
    "        ### CHUNK SPLITTING\n",
    "\n",
    "        \n",
    "        \n",
    "    train_X_aside, train_y_aside, test_X_aside, test_y_aside  = train_X, train_y, test_X, test_y \n",
    "    \n",
    "    train_X= np.concatenate((chunk_split_data(train_X_aside,0,10),chunk_split_data(test_X_aside,0,10)), axis=0)\n",
    "    train_y= np.concatenate((chunk_split_data(train_y_aside,0,10),chunk_split_data(test_y_aside,0,10)), axis=0)\n",
    "\n",
    "    \n",
    "    val_X= np.concatenate((chunk_split_data(train_X_aside,15,20),chunk_split_data(test_X_aside,15,20)), axis=0)\n",
    "    val_y= np.concatenate((chunk_split_data(train_y_aside,15,20),chunk_split_data(test_y_aside,15,20)), axis=0)\n",
    "\n",
    "    test_X= np.concatenate((chunk_split_data(train_X_aside,10,15),chunk_split_data(test_X_aside,10,15)), axis=0)\n",
    "    test_y= np.concatenate((chunk_split_data(train_y_aside,10,15),chunk_split_data(test_y_aside,10,15)), axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     ##### FURTHER TRAINING WITH SUBSEQUENCES WITH REPLACEMENT\n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#     for k in range(num_batches):\n",
    "#             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#             train_X = np.concatenate((train_X,X_seq), axis=0)\n",
    "# #             print(train_X.shape)\n",
    "#             train_y = np.concatenate((train_y, y_seq),axis=0)\n",
    "\n",
    "\n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#     train_X, train_y = X_seq, y_seq\n",
    "#     for k in range(num_batches-1):\n",
    "#             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#             train_X = np.concatenate((train_X,X_seq), axis=0)\n",
    "# #             print(train_X.shape)\n",
    "#             train_y = np.concatenate((train_y, y_seq),axis=0)\n",
    "# # # # # ##########################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### PRE TRAINING\n",
    "#     stop = int(0.7*len(train_X))\n",
    "#     print(stop)\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int((len(train_X)-stop)/2)], train_X[stop+int((len(train_X)-stop)/2):],train_y[:stop], train_y[stop:stop+int((len(train_X)-stop)/2)], train_y[stop+int((len(train_X)-stop)/2):]\n",
    "    \n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = train_X, test_X, test_X, train_y, test_y, test_y\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(val_X.shape)\n",
    "    print(val_y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "    # # center and scale\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.fit_transform(test_X)\n",
    "    val_X = scaler.fit_transform(val_X)\n",
    "\n",
    "\n",
    "    train_X = train_X[:,None,:]\n",
    "    val_X = val_X[:,None,:]\n",
    "    test_X = test_X[:,None,:]\n",
    "\n",
    "\n",
    "    # # one-hot encode the outputs\n",
    "\n",
    "    onehot_encoder = OneHotEncoder()\n",
    "    encode_categorical = train_y.reshape(len(train_y), 1)\n",
    "    encode_categorical_test = test_y.reshape(len(test_y), 1)\n",
    "    encode_categorical_val = val_y.reshape(len(val_y),1)\n",
    "\n",
    "\n",
    "    train_y = onehot_encoder.fit_transform(encode_categorical).toarray()\n",
    "    test_y = onehot_encoder.fit_transform(encode_categorical_test).toarray()\n",
    "    val_y = onehot_encoder.fit_transform(encode_categorical_val).toarray()\n",
    "\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, val_X,val_y\n",
    "#     return train_X, test_X, val_X#, test_X, test_y, val_X,val_y\n",
    "\n",
    "def build_dataset_train(data):\n",
    "    \n",
    "    return np.concatenate((data[:int(stop/3)],data[2*int(stop/3):int(stop)], data[stop+int(stop/3):stop+2*int(stop/3)]), axis=0)\n",
    "    \n",
    "\n",
    "def build_dataset_test(data):\n",
    "    \n",
    "    return np.concatenate((data[int(stop/3):int(stop/3)+int(stop/6)], data[int(stop):int(stop)+int(stop/6)], data[stop+2*int(stop/3): stop+2*int(stop/3) + int(stop/6) ]), axis=0)\n",
    "\n",
    "def build_dataset_valid(data):\n",
    "    \n",
    "    return np.concatenate((data[int(stop/3)+int(stop/6):int(stop/3)+2*int(stop/6)], data[int(stop)+int(stop/6):int(stop)+2*int(stop/6)], data[stop+2*int(stop/3) + int(stop/6) : ]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_split_data(data,start_chunk,end_chunk):\n",
    "    \n",
    "    a=[k for k in range(start_chunk,end_chunk)]\n",
    "    out=[]\n",
    "\n",
    "    for d in range(0,data.shape[0],20):\n",
    "\n",
    "        c= [c+d for c in a]\n",
    "        out = out+c\n",
    "\n",
    "    while out[-1]>=data.shape[0]-1:\n",
    "        out.pop()\n",
    "#     return out\n",
    "    return data[out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_split_data(train_X,0,10)\n",
    "# chunk_split_data(train_X,10,15)\n",
    "# chunk_split_data(train_X,15,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y): \n",
    "    reset_graph()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50000\n",
    "    batch_size = int(train_X.shape[0]/2)\n",
    "    # batch_size = 100\n",
    "    length = train_X.shape[0]\n",
    "    display = 100\n",
    "    neurons = neurons\n",
    "\n",
    "    num_batches = 100\n",
    "    seq_len = 10\n",
    "\n",
    "    percent_above_PT = 1\n",
    "\n",
    "    train_threshold = PT_R2 + percent_above_PT\n",
    "\n",
    "\n",
    "    save_step = 100\n",
    "\n",
    "\n",
    "    best_loss_val = np.infty\n",
    "    checks_since_last_progress = 0\n",
    "    max_checks_without_progress = 1000\n",
    "\n",
    "\n",
    "    # clear graph (if any) before running\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, time_steps, inputs])\n",
    "\n",
    "    y = tf.placeholder(tf.float32, [None, outputs])\n",
    "\n",
    "    # LSTM Cell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=neurons, activation=tf.nn.relu)\n",
    "    cell_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "    # pass into Dense layer\n",
    "    stacked_outputs = tf.reshape(cell_outputs, [-1, neurons])\n",
    "    out = tf.layers.dense(inputs=stacked_outputs, units=outputs)\n",
    "\n",
    "    probability = tf.nn.softmax(out)\n",
    "\n",
    "    # squared error loss or cost function for linear regression\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=y, logits=out))\n",
    "\n",
    "    # optimizer to minimize cost\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels =  tf.argmax(y, 1),\n",
    "                          predictions = tf.argmax(out, 1),\n",
    "                          name = \"accuracy\")\n",
    "    precision = tf.metrics.precision(labels=tf.argmax(y, 1),\n",
    "                                 predictions=tf.argmax(out, 1),\n",
    "                                 name=\"precision\")\n",
    "    recall = tf.metrics.recall(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"recall\")\n",
    "    f1 = 2 * accuracy[1] * recall[1] / ( precision[1] + recall[1] )\n",
    "\n",
    "    acc_up,acc_val = accuracy\n",
    "    auc = tf.metrics.auc(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"auc\")\n",
    "    \n",
    "    valid_store = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #######################\n",
    "#         saver.restore(sess, \"./checkpts/Original_RNN_LSTM_8features_v2.ckpt\")\n",
    "#         saver.restore(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "        saver.restore(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        #######################\n",
    "        \n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "\n",
    "        # Train the model\n",
    "        for steps in range(epochs):\n",
    "            mini_batch = zip(range(0, length, batch_size),\n",
    "                       range(batch_size, length+1, batch_size))\n",
    "\n",
    "            # train data in mini-batches\n",
    "            for (start, end) in mini_batch:\n",
    "    #             print(start,end)\n",
    "                sess.run(training_op, feed_dict = {X: train_X[start:end,:,:],\n",
    "                                                   y: train_y[start:end,:]}) \n",
    "\n",
    "            ## train data in batches of length subsequence\n",
    "\n",
    "    #         for k in range(num_batches):\n",
    "    #             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "\n",
    "    #             sess.run(training_op, feed_dict = {X:X_seq,y:y_seq}) \n",
    "            loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "            loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "            # print training performance \n",
    "            if (steps+1) % display == 0:\n",
    "                # evaluate loss function on training set\n",
    "\n",
    "\n",
    "                loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining loss: {}'.format((steps+1), loss_fn))\n",
    "\n",
    "                acc_train = acc_val.eval(feed_dict={X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining accuracy: {}'.format((steps+1), acc_train))\n",
    "\n",
    "\n",
    "                acc_test = acc_val.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest accuracy: {}'.format((steps+1), acc_test))\n",
    "\n",
    "                loss_test = loss.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest loss: {}'.format((steps+1), loss_test))\n",
    "\n",
    "                accu_val = acc_val.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "                loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "                print('Step: {}  \\tValid loss: {}'.format((steps+1), loss_val))\n",
    "\n",
    "                valid_store.append(loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (1 + loss_fn/np.log(0.5)) > train_threshold:\n",
    "                    print(\"Threshold achieved, quit training\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            if loss_val < best_loss_val:\n",
    "\n",
    "                        best_loss_val = loss_val\n",
    "                        checks_since_last_progress = 0\n",
    "            else:\n",
    "                            checks_since_last_progress += 1\n",
    "\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if checks_since_last_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "\n",
    "            if (steps+1) % save_step ==0:\n",
    "                                save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#                 save_path = saver.save(sess, \"./checkpts/RNN_Internet_LSTM_model_5features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     evaluate model accuracy\n",
    "        acc, prec, recall, f1, AUC = sess.run([accuracy, precision, recall, f1,auc],\n",
    "                                         feed_dict = {X: train_X, y: train_y})\n",
    "        prob_train = probability.eval(feed_dict = {X: train_X, y: train_y})\n",
    "        prob_test = probability.eval(feed_dict = {X: test_X, y: test_y})\n",
    "        prob_valid = probability.eval(feed_dict = {X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nEvaluation  on training set')\n",
    "        print('Accuracy:', acc[1])\n",
    "        print('Precision:', prec[1])\n",
    "        print('Recall:', recall[1])\n",
    "        print('F1 score:', f1)\n",
    "        print('AUC:', AUC[1])\n",
    "        \n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "        save_path = saver.save(sess, \"./checkpts/Later_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/LaterDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "    metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,AUC[1],loss_fn,accu_val,best_loss_val,acc_test,loss_test,neurons,learning_rate,epochs,steps]).reshape(-1,14),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\",\"loss\",\"accuracy_val\",\"loss_val\",\"accuracy_test\",\"loss_test\",\"neurons\",\"learning_rate\",\"n_epochs\",\"steps\"])\n",
    "    return metric_out_df, prob_train, prob_test, prob_valid\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def random_subsequence(X,y,seq_len):\n",
    "    rnd  = random.randint(0,len(X)-seq_len)\n",
    "    X_seq, y_seq = X[rnd:rnd+seq_len,:], y[rnd:rnd+seq_len,:]\n",
    "    return X_seq, y_seq\n",
    "\n",
    "    print(y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 15)\n",
      "(300, 8)\n",
      "(300, 1)\n",
      "(146, 8)\n",
      "(146, 1)\n",
      "(150, 8)\n",
      "(150, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/\"\n",
    "subj_num =11\n",
    "file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_subj29_params.csv\"\n",
    "\n",
    "file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1/generateddata300mu1params.csv\"\n",
    "\n",
    "# file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5/generateddata300mu0_5params.csv\"\n",
    "\n",
    "### ACTUAL DATA\n",
    "file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "task_df = pd.read_csv(file_name)\n",
    "dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "task_df = add_releveant_features(task_df)\n",
    "dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "\n",
    "\n",
    "task_df.head(10)\n",
    "\n",
    "\n",
    "# train_data = np.concatenate((build_dataset_train(task_df.loc[task_df.TrialNum>1]),build_dataset_train(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "# train_data_df = pd.DataFrame(train_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "# test_data = np.concatenate((build_dataset_test(task_df.loc[task_df.TrialNum>1]),build_dataset_test(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "# test_data_df = pd.DataFrame(test_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "\n",
    "\n",
    "train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n",
    "\n",
    "\n",
    "# train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v10chunks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject28\n",
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n",
      "Subject41\n"
     ]
    }
   ],
   "source": [
    "for subj_num in range(28,42):\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v2/subject_num_\"+str(subj_num)\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v6chunks/subject_num_\"+str(subj_num)\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v10chunks/subject_num_\"+str(subj_num)\n",
    "\n",
    "#     os.mkdir(file_path)\n",
    "\n",
    "    \n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    \n",
    "    ## blocking ####\n",
    "#     train_data = np.concatenate((build_dataset_train(task_df.loc[task_df.TrialNum>1]),build_dataset_train(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "#     train_data_df = pd.DataFrame(train_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "#     test_data = np.concatenate((build_dataset_test(task_df.loc[task_df.TrialNum>1]),build_dataset_test(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "#     test_data_df = pd.DataFrame(test_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "    \n",
    "#     val_data = np.concatenate((build_dataset_valid(task_df.loc[task_df.TrialNum>1]),build_dataset_valid(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "#     val_data_df = pd.DataFrame(val_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "#     ##########\n",
    "\n",
    "    ## CHUNKING\n",
    "    train_X_aside = task_df.loc[task_df.TrialNum>1].values; test_X_aside= dopa_task_df.loc[task_df.TrialNum>1].values;\n",
    "    \n",
    "    \n",
    "    train_data = np.concatenate((chunk_split_data(train_X_aside,0,10),chunk_split_data(test_X_aside,0,10)), axis=0)\n",
    "    train_data_df = pd.DataFrame(train_data, columns=task_df.columns)\n",
    "\n",
    "    val_data= np.concatenate((chunk_split_data(train_X_aside,15,20),chunk_split_data(test_X_aside,15,20)), axis=0)\n",
    "    val_data_df = pd.DataFrame(val_data, columns=task_df.columns)\n",
    "\n",
    "    test_data= np.concatenate((chunk_split_data(train_X_aside,10,15),chunk_split_data(test_X_aside,10,15)), axis=0)\n",
    "    test_data_df = pd.DataFrame(test_data, columns = task_df.columns)\n",
    "\n",
    "\n",
    "# train_data= np.concatenate((chunk_split_data(train_X_aside,0,10),chunk_split_data(test_X_aside,0,10)), axis=0)\n",
    "#     train_data_df = pd.DataFrame(train_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "    \n",
    "#     val_data= np.concatenate((chunk_split_data(train_X_aside,15,20),chunk_split_data(test_X_aside,15,20)), axis=0)\n",
    "#     val_data_df = pd.DataFrame(val_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "#     test_data= np.concatenate((chunk_split_data(train_X_aside,10,15),chunk_split_data(test_X_aside,10,15)), axis=0)\n",
    "#     test_data_df = pd.DataFrame(test_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "#     print(train_data.shape)\n",
    "#     print(val_data.shape)\n",
    "#     print(test_data.shape)\n",
    "    \n",
    "    \n",
    "    train_data_df.to_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df.to_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df.to_csv(file_path+\"/val_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialNum</th>\n",
       "      <th>SideOfScreen</th>\n",
       "      <th>Safe</th>\n",
       "      <th>BigRisky</th>\n",
       "      <th>SmallRisky</th>\n",
       "      <th>SideChosen</th>\n",
       "      <th>Choice</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>RT</th>\n",
       "      <th>Happiness</th>\n",
       "      <th>PrevOutcome</th>\n",
       "      <th>PrevChoice</th>\n",
       "      <th>PrevSafe</th>\n",
       "      <th>PrevBigRisky</th>\n",
       "      <th>PrevSmallRisky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>1.198</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>4.878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.249</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>3.007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.518229</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.523</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.399</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>2.103</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.384</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-111.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>1.568</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.768</td>\n",
       "      <td>0.516927</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.504</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-87.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-87.0</td>\n",
       "      <td>3.871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-87.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.081</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-84.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>1.551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>1.295</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>0.919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.262</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>0.918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>242.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>243.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>244.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>0.968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>245.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.565104</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>246.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.133</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>247.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-74.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>3.047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>248.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>8.254</td>\n",
       "      <td>0.559896</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>249.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>1.704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>251.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>262.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>263.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>264.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>265.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>1.567</td>\n",
       "      <td>0.553385</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>266.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>267.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>2.408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>268.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>269.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>1.173</td>\n",
       "      <td>0.542969</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>270.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>3.848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>271.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>1.278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>282.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-78.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>283.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.544271</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>284.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>285.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>0.758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>286.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>287.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.608</td>\n",
       "      <td>0.524740</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>288.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>289.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.719</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>290.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-24.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.222</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>291.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>4.837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TrialNum  SideOfScreen  Safe  BigRisky  SmallRisky  SideChosen  Choice  \\\n",
       "0         2.0           1.0   0.0      45.0       -30.0         1.0     1.0   \n",
       "1         3.0           1.0 -15.0       0.0       -47.0         2.0     0.0   \n",
       "2         4.0           1.0  30.0     108.0         0.0         1.0     1.0   \n",
       "3         5.0           1.0 -35.0       0.0       -70.0         2.0     0.0   \n",
       "4         6.0           1.0   0.0      80.0       -32.0         1.0     1.0   \n",
       "5         7.0           1.0   0.0      30.0       -25.0         1.0     1.0   \n",
       "6         8.0           1.0  25.0      70.0         0.0         1.0     1.0   \n",
       "7         9.0           1.0  35.0     126.0         0.0         1.0     1.0   \n",
       "8        10.0           1.0  30.0      60.0         0.0         2.0     0.0   \n",
       "9        11.0           2.0   0.0      30.0       -20.0         1.0     0.0   \n",
       "10       22.0           1.0 -20.0       0.0       -40.0         1.0     1.0   \n",
       "11       23.0           1.0   0.0      30.0       -60.0         2.0     0.0   \n",
       "12       24.0           1.0 -35.0       0.0      -111.0         2.0     0.0   \n",
       "13       25.0           1.0 -30.0       0.0       -55.0         1.0     1.0   \n",
       "14       26.0           1.0   0.0      20.0        -6.0         1.0     1.0   \n",
       "15       27.0           1.0   0.0      20.0       -10.0         1.0     1.0   \n",
       "16       28.0           1.0 -20.0       0.0       -34.0         1.0     1.0   \n",
       "17       29.0           1.0   0.0      30.0       -45.0         2.0     0.0   \n",
       "18       30.0           1.0 -35.0       0.0       -87.0         1.0     1.0   \n",
       "19       31.0           2.0  35.0     111.0         0.0         2.0     1.0   \n",
       "20       42.0           1.0  35.0      98.0         0.0         1.0     1.0   \n",
       "21       43.0           1.0  20.0      36.0         0.0         2.0     0.0   \n",
       "22       44.0           1.0 -30.0       0.0       -84.0         2.0     0.0   \n",
       "23       45.0           1.0   0.0      65.0       -43.0         1.0     1.0   \n",
       "24       46.0           1.0   0.0      30.0       -60.0         2.0     0.0   \n",
       "25       47.0           1.0   0.0      65.0       -26.0         1.0     1.0   \n",
       "26       48.0           1.0 -35.0       0.0       -59.0         1.0     1.0   \n",
       "27       49.0           1.0 -15.0       0.0       -33.0         1.0     1.0   \n",
       "28       50.0           1.0   0.0      80.0       -16.0         1.0     1.0   \n",
       "29       51.0           2.0   0.0      45.0       -14.0         2.0     1.0   \n",
       "..        ...           ...   ...       ...         ...         ...     ...   \n",
       "270     242.0           1.0  25.0      70.0         0.0         1.0     1.0   \n",
       "271     243.0           1.0   0.0      45.0       -68.0         2.0     0.0   \n",
       "272     244.0           1.0   0.0      80.0       -24.0         1.0     1.0   \n",
       "273     245.0           1.0   0.0      80.0      -160.0         1.0     1.0   \n",
       "274     246.0           1.0   0.0      80.0       -80.0         2.0     0.0   \n",
       "275     247.0           1.0 -30.0       0.0       -74.0         2.0     0.0   \n",
       "276     248.0           1.0 -15.0       0.0       -27.0         1.0     1.0   \n",
       "277     249.0           1.0   0.0      45.0       -45.0         2.0     0.0   \n",
       "278     250.0           1.0 -15.0       0.0       -47.0         2.0     0.0   \n",
       "279     251.0           2.0  20.0      44.0         0.0         2.0     1.0   \n",
       "280     262.0           1.0  35.0     147.0         0.0         1.0     1.0   \n",
       "281     263.0           1.0  25.0      56.0         0.0         1.0     1.0   \n",
       "282     264.0           1.0  20.0      50.0         0.0         1.0     1.0   \n",
       "283     265.0           1.0 -25.0       0.0       -62.0         2.0     0.0   \n",
       "284     266.0           1.0   0.0      45.0       -90.0         2.0     0.0   \n",
       "285     267.0           1.0 -35.0       0.0       -59.0         2.0     0.0   \n",
       "286     268.0           1.0  25.0     125.0         0.0         1.0     1.0   \n",
       "287     269.0           1.0 -25.0       0.0       -62.0         2.0     0.0   \n",
       "288     270.0           1.0 -15.0       0.0       -30.0         1.0     1.0   \n",
       "289     271.0           2.0 -25.0       0.0      -125.0         1.0     0.0   \n",
       "290     282.0           1.0   0.0      20.0       -16.0         1.0     1.0   \n",
       "291     283.0           1.0  35.0      70.0         0.0         1.0     1.0   \n",
       "292     284.0           1.0  35.0      70.0         0.0         1.0     1.0   \n",
       "293     285.0           1.0   0.0      80.0       -32.0         1.0     1.0   \n",
       "294     286.0           1.0  30.0      84.0         0.0         1.0     1.0   \n",
       "295     287.0           1.0   0.0      30.0       -60.0         2.0     0.0   \n",
       "296     288.0           1.0   0.0      30.0       -20.0         1.0     1.0   \n",
       "297     289.0           1.0  30.0      74.0         0.0         1.0     1.0   \n",
       "298     290.0           1.0   0.0      20.0       -24.0         2.0     0.0   \n",
       "299     291.0           2.0 -25.0       0.0       -42.0         2.0     1.0   \n",
       "\n",
       "     Outcome     RT  Happiness  PrevOutcome  PrevChoice  PrevSafe  \\\n",
       "0       45.0  1.727        NaN        -35.0         0.0     -35.0   \n",
       "1      -15.0  1.198   0.535156         45.0         1.0       0.0   \n",
       "2      108.0  1.202        NaN        -15.0         0.0     -15.0   \n",
       "3      -35.0  4.878        NaN        108.0         1.0      30.0   \n",
       "4       80.0  2.249   0.541667        -35.0         0.0     -35.0   \n",
       "5      -25.0  3.007        NaN         80.0         1.0       0.0   \n",
       "6       70.0  1.223        NaN        -25.0         1.0       0.0   \n",
       "7        0.0  0.902   0.518229         70.0         1.0      25.0   \n",
       "8       30.0  3.523        NaN          0.0         1.0      35.0   \n",
       "9        0.0  3.399        NaN         30.0         0.0      30.0   \n",
       "10     -40.0  2.103   0.515625        -25.0         0.0     -25.0   \n",
       "11       0.0  1.384        NaN        -40.0         1.0     -20.0   \n",
       "12     -35.0  1.568        NaN          0.0         0.0       0.0   \n",
       "13       0.0  1.768   0.516927        -35.0         0.0     -35.0   \n",
       "14      20.0  1.138        NaN          0.0         1.0     -30.0   \n",
       "15      20.0  1.390        NaN         20.0         1.0       0.0   \n",
       "16       0.0  3.504   0.531250         20.0         1.0       0.0   \n",
       "17       0.0  3.010        NaN          0.0         1.0     -20.0   \n",
       "18     -87.0  3.871        NaN          0.0         0.0       0.0   \n",
       "19       0.0  1.134        NaN        -87.0         1.0     -35.0   \n",
       "20      98.0  1.081        NaN         62.0         1.0      25.0   \n",
       "21      20.0  2.038        NaN         98.0         1.0      35.0   \n",
       "22     -30.0  1.551        NaN         20.0         0.0      20.0   \n",
       "23     -43.0  1.295   0.464844        -30.0         0.0     -30.0   \n",
       "24       0.0  1.877        NaN        -43.0         1.0       0.0   \n",
       "25     -26.0  0.919        NaN          0.0         0.0       0.0   \n",
       "26       0.0  2.198        NaN        -26.0         1.0       0.0   \n",
       "27       0.0  3.262   0.464844          0.0         1.0     -35.0   \n",
       "28     -16.0  0.918        NaN          0.0         1.0     -15.0   \n",
       "29      45.0  1.942        NaN        -16.0         1.0       0.0   \n",
       "..       ...    ...        ...          ...         ...       ...   \n",
       "270     70.0  0.715        NaN        -15.0         0.0     -15.0   \n",
       "271      0.0  1.526        NaN         70.0         1.0      25.0   \n",
       "272    -24.0  0.968        NaN          0.0         0.0       0.0   \n",
       "273     80.0  0.678   0.565104        -24.0         1.0       0.0   \n",
       "274      0.0  2.133        NaN         80.0         1.0       0.0   \n",
       "275    -30.0  3.047        NaN          0.0         0.0       0.0   \n",
       "276    -27.0  8.254   0.559896        -30.0         0.0     -30.0   \n",
       "277      0.0  1.126        NaN        -27.0         1.0     -15.0   \n",
       "278    -15.0  1.704        NaN          0.0         0.0       0.0   \n",
       "279      0.0  1.022        NaN        -15.0         0.0     -15.0   \n",
       "280      0.0  0.768   0.453125          0.0         0.0       0.0   \n",
       "281      0.0  1.575        NaN          0.0         1.0      35.0   \n",
       "282     50.0  0.870        NaN          0.0         1.0      25.0   \n",
       "283    -25.0  1.567   0.553385         50.0         1.0      20.0   \n",
       "284      0.0  1.327        NaN        -25.0         0.0     -25.0   \n",
       "285    -35.0  2.408        NaN          0.0         0.0       0.0   \n",
       "286    125.0  0.871        NaN        -35.0         0.0     -35.0   \n",
       "287    -25.0  1.173   0.542969        125.0         1.0      25.0   \n",
       "288    -30.0  3.848        NaN        -25.0         0.0     -25.0   \n",
       "289    -25.0  1.278        NaN        -30.0         1.0     -15.0   \n",
       "290     20.0  1.350        NaN        -78.0         1.0       0.0   \n",
       "291     70.0  0.718   0.544271         20.0         1.0       0.0   \n",
       "292      0.0  0.905        NaN         70.0         1.0      35.0   \n",
       "293    -32.0  0.758        NaN          0.0         1.0      35.0   \n",
       "294     84.0  0.574        NaN        -32.0         1.0       0.0   \n",
       "295      0.0  1.608   0.524740         84.0         1.0      30.0   \n",
       "296     30.0  0.962        NaN          0.0         0.0       0.0   \n",
       "297     74.0  0.719        NaN         30.0         1.0       0.0   \n",
       "298      0.0  1.222   0.533854         74.0         1.0      30.0   \n",
       "299    -42.0  4.837        NaN          0.0         0.0       0.0   \n",
       "\n",
       "     PrevBigRisky  PrevSmallRisky  \n",
       "0             0.0          -111.0  \n",
       "1            45.0           -30.0  \n",
       "2             0.0           -47.0  \n",
       "3           108.0             0.0  \n",
       "4             0.0           -70.0  \n",
       "5            80.0           -32.0  \n",
       "6            30.0           -25.0  \n",
       "7            70.0             0.0  \n",
       "8           126.0             0.0  \n",
       "9            60.0             0.0  \n",
       "10            0.0          -125.0  \n",
       "11            0.0           -40.0  \n",
       "12           30.0           -60.0  \n",
       "13            0.0          -111.0  \n",
       "14            0.0           -55.0  \n",
       "15           20.0            -6.0  \n",
       "16           20.0           -10.0  \n",
       "17            0.0           -34.0  \n",
       "18           30.0           -45.0  \n",
       "19            0.0           -87.0  \n",
       "20           62.0             0.0  \n",
       "21           98.0             0.0  \n",
       "22           36.0             0.0  \n",
       "23            0.0           -84.0  \n",
       "24           65.0           -43.0  \n",
       "25           30.0           -60.0  \n",
       "26           65.0           -26.0  \n",
       "27            0.0           -59.0  \n",
       "28            0.0           -33.0  \n",
       "29           80.0           -16.0  \n",
       "..            ...             ...  \n",
       "270           0.0           -42.0  \n",
       "271          70.0             0.0  \n",
       "272          45.0           -68.0  \n",
       "273          80.0           -24.0  \n",
       "274          80.0          -160.0  \n",
       "275          80.0           -80.0  \n",
       "276           0.0           -74.0  \n",
       "277           0.0           -27.0  \n",
       "278          45.0           -45.0  \n",
       "279           0.0           -47.0  \n",
       "280          45.0           -68.0  \n",
       "281         147.0             0.0  \n",
       "282          56.0             0.0  \n",
       "283          50.0             0.0  \n",
       "284           0.0           -62.0  \n",
       "285          45.0           -90.0  \n",
       "286           0.0           -59.0  \n",
       "287         125.0             0.0  \n",
       "288           0.0           -62.0  \n",
       "289           0.0           -30.0  \n",
       "290          65.0           -78.0  \n",
       "291          20.0           -16.0  \n",
       "292          70.0             0.0  \n",
       "293          70.0             0.0  \n",
       "294          80.0           -32.0  \n",
       "295          84.0             0.0  \n",
       "296          30.0           -60.0  \n",
       "297          30.0           -20.0  \n",
       "298          74.0             0.0  \n",
       "299          20.0           -24.0  \n",
       "\n",
       "[300 rows x 15 columns]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/\"\n",
    "# os.mkdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject36\n",
      "(301, 15)\n",
      "(300, 8)\n",
      "(300, 1)\n",
      "(146, 8)\n",
      "(146, 1)\n",
      "(150, 8)\n",
      "(150, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.5768251419067383\n",
      "Step: 100  \tTraining accuracy: 0.746666669845581\n",
      "Step: 100  \tValid loss: 0.6119353175163269\n",
      "Step: 200  \tTraining loss: 0.5098356604576111\n",
      "Step: 200  \tTraining accuracy: 0.734375\n",
      "Step: 200  \tValid loss: 0.5775622129440308\n",
      "Step: 300  \tTraining loss: 0.47863465547561646\n",
      "Step: 300  \tTraining accuracy: 0.7419571280479431\n",
      "Step: 300  \tValid loss: 0.562315046787262\n",
      "Step: 400  \tTraining loss: 0.4497145116329193\n",
      "Step: 400  \tTraining accuracy: 0.7461685538291931\n",
      "Step: 400  \tValid loss: 0.5404934883117676\n",
      "Step: 500  \tTraining loss: 0.4211510717868805\n",
      "Step: 500  \tTraining accuracy: 0.754470944404602\n",
      "Step: 500  \tValid loss: 0.5153321623802185\n",
      "Step: 600  \tTraining loss: 0.3945452868938446\n",
      "Step: 600  \tTraining accuracy: 0.7621951103210449\n",
      "Step: 600  \tValid loss: 0.4909667372703552\n",
      "Step: 700  \tTraining loss: 0.3713502585887909\n",
      "Step: 700  \tTraining accuracy: 0.7703818082809448\n",
      "Step: 700  \tValid loss: 0.46999892592430115\n",
      "Step: 800  \tTraining loss: 0.35192400217056274\n",
      "Step: 800  \tTraining accuracy: 0.777951717376709\n",
      "Step: 800  \tValid loss: 0.4530401825904846\n",
      "Step: 900  \tTraining loss: 0.3358660340309143\n",
      "Step: 900  \tTraining accuracy: 0.7841357588768005\n",
      "Step: 900  \tValid loss: 0.4396856129169464\n",
      "Step: 1000  \tTraining loss: 0.3225974142551422\n",
      "Step: 1000  \tTraining accuracy: 0.7900776863098145\n",
      "Step: 1000  \tValid loss: 0.429368793964386\n",
      "Step: 1100  \tTraining loss: 0.31151244044303894\n",
      "Step: 1100  \tTraining accuracy: 0.7955271601676941\n",
      "Step: 1100  \tValid loss: 0.4210672080516815\n",
      "Step: 1200  \tTraining loss: 0.30215638875961304\n",
      "Step: 1200  \tTraining accuracy: 0.8001750111579895\n",
      "Step: 1200  \tValid loss: 0.41440945863723755\n",
      "Step: 1300  \tTraining loss: 0.2941829264163971\n",
      "Step: 1300  \tTraining accuracy: 0.804347813129425\n",
      "Step: 1300  \tValid loss: 0.4089227020740509\n",
      "Step: 1400  \tTraining loss: 0.2873251140117645\n",
      "Step: 1400  \tTraining accuracy: 0.8080268502235413\n",
      "Step: 1400  \tValid loss: 0.40437865257263184\n",
      "Step: 1500  \tTraining loss: 0.2813700735569\n",
      "Step: 1500  \tTraining accuracy: 0.8113142251968384\n",
      "Step: 1500  \tValid loss: 0.40066200494766235\n",
      "Step: 1600  \tTraining loss: 0.2761573791503906\n",
      "Step: 1600  \tTraining accuracy: 0.8141775131225586\n",
      "Step: 1600  \tValid loss: 0.39749616384506226\n",
      "Step: 1700  \tTraining loss: 0.2715570628643036\n",
      "Step: 1700  \tTraining accuracy: 0.8166937828063965\n",
      "Step: 1700  \tValid loss: 0.39486828446388245\n",
      "Step: 1800  \tTraining loss: 0.26746419072151184\n",
      "Step: 1800  \tTraining accuracy: 0.8190184235572815\n",
      "Step: 1800  \tValid loss: 0.39264822006225586\n",
      "Step: 1900  \tTraining loss: 0.26379796862602234\n",
      "Step: 1900  \tTraining accuracy: 0.8211824297904968\n",
      "Step: 1900  \tValid loss: 0.39074981212615967\n",
      "Step: 2000  \tTraining loss: 0.26049283146858215\n",
      "Step: 2000  \tTraining accuracy: 0.8232966065406799\n",
      "Step: 2000  \tValid loss: 0.38912925124168396\n",
      "Step: 2100  \tTraining loss: 0.2574958801269531\n",
      "Step: 2100  \tTraining accuracy: 0.8252863883972168\n",
      "Step: 2100  \tValid loss: 0.3877173066139221\n",
      "Step: 2200  \tTraining loss: 0.25476324558258057\n",
      "Step: 2200  \tTraining accuracy: 0.8270131349563599\n",
      "Step: 2200  \tValid loss: 0.3865286707878113\n",
      "Step: 2300  \tTraining loss: 0.25225889682769775\n",
      "Step: 2300  \tTraining accuracy: 0.8285863399505615\n",
      "Step: 2300  \tValid loss: 0.3854999542236328\n",
      "Step: 2400  \tTraining loss: 0.24995160102844238\n",
      "Step: 2400  \tTraining accuracy: 0.8299543261528015\n",
      "Step: 2400  \tValid loss: 0.3846063017845154\n",
      "Step: 2500  \tTraining loss: 0.2478155791759491\n",
      "Step: 2500  \tTraining accuracy: 0.8313475847244263\n",
      "Step: 2500  \tValid loss: 0.3838616907596588\n",
      "Step: 2600  \tTraining loss: 0.24582748115062714\n",
      "Step: 2600  \tTraining accuracy: 0.8326315879821777\n",
      "Step: 2600  \tValid loss: 0.3832366168498993\n",
      "Step: 2700  \tTraining loss: 0.24396738409996033\n",
      "Step: 2700  \tTraining accuracy: 0.8338186740875244\n",
      "Step: 2700  \tValid loss: 0.3827223479747772\n",
      "Step: 2800  \tTraining loss: 0.24221664667129517\n",
      "Step: 2800  \tTraining accuracy: 0.8348584771156311\n",
      "Step: 2800  \tValid loss: 0.38230815529823303\n",
      "Step: 2900  \tTraining loss: 0.24055714905261993\n",
      "Step: 2900  \tTraining accuracy: 0.835884153842926\n",
      "Step: 2900  \tValid loss: 0.38199540972709656\n",
      "Step: 3000  \tTraining loss: 0.23896685242652893\n",
      "Step: 3000  \tTraining accuracy: 0.8368403315544128\n",
      "Step: 3000  \tValid loss: 0.3817780315876007\n",
      "Step: 3100  \tTraining loss: 0.23739966750144958\n",
      "Step: 3100  \tTraining accuracy: 0.8377337455749512\n",
      "Step: 3100  \tValid loss: 0.3816450834274292\n",
      "Step: 3200  \tTraining loss: 0.23564870655536652\n",
      "Step: 3200  \tTraining accuracy: 0.8384640216827393\n",
      "Step: 3200  \tValid loss: 0.38163265585899353\n",
      "Step: 3300  \tTraining loss: 0.23244044184684753\n",
      "Step: 3300  \tTraining accuracy: 0.8391492962837219\n",
      "Step: 3300  \tValid loss: 0.38253477215766907\n",
      "Step: 3400  \tTraining loss: 0.2277478277683258\n",
      "Step: 3400  \tTraining accuracy: 0.8397436141967773\n",
      "Step: 3400  \tValid loss: 0.38838639855384827\n",
      "Step: 3500  \tTraining loss: 0.22492873668670654\n",
      "Step: 3500  \tTraining accuracy: 0.8402062058448792\n",
      "Step: 3500  \tValid loss: 0.39509403705596924\n",
      "Step: 3600  \tTraining loss: 0.22286346554756165\n",
      "Step: 3600  \tTraining accuracy: 0.8405954837799072\n",
      "Step: 3600  \tValid loss: 0.3994831442832947\n",
      "Step: 3700  \tTraining loss: 0.22105351090431213\n",
      "Step: 3700  \tTraining accuracy: 0.8410093784332275\n",
      "Step: 3700  \tValid loss: 0.40258100628852844\n",
      "Step: 3800  \tTraining loss: 0.21938571333885193\n",
      "Step: 3800  \tTraining accuracy: 0.8414459824562073\n",
      "Step: 3800  \tValid loss: 0.40502941608428955\n",
      "Step: 3900  \tTraining loss: 0.21782256662845612\n",
      "Step: 3900  \tTraining accuracy: 0.8419034481048584\n",
      "Step: 3900  \tValid loss: 0.40712082386016846\n",
      "Step: 4000  \tTraining loss: 0.21634747087955475\n",
      "Step: 4000  \tTraining accuracy: 0.8423802256584167\n",
      "Step: 4000  \tValid loss: 0.40900999307632446\n",
      "Step: 4100  \tTraining loss: 0.21495145559310913\n",
      "Step: 4100  \tTraining accuracy: 0.8428334593772888\n",
      "Step: 4100  \tValid loss: 0.41076236963272095\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8432649\n",
      "Precision: 0.91129035\n",
      "Recall: 0.91129035\n",
      "F1 score: 0.8432649\n",
      "AUC: 0.9243952\n",
      "   accuracy  precision   recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.843265    0.91129  0.91129  0.843265  0.924395  0.214202      0.842241   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0   0.38161       0.842651   0.382163      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  4155.0  \n"
     ]
    }
   ],
   "source": [
    "neurons = 8\n",
    "for subj_num in range(36,37):\n",
    "# for subj_num in range(28,42):\n",
    "\n",
    "# for subj_num in [20]:##[15, 16, 17]:# 19, 20, 23, 24, 25, 26, 29, 36, 37, 40]:\n",
    "\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    PT_file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/\"\n",
    "    PT_file_name = PT_file_path  + \"PT_loss_updated.csv\"\n",
    "    PT_metrics = pd.read_csv(PT_file_name)\n",
    "    PT_metrics = PT_metrics[PT_metrics.PT_loss !=0]\n",
    "    PT_R2= PT_metrics[PT_metrics.Subject_number ==subj_num].PT_pseudoR2.iloc[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for generated data ##\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_subj29_params.csv\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_1500_subj29_params.csv\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_3000_subj29_params.csv\"\n",
    "\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1\"\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5\"\n",
    "\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1/generateddata1500mu1params.csv\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5/generateddata300mu0_5params.csv\"\n",
    "\n",
    "\n",
    "\n",
    "############# ORIGINAL ##############\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5/generateddata3000mu0_5params.csv\"\n",
    "\n",
    "\n",
    "# ############ ACTUAL ###############\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5/generateddata600mu0_5params.csv\"\n",
    "\n",
    "\n",
    "#     task_df=pd.read_csv(file_name)\n",
    "# #     task_df.TrialNum = task_df.TrialNum-1\n",
    "\n",
    "#     task_df = add_releveant_features(task_df)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### ORIGINAL\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining\"\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining_v2\"\n",
    "\n",
    "\n",
    "#     task_df = task_net_df; dopa_task_df = task_net_df;\n",
    "    \n",
    "    #############################\n",
    "#     task_df = task_net_train_df\n",
    "#     dopa_task_df  = task_net_valid_df\n",
    "\n",
    "    \n",
    "    \n",
    "     \n",
    "    ### ACTUAL DATA PER SUBJECT\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/subject_num_\"+str(subj_num)+\"/first_half_train\"\n",
    "\n",
    "    \n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v10chunks/subject_num_\"+str(subj_num)\n",
    "\n",
    "    \n",
    "    train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X,train_y= data_split(task_df,dopa_task_df)\n",
    "\n",
    "    \n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,10)\n",
    "\n",
    "    metric_out_df, prob_train, prob_test, prob_val = train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y)\n",
    "    \n",
    "    print(metric_out_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     os.mkdir(file_path + \"50_splits_combined_1sthalf\")\n",
    "#     os.mkdir(file_path + \"50_splits_combined_2ndhalf\")\n",
    "\n",
    "\n",
    "    \n",
    "#     os.mkdir(file_path + \"combined_1sthalf\")\n",
    "#     os.mkdir(file_path + \"combined_2ndhalf\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     os.mkdir(file_path + \"300_sub29\")\n",
    "#     os.mkdir(file_path + \"1500_sub29\")\n",
    "#     os.mkdir(file_path + \"3000_sub29\")\n",
    "\n",
    "#     os.mkdir(file_path + \"/300\")\n",
    "#     os.mkdir(file_path + \"/1500\")\n",
    "#     os.mkdir(file_path + \"/3000\")\n",
    "#     os.mkdir(file_path + \"/600\")\n",
    "\n",
    "\n",
    "\n",
    "#     os.mkdir(file_path)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_curr_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_v2_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "########################################\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#######################################\n",
    "    \n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_RT_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_CrossvalTESTinsess1sthalf_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_CrossvalTESTinsess2ndhalf_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"combined_1sthalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"combined_2ndhalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"50_splits_combined_1sthalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"50_splits_combined_2ndhalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "##### generated data ######\n",
    "#     metric_out_df.to_csv(file_path+\"300_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"1500_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"3000_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"3000_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/300/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"/1500/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/3000/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/3000/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\"_v2.csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\"_v2.csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "    metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "    \n",
    "    prob_train_df = pd.DataFrame(prob_train.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_test_df = pd.DataFrame(prob_test.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_val_df = pd.DataFrame(prob_val.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"prob_train_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"prob_test_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"prob_val_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"prob_train_currentopts_prev_outchoicevv2_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"prob_test_currentopts_prev_outchoicev2_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"prob_val_currentopts_prev_outchoicev2_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "################################\n",
    "    prob_train_df.to_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_test_df.to_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_val_df.to_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#############################\n",
    "\n",
    "    \n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "    \n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"2ndhalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"2ndhalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"2ndhalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"combined_1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"combined_1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"combined_2ndhalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"combined_2ndhalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"combined_2ndhalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"50_splits_combined_1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"50_splits_combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"50_splits_combined_1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### Generated data #######\n",
    "#     prob_train_df.to_csv(file_path + \"300_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"300_sub29_combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"300_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"1500_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"1500_sub29/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"1500_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"3000_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"3000_sub29/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"3000_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/300/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/300/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/300/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/1500/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/1500/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/1500/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/3000/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/3000/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/3000/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/600/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/600/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/600/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>loss_test</th>\n",
       "      <th>neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.778896</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.80198</td>\n",
       "      <td>0.804983</td>\n",
       "      <td>0.833151</td>\n",
       "      <td>0.35709</td>\n",
       "      <td>0.776734</td>\n",
       "      <td>0.509622</td>\n",
       "      <td>0.776438</td>\n",
       "      <td>0.487417</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1559.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision   recall  f1_score       auc     loss  accuracy_val  \\\n",
       "0  0.778896       0.75  0.80198  0.804983  0.833151  0.35709      0.776734   \n",
       "\n",
       "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
       "0  0.509622       0.776438   0.487417      8.0          0.001   50000.0   \n",
       "\n",
       "    steps  \n",
       "0  1559.0  "
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.236025\n",
       "Name: loss_test, dtype: float64"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+metric_out_df.loss_test/(np.log(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1, 8)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_manual(file_path):\n",
    "    \n",
    "\n",
    "    # subj_num=12\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    # file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "\n",
    "    prob_train_df = pd.read_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_test_df = pd.read_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_val_df =  pd.read_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    \n",
    "    train_data_df = pd.read_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df = pd.read_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df = pd.read_csv(file_path+\"/val_data.csv\")\n",
    "\n",
    "\n",
    "    prob_train =prob_train_df.values[:,1:]\n",
    "    prob_test =prob_test_df.values[:,1:]\n",
    "    prob_val =prob_val_df.values[:,1:]\n",
    "\n",
    "    \n",
    "\n",
    "    train_yy = train_data_df.Choice.values\n",
    "    test_yy = test_data_df.Choice.values\n",
    "    val_yy = val_data_df.Choice.values\n",
    "\n",
    "\n",
    "    encode_categorical = train_yy.reshape(len(train_yy), 1)\n",
    "    train_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "\n",
    "    encode_categorical = test_yy.reshape(len(test_yy), 1)\n",
    "    test_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "    \n",
    "    encode_categorical = val_yy.reshape(len(val_yy), 1)\n",
    "    val_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    loss_train = -(np.dot(train_yy[:,0],np.log(prob_train[:,0])) + np.dot(train_yy[:,1],np.log(prob_train[:,1]) )) / train_yy.shape[0]\n",
    "    acc_train = (np.dot(train_yy[:,0],prob_train[:,0]) + np.dot(train_yy[:,1],prob_train[:,1]))/train_yy.shape[0]\n",
    "    pseudo_R2_train = 1 + loss_train/np.log(0.5)\n",
    "\n",
    "    loss_test = -(np.dot(test_yy[:,0],np.log(prob_test[:,0])) + np.dot(test_yy[:,1],np.log(prob_test[:,1]) )) /test_yy.shape[0]\n",
    "    acc_test = (np.dot(test_yy[:,0],prob_test[:,0]) + np.dot(test_yy[:,1],prob_test[:,1]))/150\n",
    "    pseudo_R2_test  = 1 + loss_test/np.log(0.5)\n",
    "    \n",
    "    \n",
    "    loss_val = -(np.dot(val_yy[:,0],np.log(prob_val[:,0])) + np.dot(val_yy[:,1],np.log(prob_val[:,1]) )) / val_yy.shape[0]\n",
    "    acc_val = (np.dot(val_yy[:,0],prob_val[:,0]) + np.dot(val_yy[:,1],prob_val[:,1]))/val_yy.shape[0]\n",
    "    pseudo_R2_val = 1 + loss_val/np.log(0.5)\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    # pseudoR2_test  = 1 + (-((np.dot(test_yy[:,0],np.log(prob_test[:,0])) + np.dot(test_yy[:,1],np.log(prob_test[:,1]))))/150)/np.log(0.5)\n",
    "\n",
    "    # print(acc_test)\n",
    "    # print(pseudoR2_test)\n",
    "    \n",
    "    metric_manual_df= pd.DataFrame(np.array([subj_num,acc_train,loss_train,pseudo_R2_train, acc_val,loss_val,pseudo_R2_val,acc_test,loss_test,pseudo_R2_test,neurons]).reshape(-1,11),columns =[\"Subject_number\",\"accuracy_train\",\"loss_train\",\"pseudoR2_train\", \"accuracy_val\",\"loss_val\",\"pseudoR2_val\",\"accuracy_test\",\"loss_test\",\"pseudo_R2_test\",\"neurons\"])\n",
    "\n",
    "    metric_manual_df.to_csv(file_path+\"/metric_manual.csv\")\n",
    "    \n",
    "    return metric_manual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for subj_num in range(28,42):\n",
    "# for subj_num in range(28,42):\n",
    "#     file_path =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "    file_path =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v10chunks/subject_num_\"+str(subj_num)\n",
    "\n",
    "\n",
    "    metrics_manual_df = metrics_manual(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     onehot_encoder = OneHotEncoder(sparse=False)\n",
    "#     # file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "\n",
    "#     prob_train_df = pd.read_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df = pd.read_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df =  pd.read_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    \n",
    "#     train_data_df = pd.read_csv(file_path+\"/train_data.csv\")\n",
    "#     test_data_df = pd.read_csv(file_path+\"/test_data.csv\")\n",
    "#     val_data_df = pd.read_csv(file_path+\"/val_data.csv\")\n",
    "\n",
    "\n",
    "#     prob_train =prob_train_df.values[:,1:]\n",
    "#     prob_test =prob_test_df.values[:,1:]\n",
    "#     prob_val =prob_val_df.values[:,1:]\n",
    "\n",
    "    \n",
    "\n",
    "#     train_yy = train_data_df.Choice.values\n",
    "#     test_yy = test_data_df.Choice.values\n",
    "#     val_yy = val_data_df.Choice.values\n",
    "\n",
    "\n",
    "#     encode_categorical = train_yy.reshape(len(train_yy), 1)\n",
    "#     train_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "\n",
    "#     encode_categorical = test_yy.reshape(len(test_yy), 1)\n",
    "#     test_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "    \n",
    "#     encode_categorical = val_yy.reshape(len(val_yy), 1)\n",
    "#     val_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "\n",
    "\n",
    "#     ############################################\n",
    "# #     loss_train = -(np.dot(train_yy[:,0],np.log(prob_train[:,0])) + np.dot(train_yy[:,1],np.log(prob_train[:,1]) )) / train_yy.shape[0]\n",
    "# #     acc_train = (np.dot(train_yy[:,0],prob_train[:,0]) + np.dot(train_yy[:,1],prob_train[:,1]))/train_yy.shape[0]\n",
    "# #     pseudo_R2_train = 1 + loss_train/np.log(0.5)\n",
    "\n",
    "#     loss_test = -(np.dot(test_yy[:,0],np.log(prob_test[:,0])) + np.dot(test_yy[:,1],np.log(prob_test[:,1]) )) /test_yy.shape[0]\n",
    "#     acc_test = (np.dot(test_yy[:,0],prob_test[:,0]) + np.dot(test_yy[:,1],prob_test[:,1]))/150\n",
    "#     pseudo_R2_test  = 1 + loss_test/np.log(0.5)\n",
    "    \n",
    "    \n",
    "#     loss_val = -(np.dot(val_yy[:,0],np.log(prob_val[:,0])) + np.dot(val_yy[:,1],np.log(prob_val[:,1]) )) / val_yy.shape[0]\n",
    "#     acc_val = (np.dot(val_yy[:,0],prob_val[:,0]) + np.dot(val_yy[:,1],prob_val[:,1]))/val_yy.shape[0]\n",
    "#     pseudo_R2_val = 1 + loss_val/np.log(0.5)\n",
    "\n",
    "#     ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "metrics_manual_df = metrics_manual(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_out_df.accuracy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_number</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>pseudoR2_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>pseudoR2_val</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>loss_test</th>\n",
       "      <th>pseudo_R2_test</th>\n",
       "      <th>neurons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36.0</td>\n",
       "      <td>0.853764</td>\n",
       "      <td>0.214202</td>\n",
       "      <td>0.690972</td>\n",
       "      <td>0.770068</td>\n",
       "      <td>0.41171</td>\n",
       "      <td>0.406028</td>\n",
       "      <td>0.774565</td>\n",
       "      <td>0.38291</td>\n",
       "      <td>0.447577</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Subject_number  accuracy_train  loss_train  pseudoR2_train  accuracy_val  \\\n",
       "0            36.0        0.853764    0.214202        0.690972      0.770068   \n",
       "\n",
       "   loss_val  pseudoR2_val  accuracy_test  loss_test  pseudo_R2_test  neurons  \n",
       "0   0.41171      0.406028       0.774565    0.38291        0.447577      8.0  "
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_manual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v6chunks/subject_num_41'"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Subject_number</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>loss_train</th>\n",
       "      <th>pseudoR2_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>pseudoR2_val</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>loss_test</th>\n",
       "      <th>pseudo_R2_test</th>\n",
       "      <th>neurons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.752035</td>\n",
       "      <td>0.357041</td>\n",
       "      <td>0.484899</td>\n",
       "      <td>0.702774</td>\n",
       "      <td>0.479203</td>\n",
       "      <td>0.308656</td>\n",
       "      <td>0.564067</td>\n",
       "      <td>0.537597</td>\n",
       "      <td>0.224412</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Subject_number  accuracy_train  loss_train  pseudoR2_train  \\\n",
       "0           0            41.0        0.752035    0.357041        0.484899   \n",
       "\n",
       "   accuracy_val  loss_val  pseudoR2_val  accuracy_test  loss_test  \\\n",
       "0      0.702774  0.479203      0.308656       0.564067   0.537597   \n",
       "\n",
       "   pseudo_R2_test  neurons  \n",
       "0        0.224412      8.0  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(file_path+\"/metric_manual.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>loss_test</th>\n",
       "      <th>neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.760037</td>\n",
       "      <td>0.775148</td>\n",
       "      <td>0.839744</td>\n",
       "      <td>0.790438</td>\n",
       "      <td>0.826735</td>\n",
       "      <td>0.367232</td>\n",
       "      <td>0.758883</td>\n",
       "      <td>0.476922</td>\n",
       "      <td>0.759083</td>\n",
       "      <td>0.529354</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>3424.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
       "0  0.760037   0.775148  0.839744  0.790438  0.826735  0.367232      0.758883   \n",
       "\n",
       "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
       "0  0.476922       0.759083   0.529354      8.0          0.001   50000.0   \n",
       "\n",
       "    steps  \n",
       "0  3424.0  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44067895759247466\n"
     ]
    }
   ],
   "source": [
    "loss_train= - ((np.dot(train_y[:,0],np.log(prob_train[:,0])) + np.dot(train_y[:,1],np.log(prob_train[:,1]))))/300\n",
    "print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8935238987455766"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train = ((np.dot(train_y[:,0],prob_train[:,0]) + np.dot(train_y[:,1],prob_train[:,1])))/300\n",
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4236662762909496\n"
     ]
    }
   ],
   "source": [
    "loss_test= - ((np.dot(test_y[:,0],np.log(prob_test[:,0])) + np.dot(test_y[:,1],np.log(prob_test[:,1]))))/150\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5610535642039031"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test = ((np.dot(test_y[:,0],prob_test[:,0]) + np.dot(test_y[:,1],prob_test[:,1])))/150\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rechecking below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path\n",
    "train_data_df = pd.read_csv(file_path+\"/train_data.csv\")\n",
    "test_data_df = pd.read_csv(file_path+\"/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_yy = train_data_df.Choice.values\n",
    "# # print(train_yy.shape)\n",
    "\n",
    "# encode_categorical = train_yy.reshape(len(train_yy), 1)\n",
    "\n",
    "# train_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "# ((np.dot(train_yy[:,0],prob_train_1[:,0]) + np.dot(train_yy[:,1],prob_train_1[:,1])))/300\n",
    "\n",
    "\n",
    "# # train_data_df.Choice.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5610535642039031"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prob_test_df = pd.read_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "prob_test_1 =prob_test_df.values[:,1:]\n",
    "test_yy = test_data_df.Choice.values\n",
    "\n",
    "\n",
    "encode_categorical = test_yy.reshape(len(test_yy), 1)\n",
    "\n",
    "test_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "((np.dot(test_yy[:,0],prob_test[:,0]) + np.dot(test_yy[:,1],prob_test[:,1])))/150\n",
    "\n",
    "# ((np.dot(test_yy[:,0],prob_test_1[:,0]) + np.dot(test_yy[:,1],prob_test_1[:,1])))/150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the above is correct. Below is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36571429],\n",
       "       [0.82857143],\n",
       "       [0.73142857],\n",
       "       [1.        ],\n",
       "       [0.6       ],\n",
       "       [0.81714286],\n",
       "       [0.85714286],\n",
       "       [1.        ],\n",
       "       [1.        ],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0:10,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject28\n",
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n"
     ]
    }
   ],
   "source": [
    "### create a composite dataset comprising all subject's actual data\n",
    "\n",
    "# task_mega_df = pd.DataFrame(); dopa_task_mega_df = pd.DataFrame();\n",
    "\n",
    "for subj_num in range(28,41):\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    \n",
    "    task_mega_df = task_mega_df.append(task_df)\n",
    "    dopa_task_mega_df = dopa_task_mega_df.append(dopa_task_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17458, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_df=pd.concat([task_mega_df,dopa_task_mega_df])\n",
    "task_net_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17458, 15)\n",
      "12139\n",
      "(12139, 8)\n",
      "(12139, 1)\n",
      "(2602, 8)\n",
      "(2602, 1)\n",
      "(2601, 8)\n",
      "(2601, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "task_df = task_net_df; dopa_task_df = task_net_df;\n",
    "train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5       , 0.25714286, 0.82857143, ..., 0.        ,\n",
       "         0.        , 0.36571429]],\n",
       "\n",
       "       [[0.28571429, 0.        , 0.73142857, ..., 0.5       ,\n",
       "         0.25714286, 0.82857143]],\n",
       "\n",
       "       [[0.92857143, 0.61714286, 1.        , ..., 0.28571429,\n",
       "         0.        , 0.73142857]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.        , 0.63428571, 1.        , ..., 0.92857143,\n",
       "         0.28571429, 1.        ]],\n",
       "\n",
       "       [[0.28571429, 0.        , 0.76      , ..., 1.        ,\n",
       "         0.63428571, 1.        ]],\n",
       "\n",
       "       [[0.5       , 0.37142857, 0.62857143, ..., 0.28571429,\n",
       "         0.        , 0.76      ]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject28\n",
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n",
      "Subject41\n"
     ]
    }
   ],
   "source": [
    "## create a composite dataset comprising all subject's actual data\n",
    "\n",
    "\n",
    "# task_net_train_df = pd.DataFrame(); task_net_valid_df = pd.DataFrame();\n",
    "\n",
    "\n",
    "# for subj_num in range(11,27):\n",
    "for subj_num in range(28,42):\n",
    "\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    task_net_train_df = task_net_train_df.append(pd.concat([task_df.loc[(task_df.TrialNum<=241) & (task_df.TrialNum>1)],dopa_task_df.loc[(dopa_task_df.TrialNum<=241) & (dopa_task_df.TrialNum>1)]]))\n",
    "    task_net_valid_df = task_net_valid_df.append(pd.concat([task_df.loc[(task_df.TrialNum>241) ],dopa_task_df.loc[(dopa_task_df.TrialNum>241) ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14400, 15)\n",
      "(14400, 8)\n",
      "(14400, 1)\n",
      "(3540, 8)\n",
      "(3540, 1)\n",
      "(3540, 8)\n",
      "(3540, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "task_df = task_net_train_df\n",
    "dopa_task_df  = task_net_valid_df\n",
    "\n",
    "train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_train_df.shape[0]/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_valid_df.shape[0]/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dopa_task_mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
