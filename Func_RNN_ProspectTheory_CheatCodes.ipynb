{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as sc_stats\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "onehot_encoder=OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "time_steps = 1\n",
    "inputs = 8\n",
    "outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_releveant_features(task_df):\n",
    "\n",
    "\n",
    "    task_df['PrevOutcome']=task_df['Outcome'].shift(1)\n",
    "    task_df.loc[1,'PrevOutcome']= 0\n",
    "\n",
    "    task_df['PrevChoice']=task_df['Choice'].shift(1)\n",
    "    task_df.loc[1,'PrevChoice']= 0\n",
    "\n",
    "    task_df['PrevSafe']=task_df['Safe'].shift(1)\n",
    "    task_df.loc[1,'PrevSafe']= 0\n",
    "\n",
    "    task_df['PrevBigRisky']=task_df['BigRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevBigRisky']= 0\n",
    "\n",
    "    task_df['PrevSmallRisky']=task_df['SmallRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevSmallRisky']= 0\n",
    "    \n",
    "#     task_df['PrevRT']=task_df['RT'].shift(1)\n",
    "#     task_df.loc[1,'PrevRT']= N\n",
    "    \n",
    "    \n",
    "    \n",
    "    return task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=150\n",
    "num_batches=1000\n",
    "seq_len=10\n",
    "\n",
    "def data_split(task_df,dopa_task_df):\n",
    "#     stop = 200\n",
    "\n",
    "    stop = 150\n",
    "\n",
    "#     stop=300\n",
    "\n",
    "#     stop = 750\n",
    "\n",
    "#     stop=1500\n",
    "\n",
    "\n",
    "    print(task_df.shape)\n",
    " \n",
    "\n",
    "    ##----------------- UNCOMMENT BELOW\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "    \n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "####### Prev O + C+ R + CurrO--------------------\n",
    "    \n",
    "#     train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "    test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "    test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "#### - Prev RT+C+R+O + Curr O----------------------\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "    ###### when splitting data into train and validation\n",
    "\n",
    "    # train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.35, random_state=1)\n",
    "\n",
    "    \n",
    "#     train_X, val_X, train_y, val_y = train_X[:stop], train_X[stop:], train_y[:stop], train_y[stop:]\n",
    "\n",
    "\n",
    "####################\n",
    " ##### splitting data into train test valid from the same dataset ###############\n",
    "    \n",
    "#     train_X, val_X, test_X, train_y, val_y, test_y = train_X[:stop], train_X[stop:stop+int(stop/2)], train_X[stop+int(stop/2):], train_y[:stop], train_y[stop:stop+int(stop/2)], train_y[stop+int(stop/2):],\n",
    "\n",
    "#### switching the order for test and val\n",
    "#     half = 1\n",
    "\n",
    "#     if half==1:\n",
    "#         train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int(stop/2)], train_X[stop+int(stop/2):], train_y[:stop], train_y[stop:stop+int(stop/2)], train_y[stop+int(stop/2):],\n",
    "    \n",
    "#     else:\n",
    "    \n",
    "#         train_X, test_X, val_X, train_y, test_y, val_y= train_X[stop-1:], train_X[:int(stop/2)], train_X[int(stop/2):stop-1], train_y[stop-1:], train_y[:int(stop/2)], train_y[int(stop/2):stop-1]\n",
    "\n",
    "#     ##############\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(train_X)\n",
    "    \n",
    "    \n",
    "    ## Combining PLAC + LDOPA datasets\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = np.concatenate(train_X[:stop],test_X[:stop]), np.concatenate(train_X[stop:stop+int(stop/2)])#,test_X[stop:stop+int(stop/2)]),np.concatenate(train_X[stop+int(stop/2):],test_X[stop+int(stop/2):]),np.concatenate(train_y[:stop],test_y[:stop]), np.concatenate(train_y[stop:stop+int(stop/2)],test_y[stop:stop+int(stop/2)]), np.concatenate(train_y[stop+int(stop/2):],test_y[stop+int(stop/2):])\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X, test_X,val_X = np.concatenate((train_X[:stop],test_X[:stop]),axis=0), np.concatenate((train_X[stop:stop+int(stop/2)] ,test_X[stop:stop+int(stop/2)]),axis=0), np.concatenate((train_X[stop+int(stop/2):],test_X[stop+int(stop/2):]),axis=0)\n",
    "#     train_y, test_y,val_y = np.concatenate((train_y[:stop],test_y[:stop]),axis=0), np.concatenate((train_y[stop:stop+int(stop/2)] ,test_y[stop:stop+int(stop/2)]),axis=0), np.concatenate((train_y[stop+int(stop/2):],test_y[stop+int(stop/2):]),axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    ## blocking data \n",
    "    train_X_aside, train_y_aside, test_X_aside, test_y_aside  = train_X, train_y, test_X, test_y \n",
    "    \n",
    "    train_X= np.concatenate((build_dataset_train(train_X_aside),build_dataset_train(test_X_aside)), axis=0)\n",
    "    train_y= np.concatenate((build_dataset_train(train_y_aside),build_dataset_train(test_y_aside)), axis=0)\n",
    "\n",
    "    \n",
    "    val_X= np.concatenate((build_dataset_valid(train_X_aside),build_dataset_valid(test_X_aside)), axis=0)\n",
    "    val_y= np.concatenate((build_dataset_valid(train_y_aside),build_dataset_valid(test_y_aside)), axis=0)\n",
    "\n",
    "    \n",
    "    test_X= np.concatenate((build_dataset_test(train_X_aside),build_dataset_test(test_X_aside)), axis=0)\n",
    "    test_y= np.concatenate((build_dataset_test(train_y_aside),build_dataset_test(test_y_aside)), axis=0)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    ##### FURTHER TRAINING WITH SUBSEQUENCES WITH REPLACEMENT\n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#     for k in range(num_batches):\n",
    "#             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#             train_X = np.concatenate((train_X,X_seq), axis=0)\n",
    "# #             print(train_X.shape)\n",
    "#             train_y = np.concatenate((train_y, y_seq),axis=0)\n",
    "\n",
    "\n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#     train_X, train_y = X_seq, y_seq\n",
    "#     for k in range(num_batches-1):\n",
    "#             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "#             train_X = np.concatenate((train_X,X_seq), axis=0)\n",
    "# #             print(train_X.shape)\n",
    "#             train_y = np.concatenate((train_y, y_seq),axis=0)\n",
    "# # # # ##########################\n",
    "\n",
    "\n",
    "\n",
    "    #### PRE TRAINING\n",
    "#     stop = int(0.7*len(train_X))\n",
    "#     print(stop)\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int((len(train_X)-stop)/2)], train_X[stop+int((len(train_X)-stop)/2):],train_y[:stop], train_y[stop:stop+int((len(train_X)-stop)/2)], train_y[stop+int((len(train_X)-stop)/2):]\n",
    "    \n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = train_X, test_X, test_X, train_y, test_y, test_y\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(val_X.shape)\n",
    "    print(val_y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "    # # center and scale\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.fit_transform(test_X)\n",
    "    val_X = scaler.fit_transform(val_X)\n",
    "\n",
    "\n",
    "    train_X = train_X[:,None,:]\n",
    "    val_X = val_X[:,None,:]\n",
    "    test_X = test_X[:,None,:]\n",
    "\n",
    "\n",
    "    # # one-hot encode the outputs\n",
    "\n",
    "    onehot_encoder = OneHotEncoder()\n",
    "    encode_categorical = train_y.reshape(len(train_y), 1)\n",
    "    encode_categorical_test = test_y.reshape(len(test_y), 1)\n",
    "    encode_categorical_val = val_y.reshape(len(val_y),1)\n",
    "\n",
    "\n",
    "    train_y = onehot_encoder.fit_transform(encode_categorical).toarray()\n",
    "    test_y = onehot_encoder.fit_transform(encode_categorical_test).toarray()\n",
    "    val_y = onehot_encoder.fit_transform(encode_categorical_val).toarray()\n",
    "\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, val_X,val_y\n",
    "#     return train_X, test_X, val_X#, test_X, test_y, val_X,val_y\n",
    "\n",
    "def build_dataset_train(data):\n",
    "    \n",
    "    return np.concatenate((data[:int(stop/3)],data[2*int(stop/3):int(stop)], data[stop+int(stop/3):stop+2*int(stop/3)]), axis=0)\n",
    "    \n",
    "\n",
    "def build_dataset_test(data):\n",
    "    \n",
    "    return np.concatenate((data[int(stop/3):int(stop/3)+int(stop/6)], data[int(stop):int(stop)+int(stop/6)], data[stop+2*int(stop/3): stop+2*int(stop/3) + int(stop/6) ]), axis=0)\n",
    "\n",
    "def build_dataset_valid(data):\n",
    "    \n",
    "    return np.concatenate((data[int(stop/3)+int(stop/6):int(stop/3)+2*int(stop/6)], data[int(stop)+int(stop/6):int(stop)+2*int(stop/6)], data[stop+2*int(stop/3) + int(stop/6) : ]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y): \n",
    "    reset_graph()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50000\n",
    "    batch_size = int(train_X.shape[0]/2)\n",
    "    # batch_size = 100\n",
    "    length = train_X.shape[0]\n",
    "    display = 100\n",
    "    neurons = neurons\n",
    "\n",
    "    num_batches = 100\n",
    "    seq_len = 10\n",
    "\n",
    "    percent_above_PT = 1\n",
    "\n",
    "    train_threshold = PT_R2 + percent_above_PT\n",
    "\n",
    "\n",
    "    save_step = 100\n",
    "\n",
    "\n",
    "    best_loss_val = np.infty\n",
    "    checks_since_last_progress = 0\n",
    "    max_checks_without_progress = 1000\n",
    "\n",
    "\n",
    "    # clear graph (if any) before running\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, time_steps, inputs])\n",
    "\n",
    "    y = tf.placeholder(tf.float32, [None, outputs])\n",
    "\n",
    "    # LSTM Cell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=neurons, activation=tf.nn.relu)\n",
    "    cell_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "    # pass into Dense layer\n",
    "    stacked_outputs = tf.reshape(cell_outputs, [-1, neurons])\n",
    "    out = tf.layers.dense(inputs=stacked_outputs, units=outputs)\n",
    "\n",
    "    probability = tf.nn.softmax(out)\n",
    "\n",
    "    # squared error loss or cost function for linear regression\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=y, logits=out))\n",
    "\n",
    "    # optimizer to minimize cost\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels =  tf.argmax(y, 1),\n",
    "                          predictions = tf.argmax(out, 1),\n",
    "                          name = \"accuracy\")\n",
    "    precision = tf.metrics.precision(labels=tf.argmax(y, 1),\n",
    "                                 predictions=tf.argmax(out, 1),\n",
    "                                 name=\"precision\")\n",
    "    recall = tf.metrics.recall(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"recall\")\n",
    "    f1 = 2 * accuracy[1] * recall[1] / ( precision[1] + recall[1] )\n",
    "\n",
    "    acc_up,acc_val = accuracy\n",
    "    auc = tf.metrics.auc(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"auc\")\n",
    "    \n",
    "    valid_store = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #######################\n",
    "#         saver.restore(sess, \"./checkpts/Original_RNN_LSTM_8features_v2.ckpt\")\n",
    "#         saver.restore(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "        saver.restore(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        #######################\n",
    "        \n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "\n",
    "        # Train the model\n",
    "        for steps in range(epochs):\n",
    "            mini_batch = zip(range(0, length, batch_size),\n",
    "                       range(batch_size, length+1, batch_size))\n",
    "\n",
    "            # train data in mini-batches\n",
    "            for (start, end) in mini_batch:\n",
    "    #             print(start,end)\n",
    "                sess.run(training_op, feed_dict = {X: train_X[start:end,:,:],\n",
    "                                                   y: train_y[start:end,:]}) \n",
    "\n",
    "            ## train data in batches of length subsequence\n",
    "\n",
    "    #         for k in range(num_batches):\n",
    "    #             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "\n",
    "    #             sess.run(training_op, feed_dict = {X:X_seq,y:y_seq}) \n",
    "            loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "            loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "            # print training performance \n",
    "            if (steps+1) % display == 0:\n",
    "                # evaluate loss function on training set\n",
    "\n",
    "\n",
    "                loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining loss: {}'.format((steps+1), loss_fn))\n",
    "\n",
    "                acc_train = acc_val.eval(feed_dict={X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining accuracy: {}'.format((steps+1), acc_train))\n",
    "\n",
    "\n",
    "                acc_test = acc_val.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest accuracy: {}'.format((steps+1), acc_test))\n",
    "\n",
    "                loss_test = loss.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest loss: {}'.format((steps+1), loss_test))\n",
    "\n",
    "                accu_val = acc_val.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "                loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "                print('Step: {}  \\tValid loss: {}'.format((steps+1), loss_val))\n",
    "\n",
    "                valid_store.append(loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (1 + loss_fn/np.log(0.5)) > train_threshold:\n",
    "                    print(\"Threshold achieved, quit training\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            if loss_val < best_loss_val:\n",
    "\n",
    "                        best_loss_val = loss_val\n",
    "                        checks_since_last_progress = 0\n",
    "            else:\n",
    "                            checks_since_last_progress += 1\n",
    "\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if checks_since_last_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "\n",
    "            if (steps+1) % save_step ==0:\n",
    "                                save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#                 save_path = saver.save(sess, \"./checkpts/RNN_Internet_LSTM_model_5features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     evaluate model accuracy\n",
    "        acc, prec, recall, f1, AUC = sess.run([accuracy, precision, recall, f1,auc],\n",
    "                                         feed_dict = {X: train_X, y: train_y})\n",
    "        prob_train = probability.eval(feed_dict = {X: train_X, y: train_y})\n",
    "        prob_test = probability.eval(feed_dict = {X: test_X, y: test_y})\n",
    "        prob_valid = probability.eval(feed_dict = {X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nEvaluation  on training set')\n",
    "        print('Accuracy:', acc[1])\n",
    "        print('Precision:', prec[1])\n",
    "        print('Recall:', recall[1])\n",
    "        print('F1 score:', f1)\n",
    "        print('AUC:', AUC[1])\n",
    "        \n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "        save_path = saver.save(sess, \"./checkpts/Later_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/LaterDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "    metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,AUC[1],loss_fn,accu_val,best_loss_val,acc_test,loss_test,neurons,learning_rate,epochs,steps]).reshape(-1,14),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\",\"loss\",\"accuracy_val\",\"loss_val\",\"accuracy_test\",\"loss_test\",\"neurons\",\"learning_rate\",\"n_epochs\",\"steps\"])\n",
    "    return metric_out_df, prob_train, prob_test, prob_valid\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def random_subsequence(X,y,seq_len):\n",
    "    rnd  = random.randint(0,len(X)-seq_len)\n",
    "    X_seq, y_seq = X[rnd:rnd+seq_len], y[rnd:rnd+seq_len]\n",
    "    return X_seq, y_seq\n",
    "\n",
    "    print(y_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subj_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-88950e9566c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m### ACTUAL DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubj_num\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/experiment_data.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtask_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subj_num' is not defined"
     ]
    }
   ],
   "source": [
    "# file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/\"\n",
    "# file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_subj29_params.csv\"\n",
    "\n",
    "file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1/generateddata300mu1params.csv\"\n",
    "\n",
    "# file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5/generateddata300mu0_5params.csv\"\n",
    "\n",
    "### ACTUAL DATA\n",
    "file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "\n",
    "task_df=pd.read_csv(file_name)\n",
    "task_df.head(10)\n",
    "task_df = add_releveant_features(task_df)\n",
    "\n",
    "\n",
    "task_df.head(10)\n",
    "\n",
    "\n",
    "train_data = np.concatenate((build_dataset_train(task_df.loc[task_df.TrialNum>1]),build_dataset_train(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "train_data_df = pd.DataFrame(train_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "test_data = np.concatenate((build_dataset_test(task_df.loc[task_df.TrialNum>1]),build_dataset_test(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "test_data_df = pd.DataFrame(test_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "\n",
    "\n",
    "# train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n",
    "\n",
    "\n",
    "# train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n",
      "Subject41\n"
     ]
    }
   ],
   "source": [
    "for subj_num in range(29,42):\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/v2/subject_num_\"+str(subj_num)\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "#     os.mkdir(file_path)\n",
    "\n",
    "    \n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    train_data = np.concatenate((build_dataset_train(task_df.loc[task_df.TrialNum>1]),build_dataset_train(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "    train_data_df = pd.DataFrame(train_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "\n",
    "    test_data = np.concatenate((build_dataset_test(task_df.loc[task_df.TrialNum>1]),build_dataset_test(dopa_task_df.loc[dopa_task_df.TrialNum>1])),axis=0)\n",
    "    test_data_df = pd.DataFrame(test_data[:,:10],columns =['TrialNum','SideOfScreen','Safe','BigRisky','SmallRisky','SideChosen','Choice','Outcome','RT','Happiness'])\n",
    "    \n",
    "    train_data_df.to_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df.to_csv(file_path+\"/test_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/\"\n",
    "# os.mkdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject41\n",
      "(301, 15)\n",
      "(300, 8)\n",
      "(300, 1)\n",
      "(148, 8)\n",
      "(148, 1)\n",
      "(150, 8)\n",
      "(150, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\n",
      "Step: 100  \tTraining loss: 0.6132085919380188\n",
      "Step: 100  \tTraining accuracy: 0.7233333587646484\n",
      "Step: 100  \tValid loss: 0.6565061807632446\n",
      "Step: 200  \tTraining loss: 0.576703667640686\n",
      "Step: 200  \tTraining accuracy: 0.6870824098587036\n",
      "Step: 200  \tValid loss: 0.6268594861030579\n",
      "Step: 300  \tTraining loss: 0.5554911494255066\n",
      "Step: 300  \tTraining accuracy: 0.6871657967567444\n",
      "Step: 300  \tValid loss: 0.5995491743087769\n",
      "Step: 400  \tTraining loss: 0.5357575416564941\n",
      "Step: 400  \tTraining accuracy: 0.6919770836830139\n",
      "Step: 400  \tValid loss: 0.5706907510757446\n",
      "Step: 500  \tTraining loss: 0.517011284828186\n",
      "Step: 500  \tTraining accuracy: 0.6979940533638\n",
      "Step: 500  \tValid loss: 0.5423917770385742\n",
      "Step: 600  \tTraining loss: 0.4996335804462433\n",
      "Step: 600  \tTraining accuracy: 0.7048632502555847\n",
      "Step: 600  \tValid loss: 0.519123911857605\n",
      "Step: 700  \tTraining loss: 0.4836582839488983\n",
      "Step: 700  \tTraining accuracy: 0.7096193432807922\n",
      "Step: 700  \tValid loss: 0.4975278675556183\n",
      "Step: 800  \tTraining loss: 0.469452828168869\n",
      "Step: 800  \tTraining accuracy: 0.7139990925788879\n",
      "Step: 800  \tValid loss: 0.4780135154724121\n",
      "Step: 900  \tTraining loss: 0.45705386996269226\n",
      "Step: 900  \tTraining accuracy: 0.7187253832817078\n",
      "Step: 900  \tValid loss: 0.46133139729499817\n",
      "Step: 1000  \tTraining loss: 0.4463596045970917\n",
      "Step: 1000  \tTraining accuracy: 0.7231608629226685\n",
      "Step: 1000  \tValid loss: 0.4469219446182251\n",
      "Step: 1100  \tTraining loss: 0.4372071325778961\n",
      "Step: 1100  \tTraining accuracy: 0.7275477647781372\n",
      "Step: 1100  \tValid loss: 0.43477681279182434\n",
      "Step: 1200  \tTraining loss: 0.4293849766254425\n",
      "Step: 1200  \tTraining accuracy: 0.731753408908844\n",
      "Step: 1200  \tValid loss: 0.4246600270271301\n",
      "Step: 1300  \tTraining loss: 0.42275676131248474\n",
      "Step: 1300  \tTraining accuracy: 0.7354199886322021\n",
      "Step: 1300  \tValid loss: 0.4159184396266937\n",
      "Step: 1400  \tTraining loss: 0.4171443283557892\n",
      "Step: 1400  \tTraining accuracy: 0.7389150261878967\n",
      "Step: 1400  \tValid loss: 0.4082942008972168\n",
      "Step: 1500  \tTraining loss: 0.41234445571899414\n",
      "Step: 1500  \tTraining accuracy: 0.7420433759689331\n",
      "Step: 1500  \tValid loss: 0.40192848443984985\n",
      "Step: 1600  \tTraining loss: 0.4082041084766388\n",
      "Step: 1600  \tTraining accuracy: 0.7451995611190796\n",
      "Step: 1600  \tValid loss: 0.39655768871307373\n",
      "Step: 1700  \tTraining loss: 0.40463531017303467\n",
      "Step: 1700  \tTraining accuracy: 0.7477705478668213\n",
      "Step: 1700  \tValid loss: 0.3918696641921997\n",
      "Step: 1800  \tTraining loss: 0.40154728293418884\n",
      "Step: 1800  \tTraining accuracy: 0.7500478029251099\n",
      "Step: 1800  \tValid loss: 0.3875596225261688\n",
      "Step: 1900  \tTraining loss: 0.39864495396614075\n",
      "Step: 1900  \tTraining accuracy: 0.752440333366394\n",
      "Step: 1900  \tValid loss: 0.3838925361633301\n",
      "Step: 2000  \tTraining loss: 0.39604195952415466\n",
      "Step: 2000  \tTraining accuracy: 0.7544160485267639\n",
      "Step: 2000  \tValid loss: 0.381082683801651\n",
      "Step: 2100  \tTraining loss: 0.39358454942703247\n",
      "Step: 2100  \tTraining accuracy: 0.7564437389373779\n",
      "Step: 2100  \tValid loss: 0.37833014130592346\n",
      "Step: 2200  \tTraining loss: 0.3910507559776306\n",
      "Step: 2200  \tTraining accuracy: 0.7582827806472778\n",
      "Step: 2200  \tValid loss: 0.37571418285369873\n",
      "Step: 2300  \tTraining loss: 0.38814276456832886\n",
      "Step: 2300  \tTraining accuracy: 0.7599583864212036\n",
      "Step: 2300  \tValid loss: 0.37282595038414\n",
      "Step: 2400  \tTraining loss: 0.38489624857902527\n",
      "Step: 2400  \tTraining accuracy: 0.7616336941719055\n",
      "Step: 2400  \tValid loss: 0.3704487681388855\n",
      "Step: 2500  \tTraining loss: 0.3819703161716461\n",
      "Step: 2500  \tTraining accuracy: 0.7631040215492249\n",
      "Step: 2500  \tValid loss: 0.3682844936847687\n",
      "Step: 2600  \tTraining loss: 0.379489004611969\n",
      "Step: 2600  \tTraining accuracy: 0.7644590139389038\n",
      "Step: 2600  \tValid loss: 0.3660300076007843\n",
      "Step: 2700  \tTraining loss: 0.377362459897995\n",
      "Step: 2700  \tTraining accuracy: 0.7657748460769653\n",
      "Step: 2700  \tValid loss: 0.3637057840824127\n",
      "Step: 2800  \tTraining loss: 0.37545645236968994\n",
      "Step: 2800  \tTraining accuracy: 0.7668734192848206\n",
      "Step: 2800  \tValid loss: 0.36130547523498535\n",
      "Step: 2900  \tTraining loss: 0.3737214207649231\n",
      "Step: 2900  \tTraining accuracy: 0.7678948640823364\n",
      "Step: 2900  \tValid loss: 0.3589659631252289\n",
      "Step: 3000  \tTraining loss: 0.37203601002693176\n",
      "Step: 3000  \tTraining accuracy: 0.7688470482826233\n",
      "Step: 3000  \tValid loss: 0.3565133213996887\n",
      "Step: 3100  \tTraining loss: 0.37049031257629395\n",
      "Step: 3100  \tTraining accuracy: 0.7697368264198303\n",
      "Step: 3100  \tValid loss: 0.35421469807624817\n",
      "Step: 3200  \tTraining loss: 0.3690880835056305\n",
      "Step: 3200  \tTraining accuracy: 0.7705700993537903\n",
      "Step: 3200  \tValid loss: 0.35263878107070923\n",
      "Step: 3300  \tTraining loss: 0.3677635192871094\n",
      "Step: 3300  \tTraining accuracy: 0.7713521122932434\n",
      "Step: 3300  \tValid loss: 0.35109245777130127\n",
      "Step: 3400  \tTraining loss: 0.36651554703712463\n",
      "Step: 3400  \tTraining accuracy: 0.7720874547958374\n",
      "Step: 3400  \tValid loss: 0.34953999519348145\n",
      "Step: 3500  \tTraining loss: 0.3653262257575989\n",
      "Step: 3500  \tTraining accuracy: 0.7727801203727722\n",
      "Step: 3500  \tValid loss: 0.34810978174209595\n",
      "Step: 3600  \tTraining loss: 0.3641892373561859\n",
      "Step: 3600  \tTraining accuracy: 0.7735280394554138\n",
      "Step: 3600  \tValid loss: 0.3467520773410797\n",
      "Step: 3700  \tTraining loss: 0.3630947768688202\n",
      "Step: 3700  \tTraining accuracy: 0.77423495054245\n",
      "Step: 3700  \tValid loss: 0.34547480940818787\n",
      "Step: 3800  \tTraining loss: 0.3620336353778839\n",
      "Step: 3800  \tTraining accuracy: 0.7749041318893433\n",
      "Step: 3800  \tValid loss: 0.34437525272369385\n",
      "Step: 3900  \tTraining loss: 0.361022412776947\n",
      "Step: 3900  \tTraining accuracy: 0.7755820155143738\n",
      "Step: 3900  \tValid loss: 0.3434506058692932\n",
      "Step: 4000  \tTraining loss: 0.3600519895553589\n",
      "Step: 4000  \tTraining accuracy: 0.7762255668640137\n",
      "Step: 4000  \tValid loss: 0.34278446435928345\n",
      "Step: 4100  \tTraining loss: 0.35912084579467773\n",
      "Step: 4100  \tTraining accuracy: 0.7768373489379883\n",
      "Step: 4100  \tValid loss: 0.3420935571193695\n",
      "Step: 4200  \tTraining loss: 0.358221173286438\n",
      "Step: 4200  \tTraining accuracy: 0.7774599194526672\n",
      "Step: 4200  \tValid loss: 0.34146901965141296\n",
      "Step: 4300  \tTraining loss: 0.35734960436820984\n",
      "Step: 4300  \tTraining accuracy: 0.7780532240867615\n",
      "Step: 4300  \tValid loss: 0.34098947048187256\n",
      "Step: 4400  \tTraining loss: 0.35649049282073975\n",
      "Step: 4400  \tTraining accuracy: 0.7786576747894287\n",
      "Step: 4400  \tValid loss: 0.3405555784702301\n",
      "Step: 4500  \tTraining loss: 0.35545894503593445\n",
      "Step: 4500  \tTraining accuracy: 0.7792349457740784\n",
      "Step: 4500  \tValid loss: 0.3410452604293823\n",
      "Step: 4600  \tTraining loss: 0.35423773527145386\n",
      "Step: 4600  \tTraining accuracy: 0.7798603177070618\n",
      "Step: 4600  \tValid loss: 0.34179404377937317\n",
      "Step: 4700  \tTraining loss: 0.3532193601131439\n",
      "Step: 4700  \tTraining accuracy: 0.7804948091506958\n",
      "Step: 4700  \tValid loss: 0.3413584530353546\n",
      "Step: 4800  \tTraining loss: 0.35231414437294006\n",
      "Step: 4800  \tTraining accuracy: 0.7811377644538879\n",
      "Step: 4800  \tValid loss: 0.34089136123657227\n",
      "Step: 4900  \tTraining loss: 0.3514360189437866\n",
      "Step: 4900  \tTraining accuracy: 0.7817542552947998\n",
      "Step: 4900  \tValid loss: 0.3408043384552002\n",
      "Step: 5000  \tTraining loss: 0.3505217730998993\n",
      "Step: 5000  \tTraining accuracy: 0.7823457717895508\n",
      "Step: 5000  \tValid loss: 0.34106141328811646\n",
      "Step: 5100  \tTraining loss: 0.3496527671813965\n",
      "Step: 5100  \tTraining accuracy: 0.7828807830810547\n",
      "Step: 5100  \tValid loss: 0.3412127196788788\n",
      "Step: 5200  \tTraining loss: 0.34887513518333435\n",
      "Step: 5200  \tTraining accuracy: 0.7833950519561768\n",
      "Step: 5200  \tValid loss: 0.3412250578403473\n",
      "Step: 5300  \tTraining loss: 0.3481643795967102\n",
      "Step: 5300  \tTraining accuracy: 0.7838578224182129\n",
      "Step: 5300  \tValid loss: 0.34114202857017517\n",
      "Step: 5400  \tTraining loss: 0.347507506608963\n",
      "Step: 5400  \tTraining accuracy: 0.7843033075332642\n",
      "Step: 5400  \tValid loss: 0.3410515785217285\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.78473246\n",
      "Precision: 0.8064516\n",
      "Recall: 0.86206895\n",
      "F1 score: 0.8108902\n",
      "AUC: 0.8342602\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0  0.784732   0.806452  0.862069   0.81089  0.83426  0.347495      0.784281   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.340544       0.783972   0.539226      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  5401.0  \n"
     ]
    }
   ],
   "source": [
    "neurons = 8\n",
    "for subj_num in range(41,42):\n",
    "# for subj_num in range(28,42):\n",
    "\n",
    "# for subj_num in [20]:##[15, 16, 17]:# 19, 20, 23, 24, 25, 26, 29, 36, 37, 40]:\n",
    "\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    PT_file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/\"\n",
    "    PT_file_name = PT_file_path  + \"PT_loss_updated.csv\"\n",
    "    PT_metrics = pd.read_csv(PT_file_name)\n",
    "    PT_metrics = PT_metrics[PT_metrics.PT_loss !=0]\n",
    "    PT_R2= PT_metrics[PT_metrics.Subject_number ==subj_num].PT_pseudoR2.iloc[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for generated data ##\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_subj29_params.csv\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_1500_subj29_params.csv\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/generated_data_3000_subj29_params.csv\"\n",
    "\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1\"\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5\"\n",
    "\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=1/generateddata1500mu1params.csv\"\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/mu=0.5/generateddata300mu0_5params.csv\"\n",
    "\n",
    "\n",
    "\n",
    "############# ORIGINAL ##############\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5/generateddata3000mu0_5params.csv\"\n",
    "\n",
    "\n",
    "# ############ ACTUAL ###############\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5\"\n",
    "\n",
    "#     file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/generated_data/PT_fake_data/FittingProc/mu=0.5/generateddata600mu0_5params.csv\"\n",
    "\n",
    "\n",
    "#     task_df=pd.read_csv(file_name)\n",
    "# #     task_df.TrialNum = task_df.TrialNum-1\n",
    "\n",
    "#     task_df = add_releveant_features(task_df)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### ORIGINAL\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining\"\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining_v2\"\n",
    "\n",
    "\n",
    "#     task_df = task_net_df; dopa_task_df = task_net_df;\n",
    "    \n",
    "    #############################\n",
    "#     task_df = task_net_train_df\n",
    "#     dopa_task_df  = task_net_valid_df\n",
    "\n",
    "    \n",
    "    \n",
    "     \n",
    "    ### ACTUAL DATA PER SUBJECT\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/vcheck/subject_num_\"+str(subj_num)\n",
    "\n",
    "#     file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/ActualDataFitting/Pretraining/subject_num_\"+str(subj_num)+\"/first_half_train\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n",
    "\n",
    "    \n",
    "    \n",
    "#     train_X,train_y= data_split(task_df,dopa_task_df)\n",
    "\n",
    "    \n",
    "#     X_seq, y_seq = random_subsequence(train_X,train_y,10)\n",
    "\n",
    "    metric_out_df, prob_train, prob_test, prob_val = train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y)\n",
    "    \n",
    "    print(metric_out_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     os.mkdir(file_path + \"50_splits_combined_1sthalf\")\n",
    "#     os.mkdir(file_path + \"50_splits_combined_2ndhalf\")\n",
    "\n",
    "\n",
    "    \n",
    "#     os.mkdir(file_path + \"combined_1sthalf\")\n",
    "#     os.mkdir(file_path + \"combined_2ndhalf\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     os.mkdir(file_path + \"300_sub29\")\n",
    "#     os.mkdir(file_path + \"1500_sub29\")\n",
    "#     os.mkdir(file_path + \"3000_sub29\")\n",
    "\n",
    "#     os.mkdir(file_path + \"/300\")\n",
    "#     os.mkdir(file_path + \"/1500\")\n",
    "#     os.mkdir(file_path + \"/3000\")\n",
    "#     os.mkdir(file_path + \"/600\")\n",
    "\n",
    "\n",
    "\n",
    "#     os.mkdir(file_path)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_curr_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_v2_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "########################################\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#######################################\n",
    "    \n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_Crossval_currprev_RT_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_CrossvalTESTinsess1sthalf_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"LSTM_updated_CrossvalTESTinsess2ndhalf_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"combined_1sthalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"combined_2ndhalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"50_splits_combined_1sthalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"50_splits_combined_2ndhalf/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "##### generated data ######\n",
    "#     metric_out_df.to_csv(file_path+\"300_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"1500_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"3000_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"3000_sub29/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/300/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"/1500/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/3000/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metrics_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/3000/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\"_v2.csv\")\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\"_v2.csv\")\n",
    "\n",
    "\n",
    "#     metric_out_df.to_csv(file_path+\"/600/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "    metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "    \n",
    "    prob_train_df = pd.DataFrame(prob_train.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_test_df = pd.DataFrame(prob_test.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_val_df = pd.DataFrame(prob_val.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"prob_train_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"prob_test_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"prob_val_currentopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"prob_train_currentopts_prev_outchoicevv2_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"prob_test_currentopts_prev_outchoicev2_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"prob_val_currentopts_prev_outchoicev2_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "################################\n",
    "    prob_train_df.to_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_test_df.to_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "    prob_val_df.to_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#############################\n",
    "\n",
    "    \n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "    \n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"2ndhalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"2ndhalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"2ndhalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"combined_1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"combined_1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"combined_2ndhalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"combined_2ndhalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"combined_2ndhalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"50_splits_combined_1sthalf/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"50_splits_combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"50_splits_combined_1sthalf/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####### Generated data #######\n",
    "#     prob_train_df.to_csv(file_path + \"300_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"300_sub29_combined_1sthalf/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"300_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"1500_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"1500_sub29/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"1500_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"3000_sub29/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"3000_sub29/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"3000_sub29/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/300/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/300/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/300/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/1500/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/1500/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/1500/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/3000/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/3000/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/3000/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "#     prob_train_df.to_csv(file_path + \"/600/\" +\"prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_test_df.to_csv(file_path + \"/600/\"+ \"prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "#     prob_val_df.to_csv(file_path + \"/600/\"+\"prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>loss_test</th>\n",
       "      <th>neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.784732</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.81089</td>\n",
       "      <td>0.83426</td>\n",
       "      <td>0.347495</td>\n",
       "      <td>0.784281</td>\n",
       "      <td>0.340544</td>\n",
       "      <td>0.783972</td>\n",
       "      <td>0.539226</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>5401.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
       "0  0.784732   0.806452  0.862069   0.81089  0.83426  0.347495      0.784281   \n",
       "\n",
       "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
       "0  0.340544       0.783972   0.539226      8.0          0.001   50000.0   \n",
       "\n",
       "    steps  \n",
       "0  5401.0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.222061\n",
       "Name: loss_test, dtype: float64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+metric_out_df.loss_test/(np.log(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34749500210907475\n"
     ]
    }
   ],
   "source": [
    "loss_train= - ((np.dot(train_y[:,0],np.log(prob_train[:,0])) + np.dot(train_y[:,1],np.log(prob_train[:,1]))))/300\n",
    "print(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.763328478684028"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train = ((np.dot(train_y[:,0],prob_train[:,0]) + np.dot(train_y[:,1],prob_train[:,1])))/300\n",
    "acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5391888426767157\n"
     ]
    }
   ],
   "source": [
    "loss_test= - ((np.dot(test_y[:,0],np.log(prob_test[:,0])) + np.dot(test_y[:,1],np.log(prob_test[:,1]))))/150\n",
    "print(loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6847332539657752"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_test = ((np.dot(test_y[:,0],prob_test[:,0]) + np.dot(test_y[:,1],prob_test[:,1])))/150\n",
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rechecking below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path\n",
    "train_data_df = pd.read_csv(file_path+\"/train_data.csv\")\n",
    "test_data_df = pd.read_csv(file_path+\"/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_yy = train_data_df.Choice.values\n",
    "# # print(train_yy.shape)\n",
    "\n",
    "# encode_categorical = train_yy.reshape(len(train_yy), 1)\n",
    "\n",
    "# train_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "# ((np.dot(train_yy[:,0],prob_train_1[:,0]) + np.dot(train_yy[:,1],prob_train_1[:,1])))/300\n",
    "\n",
    "\n",
    "# # train_data_df.Choice.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6847332539657752"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prob_test_df = pd.read_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "prob_test_1 =prob_test_df.values[:,1:]\n",
    "test_yy = test_data_df.Choice.values\n",
    "\n",
    "\n",
    "encode_categorical = test_yy.reshape(len(test_yy), 1)\n",
    "\n",
    "test_yy = onehot_encoder.fit_transform(encode_categorical)\n",
    "((np.dot(test_yy[:,0],prob_test[:,0]) + np.dot(test_yy[:,1],prob_test[:,1])))/150\n",
    "\n",
    "# ((np.dot(test_yy[:,0],prob_test_1[:,0]) + np.dot(test_yy[:,1],prob_test_1[:,1])))/150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the above is correct. Below is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36571429],\n",
       "       [0.82857143],\n",
       "       [0.73142857],\n",
       "       [1.        ],\n",
       "       [0.6       ],\n",
       "       [0.81714286],\n",
       "       [0.85714286],\n",
       "       [1.        ],\n",
       "       [1.        ],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0:10,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject28\n",
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n"
     ]
    }
   ],
   "source": [
    "### create a composite dataset comprising all subject's actual data\n",
    "\n",
    "# task_mega_df = pd.DataFrame(); dopa_task_mega_df = pd.DataFrame();\n",
    "\n",
    "for subj_num in range(28,41):\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    \n",
    "    task_mega_df = task_mega_df.append(task_df)\n",
    "    dopa_task_mega_df = dopa_task_mega_df.append(dopa_task_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17458, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_df=pd.concat([task_mega_df,dopa_task_mega_df])\n",
    "task_net_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17458, 15)\n",
      "12139\n",
      "(12139, 8)\n",
      "(12139, 1)\n",
      "(2602, 8)\n",
      "(2602, 1)\n",
      "(2601, 8)\n",
      "(2601, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "task_df = task_net_df; dopa_task_df = task_net_df;\n",
    "train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.5       , 0.25714286, 0.82857143, ..., 0.        ,\n",
       "         0.        , 0.36571429]],\n",
       "\n",
       "       [[0.28571429, 0.        , 0.73142857, ..., 0.5       ,\n",
       "         0.25714286, 0.82857143]],\n",
       "\n",
       "       [[0.92857143, 0.61714286, 1.        , ..., 0.28571429,\n",
       "         0.        , 0.73142857]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.        , 0.63428571, 1.        , ..., 0.92857143,\n",
       "         0.28571429, 1.        ]],\n",
       "\n",
       "       [[0.28571429, 0.        , 0.76      , ..., 1.        ,\n",
       "         0.63428571, 1.        ]],\n",
       "\n",
       "       [[0.5       , 0.37142857, 0.62857143, ..., 0.28571429,\n",
       "         0.        , 0.76      ]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject28\n",
      "Subject29\n",
      "Subject30\n",
      "Subject31\n",
      "Subject32\n",
      "Subject33\n",
      "Subject34\n",
      "Subject35\n",
      "Subject36\n",
      "Subject37\n",
      "Subject38\n",
      "Subject39\n",
      "Subject40\n",
      "Subject41\n"
     ]
    }
   ],
   "source": [
    "## create a composite dataset comprising all subject's actual data\n",
    "\n",
    "\n",
    "# task_net_train_df = pd.DataFrame(); task_net_valid_df = pd.DataFrame();\n",
    "\n",
    "\n",
    "# for subj_num in range(11,27):\n",
    "for subj_num in range(28,42):\n",
    "\n",
    "    print(\"Subject\"+ str(subj_num))\n",
    "\n",
    "    file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num)+\"/\"\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\"+str(subj_num) + \"/experiment_data.csv\"\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\"+str(subj_num) +\"/dopa_experiment_data.csv\"\n",
    "    \n",
    "    task_df = pd.read_csv(file_name)\n",
    "    dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "\n",
    "    task_df = add_releveant_features(task_df)\n",
    "    dopa_task_df = add_releveant_features(dopa_task_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    task_net_train_df = task_net_train_df.append(pd.concat([task_df.loc[(task_df.TrialNum<=241) & (task_df.TrialNum>1)],dopa_task_df.loc[(dopa_task_df.TrialNum<=241) & (dopa_task_df.TrialNum>1)]]))\n",
    "    task_net_valid_df = task_net_valid_df.append(pd.concat([task_df.loc[(task_df.TrialNum>241) ],dopa_task_df.loc[(dopa_task_df.TrialNum>241) ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14400, 15)\n",
      "(14400, 8)\n",
      "(14400, 1)\n",
      "(3540, 8)\n",
      "(3540, 1)\n",
      "(3540, 8)\n",
      "(3540, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "task_df = task_net_train_df\n",
    "dopa_task_df  = task_net_valid_df\n",
    "\n",
    "train_X, train_y, test_X, test_y,val_X,val_y = data_split(task_df,dopa_task_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_train_df.shape[0]/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_net_valid_df.shape[0]/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dopa_task_mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
