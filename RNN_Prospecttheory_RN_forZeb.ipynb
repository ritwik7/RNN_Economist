{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as sc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Prospect Theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/example_RKN\"\n",
    "# file_name = file_path + \"/subj_num_39.csv\"\n",
    "# file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subj_num_39\"\n",
    "# file_name = file_path + \"/experiment_data.csv\"\n",
    "\n",
    "file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_15/\"\n",
    "file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_15/experiment_data.csv\"\n",
    "file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_15/dopa_experiment_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PT_result.mat',\n",
       " 'LSTM_metrics_all_subjs.csv',\n",
       " 'loglikelihoods_all',\n",
       " 'LSTM_metrics_10neurons.csv',\n",
       " 'LSTM_metrics.csv',\n",
       " 'experiment_data.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialNum</th>\n",
       "      <th>SideOfScreen</th>\n",
       "      <th>Safe</th>\n",
       "      <th>BigRisky</th>\n",
       "      <th>SmallRisky</th>\n",
       "      <th>SideChosen</th>\n",
       "      <th>Choice</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>RT</th>\n",
       "      <th>Happiness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.683594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-175.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>1.346</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>1.848</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>1.504</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.477</td>\n",
       "      <td>0.648438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>1.719</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>1.431</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.549</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-25.0</td>\n",
       "      <td>1.824</td>\n",
       "      <td>0.627604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.295</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TrialNum  SideOfScreen  Safe  BigRisky  SmallRisky  SideChosen  Choice  \\\n",
       "0         0           NaN   NaN       NaN         NaN         NaN     NaN   \n",
       "1         1           1.0 -35.0       0.0      -175.0         2.0     0.0   \n",
       "2         2           1.0 -15.0       0.0       -37.0         2.0     0.0   \n",
       "3         3           1.0 -15.0       0.0       -27.0         1.0     1.0   \n",
       "4         4           1.0  15.0      42.0         0.0         1.0     1.0   \n",
       "5         5           1.0 -25.0       0.0       -56.0         2.0     0.0   \n",
       "6         6           1.0   0.0      65.0       -65.0         1.0     1.0   \n",
       "7         7           1.0   0.0      30.0       -45.0         2.0     0.0   \n",
       "8         8           1.0 -25.0       0.0       -56.0         2.0     0.0   \n",
       "9         9           1.0   0.0      80.0       -66.0         1.0     1.0   \n",
       "\n",
       "   Outcome     RT  Happiness  \n",
       "0      NaN    NaN   0.683594  \n",
       "1    -35.0  1.346        NaN  \n",
       "2    -15.0  1.848        NaN  \n",
       "3    -27.0  1.504        NaN  \n",
       "4      0.0  1.477   0.648438  \n",
       "5    -25.0  1.719        NaN  \n",
       "6    -65.0  1.431        NaN  \n",
       "7      0.0  1.549        NaN  \n",
       "8    -25.0  1.824   0.627604  \n",
       "9     80.0  1.295        NaN  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_df = pd.read_csv(file_name)\n",
    "task_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301, 10)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialNum</th>\n",
       "      <th>SideOfScreen</th>\n",
       "      <th>Safe</th>\n",
       "      <th>BigRisky</th>\n",
       "      <th>SmallRisky</th>\n",
       "      <th>SideChosen</th>\n",
       "      <th>Choice</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>RT</th>\n",
       "      <th>Happiness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.418</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.327</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>1.845</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-175.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>1.033</td>\n",
       "      <td>0.552083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.798</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-126.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>1.623</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.872</td>\n",
       "      <td>0.549479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.725</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.999</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TrialNum  SideOfScreen  Safe  BigRisky  SmallRisky  SideChosen  Choice  \\\n",
       "0         0           NaN   NaN       NaN         NaN         NaN     NaN   \n",
       "1         1           2.0  15.0      54.0         0.0         2.0     1.0   \n",
       "2         2           2.0   0.0      30.0       -20.0         2.0     1.0   \n",
       "3         3           2.0 -15.0       0.0       -75.0         1.0     0.0   \n",
       "4         4           2.0 -35.0       0.0      -175.0         1.0     0.0   \n",
       "5         5           2.0  20.0      36.0         0.0         1.0     0.0   \n",
       "6         6           2.0 -35.0       0.0      -126.0         1.0     0.0   \n",
       "7         7           2.0  35.0     111.0         0.0         2.0     1.0   \n",
       "8         8           2.0  30.0      84.0         0.0         2.0     1.0   \n",
       "9         9           2.0  30.0     150.0         0.0         2.0     1.0   \n",
       "\n",
       "   Outcome     RT  Happiness  \n",
       "0      NaN    NaN   0.578125  \n",
       "1      0.0  1.418        NaN  \n",
       "2     30.0  1.327        NaN  \n",
       "3    -15.0  1.845        NaN  \n",
       "4    -35.0  1.033   0.552083  \n",
       "5     20.0  1.798        NaN  \n",
       "6    -35.0  1.623        NaN  \n",
       "7      0.0  1.872   0.549479  \n",
       "8      0.0  1.725        NaN  \n",
       "9    150.0  0.999        NaN  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dopa_task_df = pd.read_csv(file_dopa_name)\n",
    "dopa_task_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for running a batch of all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_data_df = pd.DataFrame()\n",
    "all_subj_dopa_data_df = pd.DataFrame()\n",
    "\n",
    "for subj_num in range(11,27):\n",
    "#     print(subj_num)\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\" + str(subj_num) + \"/experiment_data.csv\"\n",
    "    all_subj_data_df = all_subj_data_df.append(pd.read_csv(file_name))\n",
    "    file_dopa_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/dopadata/subject_num_\" + str(subj_num) + \"/dopa_experiment_data.csv\"\n",
    "    all_subj_dopa_data_df = all_subj_dopa_data_df.append(pd.read_csv(file_dopa_name))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_subj_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_subj_data_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df  = all_subj_data_df\n",
    "dopa_task_df = all_subj_dopa_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_releveant_features(task_df):\n",
    "\n",
    "\n",
    "    task_df['PrevOutcome']=task_df['Outcome'].shift(1)\n",
    "    task_df.loc[1,'PrevOutcome']= 0\n",
    "\n",
    "    task_df['PrevChoice']=task_df['Choice'].shift(1)\n",
    "    task_df.loc[1,'PrevChoice']= 0\n",
    "\n",
    "    task_df['PrevSafe']=task_df['Safe'].shift(1)\n",
    "    task_df.loc[1,'PrevSafe']= 0\n",
    "\n",
    "    task_df['PrevBigRisky']=task_df['BigRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevBigRisky']= 0\n",
    "\n",
    "    task_df['PrevSmallRisky']=task_df['SmallRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevSmallRisky']= 0\n",
    "    \n",
    "    return task_df\n",
    "\n",
    "task_df = add_releveant_features (task_df)\n",
    "dopa_task_df = add_releveant_features(dopa_task_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dopa_task_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define task parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 300\n",
    "inputs = 8\n",
    "outputs = 2\n",
    "\n",
    "# adjust target(t) to depend on input (t-1)\n",
    "# df.Property = df.Property.shift(-1)\n",
    "\n",
    "\n",
    "# # # remove nans as a result of the shifted values\n",
    "# df = df.iloc[:-1,:]\n",
    "# df = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.arange(0,task_df.shape[0],301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_encoder = LabelEncoder()\n",
    "# label_encoder.fit_transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_df.loc[task_df.TrialNum!=0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dopa_task_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_X, trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# include_ind = np.setdiff1d(np.arange(task_df.shape[0]),np.arange(0,task_df.shape[0],301))\n",
    "# include_ind = np.arange(1,task_df.shape[0])\n",
    "# X_y_split\n",
    "## TRAIN\n",
    "# train_X = task_df.loc[include_ind,['Safe','BigRisky','SmallRisky']].values\n",
    "# train_y = task_df.loc[include_ind,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "# train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "train_y = task_df.loc[task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "# ## TEST\n",
    "# test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky']].values\n",
    "test_y = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Choice']].values.astype(np.int32)\n",
    "\n",
    "train_X = task_df.loc[task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "test_X = dopa_task_df.loc[dopa_task_df.TrialNum!=0,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "\n",
    "extra_features = 5;\n",
    "\n",
    "\n",
    "\n",
    "# # center and scale\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "train_X = scaler.fit_transform(train_X)\n",
    "test_X = scaler.fit_transform(test_X)\n",
    "\n",
    "train_X[0,-extra_features + 1:]= 0;\n",
    "\n",
    "\n",
    "\n",
    "# # reshape input to 3D array\n",
    "# train_X = train_X[None,:,:]\n",
    "# test_X = test_X[None,:,:]\n",
    "\n",
    "\n",
    "train_X = train_X.reshape(-1,time_steps,inputs)\n",
    "test_X = test_X.reshape(-1,time_steps,inputs)\n",
    "\n",
    "\n",
    "\n",
    "# # one-hot encode the outputs\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "encode_categorical = train_y.reshape(len(train_y), 1)\n",
    "encode_categorical_test = test_y.reshape(len(test_y), 1)\n",
    "\n",
    "train_y = onehot_encoder.fit_transform(encode_categorical).toarray()\n",
    "test_y = onehot_encoder.fit_transform(encode_categorical_test).toarray()\n",
    "\n",
    "train_y =  train_y.reshape(-1,time_steps,outputs)\n",
    "test_y =  test_y.reshape(-1,time_steps,outputs)\n",
    "\n",
    "\n",
    "\n",
    "# train_y = train_y[None,:,:]\n",
    "# test_y = test_y[None,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 300, 8)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 300, 2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_X)\n",
    "# print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 300, 8)\n",
      "(16, 300, 2)\n",
      "(16, 300, 8)\n",
      "(16, 300, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "(?, 300, 8)\n",
      "(?, 300, 2)\n",
      "(?, 300, 2)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.000001\n",
    "epochs = 2000\n",
    "batch_size = int(train_X.shape[0])\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "print(batch_size)\n",
    "\n",
    "\n",
    "length = train_X.shape[0]\n",
    "display = 10 ## display loss function every display trials\n",
    "neurons = 100 ## number of neurons in LSTM cell\n",
    "save_step = 100\n",
    "\n",
    "\n",
    "best_loss_val = np.infty\n",
    "checks_since_last_progress = 0\n",
    "max_checks_without_progress = 20\n",
    "\n",
    "# # clear graph (if any) before running\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, time_steps, inputs])\n",
    "y = tf.placeholder(tf.float32, [None, time_steps, outputs])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# LSTM Cell\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=neurons, activation=tf.nn.relu)\n",
    "cell_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "# # pass into Dense layer\n",
    "stacked_outputs = tf.reshape(cell_outputs, [-1, time_steps,neurons])\n",
    "out = tf.layers.dense(inputs=stacked_outputs, units=outputs) ## logits layer kernel_initializer=he_init\n",
    "\n",
    "print(out.shape)\n",
    "\n",
    "probability = tf.nn.softmax(out)\n",
    "\n",
    "\n",
    "# squared error loss or cost function for linear regression\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=out))\n",
    "\n",
    "\n",
    "\n",
    "# # optimizer to minimize cost\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ArgMax:0\", shape=(?, 300), dtype=int64)\n",
      "Tensor(\"ArgMax_1:0\", shape=(?, 300), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(tf.argmax(y,2))\n",
    "print(tf.arg_max(out,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.metrics.accuracy(labels =  tf.argmax(y, 2),\n",
    "                          predictions = tf.argmax(out, 2),\n",
    "                          name = \"accuracy\")\n",
    "# print(accuracy) -- this is a tuple\n",
    "acc_up,acc_val = accuracy\n",
    "\n",
    "precision = tf.metrics.precision(labels=tf.argmax(y, 2),\n",
    "                                 predictions=tf.argmax(out, 2),\n",
    "                                 name=\"precision\")\n",
    "prec_up,prec_val = precision\n",
    "\n",
    "\n",
    "recall = tf.metrics.recall(labels=tf.argmax(y, 2),\n",
    "                           predictions=tf.argmax(out, 2),\n",
    "                           name=\"recall\")\n",
    "\n",
    "auc = tf.metrics.auc(labels=tf.argmax(y, 2),\n",
    "                           predictions=tf.argmax(out, 2),\n",
    "                           name=\"auc\")\n",
    "f1 = 2 * accuracy[1] * recall[1] / ( precision[1] + recall[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10  \tTraining loss: 0.6990088820457458\n",
      "Step: 10  \tTraining accuracy: 0.48625001311302185\n",
      "Step: 10  \tTest accuracy: 0.49291667342185974\n",
      "Step: 10  \tTest loss: 0.6970481872558594\n",
      "Step: 20  \tTraining loss: 0.6985108256340027\n",
      "Step: 20  \tTraining accuracy: 0.4906249940395355\n",
      "Step: 20  \tTest accuracy: 0.49286457896232605\n",
      "Step: 20  \tTest loss: 0.6966959834098816\n",
      "Step: 30  \tTraining loss: 0.6980476975440979\n",
      "Step: 30  \tTraining accuracy: 0.49149999022483826\n",
      "Step: 30  \tTest accuracy: 0.4928472340106964\n",
      "Step: 30  \tTest loss: 0.6963756680488586\n",
      "Step: 40  \tTraining loss: 0.6976175904273987\n",
      "Step: 40  \tTraining accuracy: 0.4918749928474426\n",
      "Step: 40  \tTest accuracy: 0.4928385317325592\n",
      "Step: 40  \tTest loss: 0.6960852146148682\n",
      "Step: 50  \tTraining loss: 0.6972169876098633\n",
      "Step: 50  \tTraining accuracy: 0.492083340883255\n",
      "Step: 50  \tTest accuracy: 0.49283334612846375\n",
      "Step: 50  \tTest loss: 0.6958203911781311\n",
      "Step: 60  \tTraining loss: 0.6968428492546082\n",
      "Step: 60  \tTraining accuracy: 0.4922159016132355\n",
      "Step: 60  \tTest accuracy: 0.4928298592567444\n",
      "Step: 60  \tTest loss: 0.6955775022506714\n",
      "Step: 70  \tTraining loss: 0.6964921355247498\n",
      "Step: 70  \tTraining accuracy: 0.4923076927661896\n",
      "Step: 70  \tTest accuracy: 0.4928273856639862\n",
      "Step: 70  \tTest loss: 0.6953529715538025\n",
      "Step: 80  \tTraining loss: 0.6961610317230225\n",
      "Step: 80  \tTraining accuracy: 0.492374986410141\n",
      "Step: 80  \tTest accuracy: 0.4928255081176758\n",
      "Step: 80  \tTest loss: 0.6951431632041931\n",
      "Step: 90  \tTraining loss: 0.695846676826477\n",
      "Step: 90  \tTraining accuracy: 0.49242648482322693\n",
      "Step: 90  \tTest accuracy: 0.49282407760620117\n",
      "Step: 90  \tTest loss: 0.6949459910392761\n",
      "Step: 100  \tTraining loss: 0.6955464482307434\n",
      "Step: 100  \tTraining accuracy: 0.49246710538864136\n",
      "Step: 100  \tTest accuracy: 0.49282291531562805\n",
      "Step: 100  \tTest loss: 0.6947588920593262\n",
      "Step: 110  \tTraining loss: 0.6952585577964783\n",
      "Step: 110  \tTraining accuracy: 0.49248015880584717\n",
      "Step: 110  \tTest accuracy: 0.49280303716659546\n",
      "Step: 110  \tTest loss: 0.6945809721946716\n",
      "Step: 120  \tTraining loss: 0.6949811577796936\n",
      "Step: 120  \tTraining accuracy: 0.49252718687057495\n",
      "Step: 120  \tTest accuracy: 0.49282118678092957\n",
      "Step: 120  \tTest loss: 0.6944096088409424\n",
      "Step: 130  \tTraining loss: 0.6947120428085327\n",
      "Step: 130  \tTraining accuracy: 0.49258333444595337\n",
      "Step: 130  \tTest accuracy: 0.4928525686264038\n",
      "Step: 130  \tTest loss: 0.6942429542541504\n",
      "Step: 140  \tTraining loss: 0.6944499611854553\n",
      "Step: 140  \tTraining accuracy: 0.4926157295703888\n",
      "Step: 140  \tTest accuracy: 0.4928497076034546\n",
      "Step: 140  \tTest loss: 0.6940804123878479\n",
      "Step: 150  \tTraining loss: 0.6941947340965271\n",
      "Step: 150  \tTraining accuracy: 0.4925718307495117\n",
      "Step: 150  \tTest accuracy: 0.4927777647972107\n",
      "Step: 150  \tTest loss: 0.6939213275909424\n",
      "Step: 160  \tTraining loss: 0.6939448118209839\n",
      "Step: 160  \tTraining accuracy: 0.4925537705421448\n",
      "Step: 160  \tTest accuracy: 0.4927408993244171\n",
      "Step: 160  \tTest loss: 0.6937643885612488\n",
      "Step: 170  \tTraining loss: 0.693699836730957\n",
      "Step: 170  \tTraining accuracy: 0.492683082818985\n",
      "Step: 170  \tTest accuracy: 0.49279412627220154\n",
      "Step: 170  \tTest loss: 0.6936089992523193\n",
      "Step: 180  \tTraining loss: 0.693458080291748\n",
      "Step: 180  \tTraining accuracy: 0.49277380108833313\n",
      "Step: 180  \tTest accuracy: 0.49280092120170593\n",
      "Step: 180  \tTest loss: 0.6934545636177063\n",
      "Step: 190  \tTraining loss: 0.6932177543640137\n",
      "Step: 190  \tTraining accuracy: 0.4930912256240845\n",
      "Step: 190  \tTest accuracy: 0.49314144253730774\n",
      "Step: 190  \tTest loss: 0.6933002471923828\n",
      "Step: 200  \tTraining loss: 0.6929787993431091\n",
      "Step: 200  \tTraining accuracy: 0.4934294819831848\n",
      "Step: 200  \tTest accuracy: 0.49351561069488525\n",
      "Step: 200  \tTest loss: 0.6931451559066772\n",
      "Step: 210  \tTraining loss: 0.6927394270896912\n",
      "Step: 210  \tTraining accuracy: 0.4938414692878723\n",
      "Step: 210  \tTest accuracy: 0.49396824836730957\n",
      "Step: 210  \tTest loss: 0.6929886937141418\n",
      "Step: 220  \tTraining loss: 0.6924992799758911\n",
      "Step: 220  \tTraining accuracy: 0.4944573640823364\n",
      "Step: 220  \tTest accuracy: 0.49461647868156433\n",
      "Step: 220  \tTest loss: 0.6928314566612244\n",
      "Step: 230  \tTraining loss: 0.6922581195831299\n",
      "Step: 230  \tTraining accuracy: 0.4953148066997528\n",
      "Step: 230  \tTest accuracy: 0.49568840861320496\n",
      "Step: 230  \tTest loss: 0.6926725506782532\n",
      "Step: 240  \tTraining loss: 0.6920153498649597\n",
      "Step: 240  \tTraining accuracy: 0.4965735673904419\n",
      "Step: 240  \tTest accuracy: 0.497052937746048\n",
      "Step: 240  \tTest loss: 0.6925113201141357\n",
      "Step: 250  \tTraining loss: 0.6917711496353149\n",
      "Step: 250  \tTraining accuracy: 0.49803146719932556\n",
      "Step: 250  \tTest accuracy: 0.4986875057220459\n",
      "Step: 250  \tTest loss: 0.6923465132713318\n",
      "Step: 260  \tTraining loss: 0.6915243268013\n",
      "Step: 260  \tTraining accuracy: 0.4998202621936798\n",
      "Step: 260  \tTest accuracy: 0.5005929470062256\n",
      "Step: 260  \tTest loss: 0.6921790838241577\n",
      "Step: 270  \tTraining loss: 0.6912746429443359\n",
      "Step: 270  \tTraining accuracy: 0.5017884969711304\n",
      "Step: 270  \tTest accuracy: 0.5025385618209839\n",
      "Step: 270  \tTest loss: 0.6920080780982971\n",
      "Step: 280  \tTraining loss: 0.6910207271575928\n",
      "Step: 280  \tTraining accuracy: 0.5037272572517395\n",
      "Step: 280  \tTest accuracy: 0.5045238137245178\n",
      "Step: 280  \tTest loss: 0.6918333768844604\n",
      "Step: 290  \tTraining loss: 0.6907629370689392\n",
      "Step: 290  \tTraining accuracy: 0.5056651830673218\n",
      "Step: 290  \tTest accuracy: 0.5065445303916931\n",
      "Step: 290  \tTest loss: 0.6916561722755432\n",
      "Step: 300  \tTraining loss: 0.6905001401901245\n",
      "Step: 300  \tTraining accuracy: 0.5076306462287903\n",
      "Step: 300  \tTest accuracy: 0.5085451602935791\n",
      "Step: 300  \tTest loss: 0.6914761066436768\n",
      "Step: 310  \tTraining loss: 0.6902317404747009\n",
      "Step: 310  \tTraining accuracy: 0.509586751461029\n",
      "Step: 310  \tTest accuracy: 0.5105006694793701\n",
      "Step: 310  \tTest loss: 0.6912912726402283\n",
      "Step: 320  \tTraining loss: 0.6899580359458923\n",
      "Step: 320  \tTraining accuracy: 0.5114418268203735\n",
      "Step: 320  \tTest accuracy: 0.5123144388198853\n",
      "Step: 320  \tTest loss: 0.6911019682884216\n",
      "Step: 330  \tTraining loss: 0.6896790862083435\n",
      "Step: 330  \tTraining accuracy: 0.5132692456245422\n",
      "Step: 330  \tTest accuracy: 0.5141130089759827\n",
      "Step: 330  \tTest loss: 0.6909082531929016\n",
      "Step: 340  \tTraining loss: 0.6893953680992126\n",
      "Step: 340  \tTraining accuracy: 0.5150279998779297\n",
      "Step: 340  \tTest accuracy: 0.5158271789550781\n",
      "Step: 340  \tTest loss: 0.6907090544700623\n",
      "Step: 350  \tTraining loss: 0.6891060471534729\n",
      "Step: 350  \tTraining accuracy: 0.5167179703712463\n",
      "Step: 350  \tTest accuracy: 0.5174940228462219\n",
      "Step: 350  \tTest loss: 0.6905047297477722\n",
      "Step: 360  \tTraining loss: 0.688809871673584\n",
      "Step: 360  \tTraining accuracy: 0.5183274745941162\n",
      "Step: 360  \tTest accuracy: 0.5190508961677551\n",
      "Step: 360  \tTest loss: 0.6902954578399658\n",
      "Step: 370  \tTraining loss: 0.6885059475898743\n",
      "Step: 370  \tTraining accuracy: 0.5198373198509216\n",
      "Step: 370  \tTest accuracy: 0.5205264687538147\n",
      "Step: 370  \tTest loss: 0.6900801658630371\n",
      "Step: 380  \tTraining loss: 0.6881916522979736\n",
      "Step: 380  \tTraining accuracy: 0.5213000178337097\n",
      "Step: 380  \tTest accuracy: 0.5219791531562805\n",
      "Step: 380  \tTest loss: 0.6898562908172607\n",
      "Step: 390  \tTraining loss: 0.6878640651702881\n",
      "Step: 390  \tTraining accuracy: 0.5227543115615845\n",
      "Step: 390  \tTest accuracy: 0.5234054327011108\n",
      "Step: 390  \tTest loss: 0.6896213889122009\n",
      "Step: 400  \tTraining loss: 0.6875230073928833\n",
      "Step: 400  \tTraining accuracy: 0.524153470993042\n",
      "Step: 400  \tTest accuracy: 0.524789035320282\n",
      "Step: 400  \tTest loss: 0.6893746852874756\n",
      "Step: 410  \tTraining loss: 0.6871659159660339\n",
      "Step: 410  \tTraining accuracy: 0.5255246758460999\n",
      "Step: 410  \tTest accuracy: 0.5261509418487549\n",
      "Step: 410  \tTest loss: 0.6891158819198608\n",
      "Step: 420  \tTraining loss: 0.6867892742156982\n",
      "Step: 420  \tTraining accuracy: 0.5268524289131165\n",
      "Step: 420  \tTest accuracy: 0.5274677872657776\n",
      "Step: 420  \tTest loss: 0.6888442635536194\n",
      "Step: 430  \tTraining loss: 0.6863913536071777\n",
      "Step: 430  \tTraining accuracy: 0.5281372666358948\n",
      "Step: 430  \tTest accuracy: 0.5287233591079712\n",
      "Step: 430  \tTest loss: 0.6885578036308289\n",
      "Step: 440  \tTraining loss: 0.6859685182571411\n",
      "Step: 440  \tTraining accuracy: 0.5293797850608826\n",
      "Step: 440  \tTest accuracy: 0.5299360752105713\n",
      "Step: 440  \tTest loss: 0.6882544755935669\n",
      "Step: 450  \tTraining loss: 0.6855130195617676\n",
      "Step: 450  \tTraining accuracy: 0.5305688381195068\n",
      "Step: 450  \tTest accuracy: 0.5311342477798462\n",
      "Step: 450  \tTest loss: 0.6879304051399231\n",
      "Step: 460  \tTraining loss: 0.6850142478942871\n",
      "Step: 460  \tTraining accuracy: 0.5317422151565552\n",
      "Step: 460  \tTest accuracy: 0.5322893857955933\n",
      "Step: 460  \tTest loss: 0.68758624792099\n",
      "Step: 470  \tTraining loss: 0.6844750046730042\n",
      "Step: 470  \tTraining accuracy: 0.5328629016876221\n",
      "Step: 470  \tTest accuracy: 0.5333954095840454\n",
      "Step: 470  \tTest loss: 0.6872150897979736\n",
      "Step: 480  \tTraining loss: 0.6838881373405457\n",
      "Step: 480  \tTraining accuracy: 0.5339429974555969\n",
      "Step: 480  \tTest accuracy: 0.5344704985618591\n",
      "Step: 480  \tTest loss: 0.6868124604225159\n",
      "Step: 490  \tTraining loss: 0.6832388043403625\n",
      "Step: 490  \tTraining accuracy: 0.5350064635276794\n",
      "Step: 490  \tTest accuracy: 0.5355123281478882\n",
      "Step: 490  \tTest loss: 0.6863679885864258\n",
      "Step: 500  \tTraining loss: 0.6825191974639893\n",
      "Step: 500  \tTraining accuracy: 0.5360332727432251\n",
      "Step: 500  \tTest accuracy: 0.5365041494369507\n",
      "Step: 500  \tTest loss: 0.6858826875686646\n",
      "Step: 510  \tTraining loss: 0.6817101836204529\n",
      "Step: 510  \tTraining accuracy: 0.5370193719863892\n",
      "Step: 510  \tTest accuracy: 0.5374795794487\n",
      "Step: 510  \tTest loss: 0.6853457093238831\n",
      "Step: 520  \tTraining loss: 0.680789589881897\n",
      "Step: 520  \tTraining accuracy: 0.5379571318626404\n",
      "Step: 520  \tTest accuracy: 0.5383914113044739\n",
      "Step: 520  \tTest loss: 0.6847503185272217\n",
      "Step: 530  \tTraining loss: 0.6797456741333008\n",
      "Step: 530  \tTraining accuracy: 0.5388392806053162\n",
      "Step: 530  \tTest accuracy: 0.5392492413520813\n",
      "Step: 530  \tTest loss: 0.6840830445289612\n",
      "Step: 540  \tTraining loss: 0.6785486936569214\n",
      "Step: 540  \tTraining accuracy: 0.5396865010261536\n",
      "Step: 540  \tTest accuracy: 0.5400771498680115\n",
      "Step: 540  \tTest loss: 0.6833353638648987\n",
      "Step: 550  \tTraining loss: 0.6771578788757324\n",
      "Step: 550  \tTraining accuracy: 0.5405160784721375\n",
      "Step: 550  \tTest accuracy: 0.5408920645713806\n",
      "Step: 550  \tTest loss: 0.6824913024902344\n",
      "Step: 560  \tTraining loss: 0.6755083203315735\n",
      "Step: 560  \tTraining accuracy: 0.5413194298744202\n",
      "Step: 560  \tTest accuracy: 0.5416666865348816\n",
      "Step: 560  \tTest loss: 0.6815285086631775\n",
      "Step: 570  \tTraining loss: 0.6734940409660339\n",
      "Step: 570  \tTraining accuracy: 0.5420851707458496\n",
      "Step: 570  \tTest accuracy: 0.5424250960350037\n",
      "Step: 570  \tTest loss: 0.6803904175758362\n",
      "Step: 580  \tTraining loss: 0.6708771586418152\n",
      "Step: 580  \tTraining accuracy: 0.5428152084350586\n",
      "Step: 580  \tTest accuracy: 0.5431501269340515\n",
      "Step: 580  \tTest loss: 0.6789952516555786\n",
      "Step: 590  \tTraining loss: 0.6669521331787109\n",
      "Step: 590  \tTraining accuracy: 0.5435310006141663\n",
      "Step: 590  \tTest accuracy: 0.5438541769981384\n",
      "Step: 590  \tTest loss: 0.6772206425666809\n",
      "Step: 600  \tTraining loss: 0.6619755625724792\n",
      "Step: 600  \tTraining accuracy: 0.5443172454833984\n",
      "Step: 600  \tTest accuracy: 0.5446961522102356\n",
      "Step: 600  \tTest loss: 0.674838125705719\n",
      "Step: 610  \tTraining loss: 0.6608974933624268\n",
      "Step: 610  \tTraining accuracy: 0.5451670289039612\n",
      "Step: 610  \tTest accuracy: 0.5455344915390015\n",
      "Step: 610  \tTest loss: 0.6736897230148315\n",
      "Step: 620  \tTraining loss: 0.6620824933052063\n",
      "Step: 620  \tTraining accuracy: 0.5459620356559753\n",
      "Step: 620  \tTest accuracy: 0.5462920069694519\n",
      "Step: 620  \tTest loss: 0.6737850904464722\n",
      "Step: 630  \tTraining loss: 0.6606090068817139\n",
      "Step: 630  \tTraining accuracy: 0.5466949939727783\n",
      "Step: 630  \tTest accuracy: 0.5470337271690369\n",
      "Step: 630  \tTest loss: 0.6731618642807007\n",
      "Step: 640  \tTraining loss: 0.6598452925682068\n",
      "Step: 640  \tTraining accuracy: 0.5474507808685303\n",
      "Step: 640  \tTest accuracy: 0.5477734208106995\n",
      "Step: 640  \tTest loss: 0.6727553009986877\n",
      "Step: 650  \tTraining loss: 0.6595154404640198\n",
      "Step: 650  \tTraining accuracy: 0.5481799244880676\n",
      "Step: 650  \tTest accuracy: 0.5484920144081116\n",
      "Step: 650  \tTest loss: 0.6724386811256409\n",
      "Step: 660  \tTraining loss: 0.6591957807540894\n",
      "Step: 660  \tTraining accuracy: 0.5488836169242859\n",
      "Step: 660  \tTest accuracy: 0.5491777062416077\n",
      "Step: 660  \tTest loss: 0.672133207321167\n",
      "Step: 670  \tTraining loss: 0.6588826775550842\n",
      "Step: 670  \tTraining accuracy: 0.5495473146438599\n",
      "Step: 670  \tTest accuracy: 0.5498290061950684\n",
      "Step: 670  \tTest loss: 0.6718320250511169\n",
      "Step: 680  \tTraining loss: 0.6585764288902283\n",
      "Step: 680  \tTraining accuracy: 0.550191342830658\n",
      "Step: 680  \tTest accuracy: 0.5504641532897949\n",
      "Step: 680  \tTest loss: 0.6715391278266907\n",
      "Step: 690  \tTraining loss: 0.6582721471786499\n",
      "Step: 690  \tTraining accuracy: 0.5508165955543518\n",
      "Step: 690  \tTest accuracy: 0.5510839223861694\n",
      "Step: 690  \tTest loss: 0.6712532639503479\n",
      "Step: 700  \tTraining loss: 0.6579698920249939\n",
      "Step: 700  \tTraining accuracy: 0.5514208674430847\n",
      "Step: 700  \tTest accuracy: 0.5516785979270935\n",
      "Step: 700  \tTest loss: 0.6709727048873901\n",
      "Step: 710  \tTraining loss: 0.6576704978942871\n",
      "Step: 710  \tTraining accuracy: 0.5520138740539551\n",
      "Step: 710  \tTest accuracy: 0.5522667169570923\n",
      "Step: 710  \tTest loss: 0.6706957817077637\n",
      "Step: 720  \tTraining loss: 0.6573709845542908\n",
      "Step: 720  \tTraining accuracy: 0.5525961518287659\n",
      "Step: 720  \tTest accuracy: 0.5528443455696106\n",
      "Step: 720  \tTest loss: 0.6704222559928894\n",
      "Step: 730  \tTraining loss: 0.6570756435394287\n",
      "Step: 730  \tTraining accuracy: 0.5531637668609619\n",
      "Step: 730  \tTest accuracy: 0.5534075498580933\n",
      "Step: 730  \tTest loss: 0.6701556444168091\n",
      "Step: 740  \tTraining loss: 0.656783938407898\n",
      "Step: 740  \tTraining accuracy: 0.553724467754364\n",
      "Step: 740  \tTest accuracy: 0.5539611577987671\n",
      "Step: 740  \tTest loss: 0.6698969602584839\n",
      "Step: 750  \tTraining loss: 0.6564962863922119\n",
      "Step: 750  \tTraining accuracy: 0.5542729496955872\n",
      "Step: 750  \tTest accuracy: 0.5545027852058411\n",
      "Step: 750  \tTest loss: 0.6696485280990601\n",
      "Step: 760  \tTraining loss: 0.6562177538871765\n",
      "Step: 760  \tTraining accuracy: 0.5548027157783508\n",
      "Step: 760  \tTest accuracy: 0.5550205707550049\n",
      "Step: 760  \tTest loss: 0.6694140434265137\n",
      "Step: 770  \tTraining loss: 0.655951201915741\n",
      "Step: 770  \tTraining accuracy: 0.5553131699562073\n",
      "Step: 770  \tTest accuracy: 0.555530309677124\n",
      "Step: 770  \tTest loss: 0.6691963076591492\n",
      "Step: 780  \tTraining loss: 0.655695915222168\n",
      "Step: 780  \tTraining accuracy: 0.5558225512504578\n",
      "Step: 780  \tTest accuracy: 0.5560336709022522\n",
      "Step: 780  \tTest loss: 0.668999969959259\n",
      "Step: 790  \tTraining loss: 0.6554515361785889\n",
      "Step: 790  \tTraining accuracy: 0.556313693523407\n",
      "Step: 790  \tTest accuracy: 0.5565124154090881\n",
      "Step: 790  \tTest loss: 0.6688287258148193\n",
      "Step: 800  \tTraining loss: 0.6552191376686096\n",
      "Step: 800  \tTraining accuracy: 0.556793749332428\n",
      "Step: 800  \tTest accuracy: 0.5569856762886047\n",
      "Step: 800  \tTest loss: 0.6686794757843018\n",
      "Step: 810  \tTraining loss: 0.6549999713897705\n",
      "Step: 810  \tTraining accuracy: 0.5572670698165894\n",
      "Step: 810  \tTest accuracy: 0.5574665665626526\n",
      "Step: 810  \tTest loss: 0.6685487627983093\n",
      "Step: 820  \tTraining loss: 0.6547943353652954\n",
      "Step: 820  \tTraining accuracy: 0.5577377080917358\n",
      "Step: 820  \tTest accuracy: 0.5579319000244141\n",
      "Step: 820  \tTest loss: 0.668438732624054\n",
      "Step: 830  \tTraining loss: 0.6546022891998291\n",
      "Step: 830  \tTraining accuracy: 0.5581994652748108\n",
      "Step: 830  \tTest accuracy: 0.5583986043930054\n",
      "Step: 830  \tTest loss: 0.668350100517273\n",
      "Step: 840  \tTraining loss: 0.654420793056488\n",
      "Step: 840  \tTraining accuracy: 0.5586514472961426\n",
      "Step: 840  \tTest accuracy: 0.5588516592979431\n",
      "Step: 840  \tTest loss: 0.6682679057121277\n",
      "Step: 850  \tTraining loss: 0.6542496085166931\n",
      "Step: 850  \tTraining accuracy: 0.5591074824333191\n",
      "Step: 850  \tTest accuracy: 0.5592978000640869\n",
      "Step: 850  \tTest loss: 0.6681920886039734\n",
      "Step: 860  \tTraining loss: 0.6540838479995728\n",
      "Step: 860  \tTraining accuracy: 0.5595516562461853\n",
      "Step: 860  \tTest accuracy: 0.5597395896911621\n",
      "Step: 860  \tTest loss: 0.6681228876113892\n",
      "Step: 870  \tTraining loss: 0.6539273858070374\n",
      "Step: 870  \tTraining accuracy: 0.5599879622459412\n",
      "Step: 870  \tTest accuracy: 0.5601736307144165\n",
      "Step: 870  \tTest loss: 0.66805499792099\n",
      "Step: 880  \tTraining loss: 0.6537750363349915\n",
      "Step: 880  \tTraining accuracy: 0.560413122177124\n",
      "Step: 880  \tTest accuracy: 0.5605953931808472\n",
      "Step: 880  \tTest loss: 0.6679852604866028\n",
      "Step: 890  \tTraining loss: 0.6536254286766052\n",
      "Step: 890  \tTraining accuracy: 0.5608274340629578\n",
      "Step: 890  \tTest accuracy: 0.5610077381134033\n",
      "Step: 890  \tTest loss: 0.667915940284729\n",
      "Step: 900  \tTraining loss: 0.6534779667854309\n",
      "Step: 900  \tTraining accuracy: 0.5612348914146423\n",
      "Step: 900  \tTest accuracy: 0.5614120364189148\n",
      "Step: 900  \tTest loss: 0.6678411960601807\n",
      "Step: 910  \tTraining loss: 0.6533286571502686\n",
      "Step: 910  \tTraining accuracy: 0.5616424679756165\n",
      "Step: 910  \tTest accuracy: 0.5618120431900024\n",
      "Step: 910  \tTest loss: 0.6677468419075012\n",
      "Step: 920  \tTraining loss: 0.6531776785850525\n",
      "Step: 920  \tTraining accuracy: 0.5620355010032654\n",
      "Step: 920  \tTest accuracy: 0.5622056126594543\n",
      "Step: 920  \tTest loss: 0.6676424741744995\n",
      "Step: 930  \tTraining loss: 0.653030276298523\n",
      "Step: 930  \tTraining accuracy: 0.5624222755432129\n",
      "Step: 930  \tTest accuracy: 0.5625907182693481\n",
      "Step: 930  \tTest loss: 0.6675339937210083\n",
      "Step: 940  \tTraining loss: 0.6528849601745605\n",
      "Step: 940  \tTraining accuracy: 0.5627996921539307\n",
      "Step: 940  \tTest accuracy: 0.5629621148109436\n",
      "Step: 940  \tTest loss: 0.667432427406311\n",
      "Step: 950  \tTraining loss: 0.652741014957428\n",
      "Step: 950  \tTraining accuracy: 0.5631679892539978\n",
      "Step: 950  \tTest accuracy: 0.5633267760276794\n",
      "Step: 950  \tTest loss: 0.6673349738121033\n",
      "Step: 960  \tTraining loss: 0.6525960564613342\n",
      "Step: 960  \tTraining accuracy: 0.5635263919830322\n",
      "Step: 960  \tTest accuracy: 0.56368488073349\n",
      "Step: 960  \tTest loss: 0.6672284007072449\n",
      "Step: 970  \tTraining loss: 0.6524500250816345\n",
      "Step: 970  \tTraining accuracy: 0.5638816952705383\n",
      "Step: 970  \tTest accuracy: 0.5640367269515991\n",
      "Step: 970  \tTest loss: 0.6671301126480103\n",
      "Step: 980  \tTraining loss: 0.6523008942604065\n",
      "Step: 980  \tTraining accuracy: 0.5642328858375549\n",
      "Step: 980  \tTest accuracy: 0.5643866658210754\n",
      "Step: 980  \tTest loss: 0.66701740026474\n",
      "Step: 990  \tTraining loss: 0.6521478295326233\n",
      "Step: 990  \tTraining accuracy: 0.5645801424980164\n",
      "Step: 990  \tTest accuracy: 0.5647306442260742\n",
      "Step: 990  \tTest loss: 0.6669050455093384\n",
      "Step: 1000  \tTraining loss: 0.651989758014679\n",
      "Step: 1000  \tTraining accuracy: 0.5649204254150391\n",
      "Step: 1000  \tTest accuracy: 0.565067708492279\n",
      "Step: 1000  \tTest loss: 0.6667835712432861\n",
      "Step: 1010  \tTraining loss: 0.6518272161483765\n",
      "Step: 1010  \tTraining accuracy: 0.5652559995651245\n",
      "Step: 1010  \tTest accuracy: 0.5654042959213257\n",
      "Step: 1010  \tTest loss: 0.6666615009307861\n",
      "Step: 1020  \tTraining loss: 0.6516568660736084\n",
      "Step: 1020  \tTraining accuracy: 0.5655880570411682\n",
      "Step: 1020  \tTest accuracy: 0.5657362937927246\n",
      "Step: 1020  \tTest loss: 0.666533350944519\n",
      "Step: 1030  \tTraining loss: 0.6514780521392822\n",
      "Step: 1030  \tTraining accuracy: 0.5659166574478149\n",
      "Step: 1030  \tTest accuracy: 0.5660629272460938\n",
      "Step: 1030  \tTest loss: 0.6663955450057983\n",
      "Step: 1040  \tTraining loss: 0.6512899398803711\n",
      "Step: 1040  \tTraining accuracy: 0.5662409663200378\n",
      "Step: 1040  \tTest accuracy: 0.5663852095603943\n",
      "Step: 1040  \tTest loss: 0.6662471890449524\n",
      "Step: 1050  \tTraining loss: 0.6510948538780212\n",
      "Step: 1050  \tTraining accuracy: 0.5665689706802368\n",
      "Step: 1050  \tTest accuracy: 0.5667102932929993\n",
      "Step: 1050  \tTest loss: 0.6660937666893005\n",
      "Step: 1060  \tTraining loss: 0.6508916616439819\n",
      "Step: 1060  \tTraining accuracy: 0.5668907761573792\n",
      "Step: 1060  \tTest accuracy: 0.5670302510261536\n",
      "Step: 1060  \tTest loss: 0.6659466028213501\n",
      "Step: 1070  \tTraining loss: 0.6506748199462891\n",
      "Step: 1070  \tTraining accuracy: 0.5672045946121216\n",
      "Step: 1070  \tTest accuracy: 0.5673432350158691\n",
      "Step: 1070  \tTest loss: 0.6658035516738892\n",
      "Step: 1080  \tTraining loss: 0.650438666343689\n",
      "Step: 1080  \tTraining accuracy: 0.5675135850906372\n",
      "Step: 1080  \tTest accuracy: 0.5676514506340027\n",
      "Step: 1080  \tTest loss: 0.6656970381736755\n",
      "Step: 1090  \tTraining loss: 0.6501858234405518\n",
      "Step: 1090  \tTraining accuracy: 0.5678178071975708\n",
      "Step: 1090  \tTest accuracy: 0.5679510831832886\n",
      "Step: 1090  \tTest loss: 0.6656078100204468\n",
      "Step: 1100  \tTraining loss: 0.6499656438827515\n",
      "Step: 1100  \tTraining accuracy: 0.5681145191192627\n",
      "Step: 1100  \tTest accuracy: 0.5682480931282043\n",
      "Step: 1100  \tTest loss: 0.6653481125831604\n",
      "Step: 1110  \tTraining loss: 0.6497491598129272\n",
      "Step: 1110  \tTraining accuracy: 0.568408727645874\n",
      "Step: 1110  \tTest accuracy: 0.5685388445854187\n",
      "Step: 1110  \tTest loss: 0.6650676727294922\n",
      "Step: 1120  \tTraining loss: 0.6495279669761658\n",
      "Step: 1120  \tTraining accuracy: 0.5687088966369629\n",
      "Step: 1120  \tTest accuracy: 0.5688420534133911\n",
      "Step: 1120  \tTest loss: 0.6649555563926697\n",
      "Step: 1130  \tTraining loss: 0.6493073105812073\n",
      "Step: 1130  \tTraining accuracy: 0.569007396697998\n",
      "Step: 1130  \tTest accuracy: 0.5691381096839905\n",
      "Step: 1130  \tTest loss: 0.6647515892982483\n",
      "Step: 1140  \tTraining loss: 0.6490858197212219\n",
      "Step: 1140  \tTraining accuracy: 0.5693061947822571\n",
      "Step: 1140  \tTest accuracy: 0.5694334506988525\n",
      "Step: 1140  \tTest loss: 0.6646922826766968\n",
      "Step: 1150  \tTraining loss: 0.6488608717918396\n",
      "Step: 1150  \tTraining accuracy: 0.5696006417274475\n",
      "Step: 1150  \tTest accuracy: 0.5697282552719116\n",
      "Step: 1150  \tTest loss: 0.664608895778656\n",
      "Step: 1160  \tTraining loss: 0.6486303806304932\n",
      "Step: 1160  \tTraining accuracy: 0.5698962807655334\n",
      "Step: 1160  \tTest accuracy: 0.5700206756591797\n",
      "Step: 1160  \tTest loss: 0.6646822094917297\n",
      "Step: 1170  \tTraining loss: 0.6483958959579468\n",
      "Step: 1170  \tTraining accuracy: 0.5701922178268433\n",
      "Step: 1170  \tTest accuracy: 0.5703151822090149\n",
      "Step: 1170  \tTest loss: 0.6648086309432983\n",
      "Step: 1180  \tTraining loss: 70045200.0\n",
      "Step: 1180  \tTraining accuracy: 0.5704521536827087\n",
      "Step: 1180  \tTest accuracy: 0.5705490708351135\n",
      "Step: 1180  \tTest loss: 408.85821533203125\n",
      "Step: 1190  \tTraining loss: 30963539968.0\n",
      "Step: 1190  \tTraining accuracy: 0.570569634437561\n",
      "Step: 1190  \tTest accuracy: 0.5704910755157471\n",
      "Step: 1190  \tTest loss: 5000560128.0\n",
      "Step: 1200  \tTraining loss: nan\n",
      "Step: 1200  \tTraining accuracy: 0.5702545046806335\n",
      "Step: 1200  \tTest accuracy: 0.5699635148048401\n",
      "Step: 1200  \tTest loss: nan\n",
      "Step: 1210  \tTraining loss: nan\n",
      "Step: 1210  \tTraining accuracy: 0.5697311758995056\n",
      "Step: 1210  \tTest accuracy: 0.5694447159767151\n",
      "Step: 1210  \tTest loss: nan\n",
      "Step: 1220  \tTraining loss: nan\n",
      "Step: 1220  \tTraining accuracy: 0.5692163705825806\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-72d792260cfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step: {}  \\tTest accuracy: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \"\"\"\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5179\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5180\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5181\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, \"./checkpts/RNN_LST_model_subject_num_11_final.ckpt\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    tf.local_variables_initializer().run()\n",
    "\n",
    "    # Train the model\n",
    "    for steps in range(epochs):\n",
    "        mini_batch = zip(range(0, length, batch_size),\n",
    "                   range(batch_size, length+1, batch_size))\n",
    "\n",
    "        # train data in mini-batches\n",
    "        for (start, end) in mini_batch:\n",
    "            # print(train_y[start:end,:])\n",
    "            sess.run(training_op, feed_dict = {X: train_X[start:end,:,:],\n",
    "                                               y: train_y[start:end,:,:]})\n",
    "\n",
    "        # print training performance \n",
    "        if (steps+1) % display == 0:\n",
    "            # evaluate loss function on training set\n",
    "            loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "            print('Step: {}  \\tTraining loss: {}'.format((steps+1), loss_fn))\n",
    "            \n",
    "            acc_train = acc_val.eval(feed_dict={X: train_X, y: train_y})\n",
    "            print('Step: {}  \\tTraining accuracy: {}'.format((steps+1), acc_train))\n",
    "            \n",
    "            \n",
    "            acc_test = acc_val.eval(feed_dict={X: test_X, y: test_y})\n",
    "            print('Step: {}  \\tTest accuracy: {}'.format((steps+1), acc_test))\n",
    "            \n",
    "            loss_test = loss.eval(feed_dict={X: test_X, y: test_y})\n",
    "            print('Step: {}  \\tTest loss: {}'.format((steps+1), loss_test))\n",
    "            \n",
    "            \n",
    "#             prec_test = prec_val.eval(feed_dict={X: train_X, y: train_y})\n",
    "#             print('Step: {}  \\tTraining Precision: {}'.format((steps+1), prec_test))\n",
    "\n",
    "            if loss_fn < best_loss_val:\n",
    "                    best_loss_val = loss_fn\n",
    "                    checks_since_last_progress = 0\n",
    "            else:\n",
    "                    checks_since_last_progress += 1\n",
    "        \n",
    "        \n",
    "        ## EARLY STOPPING\n",
    "        if checks_since_last_progress > max_checks_without_progress:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "            \n",
    "        if (steps+1) % save_step ==0:\n",
    "            save_path = saver.save(sess, \"./checkpts/RNN_LST_model_all_subjects_all_features.ckpt\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    # evaluate model accuracy\n",
    "    acc, prec, recall, f1,auc = sess.run([accuracy, precision, recall, f1,auc],\n",
    "                                     feed_dict = {X: train_X, y: train_y})\n",
    "    probchoice = probability.eval(feed_dict = {X: train_X, y: train_y})\n",
    "    prob_test = probability.eval(feed_dict = {X: test_X, y: test_y})\n",
    "\n",
    "    print('\\nEvaluation  on training set')\n",
    "    print('Accuracy:', acc[1])\n",
    "    print('Precision:', prec[1])\n",
    "    print('Recall:', recall[1])\n",
    "    print('F1 score:', f1)\n",
    "    print('AUC:', auc[1])\n",
    "    \n",
    "#     print(\"probability\",probchoice)\n",
    "    \n",
    "    save_path = saver.save(sess, \"./checkpts/RNN_LST_model_all_subjects_all_features.ckpt\")\n",
    "\n",
    "    \n",
    "\n",
    "metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,auc[1],loss_fn,acc_test,loss_test,neurons,learning_rate,epochs]).reshape(-1,11),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\",\"loss\",\"accuracy_test\",\"loss_test\",\"neurons\",\"learning_rate\",\"n_epochs\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(probchoice[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300, 2)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probchoice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-198.71244114637375\n"
     ]
    }
   ],
   "source": [
    "## for a specific subject\n",
    "ll = np.dot(train_y[0,:,0],np.log(probchoice[0,:,0]+0.0001)) + np.dot(train_y[0,:,1],np.log(probchoice[0,:,1]+0.0001))\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04439515531728855"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - ll/(np.log(0.5)*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300, 2)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probchoice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglike_train=[]; loglike_test =[];\n",
    "for subj_num in range(11,27):\n",
    "    loglike_train.append(np.dot(train_y[subj_num-11,:,0],np.log(probchoice[subj_num-11,:,0]+0.0001)) + np.dot(train_y[subj_num-11,:,1],np.log(probchoice[subj_num-11,:,1]+0.0001)) )\n",
    "    loglike_test.append(np.dot(test_y[subj_num-11,:,0],np.log(prob_test[subj_num-11,:,0]+0.0001)) + np.dot(test_y[subj_num-11,:,1],np.log(prob_test[subj_num-11,:,1]+0.0001)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-207.22490936517715,\n",
       " -204.16755259037018,\n",
       " -208.8415709733963,\n",
       " -211.44044595956802,\n",
       " -206.97302973270416,\n",
       " -205.5558995604515,\n",
       " -202.69466334581375,\n",
       " -205.08332860469818,\n",
       " -204.75412076711655,\n",
       " -203.12668985128403,\n",
       " -199.87487679719925,\n",
       " -201.63625067472458,\n",
       " -204.39146316051483,\n",
       " -209.07946574687958,\n",
       " -197.55535835027695,\n",
       " -200.58777582645416]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglike_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-206.7261511683464,\n",
       " -201.95139640569687,\n",
       " -206.31595557928085,\n",
       " -209.46393805742264,\n",
       " -206.60465663671494,\n",
       " -205.7004160284996,\n",
       " -205.74884968996048,\n",
       " -211.57007431983948,\n",
       " -203.55105710029602,\n",
       " -203.57157564163208,\n",
       " -201.21434617042542,\n",
       " -201.71536701917648,\n",
       " -206.82473623752594,\n",
       " -203.70757001638412,\n",
       " -199.20531463623047,\n",
       " -201.50381457805634]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loglike_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglike_df = pd.DataFrame(np.array([loglike_train,loglike_test]).T,columns = ['loglike_train','loglike_test'])\n",
    "loglike_df.to_csv(file_path+\"loglikelihoods_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.512468218803406"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll- loglike_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(probchoice.reshape(-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50145316, 0.4985469 ],\n",
       "       [0.50205415, 0.49794582],\n",
       "       [0.48375067, 0.5162493 ],\n",
       "       [0.4785777 , 0.5214223 ],\n",
       "       [0.46550062, 0.53449935],\n",
       "       [0.4628367 , 0.5371633 ],\n",
       "       [0.47923592, 0.5207641 ],\n",
       "       [0.47389728, 0.5261027 ],\n",
       "       [0.46767515, 0.53232485],\n",
       "       [0.4868466 , 0.51315343],\n",
       "       [0.4990607 , 0.50093937],\n",
       "       [0.4822271 , 0.5177729 ],\n",
       "       [0.47689328, 0.52310675],\n",
       "       [0.49527875, 0.5047213 ],\n",
       "       [0.50808525, 0.49191478],\n",
       "       [0.48686492, 0.5131351 ],\n",
       "       [0.47626802, 0.52373195],\n",
       "       [0.46770725, 0.53229284],\n",
       "       [0.47002116, 0.5299789 ],\n",
       "       [0.4569358 , 0.5430642 ],\n",
       "       [0.46574152, 0.5342585 ],\n",
       "       [0.4673424 , 0.53265756],\n",
       "       [0.46310827, 0.53689176],\n",
       "       [0.45976675, 0.54023325],\n",
       "       [0.4604648 , 0.5395352 ],\n",
       "       [0.49489945, 0.50510055],\n",
       "       [0.47340846, 0.52659154],\n",
       "       [0.47672725, 0.52327275],\n",
       "       [0.48089767, 0.51910233],\n",
       "       [0.47123   , 0.52876997],\n",
       "       [0.48177573, 0.51822424],\n",
       "       [0.48912784, 0.5108722 ],\n",
       "       [0.48847008, 0.5115299 ],\n",
       "       [0.50024056, 0.49975944],\n",
       "       [0.52003735, 0.47996268],\n",
       "       [0.5387519 , 0.4612481 ],\n",
       "       [0.5150454 , 0.48495463],\n",
       "       [0.5203664 , 0.4796336 ],\n",
       "       [0.5204016 , 0.47959837],\n",
       "       [0.52213633, 0.47786364],\n",
       "       [0.5335364 , 0.46646363],\n",
       "       [0.522691  , 0.47730908],\n",
       "       [0.49223244, 0.5077676 ],\n",
       "       [0.5040776 , 0.4959224 ],\n",
       "       [0.49007696, 0.5099231 ],\n",
       "       [0.51976526, 0.4802348 ],\n",
       "       [0.5385585 , 0.46144152],\n",
       "       [0.54772   , 0.45228   ],\n",
       "       [0.5560783 , 0.4439217 ],\n",
       "       [0.5396706 , 0.4603294 ],\n",
       "       [0.5113705 , 0.48862958],\n",
       "       [0.5121391 , 0.48786095],\n",
       "       [0.5281345 , 0.47186542],\n",
       "       [0.517617  , 0.482383  ],\n",
       "       [0.5398634 , 0.46013656],\n",
       "       [0.5418255 , 0.45817456],\n",
       "       [0.5331857 , 0.46681428],\n",
       "       [0.5461951 , 0.45380488],\n",
       "       [0.55604196, 0.44395807],\n",
       "       [0.54321814, 0.45678183],\n",
       "       [0.53071964, 0.4692804 ],\n",
       "       [0.5058629 , 0.49413717],\n",
       "       [0.49565652, 0.5043435 ],\n",
       "       [0.5014291 , 0.49857092],\n",
       "       [0.48983425, 0.51016575],\n",
       "       [0.493307  , 0.50669295],\n",
       "       [0.49030632, 0.5096937 ],\n",
       "       [0.5032993 , 0.49670076],\n",
       "       [0.5087042 , 0.4912958 ],\n",
       "       [0.51171255, 0.4882875 ],\n",
       "       [0.5284089 , 0.47159114],\n",
       "       [0.53616196, 0.46383804],\n",
       "       [0.5117798 , 0.48822019],\n",
       "       [0.5059831 , 0.49401683],\n",
       "       [0.49300268, 0.5069973 ],\n",
       "       [0.500657  , 0.49934292],\n",
       "       [0.4949219 , 0.5050782 ],\n",
       "       [0.49842152, 0.5015785 ],\n",
       "       [0.50891733, 0.4910826 ],\n",
       "       [0.5009948 , 0.49900523],\n",
       "       [0.525757  , 0.47424296],\n",
       "       [0.5171976 , 0.48280236],\n",
       "       [0.5237642 , 0.47623578],\n",
       "       [0.49580896, 0.504191  ],\n",
       "       [0.5021003 , 0.49789968],\n",
       "       [0.5083802 , 0.49161988],\n",
       "       [0.51064223, 0.48935777],\n",
       "       [0.5094718 , 0.4905282 ],\n",
       "       [0.5321302 , 0.46786982],\n",
       "       [0.51352656, 0.48647347],\n",
       "       [0.5402519 , 0.4597481 ],\n",
       "       [0.54729843, 0.45270154],\n",
       "       [0.564278  , 0.435722  ],\n",
       "       [0.54148424, 0.45851573],\n",
       "       [0.55300415, 0.44699585],\n",
       "       [0.5460384 , 0.45396152],\n",
       "       [0.5344855 , 0.46551448],\n",
       "       [0.5077785 , 0.49222153],\n",
       "       [0.52839607, 0.4716039 ],\n",
       "       [0.52168036, 0.47831964],\n",
       "       [0.5255549 , 0.47444513],\n",
       "       [0.5414334 , 0.45856664],\n",
       "       [0.5500784 , 0.4499216 ],\n",
       "       [0.5324706 , 0.46752945],\n",
       "       [0.5301423 , 0.4698577 ],\n",
       "       [0.5485079 , 0.45149204],\n",
       "       [0.5509855 , 0.44901446],\n",
       "       [0.5283959 , 0.47160405],\n",
       "       [0.5414142 , 0.45858583],\n",
       "       [0.53203845, 0.46796152],\n",
       "       [0.5167045 , 0.48329544],\n",
       "       [0.5047612 , 0.49523878],\n",
       "       [0.51299524, 0.4870048 ],\n",
       "       [0.536679  , 0.46332094],\n",
       "       [0.54639715, 0.4536029 ],\n",
       "       [0.5388135 , 0.46118656],\n",
       "       [0.5492806 , 0.45071945],\n",
       "       [0.5257813 , 0.4742187 ],\n",
       "       [0.5142491 , 0.4857509 ],\n",
       "       [0.51323825, 0.48676178],\n",
       "       [0.5029271 , 0.49707285],\n",
       "       [0.5134858 , 0.48651415],\n",
       "       [0.5159173 , 0.48408273],\n",
       "       [0.5362895 , 0.46371046],\n",
       "       [0.5102055 , 0.4897945 ],\n",
       "       [0.51482123, 0.48517874],\n",
       "       [0.5181533 , 0.48184663],\n",
       "       [0.54181135, 0.45818865],\n",
       "       [0.5514605 , 0.44853947],\n",
       "       [0.55292046, 0.4470796 ],\n",
       "       [0.543618  , 0.456382  ],\n",
       "       [0.53655493, 0.4634451 ],\n",
       "       [0.53452915, 0.46547076],\n",
       "       [0.51675314, 0.48324686],\n",
       "       [0.50772345, 0.49227652],\n",
       "       [0.50285757, 0.49714237],\n",
       "       [0.48355561, 0.51644444],\n",
       "       [0.50026494, 0.49973506],\n",
       "       [0.52806187, 0.47193816],\n",
       "       [0.52519566, 0.4748043 ],\n",
       "       [0.52813053, 0.47186944],\n",
       "       [0.52474684, 0.47525313],\n",
       "       [0.52563363, 0.47436637],\n",
       "       [0.50735635, 0.49264365],\n",
       "       [0.52693534, 0.47306466],\n",
       "       [0.5359288 , 0.46407118],\n",
       "       [0.54558855, 0.45441142],\n",
       "       [0.5538488 , 0.44615117],\n",
       "       [0.5238844 , 0.4761156 ],\n",
       "       [0.53613025, 0.46386972],\n",
       "       [0.54265475, 0.45734522],\n",
       "       [0.5356143 , 0.4643857 ],\n",
       "       [0.52656174, 0.47343826],\n",
       "       [0.5439413 , 0.45605868],\n",
       "       [0.55556375, 0.4444363 ],\n",
       "       [0.5433921 , 0.45660788],\n",
       "       [0.5367859 , 0.46321413],\n",
       "       [0.52884144, 0.47115853],\n",
       "       [0.52581656, 0.4741834 ],\n",
       "       [0.5060708 , 0.4939292 ],\n",
       "       [0.533053  , 0.466947  ],\n",
       "       [0.54426014, 0.45573977],\n",
       "       [0.55254805, 0.44745192],\n",
       "       [0.54939187, 0.45060813],\n",
       "       [0.5337308 , 0.4662692 ],\n",
       "       [0.5219385 , 0.47806144],\n",
       "       [0.502144  , 0.4978561 ],\n",
       "       [0.48910758, 0.51089245],\n",
       "       [0.4973956 , 0.50260437],\n",
       "       [0.5195872 , 0.48041278],\n",
       "       [0.51812017, 0.48187983],\n",
       "       [0.5301374 , 0.46986255],\n",
       "       [0.5175216 , 0.48247838],\n",
       "       [0.5097512 , 0.49024883],\n",
       "       [0.50907403, 0.49092606],\n",
       "       [0.50504833, 0.49495175],\n",
       "       [0.49667737, 0.5033226 ],\n",
       "       [0.5169419 , 0.4830581 ],\n",
       "       [0.52993345, 0.47006652],\n",
       "       [0.5120819 , 0.48791808],\n",
       "       [0.5108409 , 0.48915914],\n",
       "       [0.5314935 , 0.46850654],\n",
       "       [0.51685464, 0.4831453 ],\n",
       "       [0.512622  , 0.48737806],\n",
       "       [0.51557636, 0.48442358],\n",
       "       [0.5174774 , 0.48252264],\n",
       "       [0.5119388 , 0.48806116],\n",
       "       [0.5002746 , 0.49972546],\n",
       "       [0.486461  , 0.5135389 ],\n",
       "       [0.4998971 , 0.5001029 ],\n",
       "       [0.52033347, 0.47966647],\n",
       "       [0.5213823 , 0.4786177 ],\n",
       "       [0.5420129 , 0.45798713],\n",
       "       [0.5232893 , 0.47671074],\n",
       "       [0.52433276, 0.4756672 ],\n",
       "       [0.5251137 , 0.4748864 ],\n",
       "       [0.53451747, 0.46548247],\n",
       "       [0.5268536 , 0.4731464 ],\n",
       "       [0.5437973 , 0.4562027 ],\n",
       "       [0.4996572 , 0.5003427 ],\n",
       "       [0.48417658, 0.5158235 ],\n",
       "       [0.5006479 , 0.4993521 ],\n",
       "       [0.52600825, 0.47399175],\n",
       "       [0.5384981 , 0.46150187],\n",
       "       [0.51285404, 0.487146  ],\n",
       "       [0.5119765 , 0.4880235 ],\n",
       "       [0.5090416 , 0.49095842],\n",
       "       [0.5124281 , 0.48757195],\n",
       "       [0.53193164, 0.4680683 ],\n",
       "       [0.5478084 , 0.45219162],\n",
       "       [0.5400847 , 0.45991522],\n",
       "       [0.53479284, 0.46520713],\n",
       "       [0.53174585, 0.4682541 ],\n",
       "       [0.522955  , 0.47704497],\n",
       "       [0.5025598 , 0.49744022],\n",
       "       [0.51109666, 0.48890334],\n",
       "       [0.51143914, 0.48856091],\n",
       "       [0.5365666 , 0.46343344],\n",
       "       [0.53202677, 0.46797314],\n",
       "       [0.5468701 , 0.45312992],\n",
       "       [0.5266207 , 0.47337928],\n",
       "       [0.53844494, 0.46155512],\n",
       "       [0.5506782 , 0.44932184],\n",
       "       [0.5499615 , 0.45003852],\n",
       "       [0.5492781 , 0.45072192],\n",
       "       [0.55329937, 0.44670066],\n",
       "       [0.56020194, 0.43979815],\n",
       "       [0.5254646 , 0.4745354 ],\n",
       "       [0.536522  , 0.463478  ],\n",
       "       [0.5265455 , 0.4734545 ],\n",
       "       [0.5059025 , 0.49409744],\n",
       "       [0.5268623 , 0.47313765],\n",
       "       [0.51911986, 0.4808802 ],\n",
       "       [0.5184739 , 0.48152614],\n",
       "       [0.50826794, 0.491732  ],\n",
       "       [0.51035005, 0.48964998],\n",
       "       [0.51152873, 0.48847127],\n",
       "       [0.52642936, 0.47357064],\n",
       "       [0.5128096 , 0.48719046],\n",
       "       [0.49694934, 0.5030506 ],\n",
       "       [0.48116043, 0.51883954],\n",
       "       [0.49255002, 0.50745   ],\n",
       "       [0.517271  , 0.48272902],\n",
       "       [0.50753593, 0.49246404],\n",
       "       [0.52542305, 0.47457695],\n",
       "       [0.52396214, 0.4760379 ],\n",
       "       [0.5420059 , 0.4579941 ],\n",
       "       [0.5273194 , 0.4726807 ],\n",
       "       [0.5187337 , 0.4812663 ],\n",
       "       [0.49637714, 0.5036229 ],\n",
       "       [0.522177  , 0.477823  ],\n",
       "       [0.5202979 , 0.47970214],\n",
       "       [0.5064069 , 0.49359307],\n",
       "       [0.48806617, 0.51193386],\n",
       "       [0.4998638 , 0.5001362 ],\n",
       "       [0.52682734, 0.4731727 ],\n",
       "       [0.5230162 , 0.47698382],\n",
       "       [0.53913677, 0.4608632 ],\n",
       "       [0.52857476, 0.4714252 ],\n",
       "       [0.5422693 , 0.45773065],\n",
       "       [0.5295238 , 0.47047627],\n",
       "       [0.5243005 , 0.47569945],\n",
       "       [0.5031165 , 0.49688354],\n",
       "       [0.49314466, 0.50685537],\n",
       "       [0.48138908, 0.51861095],\n",
       "       [0.508917  , 0.491083  ],\n",
       "       [0.4836349 , 0.5163652 ],\n",
       "       [0.5133276 , 0.48667237],\n",
       "       [0.49478596, 0.50521404],\n",
       "       [0.47321975, 0.5267802 ],\n",
       "       [0.5061955 , 0.49380445],\n",
       "       [0.50783485, 0.49216518],\n",
       "       [0.4999646 , 0.50003546],\n",
       "       [0.53033006, 0.46966997],\n",
       "       [0.5258121 , 0.47418794],\n",
       "       [0.5139107 , 0.4860893 ],\n",
       "       [0.53794485, 0.4620551 ],\n",
       "       [0.5456686 , 0.4543314 ],\n",
       "       [0.5540007 , 0.4459993 ],\n",
       "       [0.56063306, 0.43936688],\n",
       "       [0.5440591 , 0.45594096],\n",
       "       [0.51606643, 0.48393357],\n",
       "       [0.53967404, 0.460326  ],\n",
       "       [0.5226591 , 0.47734088],\n",
       "       [0.53625923, 0.46374077],\n",
       "       [0.53168684, 0.4683132 ],\n",
       "       [0.51236284, 0.48763722],\n",
       "       [0.51286083, 0.48713917],\n",
       "       [0.52014756, 0.47985244],\n",
       "       [0.5172096 , 0.48279044],\n",
       "       [0.5147623 , 0.4852377 ],\n",
       "       [0.53379875, 0.46620125],\n",
       "       [0.4995301 , 0.5004698 ],\n",
       "       [0.51359344, 0.48640662],\n",
       "       [0.538603  , 0.46139696],\n",
       "       [0.5246489 , 0.47535115],\n",
       "       [0.52571297, 0.47428703],\n",
       "       [0.5419558 , 0.4580442 ],\n",
       "       [0.551857  , 0.448143  ],\n",
       "       [0.5637179 , 0.43628213]], dtype=float32)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probchoice[15,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 300, 2)"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loglike = np.dot(train_y.reshape(-1,2)[:,0],np.log(probchoice.reshape(-1,2)+0.0001)[:,0]) + np.dot(train_y.reshape(-1,2)[:,1],np.log(probchoice.reshape(-1,2)+0.0001)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-417.8953369855881\n"
     ]
    }
   ],
   "source": [
    "# print(loglike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0096517675991006"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 - loglike/(np.log(0.5)*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,auc[1],loss_fn,probchoice,acc_test,loss_test,prob_test,neurons,learning_rate,epochs]).reshape(-1,13),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\",\"loss\",\"probchoice\",\"accuracy_test\",\"loss_test\",\"prob_test\",\"neurons\",\"learning_rate\",\"n_epochs\"])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "      <th>loss</th>\n",
       "      <th>probchoice</th>\n",
       "      <th>accuracy_test</th>\n",
       "      <th>loss_test</th>\n",
       "      <th>prob_test</th>\n",
       "      <th>neurons</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.57285</td>\n",
       "      <td>0.689693</td>\n",
       "      <td>0.26961</td>\n",
       "      <td>0.321996</td>\n",
       "      <td>0.577448</td>\n",
       "      <td>0.68207</td>\n",
       "      <td>[[[0.5028481, 0.4971519], [0.4913881, 0.508611...</td>\n",
       "      <td>0.57279</td>\n",
       "      <td>0.682568</td>\n",
       "      <td>[[[0.49896055, 0.50103945], [0.48632938, 0.513...</td>\n",
       "      <td>100</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  accuracy precision   recall  f1_score       auc     loss  \\\n",
       "0  0.57285  0.689693  0.26961  0.321996  0.577448  0.68207   \n",
       "\n",
       "                                          probchoice accuracy_test loss_test  \\\n",
       "0  [[[0.5028481, 0.4971519], [0.4913881, 0.508611...       0.57279  0.682568   \n",
       "\n",
       "                                           prob_test neurons learning_rate  \\\n",
       "0  [[[0.49896055, 0.50103945], [0.48632938, 0.513...     100         1e-05   \n",
       "\n",
       "  n_epochs  \n",
       "0     2000  "
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path\n",
    "metric_out_df.to_csv(file_path+\"LSTM_metrics_all_subjs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probchoice_all_df = pd.DataFrame([probchoice,prob_test],columns =['probchoice','prob_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpts/RNN_LST_model_subject_num_11_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./checkpts/RNN_LST_model_subject_num_11_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(probchoice[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df = pd.DataFrame(probchoice.reshape(-1,2),columns = {'action_0','action_1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_0</th>\n",
       "      <th>action_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.502848</td>\n",
       "      <td>0.497152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.491388</td>\n",
       "      <td>0.508612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.491984</td>\n",
       "      <td>0.508016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.469915</td>\n",
       "      <td>0.530085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.484155</td>\n",
       "      <td>0.515845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.473913</td>\n",
       "      <td>0.526087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.474276</td>\n",
       "      <td>0.525724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.463028</td>\n",
       "      <td>0.536972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.447558</td>\n",
       "      <td>0.552442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.447536</td>\n",
       "      <td>0.552464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_0  action_1\n",
       "0  0.502848  0.497152\n",
       "1  0.491388  0.508612\n",
       "2  0.491984  0.508016\n",
       "3  0.469915  0.530085\n",
       "4  0.484155  0.515845\n",
       "5  0.473913  0.526087\n",
       "6  0.474276  0.525724\n",
       "7  0.463028  0.536972\n",
       "8  0.447558  0.552442\n",
       "9  0.447536  0.552464"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(probchoice.reshape(-1,2))\n",
    "plt.ylim([0.25,0.75])\n",
    "plt.ylabel(\"Probchoice\");\n",
    "plt.xlabel(\"Trials\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5866916"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_15/'"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,auc[1]]).reshape(-1,5),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array([acc[1],prec[1],recall[1],f1,auc[1]]).reshape(-1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric_out_df.to_csv(file_path+\"LSTM_metrics.csv\")\n",
    "# metric_out_df.to_csv(file_path+\"LSTM_metrics_lr05.csv\")\n",
    "metric_out_df.to_csv(file_path+\"LSTM_metrics_10neurons.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_metrics_df = pd.DataFrame()\n",
    "for subj_num in range(11,26):\n",
    "#     print(subj_num)\n",
    "    file_name = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/subject_num_\" + str(subj_num) + \"/LSTM_metrics.csv\"\n",
    "    all_subj_metrics_df = all_subj_metrics_df.append(pd.read_csv(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>auc</th>\n",
       "      <th>loss</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>n_epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.762887</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.653649</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.371212</td>\n",
       "      <td>0.472657</td>\n",
       "      <td>0.578463</td>\n",
       "      <td>0.673517</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.889571</td>\n",
       "      <td>0.699521</td>\n",
       "      <td>0.543325</td>\n",
       "      <td>0.676040</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.834951</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.589942</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.742138</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.661651</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.563333</td>\n",
       "      <td>0.562712</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.717855</td>\n",
       "      <td>0.505411</td>\n",
       "      <td>0.660161</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.593333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.219753</td>\n",
       "      <td>0.545996</td>\n",
       "      <td>0.663565</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.605948</td>\n",
       "      <td>0.942197</td>\n",
       "      <td>0.746546</td>\n",
       "      <td>0.553775</td>\n",
       "      <td>0.648313</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.608295</td>\n",
       "      <td>0.874172</td>\n",
       "      <td>0.770507</td>\n",
       "      <td>0.651851</td>\n",
       "      <td>0.684488</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.147009</td>\n",
       "      <td>0.533178</td>\n",
       "      <td>0.674514</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.539802</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.035282</td>\n",
       "      <td>0.495348</td>\n",
       "      <td>0.660351</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.582996</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.755914</td>\n",
       "      <td>0.603900</td>\n",
       "      <td>0.672934</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.726115</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.681617</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.438561</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  accuracy  precision    recall  f1_score       auc      loss  \\\n",
       "0           0  0.616667   0.616667  1.000000  0.762887  0.500000  0.653649   \n",
       "0           0  0.603333   0.576471  0.371212  0.472657  0.578463  0.673517   \n",
       "0           0  0.573333   0.568627  0.889571  0.699521  0.543325  0.676040   \n",
       "0           0  0.716667   0.716667  1.000000  0.834951  0.500000  0.589942   \n",
       "0           0  0.590000   0.590000  1.000000  0.742138  0.500000  0.661651   \n",
       "0           0  0.563333   0.562712  0.988095  0.717855  0.505411  0.660161   \n",
       "0           0  0.593333   0.666667  0.151515  0.219753  0.545996  0.663565   \n",
       "0           0  0.613333   0.605948  0.942197  0.746546  0.553775  0.648313   \n",
       "0           0  0.653333   0.608295  0.874172  0.770507  0.651851  0.684488   \n",
       "0           0  0.573333   0.700000  0.102941  0.147009  0.533178  0.674514   \n",
       "0           0  0.773333   0.000000  0.000000       NaN  0.500000  0.539802   \n",
       "0           0  0.573333   0.250000  0.007937  0.035282  0.495348  0.660351   \n",
       "0           0  0.616667   0.582996  0.923077  0.755914  0.603900  0.672934   \n",
       "0           0  0.570000   0.570000  1.000000  0.726115  0.500000  0.681617   \n",
       "0           0  0.830000   0.000000  0.000000       NaN  0.500000  0.438561   \n",
       "\n",
       "   learning_rate  n_epochs  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00005    2000.0  \n",
       "0        0.00005    2000.0  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00001    2000.0  \n",
       "0        0.00001    2000.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subj_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_metrics_df[\"pseudoR2\"]=1+all_subj_metrics_df.loss/(np.log(0.5)*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_file_path = \"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/placdata/\"\n",
    "PT_file_name = PT_file_path  + \"PT_loss.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_metrics = pd.read_csv(PT_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PT_metrics.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_number</th>\n",
       "      <th>PT_loss</th>\n",
       "      <th>PT_pseudoR2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>81.160098</td>\n",
       "      <td>0.609702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>159.474231</td>\n",
       "      <td>0.233091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>171.936892</td>\n",
       "      <td>0.173158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>113.557085</td>\n",
       "      <td>0.453906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>8.021925</td>\n",
       "      <td>0.961423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>64.471021</td>\n",
       "      <td>0.689960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17</td>\n",
       "      <td>111.244149</td>\n",
       "      <td>0.465029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18</td>\n",
       "      <td>133.319149</td>\n",
       "      <td>0.358870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>16.799920</td>\n",
       "      <td>0.919209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>43.594714</td>\n",
       "      <td>0.790354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21</td>\n",
       "      <td>117.752046</td>\n",
       "      <td>0.433732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22</td>\n",
       "      <td>60.262084</td>\n",
       "      <td>0.710201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23</td>\n",
       "      <td>107.505168</td>\n",
       "      <td>0.483009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24</td>\n",
       "      <td>149.257056</td>\n",
       "      <td>0.282225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>96.780830</td>\n",
       "      <td>0.534583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26</td>\n",
       "      <td>102.310428</td>\n",
       "      <td>0.507991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28</td>\n",
       "      <td>127.181507</td>\n",
       "      <td>0.388386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29</td>\n",
       "      <td>37.328830</td>\n",
       "      <td>0.820486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>30</td>\n",
       "      <td>163.100213</td>\n",
       "      <td>0.215654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>117.658301</td>\n",
       "      <td>0.434183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>91.442263</td>\n",
       "      <td>0.560256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>158.940983</td>\n",
       "      <td>0.235655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>34</td>\n",
       "      <td>146.488302</td>\n",
       "      <td>0.295540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>35</td>\n",
       "      <td>63.927930</td>\n",
       "      <td>0.692572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>36</td>\n",
       "      <td>88.129209</td>\n",
       "      <td>0.576188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>37</td>\n",
       "      <td>62.169885</td>\n",
       "      <td>0.701026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>38</td>\n",
       "      <td>65.596010</td>\n",
       "      <td>0.684550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>39</td>\n",
       "      <td>137.085788</td>\n",
       "      <td>0.340757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>40</td>\n",
       "      <td>93.823903</td>\n",
       "      <td>0.548802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>41</td>\n",
       "      <td>114.537550</td>\n",
       "      <td>0.449191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subject_number     PT_loss  PT_pseudoR2\n",
       "0               11   81.160098     0.609702\n",
       "1               12  159.474231     0.233091\n",
       "2               13  171.936892     0.173158\n",
       "3               14  113.557085     0.453906\n",
       "4               15    8.021925     0.961423\n",
       "5               16   64.471021     0.689960\n",
       "6               17  111.244149     0.465029\n",
       "7               18  133.319149     0.358870\n",
       "8               19   16.799920     0.919209\n",
       "9               20   43.594714     0.790354\n",
       "10              21  117.752046     0.433732\n",
       "11              22   60.262084     0.710201\n",
       "12              23  107.505168     0.483009\n",
       "13              24  149.257056     0.282225\n",
       "14              25   96.780830     0.534583\n",
       "15              26  102.310428     0.507991\n",
       "17              28  127.181507     0.388386\n",
       "18              29   37.328830     0.820486\n",
       "19              30  163.100213     0.215654\n",
       "20              31  117.658301     0.434183\n",
       "21              32   91.442263     0.560256\n",
       "22              33  158.940983     0.235655\n",
       "23              34  146.488302     0.295540\n",
       "24              35   63.927930     0.692572\n",
       "25              36   88.129209     0.576188\n",
       "26              37   62.169885     0.701026\n",
       "27              38   65.596010     0.684550\n",
       "28              39  137.085788     0.340757\n",
       "29              40   93.823903     0.548802\n",
       "30              41  114.537550     0.449191"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT_metrics = PT_metrics[PT_metrics.PT_loss !=0]\n",
    "PT_metrics.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [y for y in a if y not in b]\n",
    "# [PT_metrics.Subject_number(a) for a in range(11,22)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PT_metrics.loc[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_number</th>\n",
       "      <th>PT_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>81.160098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>159.474231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>171.936892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>113.557085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>8.021925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "      <td>64.471021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17</td>\n",
       "      <td>111.244149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18</td>\n",
       "      <td>133.319149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>19</td>\n",
       "      <td>16.799920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20</td>\n",
       "      <td>43.594714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21</td>\n",
       "      <td>117.752046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>22</td>\n",
       "      <td>60.262084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23</td>\n",
       "      <td>107.505168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24</td>\n",
       "      <td>149.257056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>96.780830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Subject_number     PT_loss\n",
       "0               11   81.160098\n",
       "1               12  159.474231\n",
       "2               13  171.936892\n",
       "3               14  113.557085\n",
       "4               15    8.021925\n",
       "5               16   64.471021\n",
       "6               17  111.244149\n",
       "7               18  133.319149\n",
       "8               19   16.799920\n",
       "9               20   43.594714\n",
       "10              21  117.752046\n",
       "11              22   60.262084\n",
       "12              23  107.505168\n",
       "13              24  149.257056\n",
       "14              25   96.780830"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PT_metrics.loc[0:14,['Subject_number','PT_loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_subj_metrics_df.loc[0:12,['Subject_number']]= PT_metrics.loc[0:12,['Subject_number']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# sns.set(style=\"ticks\")\n",
    "ax = sns.barplot(y=\"accuracy\", data=all_subj_metrics_df, capsize=.2)\n",
    "# sns.catplot(y=\"accuracy\",data=all_subj_metrics_df)sns.catplot(y=\"accuracy\",data=all_subj_metrics_df,kind=\"box\")\n",
    "\n",
    "plt.ylim([0 ,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sns.catplot(y=\"accuracy\",data=all_subj_metrics_df)\n",
    "sns.catplot(y=\"accuracy\",data=all_subj_metrics_df,kind=\"box\")\n",
    "# sns.catplot(y=\"accuracy\",data=all_subj_metrics_df,kind=\"violin\")\n",
    "\n",
    "plt.ylim([0.5,0.8])\n",
    "plt.xlabel('RNN (100 neurons)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(y=\"loss\",data=all_subj_metrics_df,kind=\"box\",color=\"grey\")\n",
    "# plt.title(\"n=\"+str(all_subj_metrics_df.shape[0]))\n",
    "plt.ylim([0.5,0.75])\n",
    "plt.xlabel('RNN (100 neurons)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(y=\"pseudoR2\",data=all_subj_metrics_df,kind=\"box\",color=\"grey\")\n",
    "# plt.title(\"n=\"+str(all_subj_metrics_df.shape[0]))\n",
    "# plt.ylim([0.5,0.75])\n",
    "plt.ylim([0.9,1])\n",
    "plt.xlabel('RNN (100 neurons)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(y=\"PT_loss\",data = PT_metrics.loc[0:14,:],kind=\"box\",color=\"grey\")\n",
    "# plt.title(\"n=\"+str(all_subj_metrics_df.shape[0]))\n",
    "# plt.ylim([0.5,0.75])\n",
    "plt.xlabel('PT');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.catplot(y=\"PT_pseudoR2\",data = PT_metrics.loc[0:14,:],kind=\"box\",color=\"grey\")\n",
    "# plt.title(\"n=\"+str(all_subj_metrics_df.shape[0]))\n",
    "# plt.ylim([0.5,0.75])\n",
    "plt.xlabel('PT');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons and WilcoxonSigned Rank test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=0.0, pvalue=0.0006549583433856954)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_stats.wilcoxon(all_subj_metrics_df.loss,PT_metrics.loc[0:14].PT_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax= plt.subplots(figsize=(10,10))\n",
    "ax.scatter(all_subj_metrics_df.pseudoR2,PT_metrics.loc[0:14].PT_pseudoR2)\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes\n",
    "]\n",
    "ax.plot(lims,lims,color='black',alpha=0.75, zorder=0)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('Pseudo R2 RNN (100 Neurons)',Fontsize=14);\n",
    "ax.set_ylabel('Pseudo R2 Prospect Theory',Fontsize=14);\n",
    "\n",
    "# ax.set_xlim([100,300])\n",
    "# ax.set_ylim([100,300]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=0.0, pvalue=0.0006549583433856954)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_stats.wilcoxon(all_subj_metrics_df.pseudoR2,PT_metrics.loc[0:14].PT_pseudoR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
