{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as sc_stats\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "onehot_encoder=OneHotEncoder(sparse=False)\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "time_steps = 1\n",
    "inputs = 3\n",
    "outputs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_releveant_features(task_df):\n",
    "\n",
    "\n",
    "    task_df['PrevOutcome']=task_df['Outcome'].shift(1)\n",
    "    task_df.loc[1,'PrevOutcome']= 0\n",
    "\n",
    "    task_df['PrevChoice']=task_df['Choice'].shift(1)\n",
    "    task_df.loc[1,'PrevChoice']= 0\n",
    "\n",
    "    task_df['PrevSafe']=task_df['Safe'].shift(1)\n",
    "    task_df.loc[1,'PrevSafe']= 0\n",
    "\n",
    "    task_df['PrevBigRisky']=task_df['BigRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevBigRisky']= 0\n",
    "\n",
    "    task_df['PrevSmallRisky']=task_df['SmallRisky'].shift(1)\n",
    "    task_df.loc[1,'PrevSmallRisky']= 0\n",
    "    \n",
    "    task_df['PrevRT']=task_df['RT'].shift(1)\n",
    "    task_df.loc[1,'PrevRT']= 0\n",
    "    \n",
    "#     task_df['RPE'] =  task_df['Outcome'].shift(1) - 0.5*(task_df['BigRisky'].shift(1) + \n",
    "    \n",
    "    return task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_kback_features(task_df):\n",
    "\n",
    "    for k in range(1,11):\n",
    "        task_df[str(k)+'backOutcome']=task_df['Outcome'].shift(k)\n",
    "        task_df[str(k)+'backChoice']=task_df['Choice'].shift(k)\n",
    "        task_df[str(k)+'backSafe']=task_df['Safe'].shift(k)\n",
    "        task_df[str(k)+'backBigRisky']=task_df['BigRisky'].shift(k)\n",
    "        task_df[str(k)+'backSmallRisky']=task_df['SmallRisky'].shift(k)\n",
    "\n",
    "    return task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_split_data(data,start_chunk,end_chunk):\n",
    "    \n",
    "    a=[k for k in range(start_chunk,end_chunk)]\n",
    "    out=[]\n",
    "\n",
    "    for d in range(0,data.shape[0],20):\n",
    "\n",
    "        c= [c+d for c in a]\n",
    "        out = out+c\n",
    "\n",
    "    while out[-1]>=data.shape[0]-1:\n",
    "        out.pop()\n",
    "#     return out\n",
    "    return data[out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y): \n",
    "    reset_graph()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50000\n",
    "    batch_size = int(train_X.shape[0]/2)\n",
    "    # batch_size = 100\n",
    "    length = train_X.shape[0]\n",
    "    display = 100\n",
    "    neurons = neurons\n",
    "\n",
    "    num_batches = 100\n",
    "    seq_len = 10\n",
    "\n",
    "    percent_above_PT = 1\n",
    "\n",
    "    train_threshold = 1.5#PT_R2 + percent_above_PT\n",
    "\n",
    "\n",
    "    save_step = 100\n",
    "\n",
    "\n",
    "    best_loss_val = np.infty\n",
    "    checks_since_last_progress = 0\n",
    "    max_checks_without_progress = 1000\n",
    "\n",
    "\n",
    "    # clear graph (if any) before running\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, time_steps, inputs])\n",
    "\n",
    "    y = tf.placeholder(tf.float32, [None, outputs])\n",
    "\n",
    "    # LSTM Cell\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=neurons, activation=tf.nn.relu)\n",
    "    cell_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "    # pass into Dense layer\n",
    "    stacked_outputs = tf.reshape(cell_outputs, [-1, neurons])\n",
    "    out = tf.layers.dense(inputs=stacked_outputs, units=outputs)\n",
    "\n",
    "    probability = tf.nn.softmax(out)\n",
    "\n",
    "    # squared error loss or cost function for linear regression\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=y, logits=out))\n",
    "\n",
    "    # optimizer to minimize cost\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    accuracy = tf.metrics.accuracy(labels =  tf.argmax(y, 1),\n",
    "                          predictions = tf.argmax(out, 1),\n",
    "                          name = \"accuracy\")\n",
    "    precision = tf.metrics.precision(labels=tf.argmax(y, 1),\n",
    "                                 predictions=tf.argmax(out, 1),\n",
    "                                 name=\"precision\")\n",
    "    recall = tf.metrics.recall(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"recall\")\n",
    "    f1 = 2 * accuracy[1] * recall[1] / ( precision[1] + recall[1] )\n",
    "\n",
    "    acc_up,acc_val = accuracy\n",
    "    auc = tf.metrics.auc(labels=tf.argmax(y, 1),\n",
    "                           predictions=tf.argmax(out, 1),\n",
    "                           name=\"auc\")\n",
    "    \n",
    "    valid_store = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #######################\n",
    "#         saver.restore(sess, \"./checkpts/Original_RNN_LSTM_8features_v2.ckpt\")\n",
    "#         saver.restore(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "        \n",
    "        if pretraining == True:\n",
    "\n",
    "            saver.restore(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        #######################\n",
    "        \n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        tf.local_variables_initializer().run()\n",
    "\n",
    "        # Train the model\n",
    "        for steps in range(epochs):\n",
    "            mini_batch = zip(range(0, length, batch_size),\n",
    "                       range(batch_size, length+1, batch_size))\n",
    "\n",
    "            # train data in mini-batches\n",
    "            for (start, end) in mini_batch:\n",
    "    #             print(start,end)\n",
    "                sess.run(training_op, feed_dict = {X: train_X[start:end,:,:],\n",
    "                                                   y: train_y[start:end,:]}) \n",
    "\n",
    "            ## train data in batches of length subsequence\n",
    "\n",
    "    #         for k in range(num_batches):\n",
    "    #             X_seq, y_seq = random_subsequence(train_X,train_y,seq_len)\n",
    "\n",
    "    #             sess.run(training_op, feed_dict = {X:X_seq,y:y_seq}) \n",
    "            loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "            loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "            # print training performance \n",
    "            if (steps+1) % display == 0:\n",
    "                # evaluate loss function on training set\n",
    "\n",
    "\n",
    "                loss_fn = loss.eval(feed_dict = {X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining loss: {}'.format((steps+1), loss_fn))\n",
    "\n",
    "                acc_train = acc_val.eval(feed_dict={X: train_X, y: train_y})\n",
    "                print('Step: {}  \\tTraining accuracy: {}'.format((steps+1), acc_train))\n",
    "\n",
    "\n",
    "                acc_test = acc_val.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest accuracy: {}'.format((steps+1), acc_test))\n",
    "\n",
    "                loss_test = loss.eval(feed_dict={X: test_X, y: test_y})\n",
    "    #             print('Step: {}  \\tTest loss: {}'.format((steps+1), loss_test))\n",
    "\n",
    "                accu_val = acc_val.eval(feed_dict={X: val_X, y: val_y})\n",
    "\n",
    "                loss_val = loss.eval(feed_dict={X: val_X, y: val_y})\n",
    "                print('Step: {}  \\tValid loss: {}'.format((steps+1), loss_val))\n",
    "\n",
    "                valid_store.append(loss_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if (1 + loss_fn/np.log(0.5)) > train_threshold:\n",
    "                    print(\"Threshold achieved, quit training\")\n",
    "                    break\n",
    "\n",
    "\n",
    "            if loss_val < best_loss_val:\n",
    "\n",
    "                        best_loss_val = loss_val\n",
    "                        checks_since_last_progress = 0\n",
    "            else:\n",
    "                            checks_since_last_progress += 1\n",
    "\n",
    "\n",
    "            # EARLY STOPPING\n",
    "            if checks_since_last_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "\n",
    "\n",
    "            if (steps+1) % save_step ==0:\n",
    "                                save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#                 save_path = saver.save(sess, \"./checkpts/RNN_Internet_LSTM_model_5features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     evaluate model accuracy\n",
    "        acc, prec, recall, f1, AUC = sess.run([accuracy, precision, recall, f1,auc],\n",
    "                                         feed_dict = {X: train_X, y: train_y})\n",
    "        prob_train = probability.eval(feed_dict = {X: train_X, y: train_y})\n",
    "        prob_test = probability.eval(feed_dict = {X: test_X, y: test_y})\n",
    "        prob_valid = probability.eval(feed_dict = {X: val_X, y: val_y})\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nEvaluation  on training set')\n",
    "        print('Accuracy:', acc[1])\n",
    "        print('Precision:', prec[1])\n",
    "        print('Recall:', recall[1])\n",
    "        print('F1 score:', f1)\n",
    "        print('AUC:', AUC[1])\n",
    "        \n",
    "      \n",
    "    \n",
    "    \n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_v2_DATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "        \n",
    "#         save_path = saver.save(sess, \"./checkpts/OriginalDATA_RNN_LSTM_8features.ckpt\")\n",
    "#         save_path = saver.save(sess, \"./checkpts/LaterDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "#         save_path = saver.save(sess, \"./checkpts/Later_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## APP DATA\n",
    "#         save_path = saver.save(sess, \"./checkpts/Original_v2_APPDATA_RNN_LSTM_8features.ckpt\")\n",
    "        save_path = saver.save(sess, \"./checkpts/Later_v2_APPDATA_RNN_LSTM_8features.ckpt\")\n",
    "\n",
    "\n",
    "    metric_out_df= pd.DataFrame(np.array([acc[1],prec[1],recall[1],f1,AUC[1],loss_fn,accu_val,best_loss_val,acc_test,loss_test,neurons,learning_rate,epochs,steps]).reshape(-1,14),columns =[\"accuracy\",\"precision\",\"recall\",\"f1_score\",\"auc\",\"loss\",\"accuracy_val\",\"loss_val\",\"accuracy_test\",\"loss_test\",\"neurons\",\"learning_rate\",\"n_epochs\",\"steps\"])\n",
    "    return metric_out_df, prob_train, prob_test, prob_valid\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def random_subsequence(X,y,seq_len):\n",
    "    rnd  = random.randint(0,len(X)-seq_len)\n",
    "    X_seq, y_seq = X[rnd:rnd+seq_len,:], y[rnd:rnd+seq_len,:]\n",
    "    return X_seq, y_seq\n",
    "\n",
    "    print(y_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odd plays train, even plays test and valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate subject files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\")\n",
    "dir_path =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"\n",
    "\n",
    "subj_files_list =[]; ## list of subject_files fullfilling a criteria\n",
    "\n",
    "dir_files = [i for i in os.listdir(dir_path) if i.startswith('sub')]\n",
    "\n",
    "for subj_file_path in dir_files:\n",
    "\n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "    mypath =file_path\n",
    "    \n",
    "    play_names = [i for i in os.listdir(mypath) if os.path.isfile(os.path.join(mypath,i)) and i.startswith('app')]   \n",
    "    \n",
    "    if len(play_names) >= 50: ## criteria\n",
    "        subj_files_list.append(subj_file_path)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subj_files_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate train, valid and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_shuffles=1\n",
    "# for num, subj_file_path in enumerate(subj_files_list):\n",
    "#     print(num)\n",
    "# # for subj_file_path in [subj_files_list[0]]:\n",
    "    \n",
    "# #     train_data,test_data, val_data = np.empty((0,task_df.columns.shape[0])),  np.empty((0,task_df.columns.shape[0])), np.empty((0,task_df.columns.shape[0]))\n",
    "#     train_data,test_data, val_data = np.empty((0,15)),  np.empty((0,15)), np.empty((0,15))\n",
    "\n",
    "#     file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "#     mypath =file_path\n",
    "        \n",
    "#     comp_task_train_df = pd.DataFrame()\n",
    "\n",
    "#     play_names = [i for i in os.listdir(mypath) if os.path.isfile(os.path.join(mypath,i)) and i.startswith('app')]    \n",
    "\n",
    "#     for randomization_counter in range(0,num_shuffles):\n",
    "#             randomized_play_names= random.sample(play_names,len(play_names))\n",
    "            \n",
    "#             for play_num, play_name in enumerate(randomized_play_names):\n",
    "# #         for play_num,play_name in enumerate(play_names):\n",
    "\n",
    "#                 file_name = file_path + \"/\" + play_name\n",
    "#                 task_df = pd.read_csv(file_name)\n",
    "#                 task_df = add_releveant_features(task_df)\n",
    "\n",
    "#                 if np.mod(play_num,2)==0: ## odd trials\n",
    "#                     train_data = np.append(train_data,task_df[task_df.TrialNum>1].values, axis=0)\n",
    "\n",
    "#                 else:\n",
    "#                     test_data =  np.append(test_data, task_df[task_df.TrialNum>1].values[0:16], axis=0)\n",
    "#                     val_data =  np.append(val_data, task_df[task_df.TrialNum>1].values[16:], axis=0)\n",
    "\n",
    "\n",
    "#     train_data_df= pd.DataFrame(train_data,columns=task_df.columns)\n",
    "#     val_data_df = pd.DataFrame(test_data,columns=task_df.columns)\n",
    "#     test_data_df= pd.DataFrame(val_data,columns=task_df.columns)\n",
    "\n",
    "# #     file_path = file_path + \"/OddEvenPlays/\"\n",
    "#     file_path = file_path + \"/OddEvenPlays/RandomizedPlays1\"\n",
    "\n",
    "# #     os.mkdir(file_path)\n",
    "#     train_data_df.to_csv(file_path+\"/train_data.csv\")\n",
    "#     test_data_df.to_csv(file_path+\"/test_data.csv\")\n",
    "#     val_data_df.to_csv(file_path+\"/val_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Randomizing Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_trials(df):\n",
    "\n",
    "    locs= random.sample([a for a in range(0,df.shape[0])],df.shape[0])\n",
    "    # len(locs)\n",
    "    df = add_releveant_features(df.loc[locs])\n",
    "    \n",
    "    ### get rid of first index since it contains NaN for previous trials\n",
    "    df  = df.iloc[1:]\n",
    "    \n",
    "    return df , pd.DataFrame(locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_randomizations(train_data_df,test_data_df,val_data_df):\n",
    "        train_data_df,train_locs = randomize_trials(train_data_df)\n",
    "        test_data_df,test_locs = randomize_trials(test_data_df)\n",
    "        val_data_df,val_locs = randomize_trials(val_data_df)\n",
    "        \n",
    "        #         os.mkdir(file_path)\n",
    "        train_locs.to_csv(file_path+\"/train_locs.csv\")\n",
    "        test_locs.to_csv(file_path+\"/test_locs.csv\")\n",
    "        val_locs.to_csv(file_path+\"/val_locs.csv\")\n",
    "        \n",
    "        return train_locs, test_locs, val_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shuffled_data():\n",
    "        train_locs = pd.read_csv(file_path+\"/train_locs.csv\")\n",
    "        test_locs = pd.read_csv(file_path+\"/test_locs.csv\")\n",
    "        val_locs = pd.read_csv(file_path+\"/val_locs.csv\")\n",
    "        train_data_random_df  = train_data_df.iloc[train_locs.iloc[:,1]]\n",
    "        test_data_random_df  = test_data_df.iloc[test_locs.iloc[:,1]]\n",
    "        val_data_random_df  = val_data_df.iloc[val_locs.iloc[:,1]]\n",
    "        \n",
    "        train_data_random_df = add_releveant_features(train_data_random_df)\n",
    "        train_data_random_df = add_kback_features(train_data_random_df)\n",
    "        test_data_random_df = add_releveant_features(test_data_random_df)\n",
    "        test_data_random_df = add_kback_features(test_data_random_df)\n",
    "        val_data_random_df = add_releveant_features(val_data_random_df)\n",
    "        val_data_random_df = add_kback_features(val_data_random_df)\n",
    "        \n",
    "        return train_data_random_df, test_data_random_df,val_data_random_df, train_locs, test_locs, val_locs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_input_feature(randomize_trials_flag,hist_flag):\n",
    "\n",
    "    if randomize_trials_flag==True:\n",
    "        if hist_flag==0: ## CURR OPTIONS ONLY\n",
    "            print(\"return as is\")\n",
    "            \n",
    "    \n",
    "        if hist_flag==1: ## CURR OPTIONS, PREV ACTIONS:\n",
    "            train_data_random_df.loc[train_locs.iloc[:,1],'PrevChoice'] = train_data_df.loc[train_locs.iloc[:,1],'PrevChoice']\n",
    "            test_data_random_df.loc[test_locs.iloc[:,1],'PrevChoice'] = test_data_df.loc[test_locs.iloc[:,1],'PrevChoice']\n",
    "            val_data_random_df.loc[val_locs.iloc[:,1],'PrevChoice'] = val_data_df.loc[val_locs.iloc[:,1],'PrevChoice']\n",
    "\n",
    "\n",
    "\n",
    "        elif hist_flag==2: # CURR OPTIONS, PREV OUTCOME        \n",
    "\n",
    "            train_data_random_df.loc[train_locs.iloc[:,1],'PrevOutcome'] = train_data_df.loc[train_locs.iloc[:,1],'PrevOutcome']\n",
    "            test_data_random_df.loc[test_locs.iloc[:,1],'PrevOutcome'] = test_data_df.loc[test_locs.iloc[:,1],'PrevOutcome']\n",
    "            val_data_random_df.loc[val_locs.iloc[:,1],'PrevOutcome'] = val_data_df.loc[val_locs.iloc[:,1],'PrevOutcome']\n",
    "\n",
    "\n",
    "        elif hist_flag==3: ## CURR OPTIONS, PREV ACTIONS, PREV OUTCOME\n",
    "\n",
    "            train_data_random_df.loc[train_locs.iloc[:,1],'PrevChoice','PrevOutcome'] = train_data_df.loc[train_locs.iloc[:,1],'PrevChoice','PrevOutcome']\n",
    "            test_data_random_df.loc[test_locs.iloc[:,1],'PrevChoice','PrevOutcome'] = test_data_df.loc[test_locs.iloc[:,1],'PrevChoice','PrevOutcome']\n",
    "            val_data_random_df.loc[val_locs.iloc[:,1],'PrevChoice','PrevOutcome'] = val_data_df.loc[val_locs.iloc[:,1],'PrevChoice','PrevOutcome']\n",
    "\n",
    "\n",
    "\n",
    "    ####### Prev O + C+ R + CurrO--------------------\n",
    "        elif hist_flag==4: # CURR OPTIONS, PREV ACTIONS, PREV OUTCOME, PREV OPTIONS\n",
    "            print(hist_flag)\n",
    "   \n",
    "    return train_data_random_df, test_data_random_df,val_data_random_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split_odd_even(train_data_df,test_data_df,val_data_df):\n",
    "\n",
    "#     train_len = 29\n",
    "#     test_len = 14\n",
    "#     val_len = 15\n",
    "\n",
    "    ##----------------- UNCOMMENT BELOW\n",
    "    \n",
    "    if hist_flag==0: ## CURR OPTIONS ONLY\n",
    "        \n",
    "    \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        val_X = val_data_df[['Safe','BigRisky','SmallRisky']].values\n",
    "        val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "    elif hist_flag==1: ## CURR OPTIONS, PREV ACTIONS:\n",
    "        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevChoice']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevChoice']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevChoice']].values\n",
    "        val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "        \n",
    "        \n",
    "      \n",
    "    elif hist_flag==2: # CURR OPTIONS, PREV OUTCOME        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome']].values\n",
    "        val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    elif hist_flag==3: ## CURR OPTIONS, PREV ACTIONS, PREV OUTCOME\n",
    "        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevChoice','PrevOutcome']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevChoice','PrevOutcome']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevChoice','PrevOutcome']].values\n",
    "        val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "             \n",
    "        \n",
    "\n",
    "####### Prev O + C+ R + CurrO--------------------\n",
    "    elif hist_flag==4: # CURR OPTIONS, PREV ACTIONS, PREV OUTCOME, PREV OPTIONS\n",
    "        \n",
    "        train_X = train_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "        train_y = train_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        test_X = test_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "        test_y = test_data_df[['Choice']].values.astype(np.int32)\n",
    "\n",
    "        val_X = val_data_df[['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky']].values\n",
    "        val_y = val_data_df[['Choice']].values.astype(np.int32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ######## sampling \n",
    "    \n",
    "    \n",
    "#### - Prev RT+C+R+O + Curr O----------------------\n",
    "\n",
    "#     train_X = task_df.loc[task_df.TrialNum>1, ['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     train_y = task_df.loc[task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "#     test_X = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Safe','BigRisky','SmallRisky','PrevOutcome','PrevChoice','PrevSafe','PrevBigRisky','PrevSmallRisky','PrevRT']].values\n",
    "#     test_y = dopa_task_df.loc[dopa_task_df.TrialNum>1,['Choice']].values.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### PRE TRAINING\n",
    "#     stop = int(0.7*len(train_X))\n",
    "#     print(stop)\n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y= train_X[:stop], train_X[stop:stop+int((len(train_X)-stop)/2)], train_X[stop+int((len(train_X)-stop)/2):],train_y[:stop], train_y[stop:stop+int((len(train_X)-stop)/2)], train_y[stop+int((len(train_X)-stop)/2):]\n",
    "    \n",
    "#     train_X, test_X, val_X, train_y, test_y, val_y = train_X, test_X, test_X, train_y, test_y, test_y\n",
    "    ###################################################################\n",
    "\n",
    "\n",
    "    print(train_X.shape)\n",
    "    print(train_y.shape)\n",
    "    print(val_X.shape)\n",
    "    print(val_y.shape)\n",
    "    print(test_X.shape)\n",
    "    print(test_y.shape)\n",
    "\n",
    "    # # center and scale\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))    \n",
    "    train_X = scaler.fit_transform(train_X)\n",
    "    test_X = scaler.fit_transform(test_X)\n",
    "    val_X = scaler.fit_transform(val_X)\n",
    "\n",
    "\n",
    "    train_X = train_X[:,None,:]\n",
    "    val_X = val_X[:,None,:]\n",
    "    test_X = test_X[:,None,:]\n",
    "\n",
    "\n",
    "    # # one-hot encode the outputs\n",
    "\n",
    "    onehot_encoder = OneHotEncoder()\n",
    "    encode_categorical = train_y.reshape(len(train_y), 1)\n",
    "    encode_categorical_test = test_y.reshape(len(test_y), 1)\n",
    "    encode_categorical_val = val_y.reshape(len(val_y),1)\n",
    "\n",
    "\n",
    "    train_y = onehot_encoder.fit_transform(encode_categorical).toarray()\n",
    "    test_y = onehot_encoder.fit_transform(encode_categorical_test).toarray()\n",
    "    val_y = onehot_encoder.fit_transform(encode_categorical_val).toarray()\n",
    "\n",
    "    \n",
    "    return train_X, train_y, test_X, test_y, val_X,val_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run main training and save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_27883/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(1189, 3)\n",
      "(1189, 1)\n",
      "(640, 3)\n",
      "(640, 1)\n",
      "(520, 3)\n",
      "(520, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4570558965206146\n",
      "Step: 100  \tTraining accuracy: 0.8696383237838745\n",
      "Step: 100  \tValid loss: 0.4564363956451416\n",
      "Step: 200  \tTraining loss: 0.4131602346897125\n",
      "Step: 200  \tTraining accuracy: 0.8651780486106873\n",
      "Step: 200  \tValid loss: 0.41126784682273865\n",
      "Step: 300  \tTraining loss: 0.40119293332099915\n",
      "Step: 300  \tTraining accuracy: 0.8642772436141968\n",
      "Step: 300  \tValid loss: 0.40256696939468384\n",
      "Step: 400  \tTraining loss: 0.3928319811820984\n",
      "Step: 400  \tTraining accuracy: 0.8638902306556702\n",
      "Step: 400  \tValid loss: 0.3980369567871094\n",
      "Step: 500  \tTraining loss: 0.3882521688938141\n",
      "Step: 500  \tTraining accuracy: 0.8636749982833862\n",
      "Step: 500  \tValid loss: 0.3970620036125183\n",
      "Step: 600  \tTraining loss: 0.3863036334514618\n",
      "Step: 600  \tTraining accuracy: 0.8635379672050476\n",
      "Step: 600  \tValid loss: 0.39786943793296814\n",
      "Step: 700  \tTraining loss: 0.3855426013469696\n",
      "Step: 700  \tTraining accuracy: 0.8634430170059204\n",
      "Step: 700  \tValid loss: 0.3988562822341919\n",
      "Step: 800  \tTraining loss: 0.38514962792396545\n",
      "Step: 800  \tTraining accuracy: 0.8633733987808228\n",
      "Step: 800  \tValid loss: 0.3994700312614441\n",
      "Step: 900  \tTraining loss: 0.38485658168792725\n",
      "Step: 900  \tTraining accuracy: 0.8633201718330383\n",
      "Step: 900  \tValid loss: 0.3997817039489746\n",
      "Step: 1000  \tTraining loss: 0.3846016526222229\n",
      "Step: 1000  \tTraining accuracy: 0.8632780909538269\n",
      "Step: 1000  \tValid loss: 0.39994698762893677\n",
      "Step: 1100  \tTraining loss: 0.38437119126319885\n",
      "Step: 1100  \tTraining accuracy: 0.8632440567016602\n",
      "Step: 1100  \tValid loss: 0.4000561833381653\n",
      "Step: 1200  \tTraining loss: 0.38416004180908203\n",
      "Step: 1200  \tTraining accuracy: 0.8632159233093262\n",
      "Step: 1200  \tValid loss: 0.40014562010765076\n",
      "Step: 1300  \tTraining loss: 0.3839651346206665\n",
      "Step: 1300  \tTraining accuracy: 0.8631923198699951\n",
      "Step: 1300  \tValid loss: 0.40022796392440796\n",
      "Step: 1400  \tTraining loss: 0.3837839663028717\n",
      "Step: 1400  \tTraining accuracy: 0.863172173500061\n",
      "Step: 1400  \tValid loss: 0.4003064036369324\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8631548\n",
      "Precision: 0.8696383\n",
      "Recall: 1.0\n",
      "F1 score: 0.92333883\n",
      "AUC: 0.5\n",
      "   accuracy  precision  recall  f1_score  auc      loss  accuracy_val  \\\n",
      "0  0.863155   0.869638     1.0  0.923339  0.5  0.383636       0.86292   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.397045       0.862898     0.4267      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  1486.0  \n",
      "1\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_86257/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(4263, 3)\n",
      "(4263, 1)\n",
      "(2352, 3)\n",
      "(2352, 1)\n",
      "(1911, 3)\n",
      "(1911, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5664170980453491\n",
      "Step: 100  \tTraining accuracy: 0.7593244314193726\n",
      "Step: 100  \tValid loss: 0.5785761475563049\n",
      "Step: 200  \tTraining loss: 0.5290668606758118\n",
      "Step: 200  \tTraining accuracy: 0.7523652911186218\n",
      "Step: 200  \tValid loss: 0.5527979135513306\n",
      "Step: 300  \tTraining loss: 0.5248906016349792\n",
      "Step: 300  \tTraining accuracy: 0.7509734630584717\n",
      "Step: 300  \tValid loss: 0.5487770438194275\n",
      "Step: 400  \tTraining loss: 0.5169987678527832\n",
      "Step: 400  \tTraining accuracy: 0.7503769993782043\n",
      "Step: 400  \tValid loss: 0.5403470396995544\n",
      "Step: 500  \tTraining loss: 0.5024215579032898\n",
      "Step: 500  \tTraining accuracy: 0.7500455975532532\n",
      "Step: 500  \tValid loss: 0.5243799090385437\n",
      "Step: 600  \tTraining loss: 0.48167943954467773\n",
      "Step: 600  \tTraining accuracy: 0.7498347163200378\n",
      "Step: 600  \tValid loss: 0.5014874935150146\n",
      "Step: 700  \tTraining loss: 0.4592260420322418\n",
      "Step: 700  \tTraining accuracy: 0.7496887445449829\n",
      "Step: 700  \tValid loss: 0.47566452622413635\n",
      "Step: 800  \tTraining loss: 0.4416685998439789\n",
      "Step: 800  \tTraining accuracy: 0.7495816946029663\n",
      "Step: 800  \tValid loss: 0.45554423332214355\n",
      "Step: 900  \tTraining loss: 0.430240273475647\n",
      "Step: 900  \tTraining accuracy: 0.750617504119873\n",
      "Step: 900  \tValid loss: 0.4419805705547333\n",
      "Step: 1000  \tTraining loss: 0.42334306240081787\n",
      "Step: 1000  \tTraining accuracy: 0.7532747983932495\n",
      "Step: 1000  \tValid loss: 0.4331170320510864\n",
      "Step: 1100  \tTraining loss: 0.4193573594093323\n",
      "Step: 1100  \tTraining accuracy: 0.7562637329101562\n",
      "Step: 1100  \tValid loss: 0.42773759365081787\n",
      "Step: 1200  \tTraining loss: 0.4172055125236511\n",
      "Step: 1200  \tTraining accuracy: 0.7589470744132996\n",
      "Step: 1200  \tValid loss: 0.4244147837162018\n",
      "Step: 1300  \tTraining loss: 0.4161325693130493\n",
      "Step: 1300  \tTraining accuracy: 0.7617077231407166\n",
      "Step: 1300  \tValid loss: 0.4226793944835663\n",
      "Step: 1400  \tTraining loss: 0.4156390428543091\n",
      "Step: 1400  \tTraining accuracy: 0.7642418146133423\n",
      "Step: 1400  \tValid loss: 0.4215990900993347\n",
      "Step: 1500  \tTraining loss: 0.4154060482978821\n",
      "Step: 1500  \tTraining accuracy: 0.7664264440536499\n",
      "Step: 1500  \tValid loss: 0.4210587441921234\n",
      "Step: 1600  \tTraining loss: 0.41528892517089844\n",
      "Step: 1600  \tTraining accuracy: 0.7683291435241699\n",
      "Step: 1600  \tValid loss: 0.42070016264915466\n",
      "Step: 1700  \tTraining loss: 0.4152166247367859\n",
      "Step: 1700  \tTraining accuracy: 0.770001232624054\n",
      "Step: 1700  \tValid loss: 0.42051443457603455\n",
      "Step: 1800  \tTraining loss: 0.415155827999115\n",
      "Step: 1800  \tTraining accuracy: 0.7714821696281433\n",
      "Step: 1800  \tValid loss: 0.42038843035697937\n",
      "Step: 1900  \tTraining loss: 0.4150931239128113\n",
      "Step: 1900  \tTraining accuracy: 0.7728030681610107\n",
      "Step: 1900  \tValid loss: 0.4203343689441681\n",
      "Step: 2000  \tTraining loss: 0.41499096155166626\n",
      "Step: 2000  \tTraining accuracy: 0.7739884853363037\n",
      "Step: 2000  \tValid loss: 0.42031219601631165\n",
      "Step: 2100  \tTraining loss: 0.4148796796798706\n",
      "Step: 2100  \tTraining accuracy: 0.7750582098960876\n",
      "Step: 2100  \tValid loss: 0.4201776087284088\n",
      "Step: 2200  \tTraining loss: 0.4147360920906067\n",
      "Step: 2200  \tTraining accuracy: 0.7760284543037415\n",
      "Step: 2200  \tValid loss: 0.42006614804267883\n",
      "Step: 2300  \tTraining loss: 0.41455626487731934\n",
      "Step: 2300  \tTraining accuracy: 0.7769124507904053\n",
      "Step: 2300  \tValid loss: 0.41987520456314087\n",
      "Step: 2400  \tTraining loss: 0.41428080201148987\n",
      "Step: 2400  \tTraining accuracy: 0.7777212262153625\n",
      "Step: 2400  \tValid loss: 0.41981273889541626\n",
      "Step: 2500  \tTraining loss: 0.4140000343322754\n",
      "Step: 2500  \tTraining accuracy: 0.7784639596939087\n",
      "Step: 2500  \tValid loss: 0.41950997710227966\n",
      "Step: 2600  \tTraining loss: 0.41366830468177795\n",
      "Step: 2600  \tTraining accuracy: 0.7791484594345093\n",
      "Step: 2600  \tValid loss: 0.4191630780696869\n",
      "Step: 2700  \tTraining loss: 0.4132811725139618\n",
      "Step: 2700  \tTraining accuracy: 0.7797812819480896\n",
      "Step: 2700  \tValid loss: 0.4188280701637268\n",
      "Step: 2800  \tTraining loss: 0.4129035472869873\n",
      "Step: 2800  \tTraining accuracy: 0.7803680896759033\n",
      "Step: 2800  \tValid loss: 0.4184543192386627\n",
      "Step: 2900  \tTraining loss: 0.4125380516052246\n",
      "Step: 2900  \tTraining accuracy: 0.7809137105941772\n",
      "Step: 2900  \tValid loss: 0.418124258518219\n",
      "Step: 3000  \tTraining loss: 0.4121781587600708\n",
      "Step: 3000  \tTraining accuracy: 0.7814223170280457\n",
      "Step: 3000  \tValid loss: 0.41774335503578186\n",
      "Step: 3100  \tTraining loss: 0.4118342399597168\n",
      "Step: 3100  \tTraining accuracy: 0.7818976044654846\n",
      "Step: 3100  \tValid loss: 0.4173618257045746\n",
      "Step: 3200  \tTraining loss: 0.4114847779273987\n",
      "Step: 3200  \tTraining accuracy: 0.7823427319526672\n",
      "Step: 3200  \tValid loss: 0.4171014130115509\n",
      "Step: 3300  \tTraining loss: 0.411099374294281\n",
      "Step: 3300  \tTraining accuracy: 0.7827604413032532\n",
      "Step: 3300  \tValid loss: 0.41658350825309753\n",
      "Step: 3400  \tTraining loss: 0.4106621742248535\n",
      "Step: 3400  \tTraining accuracy: 0.7831847071647644\n",
      "Step: 3400  \tValid loss: 0.4161681532859802\n",
      "Step: 3500  \tTraining loss: 0.4102564752101898\n",
      "Step: 3500  \tTraining accuracy: 0.7835946083068848\n",
      "Step: 3500  \tValid loss: 0.41567400097846985\n",
      "Step: 3600  \tTraining loss: 0.40989333391189575\n",
      "Step: 3600  \tTraining accuracy: 0.7839813828468323\n",
      "Step: 3600  \tValid loss: 0.415225088596344\n",
      "Step: 3700  \tTraining loss: 0.4095456302165985\n",
      "Step: 3700  \tTraining accuracy: 0.7843469977378845\n",
      "Step: 3700  \tValid loss: 0.4148412048816681\n",
      "Step: 3800  \tTraining loss: 0.4092394709587097\n",
      "Step: 3800  \tTraining accuracy: 0.7847431302070618\n",
      "Step: 3800  \tValid loss: 0.41443613171577454\n",
      "Step: 3900  \tTraining loss: 0.40896451473236084\n",
      "Step: 3900  \tTraining accuracy: 0.7851186990737915\n",
      "Step: 3900  \tValid loss: 0.4140712320804596\n",
      "Step: 4000  \tTraining loss: 0.40872061252593994\n",
      "Step: 4000  \tTraining accuracy: 0.7854752540588379\n",
      "Step: 4000  \tValid loss: 0.4138115346431732\n",
      "Step: 4100  \tTraining loss: 0.40850162506103516\n",
      "Step: 4100  \tTraining accuracy: 0.785825788974762\n",
      "Step: 4100  \tValid loss: 0.4135119318962097\n",
      "Step: 4200  \tTraining loss: 0.4083023965358734\n",
      "Step: 4200  \tTraining accuracy: 0.7861679196357727\n",
      "Step: 4200  \tValid loss: 0.4133317172527313\n",
      "Step: 4300  \tTraining loss: 0.40812283754348755\n",
      "Step: 4300  \tTraining accuracy: 0.7864938974380493\n",
      "Step: 4300  \tValid loss: 0.4131549298763275\n",
      "Step: 4400  \tTraining loss: 0.4079582989215851\n",
      "Step: 4400  \tTraining accuracy: 0.7868291735649109\n",
      "Step: 4400  \tValid loss: 0.4129292368888855\n",
      "Step: 4500  \tTraining loss: 0.40782707929611206\n",
      "Step: 4500  \tTraining accuracy: 0.7871415019035339\n",
      "Step: 4500  \tValid loss: 0.4126509130001068\n",
      "Step: 4600  \tTraining loss: 0.4077119827270508\n",
      "Step: 4600  \tTraining accuracy: 0.7874168753623962\n",
      "Step: 4600  \tValid loss: 0.4126662611961365\n",
      "Step: 4700  \tTraining loss: 0.40762677788734436\n",
      "Step: 4700  \tTraining accuracy: 0.7876880168914795\n",
      "Step: 4700  \tValid loss: 0.41257598996162415\n",
      "Step: 4800  \tTraining loss: 0.4075515866279602\n",
      "Step: 4800  \tTraining accuracy: 0.7879477143287659\n",
      "Step: 4800  \tValid loss: 0.4125107526779175\n",
      "Step: 4900  \tTraining loss: 0.4074860215187073\n",
      "Step: 4900  \tTraining accuracy: 0.7881966829299927\n",
      "Step: 4900  \tValid loss: 0.4123908281326294\n",
      "Step: 5000  \tTraining loss: 0.40742647647857666\n",
      "Step: 5000  \tTraining accuracy: 0.788456916809082\n",
      "Step: 5000  \tValid loss: 0.4123421609401703\n",
      "Step: 5100  \tTraining loss: 0.4073711037635803\n",
      "Step: 5100  \tTraining accuracy: 0.7886999249458313\n",
      "Step: 5100  \tValid loss: 0.41228264570236206\n",
      "Step: 5200  \tTraining loss: 0.4073188900947571\n",
      "Step: 5200  \tTraining accuracy: 0.7889334559440613\n",
      "Step: 5200  \tValid loss: 0.41225069761276245\n",
      "Step: 5300  \tTraining loss: 0.40726956725120544\n",
      "Step: 5300  \tTraining accuracy: 0.7891581058502197\n",
      "Step: 5300  \tValid loss: 0.4122002124786377\n",
      "Step: 5400  \tTraining loss: 0.40722474455833435\n",
      "Step: 5400  \tTraining accuracy: 0.7893743515014648\n",
      "Step: 5400  \tValid loss: 0.41212818026542664\n",
      "Step: 5500  \tTraining loss: 0.40717655420303345\n",
      "Step: 5500  \tTraining accuracy: 0.7895826697349548\n",
      "Step: 5500  \tValid loss: 0.41207486391067505\n",
      "Step: 5600  \tTraining loss: 0.4071323275566101\n",
      "Step: 5600  \tTraining accuracy: 0.7897602319717407\n",
      "Step: 5600  \tValid loss: 0.41206610202789307\n",
      "Step: 5700  \tTraining loss: 0.4070869982242584\n",
      "Step: 5700  \tTraining accuracy: 0.7898879051208496\n",
      "Step: 5700  \tValid loss: 0.41202831268310547\n",
      "Step: 5800  \tTraining loss: 0.40703895688056946\n",
      "Step: 5800  \tTraining accuracy: 0.7900111079216003\n",
      "Step: 5800  \tValid loss: 0.411996454000473\n",
      "Step: 5900  \tTraining loss: 0.40698134899139404\n",
      "Step: 5900  \tTraining accuracy: 0.7901301383972168\n",
      "Step: 5900  \tValid loss: 0.4119735062122345\n",
      "Step: 6000  \tTraining loss: 0.406909704208374\n",
      "Step: 6000  \tTraining accuracy: 0.7902451753616333\n",
      "Step: 6000  \tValid loss: 0.4119000732898712\n",
      "Step: 6100  \tTraining loss: 0.4068147838115692\n",
      "Step: 6100  \tTraining accuracy: 0.7903563976287842\n",
      "Step: 6100  \tValid loss: 0.411885529756546\n",
      "Step: 6200  \tTraining loss: 0.406704843044281\n",
      "Step: 6200  \tTraining accuracy: 0.7904639840126038\n",
      "Step: 6200  \tValid loss: 0.4118565320968628\n",
      "Step: 6300  \tTraining loss: 0.40648192167282104\n",
      "Step: 6300  \tTraining accuracy: 0.7905718684196472\n",
      "Step: 6300  \tValid loss: 0.41201403737068176\n",
      "Step: 6400  \tTraining loss: 0.4061926007270813\n",
      "Step: 6400  \tTraining accuracy: 0.7907207608222961\n",
      "Step: 6400  \tValid loss: 0.4120611548423767\n",
      "Step: 6500  \tTraining loss: 0.40583306550979614\n",
      "Step: 6500  \tTraining accuracy: 0.7908649444580078\n",
      "Step: 6500  \tValid loss: 0.4119795858860016\n",
      "Step: 6600  \tTraining loss: 0.4056650996208191\n",
      "Step: 6600  \tTraining accuracy: 0.7910047769546509\n",
      "Step: 6600  \tValid loss: 0.4121015965938568\n",
      "Step: 6700  \tTraining loss: 0.4055604934692383\n",
      "Step: 6700  \tTraining accuracy: 0.7911404371261597\n",
      "Step: 6700  \tValid loss: 0.4121495485305786\n",
      "Step: 6800  \tTraining loss: 0.4054681956768036\n",
      "Step: 6800  \tTraining accuracy: 0.791271984577179\n",
      "Step: 6800  \tValid loss: 0.41218996047973633\n",
      "Step: 6900  \tTraining loss: 0.4053843021392822\n",
      "Step: 6900  \tTraining accuracy: 0.7913929224014282\n",
      "Step: 6900  \tValid loss: 0.4122103154659271\n",
      "Step: 7000  \tTraining loss: 0.4053051173686981\n",
      "Step: 7000  \tTraining accuracy: 0.7915170788764954\n",
      "Step: 7000  \tValid loss: 0.41223347187042236\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7916378\n",
      "Precision: 0.8436782\n",
      "Recall: 0.90701264\n",
      "F1 score: 0.8202767\n",
      "AUC: 0.68839914\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.791638   0.843678  0.907013  0.820277  0.688399  0.405231      0.791565   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.411836       0.791518   0.413577      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  7095.0  \n",
      "2\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_31983/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(754, 3)\n",
      "(754, 1)\n",
      "(416, 3)\n",
      "(416, 1)\n",
      "(338, 3)\n",
      "(338, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6263323426246643\n",
      "Step: 100  \tTraining accuracy: 0.6273209452629089\n",
      "Step: 100  \tValid loss: 0.5753650665283203\n",
      "Step: 200  \tTraining loss: 0.6012600660324097\n",
      "Step: 200  \tTraining accuracy: 0.6520777940750122\n",
      "Step: 200  \tValid loss: 0.5694323778152466\n",
      "Step: 300  \tTraining loss: 0.5920405387878418\n",
      "Step: 300  \tTraining accuracy: 0.6583554148674011\n",
      "Step: 300  \tValid loss: 0.5707835555076599\n",
      "Step: 400  \tTraining loss: 0.5866808295249939\n",
      "Step: 400  \tTraining accuracy: 0.6576354503631592\n",
      "Step: 400  \tValid loss: 0.5705583095550537\n",
      "Step: 500  \tTraining loss: 0.58162921667099\n",
      "Step: 500  \tTraining accuracy: 0.6575301885604858\n",
      "Step: 500  \tValid loss: 0.5667098760604858\n",
      "Step: 600  \tTraining loss: 0.5760268568992615\n",
      "Step: 600  \tTraining accuracy: 0.6586689352989197\n",
      "Step: 600  \tValid loss: 0.561038076877594\n",
      "Step: 700  \tTraining loss: 0.571063756942749\n",
      "Step: 700  \tTraining accuracy: 0.661293625831604\n",
      "Step: 700  \tValid loss: 0.5560649037361145\n",
      "Step: 800  \tTraining loss: 0.5675008893013\n",
      "Step: 800  \tTraining accuracy: 0.6645446419715881\n",
      "Step: 800  \tValid loss: 0.5525791645050049\n",
      "Step: 900  \tTraining loss: 0.5652486085891724\n",
      "Step: 900  \tTraining accuracy: 0.6674988269805908\n",
      "Step: 900  \tValid loss: 0.5507465600967407\n",
      "Step: 1000  \tTraining loss: 0.5633975863456726\n",
      "Step: 1000  \tTraining accuracy: 0.670249879360199\n",
      "Step: 1000  \tValid loss: 0.5506880283355713\n",
      "Step: 1100  \tTraining loss: 0.5627231001853943\n",
      "Step: 1100  \tTraining accuracy: 0.6728558540344238\n",
      "Step: 1100  \tValid loss: 0.5509199500083923\n",
      "Step: 1200  \tTraining loss: 0.5614285469055176\n",
      "Step: 1200  \tTraining accuracy: 0.6752969622612\n",
      "Step: 1200  \tValid loss: 0.551327645778656\n",
      "Step: 1300  \tTraining loss: 0.5609304904937744\n",
      "Step: 1300  \tTraining accuracy: 0.6774535775184631\n",
      "Step: 1300  \tValid loss: 0.5520563125610352\n",
      "Step: 1400  \tTraining loss: 0.5607017278671265\n",
      "Step: 1400  \tTraining accuracy: 0.6792907118797302\n",
      "Step: 1400  \tValid loss: 0.5526498556137085\n",
      "Step: 1500  \tTraining loss: 0.5606021285057068\n",
      "Step: 1500  \tTraining accuracy: 0.6808744072914124\n",
      "Step: 1500  \tValid loss: 0.5528684854507446\n",
      "Step: 1600  \tTraining loss: 0.560537576675415\n",
      "Step: 1600  \tTraining accuracy: 0.6821682453155518\n",
      "Step: 1600  \tValid loss: 0.5529478788375854\n",
      "Step: 1700  \tTraining loss: 0.5604910850524902\n",
      "Step: 1700  \tTraining accuracy: 0.6833855509757996\n",
      "Step: 1700  \tValid loss: 0.5531398057937622\n",
      "Step: 1800  \tTraining loss: 0.5604301691055298\n",
      "Step: 1800  \tTraining accuracy: 0.6844637989997864\n",
      "Step: 1800  \tValid loss: 0.5531362295150757\n",
      "Step: 1900  \tTraining loss: 0.5603645443916321\n",
      "Step: 1900  \tTraining accuracy: 0.6854254603385925\n",
      "Step: 1900  \tValid loss: 0.5530099272727966\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.68628854\n",
      "Precision: 0.72962964\n",
      "Recall: 0.832981\n",
      "F1 score: 0.73167974\n",
      "AUC: 0.656704\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.686289    0.72963  0.832981   0.73168  0.656704  0.560339      0.685886   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.550302       0.685437   0.578105      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  1944.0  \n",
      "3\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_94509/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(783, 3)\n",
      "(783, 1)\n",
      "(416, 3)\n",
      "(416, 1)\n",
      "(338, 3)\n",
      "(338, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5314882397651672\n",
      "Step: 100  \tTraining accuracy: 0.7522349953651428\n",
      "Step: 100  \tValid loss: 0.5975150465965271\n",
      "Step: 200  \tTraining loss: 0.5221245288848877\n",
      "Step: 200  \tTraining accuracy: 0.7431034445762634\n",
      "Step: 200  \tValid loss: 0.5906382203102112\n",
      "Step: 300  \tTraining loss: 0.5032116174697876\n",
      "Step: 300  \tTraining accuracy: 0.741249680519104\n",
      "Step: 300  \tValid loss: 0.5721123814582825\n",
      "Step: 400  \tTraining loss: 0.4810274541378021\n",
      "Step: 400  \tTraining accuracy: 0.7404523491859436\n",
      "Step: 400  \tValid loss: 0.5481474995613098\n",
      "Step: 500  \tTraining loss: 0.46467456221580505\n",
      "Step: 500  \tTraining accuracy: 0.7434713840484619\n",
      "Step: 500  \tValid loss: 0.5320689082145691\n",
      "Step: 600  \tTraining loss: 0.4510752558708191\n",
      "Step: 600  \tTraining accuracy: 0.7470477223396301\n",
      "Step: 600  \tValid loss: 0.5182134509086609\n",
      "Step: 700  \tTraining loss: 0.432783842086792\n",
      "Step: 700  \tTraining accuracy: 0.7525237202644348\n",
      "Step: 700  \tValid loss: 0.5009050369262695\n",
      "Step: 800  \tTraining loss: 0.4071941077709198\n",
      "Step: 800  \tTraining accuracy: 0.7568012475967407\n",
      "Step: 800  \tValid loss: 0.47803157567977905\n",
      "Step: 900  \tTraining loss: 0.3933747112751007\n",
      "Step: 900  \tTraining accuracy: 0.7613732218742371\n",
      "Step: 900  \tValid loss: 0.46521222591400146\n",
      "Step: 1000  \tTraining loss: 0.384888231754303\n",
      "Step: 1000  \tTraining accuracy: 0.7649151682853699\n",
      "Step: 1000  \tValid loss: 0.4591580033302307\n",
      "Step: 1100  \tTraining loss: 0.3780501186847687\n",
      "Step: 1100  \tTraining accuracy: 0.7680926322937012\n",
      "Step: 1100  \tValid loss: 0.4528241753578186\n",
      "Step: 1200  \tTraining loss: 0.3725818693637848\n",
      "Step: 1200  \tTraining accuracy: 0.771226704120636\n",
      "Step: 1200  \tValid loss: 0.44966375827789307\n",
      "Step: 1300  \tTraining loss: 0.36742672324180603\n",
      "Step: 1300  \tTraining accuracy: 0.7743797898292542\n",
      "Step: 1300  \tValid loss: 0.447997510433197\n",
      "Step: 1400  \tTraining loss: 0.36245235800743103\n",
      "Step: 1400  \tTraining accuracy: 0.7770660519599915\n",
      "Step: 1400  \tValid loss: 0.4454447627067566\n",
      "Step: 1500  \tTraining loss: 0.35590457916259766\n",
      "Step: 1500  \tTraining accuracy: 0.779606282711029\n",
      "Step: 1500  \tValid loss: 0.44155266880989075\n",
      "Step: 1600  \tTraining loss: 0.35121726989746094\n",
      "Step: 1600  \tTraining accuracy: 0.7819448113441467\n",
      "Step: 1600  \tValid loss: 0.4378809928894043\n",
      "Step: 1700  \tTraining loss: 0.3485739231109619\n",
      "Step: 1700  \tTraining accuracy: 0.7841182351112366\n",
      "Step: 1700  \tValid loss: 0.43672847747802734\n",
      "Step: 1800  \tTraining loss: 0.3469105660915375\n",
      "Step: 1800  \tTraining accuracy: 0.7863035202026367\n",
      "Step: 1800  \tValid loss: 0.4361804723739624\n",
      "Step: 1900  \tTraining loss: 0.34588733315467834\n",
      "Step: 1900  \tTraining accuracy: 0.7882526516914368\n",
      "Step: 1900  \tValid loss: 0.4362931251525879\n",
      "Step: 2000  \tTraining loss: 0.3451874852180481\n",
      "Step: 2000  \tTraining accuracy: 0.7900019884109497\n",
      "Step: 2000  \tValid loss: 0.4366132318973541\n",
      "Step: 2100  \tTraining loss: 0.34468385577201843\n",
      "Step: 2100  \tTraining accuracy: 0.7915807366371155\n",
      "Step: 2100  \tValid loss: 0.4371609389781952\n",
      "Step: 2200  \tTraining loss: 0.344224750995636\n",
      "Step: 2200  \tTraining accuracy: 0.7930126786231995\n",
      "Step: 2200  \tValid loss: 0.43698829412460327\n",
      "Step: 2300  \tTraining loss: 0.34383633732795715\n",
      "Step: 2300  \tTraining accuracy: 0.7943174242973328\n",
      "Step: 2300  \tValid loss: 0.4370051622390747\n",
      "Step: 2400  \tTraining loss: 0.34345129132270813\n",
      "Step: 2400  \tTraining accuracy: 0.7955111265182495\n",
      "Step: 2400  \tValid loss: 0.4370339810848236\n",
      "Step: 2500  \tTraining loss: 0.3430669903755188\n",
      "Step: 2500  \tTraining accuracy: 0.796607494354248\n",
      "Step: 2500  \tValid loss: 0.43705442547798157\n",
      "Step: 2600  \tTraining loss: 0.3426908552646637\n",
      "Step: 2600  \tTraining accuracy: 0.7976178526878357\n",
      "Step: 2600  \tValid loss: 0.4372381865978241\n",
      "Step: 2700  \tTraining loss: 0.34231892228126526\n",
      "Step: 2700  \tTraining accuracy: 0.7985519766807556\n",
      "Step: 2700  \tValid loss: 0.43723243474960327\n",
      "Step: 2800  \tTraining loss: 0.3418361246585846\n",
      "Step: 2800  \tTraining accuracy: 0.7994182109832764\n",
      "Step: 2800  \tValid loss: 0.4367702901363373\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8002465\n",
      "Precision: 0.8817734\n",
      "Recall: 0.9117148\n",
      "F1 score: 0.81360614\n",
      "AUC: 0.7702904\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0  0.800246   0.881773  0.911715  0.813606  0.77029  0.341675      0.799493   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.435886       0.799484   0.393307      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  2835.0  \n",
      "4\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_35723/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(725, 3)\n",
      "(725, 1)\n",
      "(400, 3)\n",
      "(400, 1)\n",
      "(325, 3)\n",
      "(325, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.615144670009613\n",
      "Step: 100  \tTraining accuracy: 0.8675861954689026\n",
      "Step: 100  \tValid loss: 0.62337726354599\n",
      "Step: 200  \tTraining loss: 0.48515549302101135\n",
      "Step: 200  \tTraining accuracy: 0.8505747318267822\n",
      "Step: 200  \tValid loss: 0.5108481049537659\n",
      "Step: 300  \tTraining loss: 0.43144550919532776\n",
      "Step: 300  \tTraining accuracy: 0.8328275680541992\n",
      "Step: 300  \tValid loss: 0.472306489944458\n",
      "Step: 400  \tTraining loss: 0.4100683033466339\n",
      "Step: 400  \tTraining accuracy: 0.814975380897522\n",
      "Step: 400  \tValid loss: 0.460165798664093\n",
      "Step: 500  \tTraining loss: 0.3942187428474426\n",
      "Step: 500  \tTraining accuracy: 0.8049042224884033\n",
      "Step: 500  \tValid loss: 0.45002731680870056\n",
      "Step: 600  \tTraining loss: 0.3803699016571045\n",
      "Step: 600  \tTraining accuracy: 0.7998746037483215\n",
      "Step: 600  \tValid loss: 0.4401722848415375\n",
      "Step: 700  \tTraining loss: 0.3680582046508789\n",
      "Step: 700  \tTraining accuracy: 0.7962864637374878\n",
      "Step: 700  \tValid loss: 0.4308881461620331\n",
      "Step: 800  \tTraining loss: 0.35694095492362976\n",
      "Step: 800  \tTraining accuracy: 0.7936551570892334\n",
      "Step: 800  \tValid loss: 0.42198750376701355\n",
      "Step: 900  \tTraining loss: 0.34666404128074646\n",
      "Step: 900  \tTraining accuracy: 0.7914807200431824\n",
      "Step: 900  \tValid loss: 0.41331085562705994\n",
      "Step: 1000  \tTraining loss: 0.3369121551513672\n",
      "Step: 1000  \tTraining accuracy: 0.7903448343276978\n",
      "Step: 1000  \tValid loss: 0.404703289270401\n",
      "Step: 1100  \tTraining loss: 0.3274223506450653\n",
      "Step: 1100  \tTraining accuracy: 0.789556622505188\n",
      "Step: 1100  \tValid loss: 0.3961268365383148\n",
      "Step: 1200  \tTraining loss: 0.3176271915435791\n",
      "Step: 1200  \tTraining accuracy: 0.7892053723335266\n",
      "Step: 1200  \tValid loss: 0.3869577646255493\n",
      "Step: 1300  \tTraining loss: 0.30766135454177856\n",
      "Step: 1300  \tTraining accuracy: 0.7906206846237183\n",
      "Step: 1300  \tValid loss: 0.378306120634079\n",
      "Step: 1400  \tTraining loss: 0.29809698462486267\n",
      "Step: 1400  \tTraining accuracy: 0.7931545376777649\n",
      "Step: 1400  \tValid loss: 0.36899879574775696\n",
      "Step: 1500  \tTraining loss: 0.28291380405426025\n",
      "Step: 1500  \tTraining accuracy: 0.796718180179596\n",
      "Step: 1500  \tValid loss: 0.353881299495697\n",
      "Step: 1600  \tTraining loss: 0.27254390716552734\n",
      "Step: 1600  \tTraining accuracy: 0.800934374332428\n",
      "Step: 1600  \tValid loss: 0.34534817934036255\n",
      "Step: 1700  \tTraining loss: 0.2654387652873993\n",
      "Step: 1700  \tTraining accuracy: 0.8047648668289185\n",
      "Step: 1700  \tValid loss: 0.33967849612236023\n",
      "Step: 1800  \tTraining loss: 0.259816437959671\n",
      "Step: 1800  \tTraining accuracy: 0.8083152770996094\n",
      "Step: 1800  \tValid loss: 0.3352852761745453\n",
      "Step: 1900  \tTraining loss: 0.25532999634742737\n",
      "Step: 1900  \tTraining accuracy: 0.8114818334579468\n",
      "Step: 1900  \tValid loss: 0.3319818079471588\n",
      "Step: 2000  \tTraining loss: 0.25169482827186584\n",
      "Step: 2000  \tTraining accuracy: 0.8143236041069031\n",
      "Step: 2000  \tValid loss: 0.3294939398765564\n",
      "Step: 2100  \tTraining loss: 0.24869172275066376\n",
      "Step: 2100  \tTraining accuracy: 0.816888153553009\n",
      "Step: 2100  \tValid loss: 0.32757890224456787\n",
      "Step: 2200  \tTraining loss: 0.2461743950843811\n",
      "Step: 2200  \tTraining accuracy: 0.8193424344062805\n",
      "Step: 2200  \tValid loss: 0.32612183690071106\n",
      "Step: 2300  \tTraining loss: 0.24403676390647888\n",
      "Step: 2300  \tTraining accuracy: 0.8216398358345032\n",
      "Step: 2300  \tValid loss: 0.32496216893196106\n",
      "Step: 2400  \tTraining loss: 0.24218998849391937\n",
      "Step: 2400  \tTraining accuracy: 0.8238297700881958\n",
      "Step: 2400  \tValid loss: 0.3240244686603546\n",
      "Step: 2500  \tTraining loss: 0.24056144058704376\n",
      "Step: 2500  \tTraining accuracy: 0.825869083404541\n",
      "Step: 2500  \tValid loss: 0.3232462406158447\n",
      "Step: 2600  \tTraining loss: 0.23910827934741974\n",
      "Step: 2600  \tTraining accuracy: 0.8278295993804932\n",
      "Step: 2600  \tValid loss: 0.3226017653942108\n",
      "Step: 2700  \tTraining loss: 0.2377973198890686\n",
      "Step: 2700  \tTraining accuracy: 0.8296421766281128\n",
      "Step: 2700  \tValid loss: 0.3220243752002716\n",
      "Step: 2800  \tTraining loss: 0.2365986406803131\n",
      "Step: 2800  \tTraining accuracy: 0.8313730359077454\n",
      "Step: 2800  \tValid loss: 0.32154926657676697\n",
      "Step: 2900  \tTraining loss: 0.2354988008737564\n",
      "Step: 2900  \tTraining accuracy: 0.832909882068634\n",
      "Step: 2900  \tValid loss: 0.32113197445869446\n",
      "Step: 3000  \tTraining loss: 0.2344851791858673\n",
      "Step: 3000  \tTraining accuracy: 0.8343424797058105\n",
      "Step: 3000  \tValid loss: 0.320746511220932\n",
      "Step: 3100  \tTraining loss: 0.23354536294937134\n",
      "Step: 3100  \tTraining accuracy: 0.8356812000274658\n",
      "Step: 3100  \tValid loss: 0.32042449712753296\n",
      "Step: 3200  \tTraining loss: 0.23267309367656708\n",
      "Step: 3200  \tTraining accuracy: 0.8370005488395691\n",
      "Step: 3200  \tValid loss: 0.32012051343917847\n",
      "Step: 3300  \tTraining loss: 0.23186036944389343\n",
      "Step: 3300  \tTraining accuracy: 0.8382174968719482\n",
      "Step: 3300  \tValid loss: 0.31992462277412415\n",
      "Step: 3400  \tTraining loss: 0.23110118508338928\n",
      "Step: 3400  \tTraining accuracy: 0.8393824100494385\n",
      "Step: 3400  \tValid loss: 0.3195120692253113\n",
      "Step: 3500  \tTraining loss: 0.23038792610168457\n",
      "Step: 3500  \tTraining accuracy: 0.8404797315597534\n",
      "Step: 3500  \tValid loss: 0.31926771998405457\n",
      "Step: 3600  \tTraining loss: 0.22972510755062103\n",
      "Step: 3600  \tTraining accuracy: 0.841515302658081\n",
      "Step: 3600  \tValid loss: 0.3192020356655121\n",
      "Step: 3700  \tTraining loss: 0.2291097342967987\n",
      "Step: 3700  \tTraining accuracy: 0.8424940705299377\n",
      "Step: 3700  \tValid loss: 0.3191767930984497\n",
      "Step: 3800  \tTraining loss: 0.22853799164295197\n",
      "Step: 3800  \tTraining accuracy: 0.8434023261070251\n",
      "Step: 3800  \tValid loss: 0.3191835880279541\n",
      "Step: 3900  \tTraining loss: 0.22800633311271667\n",
      "Step: 3900  \tTraining accuracy: 0.8442633152008057\n",
      "Step: 3900  \tValid loss: 0.31923142075538635\n",
      "Step: 4000  \tTraining loss: 0.22751013934612274\n",
      "Step: 4000  \tTraining accuracy: 0.8450807332992554\n",
      "Step: 4000  \tValid loss: 0.3192954957485199\n",
      "Step: 4100  \tTraining loss: 0.2270466536283493\n",
      "Step: 4100  \tTraining accuracy: 0.8458577990531921\n",
      "Step: 4100  \tValid loss: 0.3193906545639038\n",
      "Step: 4200  \tTraining loss: 0.22661246359348297\n",
      "Step: 4200  \tTraining accuracy: 0.84659743309021\n",
      "Step: 4200  \tValid loss: 0.319515585899353\n",
      "Step: 4300  \tTraining loss: 0.22621935606002808\n",
      "Step: 4300  \tTraining accuracy: 0.847302258014679\n",
      "Step: 4300  \tValid loss: 0.31924968957901\n",
      "Step: 4400  \tTraining loss: 0.2258194088935852\n",
      "Step: 4400  \tTraining accuracy: 0.8479746580123901\n",
      "Step: 4400  \tValid loss: 0.3196520507335663\n",
      "Step: 4500  \tTraining loss: 0.22544503211975098\n",
      "Step: 4500  \tTraining accuracy: 0.8486168384552002\n",
      "Step: 4500  \tValid loss: 0.31990182399749756\n",
      "Step: 4600  \tTraining loss: 0.22508856654167175\n",
      "Step: 4600  \tTraining accuracy: 0.8492307662963867\n",
      "Step: 4600  \tValid loss: 0.32006266713142395\n",
      "Step: 4700  \tTraining loss: 0.22474485635757446\n",
      "Step: 4700  \tTraining accuracy: 0.8498479723930359\n",
      "Step: 4700  \tValid loss: 0.3201957643032074\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8504392\n",
      "Precision: 0.86989796\n",
      "Recall: 0.94722223\n",
      "F1 score: 0.8866281\n",
      "AUC: 0.9037481\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.850439   0.869898  0.947222  0.886628  0.903748  0.224574      0.849875   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.319174       0.849904   0.293944      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  4750.0  \n",
      "5\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_13416/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(841, 3)\n",
      "(841, 1)\n",
      "(464, 3)\n",
      "(464, 1)\n",
      "(377, 3)\n",
      "(377, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6719677448272705\n",
      "Step: 100  \tTraining accuracy: 0.5624256730079651\n",
      "Step: 100  \tValid loss: 0.6559136509895325\n",
      "Step: 200  \tTraining loss: 0.6605202555656433\n",
      "Step: 200  \tTraining accuracy: 0.5862069129943848\n",
      "Step: 200  \tValid loss: 0.645237386226654\n",
      "Step: 300  \tTraining loss: 0.6477357745170593\n",
      "Step: 300  \tTraining accuracy: 0.5985731482505798\n",
      "Step: 300  \tValid loss: 0.6398431062698364\n",
      "Step: 400  \tTraining loss: 0.6350820064544678\n",
      "Step: 400  \tTraining accuracy: 0.609308660030365\n",
      "Step: 400  \tValid loss: 0.6338953375816345\n",
      "Step: 500  \tTraining loss: 0.6247261166572571\n",
      "Step: 500  \tTraining accuracy: 0.6163297891616821\n",
      "Step: 500  \tValid loss: 0.6287651658058167\n",
      "Step: 600  \tTraining loss: 0.6178786754608154\n",
      "Step: 600  \tTraining accuracy: 0.62220299243927\n",
      "Step: 600  \tValid loss: 0.6257103085517883\n",
      "Step: 700  \tTraining loss: 0.6136744618415833\n",
      "Step: 700  \tTraining accuracy: 0.6270008087158203\n",
      "Step: 700  \tValid loss: 0.6235692501068115\n",
      "Step: 800  \tTraining loss: 0.6106497645378113\n",
      "Step: 800  \tTraining accuracy: 0.6298058032989502\n",
      "Step: 800  \tValid loss: 0.6212723851203918\n",
      "Step: 900  \tTraining loss: 0.6078184843063354\n",
      "Step: 900  \tTraining accuracy: 0.6319507360458374\n",
      "Step: 900  \tValid loss: 0.6178018450737\n",
      "Step: 1000  \tTraining loss: 0.6048480272293091\n",
      "Step: 1000  \tTraining accuracy: 0.633644163608551\n",
      "Step: 1000  \tValid loss: 0.612718403339386\n",
      "Step: 1100  \tTraining loss: 0.6025009751319885\n",
      "Step: 1100  \tTraining accuracy: 0.6351282596588135\n",
      "Step: 1100  \tValid loss: 0.6083589792251587\n",
      "Step: 1200  \tTraining loss: 0.6006824970245361\n",
      "Step: 1200  \tTraining accuracy: 0.6363542079925537\n",
      "Step: 1200  \tValid loss: 0.6056652069091797\n",
      "Step: 1300  \tTraining loss: 0.5993257761001587\n",
      "Step: 1300  \tTraining accuracy: 0.6378597021102905\n",
      "Step: 1300  \tValid loss: 0.6028859615325928\n",
      "Step: 1400  \tTraining loss: 0.5982959866523743\n",
      "Step: 1400  \tTraining accuracy: 0.640066921710968\n",
      "Step: 1400  \tValid loss: 0.6001576781272888\n",
      "Step: 1500  \tTraining loss: 0.5975746512413025\n",
      "Step: 1500  \tTraining accuracy: 0.6418877243995667\n",
      "Step: 1500  \tValid loss: 0.5978543758392334\n",
      "Step: 1600  \tTraining loss: 0.5971011519432068\n",
      "Step: 1600  \tTraining accuracy: 0.6433201432228088\n",
      "Step: 1600  \tValid loss: 0.5969154238700867\n",
      "Step: 1700  \tTraining loss: 0.5967729687690735\n",
      "Step: 1700  \tTraining accuracy: 0.6445789933204651\n",
      "Step: 1700  \tValid loss: 0.5957162976264954\n",
      "Step: 1800  \tTraining loss: 0.5965186953544617\n",
      "Step: 1800  \tTraining accuracy: 0.6459317207336426\n",
      "Step: 1800  \tValid loss: 0.594622015953064\n",
      "Step: 1900  \tTraining loss: 0.5963040590286255\n",
      "Step: 1900  \tTraining accuracy: 0.6472024917602539\n",
      "Step: 1900  \tValid loss: 0.5937848687171936\n",
      "Step: 2000  \tTraining loss: 0.5960447192192078\n",
      "Step: 2000  \tTraining accuracy: 0.6484344005584717\n",
      "Step: 2000  \tValid loss: 0.5927488803863525\n",
      "Step: 2100  \tTraining loss: 0.5957945585250854\n",
      "Step: 2100  \tTraining accuracy: 0.6496621370315552\n",
      "Step: 2100  \tValid loss: 0.5922440886497498\n",
      "Step: 2200  \tTraining loss: 0.5954592823982239\n",
      "Step: 2200  \tTraining accuracy: 0.6509692072868347\n",
      "Step: 2200  \tValid loss: 0.5915799736976624\n",
      "Step: 2300  \tTraining loss: 0.5949721336364746\n",
      "Step: 2300  \tTraining accuracy: 0.6521865725517273\n",
      "Step: 2300  \tValid loss: 0.5920813083648682\n",
      "Step: 2400  \tTraining loss: 0.5948173403739929\n",
      "Step: 2400  \tTraining accuracy: 0.6533002853393555\n",
      "Step: 2400  \tValid loss: 0.5919868350028992\n",
      "Step: 2500  \tTraining loss: 0.5947214365005493\n",
      "Step: 2500  \tTraining accuracy: 0.6543231010437012\n",
      "Step: 2500  \tValid loss: 0.5920516848564148\n",
      "Step: 2600  \tTraining loss: 0.5946356058120728\n",
      "Step: 2600  \tTraining accuracy: 0.6552656888961792\n",
      "Step: 2600  \tValid loss: 0.5920129418373108\n",
      "Step: 2700  \tTraining loss: 0.5945609211921692\n",
      "Step: 2700  \tTraining accuracy: 0.6561371088027954\n",
      "Step: 2700  \tValid loss: 0.5920867919921875\n",
      "Step: 2800  \tTraining loss: 0.5945055484771729\n",
      "Step: 2800  \tTraining accuracy: 0.6569451689720154\n",
      "Step: 2800  \tValid loss: 0.5919777154922485\n",
      "Step: 2900  \tTraining loss: 0.594458281993866\n",
      "Step: 2900  \tTraining accuracy: 0.657696545124054\n",
      "Step: 2900  \tValid loss: 0.5922643542289734\n",
      "Step: 3000  \tTraining loss: 0.5944108366966248\n",
      "Step: 3000  \tTraining accuracy: 0.6583969593048096\n",
      "Step: 3000  \tValid loss: 0.5922197699546814\n",
      "Step: 3100  \tTraining loss: 0.594366729259491\n",
      "Step: 3100  \tTraining accuracy: 0.6590514779090881\n",
      "Step: 3100  \tValid loss: 0.5922774076461792\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.6596644\n",
      "Precision: 0.7043121\n",
      "Recall: 0.7236287\n",
      "F1 score: 0.66858804\n",
      "AUC: 0.665629\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.659664   0.704312  0.723629  0.668588  0.665629  0.594326      0.659449   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.591566       0.659178   0.635747      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3196.0  \n",
      "6\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_25776/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(1131, 3)\n",
      "(1131, 1)\n",
      "(624, 3)\n",
      "(624, 1)\n",
      "(507, 3)\n",
      "(507, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5654425024986267\n",
      "Step: 100  \tTraining accuracy: 0.831122875213623\n",
      "Step: 100  \tValid loss: 0.5737563967704773\n",
      "Step: 200  \tTraining loss: 0.4433384835720062\n",
      "Step: 200  \tTraining accuracy: 0.8331859707832336\n",
      "Step: 200  \tValid loss: 0.4540248215198517\n",
      "Step: 300  \tTraining loss: 0.42280635237693787\n",
      "Step: 300  \tTraining accuracy: 0.8335986137390137\n",
      "Step: 300  \tValid loss: 0.4374741315841675\n",
      "Step: 400  \tTraining loss: 0.4127946197986603\n",
      "Step: 400  \tTraining accuracy: 0.8337754011154175\n",
      "Step: 400  \tValid loss: 0.4308621287345886\n",
      "Step: 500  \tTraining loss: 0.4055790305137634\n",
      "Step: 500  \tTraining accuracy: 0.8341683745384216\n",
      "Step: 500  \tValid loss: 0.42635491490364075\n",
      "Step: 600  \tTraining loss: 0.40024229884147644\n",
      "Step: 600  \tTraining accuracy: 0.8345792293548584\n",
      "Step: 600  \tValid loss: 0.42345505952835083\n",
      "Step: 700  \tTraining loss: 0.39629116654396057\n",
      "Step: 700  \tTraining accuracy: 0.8352037072181702\n",
      "Step: 700  \tValid loss: 0.4216940402984619\n",
      "Step: 800  \tTraining loss: 0.39323586225509644\n",
      "Step: 800  \tTraining accuracy: 0.8364868760108948\n",
      "Step: 800  \tValid loss: 0.4209327697753906\n",
      "Step: 900  \tTraining loss: 0.3909127116203308\n",
      "Step: 900  \tTraining accuracy: 0.8381962776184082\n",
      "Step: 900  \tValid loss: 0.42051222920417786\n",
      "Step: 1000  \tTraining loss: 0.38916248083114624\n",
      "Step: 1000  \tTraining accuracy: 0.8398715853691101\n",
      "Step: 1000  \tValid loss: 0.4203346371650696\n",
      "Step: 1100  \tTraining loss: 0.3878507912158966\n",
      "Step: 1100  \tTraining accuracy: 0.8412277102470398\n",
      "Step: 1100  \tValid loss: 0.4203238785266876\n",
      "Step: 1200  \tTraining loss: 0.38688138127326965\n",
      "Step: 1200  \tTraining accuracy: 0.842348039150238\n",
      "Step: 1200  \tValid loss: 0.4204915463924408\n",
      "Step: 1300  \tTraining loss: 0.3861493170261383\n",
      "Step: 1300  \tTraining accuracy: 0.8432891368865967\n",
      "Step: 1300  \tValid loss: 0.42069146037101746\n",
      "Step: 1400  \tTraining loss: 0.38557663559913635\n",
      "Step: 1400  \tTraining accuracy: 0.8441562652587891\n",
      "Step: 1400  \tValid loss: 0.42086371779441833\n",
      "Step: 1500  \tTraining loss: 0.38510802388191223\n",
      "Step: 1500  \tTraining accuracy: 0.844903826713562\n",
      "Step: 1500  \tValid loss: 0.4210186004638672\n",
      "Step: 1600  \tTraining loss: 0.3847044110298157\n",
      "Step: 1600  \tTraining accuracy: 0.8454978466033936\n",
      "Step: 1600  \tValid loss: 0.4210987687110901\n",
      "Step: 1700  \tTraining loss: 0.38434016704559326\n",
      "Step: 1700  \tTraining accuracy: 0.8458591103553772\n",
      "Step: 1700  \tValid loss: 0.4211087226867676\n",
      "Step: 1800  \tTraining loss: 0.3840017020702362\n",
      "Step: 1800  \tTraining accuracy: 0.846204400062561\n",
      "Step: 1800  \tValid loss: 0.42107558250427246\n",
      "Step: 1900  \tTraining loss: 0.38368237018585205\n",
      "Step: 1900  \tTraining accuracy: 0.8465123176574707\n",
      "Step: 1900  \tValid loss: 0.42101195454597473\n",
      "Step: 2000  \tTraining loss: 0.3833793103694916\n",
      "Step: 2000  \tTraining accuracy: 0.8467886447906494\n",
      "Step: 2000  \tValid loss: 0.42092788219451904\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.84703803\n",
      "Precision: 0.802935\n",
      "Recall: 0.83442265\n",
      "F1 score: 0.8633272\n",
      "AUC: 0.84727085\n",
      "   accuracy  precision    recall  f1_score       auc    loss  accuracy_val  \\\n",
      "0  0.847038   0.802935  0.834423  0.863327  0.847271  0.3832      0.846972   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.420276       0.847028   0.366738      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  2061.0  \n",
      "7\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_78058/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(986, 3)\n",
      "(986, 1)\n",
      "(544, 3)\n",
      "(544, 1)\n",
      "(442, 3)\n",
      "(442, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6529010534286499\n",
      "Step: 100  \tTraining accuracy: 0.6227180361747742\n",
      "Step: 100  \tValid loss: 0.6454319953918457\n",
      "Step: 200  \tTraining loss: 0.6131405234336853\n",
      "Step: 200  \tTraining accuracy: 0.6304935812950134\n",
      "Step: 200  \tValid loss: 0.5919963717460632\n",
      "Step: 300  \tTraining loss: 0.5763292908668518\n",
      "Step: 300  \tTraining accuracy: 0.6494929194450378\n",
      "Step: 300  \tValid loss: 0.539278507232666\n",
      "Step: 400  \tTraining loss: 0.5612725019454956\n",
      "Step: 400  \tTraining accuracy: 0.6740075349807739\n",
      "Step: 400  \tValid loss: 0.5123038291931152\n",
      "Step: 500  \tTraining loss: 0.5573777556419373\n",
      "Step: 500  \tTraining accuracy: 0.6887536644935608\n",
      "Step: 500  \tValid loss: 0.502503514289856\n",
      "Step: 600  \tTraining loss: 0.5561087727546692\n",
      "Step: 600  \tTraining accuracy: 0.6978609561920166\n",
      "Step: 600  \tValid loss: 0.49951064586639404\n",
      "Step: 700  \tTraining loss: 0.555637001991272\n",
      "Step: 700  \tTraining accuracy: 0.7040879726409912\n",
      "Step: 700  \tValid loss: 0.49844130873680115\n",
      "Step: 800  \tTraining loss: 0.555351734161377\n",
      "Step: 800  \tTraining accuracy: 0.708654522895813\n",
      "Step: 800  \tValid loss: 0.4980253279209137\n",
      "Step: 900  \tTraining loss: 0.5551435947418213\n",
      "Step: 900  \tTraining accuracy: 0.712146520614624\n",
      "Step: 900  \tValid loss: 0.49788016080856323\n",
      "Step: 1000  \tTraining loss: 0.5549451112747192\n",
      "Step: 1000  \tTraining accuracy: 0.7150101661682129\n",
      "Step: 1000  \tValid loss: 0.4978875517845154\n",
      "Step: 1100  \tTraining loss: 0.554753303527832\n",
      "Step: 1100  \tTraining accuracy: 0.7174732089042664\n",
      "Step: 1100  \tValid loss: 0.49795079231262207\n",
      "Step: 1200  \tTraining loss: 0.5545918345451355\n",
      "Step: 1200  \tTraining accuracy: 0.7195078730583191\n",
      "Step: 1200  \tValid loss: 0.4979873597621918\n",
      "Step: 1300  \tTraining loss: 0.5544338822364807\n",
      "Step: 1300  \tTraining accuracy: 0.7212170362472534\n",
      "Step: 1300  \tValid loss: 0.4980628490447998\n",
      "Step: 1400  \tTraining loss: 0.5542786121368408\n",
      "Step: 1400  \tTraining accuracy: 0.7227856516838074\n",
      "Step: 1400  \tValid loss: 0.4981206953525543\n",
      "Step: 1500  \tTraining loss: 0.5541132092475891\n",
      "Step: 1500  \tTraining accuracy: 0.7241379022598267\n",
      "Step: 1500  \tValid loss: 0.4981802701950073\n",
      "Step: 1600  \tTraining loss: 0.5539212822914124\n",
      "Step: 1600  \tTraining accuracy: 0.7253156900405884\n",
      "Step: 1600  \tValid loss: 0.49825453758239746\n",
      "Step: 1700  \tTraining loss: 0.5537132620811462\n",
      "Step: 1700  \tTraining accuracy: 0.726350724697113\n",
      "Step: 1700  \tValid loss: 0.4983172118663788\n",
      "Step: 1800  \tTraining loss: 0.5534251928329468\n",
      "Step: 1800  \tTraining accuracy: 0.7272384762763977\n",
      "Step: 1800  \tValid loss: 0.49838492274284363\n",
      "Step: 1900  \tTraining loss: 0.5531412363052368\n",
      "Step: 1900  \tTraining accuracy: 0.7280028462409973\n",
      "Step: 1900  \tValid loss: 0.49832165241241455\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.72881883\n",
      "Precision: 0.6964286\n",
      "Recall: 0.5241935\n",
      "F1 score: 0.62597936\n",
      "AUC: 0.6928785\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.728819   0.696429  0.524194  0.625979  0.692878  0.552888      0.728675   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0   0.49788       0.728063   0.551525      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  1985.0  \n",
      "8\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_115395/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(2668, 3)\n",
      "(2668, 1)\n",
      "(1472, 3)\n",
      "(1472, 1)\n",
      "(1196, 3)\n",
      "(1196, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6399394273757935\n",
      "Step: 100  \tTraining accuracy: 0.6517990827560425\n",
      "Step: 100  \tValid loss: 0.6543242931365967\n",
      "Step: 200  \tTraining loss: 0.539897620677948\n",
      "Step: 200  \tTraining accuracy: 0.6869065761566162\n",
      "Step: 200  \tValid loss: 0.5454822182655334\n",
      "Step: 300  \tTraining loss: 0.42983949184417725\n",
      "Step: 300  \tTraining accuracy: 0.7407046556472778\n",
      "Step: 300  \tValid loss: 0.42518433928489685\n",
      "Step: 400  \tTraining loss: 0.3684976398944855\n",
      "Step: 400  \tTraining accuracy: 0.7744163870811462\n",
      "Step: 400  \tValid loss: 0.3564607501029968\n",
      "Step: 500  \tTraining loss: 0.3421682119369507\n",
      "Step: 500  \tTraining accuracy: 0.7937281131744385\n",
      "Step: 500  \tValid loss: 0.32608866691589355\n",
      "Step: 600  \tTraining loss: 0.3313628137111664\n",
      "Step: 600  \tTraining accuracy: 0.8063241243362427\n",
      "Step: 600  \tValid loss: 0.3130953013896942\n",
      "Step: 700  \tTraining loss: 0.3268212676048279\n",
      "Step: 700  \tTraining accuracy: 0.8151597380638123\n",
      "Step: 700  \tValid loss: 0.3072308301925659\n",
      "Step: 800  \tTraining loss: 0.32464128732681274\n",
      "Step: 800  \tTraining accuracy: 0.8216391801834106\n",
      "Step: 800  \tValid loss: 0.3043719232082367\n",
      "Step: 900  \tTraining loss: 0.32295241951942444\n",
      "Step: 900  \tTraining accuracy: 0.8265940546989441\n",
      "Step: 900  \tValid loss: 0.30249086022377014\n",
      "Step: 1000  \tTraining loss: 0.321438729763031\n",
      "Step: 1000  \tTraining accuracy: 0.8305057883262634\n",
      "Step: 1000  \tValid loss: 0.301770955324173\n",
      "Step: 1100  \tTraining loss: 0.3206244111061096\n",
      "Step: 1100  \tTraining accuracy: 0.8336724638938904\n",
      "Step: 1100  \tValid loss: 0.3006765842437744\n",
      "Step: 1200  \tTraining loss: 0.31994614005088806\n",
      "Step: 1200  \tTraining accuracy: 0.8362883925437927\n",
      "Step: 1200  \tValid loss: 0.29979607462882996\n",
      "Step: 1300  \tTraining loss: 0.31935611367225647\n",
      "Step: 1300  \tTraining accuracy: 0.8384857773780823\n",
      "Step: 1300  \tValid loss: 0.29900914430618286\n",
      "Step: 1400  \tTraining loss: 0.31877434253692627\n",
      "Step: 1400  \tTraining accuracy: 0.8403576016426086\n",
      "Step: 1400  \tValid loss: 0.2982235848903656\n",
      "Step: 1500  \tTraining loss: 0.3181760311126709\n",
      "Step: 1500  \tTraining accuracy: 0.8419712781906128\n",
      "Step: 1500  \tValid loss: 0.29738885164260864\n",
      "Step: 1600  \tTraining loss: 0.3175298273563385\n",
      "Step: 1600  \tTraining accuracy: 0.8433766961097717\n",
      "Step: 1600  \tValid loss: 0.2964828610420227\n",
      "Step: 1700  \tTraining loss: 0.3168187141418457\n",
      "Step: 1700  \tTraining accuracy: 0.8446117639541626\n",
      "Step: 1700  \tValid loss: 0.2954428195953369\n",
      "Step: 1800  \tTraining loss: 0.3160497844219208\n",
      "Step: 1800  \tTraining accuracy: 0.8458021283149719\n",
      "Step: 1800  \tValid loss: 0.294333815574646\n",
      "Step: 1900  \tTraining loss: 0.3152279853820801\n",
      "Step: 1900  \tTraining accuracy: 0.8469650149345398\n",
      "Step: 1900  \tValid loss: 0.2933557331562042\n",
      "Step: 2000  \tTraining loss: 0.31460291147232056\n",
      "Step: 2000  \tTraining accuracy: 0.8480086922645569\n",
      "Step: 2000  \tValid loss: 0.29242458939552307\n",
      "Step: 2100  \tTraining loss: 0.3140466511249542\n",
      "Step: 2100  \tTraining accuracy: 0.8489505052566528\n",
      "Step: 2100  \tValid loss: 0.2916467785835266\n",
      "Step: 2200  \tTraining loss: 0.31362226605415344\n",
      "Step: 2200  \tTraining accuracy: 0.8498047590255737\n",
      "Step: 2200  \tValid loss: 0.2910035252571106\n",
      "Step: 2300  \tTraining loss: 0.31327006220817566\n",
      "Step: 2300  \tTraining accuracy: 0.850583016872406\n",
      "Step: 2300  \tValid loss: 0.2905091345310211\n",
      "Step: 2400  \tTraining loss: 0.3129613995552063\n",
      "Step: 2400  \tTraining accuracy: 0.8512951135635376\n",
      "Step: 2400  \tValid loss: 0.2901301681995392\n",
      "Step: 2500  \tTraining loss: 0.3126792311668396\n",
      "Step: 2500  \tTraining accuracy: 0.8519490361213684\n",
      "Step: 2500  \tValid loss: 0.2898254692554474\n",
      "Step: 2600  \tTraining loss: 0.31241244077682495\n",
      "Step: 2600  \tTraining accuracy: 0.8525516390800476\n",
      "Step: 2600  \tValid loss: 0.2896469533443451\n",
      "Step: 2700  \tTraining loss: 0.3121548295021057\n",
      "Step: 2700  \tTraining accuracy: 0.853108823299408\n",
      "Step: 2700  \tValid loss: 0.2894047796726227\n",
      "Step: 2800  \tTraining loss: 0.3119020462036133\n",
      "Step: 2800  \tTraining accuracy: 0.853625476360321\n",
      "Step: 2800  \tValid loss: 0.2893248498439789\n",
      "Step: 2900  \tTraining loss: 0.31165453791618347\n",
      "Step: 2900  \tTraining accuracy: 0.8541058301925659\n",
      "Step: 2900  \tValid loss: 0.28922826051712036\n",
      "Step: 3000  \tTraining loss: 0.31141331791877747\n",
      "Step: 3000  \tTraining accuracy: 0.8545536398887634\n",
      "Step: 3000  \tValid loss: 0.28922638297080994\n",
      "Step: 3100  \tTraining loss: 0.31117966771125793\n",
      "Step: 3100  \tTraining accuracy: 0.8549721240997314\n",
      "Step: 3100  \tValid loss: 0.2890959680080414\n",
      "Step: 3200  \tTraining loss: 0.3109537661075592\n",
      "Step: 3200  \tTraining accuracy: 0.8553639650344849\n",
      "Step: 3200  \tValid loss: 0.288990318775177\n",
      "Step: 3300  \tTraining loss: 0.3107343316078186\n",
      "Step: 3300  \tTraining accuracy: 0.855731725692749\n",
      "Step: 3300  \tValid loss: 0.2889009714126587\n",
      "Step: 3400  \tTraining loss: 0.3105221092700958\n",
      "Step: 3400  \tTraining accuracy: 0.8560775518417358\n",
      "Step: 3400  \tValid loss: 0.2888859510421753\n",
      "Step: 3500  \tTraining loss: 0.3103165626525879\n",
      "Step: 3500  \tTraining accuracy: 0.8564032912254333\n",
      "Step: 3500  \tValid loss: 0.2888365685939789\n",
      "Step: 3600  \tTraining loss: 0.31011220812797546\n",
      "Step: 3600  \tTraining accuracy: 0.8567107319831848\n",
      "Step: 3600  \tValid loss: 0.28869912028312683\n",
      "Step: 3700  \tTraining loss: 0.3099097013473511\n",
      "Step: 3700  \tTraining accuracy: 0.8570013046264648\n",
      "Step: 3700  \tValid loss: 0.2887265682220459\n",
      "Step: 3800  \tTraining loss: 0.30972832441329956\n",
      "Step: 3800  \tTraining accuracy: 0.8572763800621033\n",
      "Step: 3800  \tValid loss: 0.28864288330078125\n",
      "Step: 3900  \tTraining loss: 0.30955931544303894\n",
      "Step: 3900  \tTraining accuracy: 0.8575371503829956\n",
      "Step: 3900  \tValid loss: 0.2886056900024414\n",
      "Step: 4000  \tTraining loss: 0.3093988597393036\n",
      "Step: 4000  \tTraining accuracy: 0.8577846884727478\n",
      "Step: 4000  \tValid loss: 0.28856945037841797\n",
      "Step: 4100  \tTraining loss: 0.3092474937438965\n",
      "Step: 4100  \tTraining accuracy: 0.8580200672149658\n",
      "Step: 4100  \tValid loss: 0.2885034680366516\n",
      "Step: 4200  \tTraining loss: 0.30910375714302063\n",
      "Step: 4200  \tTraining accuracy: 0.8582440614700317\n",
      "Step: 4200  \tValid loss: 0.28845036029815674\n",
      "Step: 4300  \tTraining loss: 0.3089679181575775\n",
      "Step: 4300  \tTraining accuracy: 0.8584575653076172\n",
      "Step: 4300  \tValid loss: 0.2883751094341278\n",
      "Step: 4400  \tTraining loss: 0.30883967876434326\n",
      "Step: 4400  \tTraining accuracy: 0.8586611747741699\n",
      "Step: 4400  \tValid loss: 0.28831979632377625\n",
      "Step: 4500  \tTraining loss: 0.3087177276611328\n",
      "Step: 4500  \tTraining accuracy: 0.858855664730072\n",
      "Step: 4500  \tValid loss: 0.2882422208786011\n",
      "Step: 4600  \tTraining loss: 0.30860215425491333\n",
      "Step: 4600  \tTraining accuracy: 0.8590416312217712\n",
      "Step: 4600  \tValid loss: 0.28819578886032104\n",
      "Step: 4700  \tTraining loss: 0.3084928095340729\n",
      "Step: 4700  \tTraining accuracy: 0.8592196106910706\n",
      "Step: 4700  \tValid loss: 0.28814733028411865\n",
      "Step: 4800  \tTraining loss: 0.3083885908126831\n",
      "Step: 4800  \tTraining accuracy: 0.8593900203704834\n",
      "Step: 4800  \tValid loss: 0.28810814023017883\n",
      "Step: 4900  \tTraining loss: 0.30828967690467834\n",
      "Step: 4900  \tTraining accuracy: 0.8595534563064575\n",
      "Step: 4900  \tValid loss: 0.28806254267692566\n",
      "Step: 5000  \tTraining loss: 0.3081955909729004\n",
      "Step: 5000  \tTraining accuracy: 0.8597102761268616\n",
      "Step: 5000  \tValid loss: 0.288038045167923\n",
      "Step: 5100  \tTraining loss: 0.3081050515174866\n",
      "Step: 5100  \tTraining accuracy: 0.859860897064209\n",
      "Step: 5100  \tValid loss: 0.2880038022994995\n",
      "Step: 5200  \tTraining loss: 0.3080187439918518\n",
      "Step: 5200  \tTraining accuracy: 0.8600056767463684\n",
      "Step: 5200  \tValid loss: 0.28797072172164917\n",
      "Step: 5300  \tTraining loss: 0.30793577432632446\n",
      "Step: 5300  \tTraining accuracy: 0.8601413369178772\n",
      "Step: 5300  \tValid loss: 0.2879495918750763\n",
      "Step: 5400  \tTraining loss: 0.3078557550907135\n",
      "Step: 5400  \tTraining accuracy: 0.860289454460144\n",
      "Step: 5400  \tValid loss: 0.2879297733306885\n",
      "Step: 5500  \tTraining loss: 0.30777883529663086\n",
      "Step: 5500  \tTraining accuracy: 0.8604321479797363\n",
      "Step: 5500  \tValid loss: 0.2879168391227722\n",
      "Step: 5600  \tTraining loss: 0.30770444869995117\n",
      "Step: 5600  \tTraining accuracy: 0.8605697154998779\n",
      "Step: 5600  \tValid loss: 0.28790485858917236\n",
      "Step: 5700  \tTraining loss: 0.30763208866119385\n",
      "Step: 5700  \tTraining accuracy: 0.860702395439148\n",
      "Step: 5700  \tValid loss: 0.287898451089859\n",
      "Step: 5800  \tTraining loss: 0.30756238102912903\n",
      "Step: 5800  \tTraining accuracy: 0.8608304262161255\n",
      "Step: 5800  \tValid loss: 0.28789710998535156\n",
      "Step: 5900  \tTraining loss: 0.30749446153640747\n",
      "Step: 5900  \tTraining accuracy: 0.8609541654586792\n",
      "Step: 5900  \tValid loss: 0.28789931535720825\n",
      "Step: 6000  \tTraining loss: 0.30742788314819336\n",
      "Step: 6000  \tTraining accuracy: 0.8610736727714539\n",
      "Step: 6000  \tValid loss: 0.2879064679145813\n",
      "Step: 6100  \tTraining loss: 0.3073628842830658\n",
      "Step: 6100  \tTraining accuracy: 0.8611892461776733\n",
      "Step: 6100  \tValid loss: 0.28791722655296326\n",
      "Step: 6200  \tTraining loss: 0.3072987198829651\n",
      "Step: 6200  \tTraining accuracy: 0.861301064491272\n",
      "Step: 6200  \tValid loss: 0.2879331707954407\n",
      "Step: 6300  \tTraining loss: 0.3072371184825897\n",
      "Step: 6300  \tTraining accuracy: 0.8614093065261841\n",
      "Step: 6300  \tValid loss: 0.2879484295845032\n",
      "Step: 6400  \tTraining loss: 0.307176411151886\n",
      "Step: 6400  \tTraining accuracy: 0.861514151096344\n",
      "Step: 6400  \tValid loss: 0.2879681885242462\n",
      "Step: 6500  \tTraining loss: 0.3071189522743225\n",
      "Step: 6500  \tTraining accuracy: 0.8616157174110413\n",
      "Step: 6500  \tValid loss: 0.2880033552646637\n",
      "Step: 6600  \tTraining loss: 0.3070693910121918\n",
      "Step: 6600  \tTraining accuracy: 0.8617141842842102\n",
      "Step: 6600  \tValid loss: 0.2880572974681854\n",
      "Step: 6700  \tTraining loss: 0.3070233166217804\n",
      "Step: 6700  \tTraining accuracy: 0.8618096709251404\n",
      "Step: 6700  \tValid loss: 0.2881225645542145\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.86190236\n",
      "Precision: 0.89787525\n",
      "Recall: 0.8641161\n",
      "F1 score: 0.84538865\n",
      "AUC: 0.8673879\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.861902   0.897875  0.864116  0.845389  0.867388  0.306986      0.861865   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.287893        0.86178   0.333735      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  6785.0  \n",
      "9\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_12230/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(754, 3)\n",
      "(754, 1)\n",
      "(400, 3)\n",
      "(400, 1)\n",
      "(325, 3)\n",
      "(325, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5905454158782959\n",
      "Step: 100  \tTraining accuracy: 0.6909814476966858\n",
      "Step: 100  \tValid loss: 0.6805668473243713\n",
      "Step: 200  \tTraining loss: 0.5097231268882751\n",
      "Step: 200  \tTraining accuracy: 0.7057769894599915\n",
      "Step: 200  \tValid loss: 0.6631883382797241\n",
      "Step: 300  \tTraining loss: 0.48901060223579407\n",
      "Step: 300  \tTraining accuracy: 0.735722005367279\n",
      "Step: 300  \tValid loss: 0.6662518382072449\n",
      "Step: 400  \tTraining loss: 0.482147753238678\n",
      "Step: 400  \tTraining accuracy: 0.748603343963623\n",
      "Step: 400  \tValid loss: 0.6609745621681213\n",
      "Step: 500  \tTraining loss: 0.4773680865764618\n",
      "Step: 500  \tTraining accuracy: 0.7557721138000488\n",
      "Step: 500  \tValid loss: 0.6538183689117432\n",
      "Step: 600  \tTraining loss: 0.4731300473213196\n",
      "Step: 600  \tTraining accuracy: 0.7603386640548706\n",
      "Step: 600  \tValid loss: 0.6471761465072632\n",
      "Step: 700  \tTraining loss: 0.46894997358322144\n",
      "Step: 700  \tTraining accuracy: 0.763502299785614\n",
      "Step: 700  \tValid loss: 0.6406494975090027\n",
      "Step: 800  \tTraining loss: 0.4648974537849426\n",
      "Step: 800  \tTraining accuracy: 0.7658233642578125\n",
      "Step: 800  \tValid loss: 0.6341797113418579\n",
      "Step: 900  \tTraining loss: 0.46092844009399414\n",
      "Step: 900  \tTraining accuracy: 0.7675989270210266\n",
      "Step: 900  \tValid loss: 0.627873420715332\n",
      "Step: 1000  \tTraining loss: 0.45708346366882324\n",
      "Step: 1000  \tTraining accuracy: 0.7690010666847229\n",
      "Step: 1000  \tValid loss: 0.6210755705833435\n",
      "Step: 1100  \tTraining loss: 0.45351362228393555\n",
      "Step: 1100  \tTraining accuracy: 0.7701364159584045\n",
      "Step: 1100  \tValid loss: 0.6140202283859253\n",
      "Step: 1200  \tTraining loss: 0.4503573775291443\n",
      "Step: 1200  \tTraining accuracy: 0.7710744142532349\n",
      "Step: 1200  \tValid loss: 0.6071699857711792\n",
      "Step: 1300  \tTraining loss: 0.44772154092788696\n",
      "Step: 1300  \tTraining accuracy: 0.7718625068664551\n",
      "Step: 1300  \tValid loss: 0.6010100245475769\n",
      "Step: 1400  \tTraining loss: 0.4456630051136017\n",
      "Step: 1400  \tTraining accuracy: 0.7725338935852051\n",
      "Step: 1400  \tValid loss: 0.5955629944801331\n",
      "Step: 1500  \tTraining loss: 0.4441477060317993\n",
      "Step: 1500  \tTraining accuracy: 0.7731127738952637\n",
      "Step: 1500  \tValid loss: 0.5908982753753662\n",
      "Step: 1600  \tTraining loss: 0.44307243824005127\n",
      "Step: 1600  \tTraining accuracy: 0.7736169695854187\n",
      "Step: 1600  \tValid loss: 0.5873106718063354\n",
      "Step: 1700  \tTraining loss: 0.44231975078582764\n",
      "Step: 1700  \tTraining accuracy: 0.7740601301193237\n",
      "Step: 1700  \tValid loss: 0.5845437049865723\n",
      "Step: 1800  \tTraining loss: 0.44167086482048035\n",
      "Step: 1800  \tTraining accuracy: 0.7744526267051697\n",
      "Step: 1800  \tValid loss: 0.5832027196884155\n",
      "Step: 1900  \tTraining loss: 0.4412342309951782\n",
      "Step: 1900  \tTraining accuracy: 0.7748027443885803\n",
      "Step: 1900  \tValid loss: 0.5813118815422058\n",
      "Step: 2000  \tTraining loss: 0.44091951847076416\n",
      "Step: 2000  \tTraining accuracy: 0.7751169800758362\n",
      "Step: 2000  \tValid loss: 0.5797688961029053\n",
      "Step: 2100  \tTraining loss: 0.4406754970550537\n",
      "Step: 2100  \tTraining accuracy: 0.7754005193710327\n",
      "Step: 2100  \tValid loss: 0.5787767767906189\n",
      "Step: 2200  \tTraining loss: 0.4404808580875397\n",
      "Step: 2200  \tTraining accuracy: 0.7756577730178833\n",
      "Step: 2200  \tValid loss: 0.5776535272598267\n",
      "Step: 2300  \tTraining loss: 0.4403400719165802\n",
      "Step: 2300  \tTraining accuracy: 0.7758920788764954\n",
      "Step: 2300  \tValid loss: 0.5768653154373169\n",
      "Step: 2400  \tTraining loss: 0.43999916315078735\n",
      "Step: 2400  \tTraining accuracy: 0.7761065363883972\n",
      "Step: 2400  \tValid loss: 0.577347993850708\n",
      "Step: 2500  \tTraining loss: 0.4398241937160492\n",
      "Step: 2500  \tTraining accuracy: 0.7763034701347351\n",
      "Step: 2500  \tValid loss: 0.5764167308807373\n",
      "Step: 2600  \tTraining loss: 0.439664751291275\n",
      "Step: 2600  \tTraining accuracy: 0.7764849066734314\n",
      "Step: 2600  \tValid loss: 0.5759592652320862\n",
      "Step: 2700  \tTraining loss: 0.43951416015625\n",
      "Step: 2700  \tTraining accuracy: 0.7766527533531189\n",
      "Step: 2700  \tValid loss: 0.5756161212921143\n",
      "Step: 2800  \tTraining loss: 0.4393811523914337\n",
      "Step: 2800  \tTraining accuracy: 0.7768083214759827\n",
      "Step: 2800  \tValid loss: 0.5750149488449097\n",
      "Step: 2900  \tTraining loss: 0.43925970792770386\n",
      "Step: 2900  \tTraining accuracy: 0.7769529819488525\n",
      "Step: 2900  \tValid loss: 0.574420690536499\n",
      "Step: 3000  \tTraining loss: 0.43914932012557983\n",
      "Step: 3000  \tTraining accuracy: 0.7770878672599792\n",
      "Step: 3000  \tValid loss: 0.5738267302513123\n",
      "Step: 3100  \tTraining loss: 0.4390462338924408\n",
      "Step: 3100  \tTraining accuracy: 0.7772138714790344\n",
      "Step: 3100  \tValid loss: 0.5735764503479004\n",
      "Step: 3200  \tTraining loss: 0.43894651532173157\n",
      "Step: 3200  \tTraining accuracy: 0.7773319482803345\n",
      "Step: 3200  \tValid loss: 0.5733214616775513\n",
      "Step: 3300  \tTraining loss: 0.4388493001461029\n",
      "Step: 3300  \tTraining accuracy: 0.7774426937103271\n",
      "Step: 3300  \tValid loss: 0.5730884075164795\n",
      "Step: 3400  \tTraining loss: 0.43875187635421753\n",
      "Step: 3400  \tTraining accuracy: 0.7775468826293945\n",
      "Step: 3400  \tValid loss: 0.5728303790092468\n",
      "Step: 3500  \tTraining loss: 0.43865540623664856\n",
      "Step: 3500  \tTraining accuracy: 0.7776449918746948\n",
      "Step: 3500  \tValid loss: 0.5725618600845337\n",
      "Step: 3600  \tTraining loss: 0.43856096267700195\n",
      "Step: 3600  \tTraining accuracy: 0.777737557888031\n",
      "Step: 3600  \tValid loss: 0.5722976922988892\n",
      "Step: 3700  \tTraining loss: 0.4384671449661255\n",
      "Step: 3700  \tTraining accuracy: 0.777825117111206\n",
      "Step: 3700  \tValid loss: 0.5720363855361938\n",
      "Step: 3800  \tTraining loss: 0.4383743107318878\n",
      "Step: 3800  \tTraining accuracy: 0.7779079675674438\n",
      "Step: 3800  \tValid loss: 0.5718274712562561\n",
      "Step: 3900  \tTraining loss: 0.4382820427417755\n",
      "Step: 3900  \tTraining accuracy: 0.7779865264892578\n",
      "Step: 3900  \tValid loss: 0.57157301902771\n",
      "Step: 4000  \tTraining loss: 0.43819060921669006\n",
      "Step: 4000  \tTraining accuracy: 0.7780610918998718\n",
      "Step: 4000  \tValid loss: 0.571343183517456\n",
      "Step: 4100  \tTraining loss: 0.43809974193573\n",
      "Step: 4100  \tTraining accuracy: 0.7781319618225098\n",
      "Step: 4100  \tValid loss: 0.5711252689361572\n",
      "Step: 4200  \tTraining loss: 0.4380090534687042\n",
      "Step: 4200  \tTraining accuracy: 0.7781994938850403\n",
      "Step: 4200  \tValid loss: 0.5708901882171631\n",
      "Step: 4300  \tTraining loss: 0.4379197955131531\n",
      "Step: 4300  \tTraining accuracy: 0.7782637476921082\n",
      "Step: 4300  \tValid loss: 0.5706654191017151\n",
      "Step: 4400  \tTraining loss: 0.43783092498779297\n",
      "Step: 4400  \tTraining accuracy: 0.7783251404762268\n",
      "Step: 4400  \tValid loss: 0.5704305171966553\n",
      "Step: 4500  \tTraining loss: 0.4377431273460388\n",
      "Step: 4500  \tTraining accuracy: 0.778383731842041\n",
      "Step: 4500  \tValid loss: 0.5701817870140076\n",
      "Step: 4600  \tTraining loss: 0.4376557171344757\n",
      "Step: 4600  \tTraining accuracy: 0.7784397602081299\n",
      "Step: 4600  \tValid loss: 0.5699661374092102\n",
      "Step: 4700  \tTraining loss: 0.4375702142715454\n",
      "Step: 4700  \tTraining accuracy: 0.778493344783783\n",
      "Step: 4700  \tValid loss: 0.5697231888771057\n",
      "Step: 4800  \tTraining loss: 0.4374859631061554\n",
      "Step: 4800  \tTraining accuracy: 0.7785446643829346\n",
      "Step: 4800  \tValid loss: 0.5694892406463623\n",
      "Step: 4900  \tTraining loss: 0.43740200996398926\n",
      "Step: 4900  \tTraining accuracy: 0.7785939574241638\n",
      "Step: 4900  \tValid loss: 0.5692754983901978\n",
      "Step: 5000  \tTraining loss: 0.4373202621936798\n",
      "Step: 5000  \tTraining accuracy: 0.7786411643028259\n",
      "Step: 5000  \tValid loss: 0.5690425038337708\n",
      "Step: 5100  \tTraining loss: 0.4372406005859375\n",
      "Step: 5100  \tTraining accuracy: 0.7786865234375\n",
      "Step: 5100  \tValid loss: 0.5688199400901794\n",
      "Step: 5200  \tTraining loss: 0.43716058135032654\n",
      "Step: 5200  \tTraining accuracy: 0.7787301540374756\n",
      "Step: 5200  \tValid loss: 0.5685918927192688\n",
      "Step: 5300  \tTraining loss: 0.43708336353302\n",
      "Step: 5300  \tTraining accuracy: 0.7787721157073975\n",
      "Step: 5300  \tValid loss: 0.568372368812561\n",
      "Step: 5400  \tTraining loss: 0.43700745701789856\n",
      "Step: 5400  \tTraining accuracy: 0.7788125276565552\n",
      "Step: 5400  \tValid loss: 0.5681455135345459\n",
      "Step: 5500  \tTraining loss: 0.43693217635154724\n",
      "Step: 5500  \tTraining accuracy: 0.7788513898849487\n",
      "Step: 5500  \tValid loss: 0.5679699182510376\n",
      "Step: 5600  \tTraining loss: 0.4368589222431183\n",
      "Step: 5600  \tTraining accuracy: 0.7788888812065125\n",
      "Step: 5600  \tValid loss: 0.5678042769432068\n",
      "Step: 5700  \tTraining loss: 0.43678611516952515\n",
      "Step: 5700  \tTraining accuracy: 0.7789250612258911\n",
      "Step: 5700  \tValid loss: 0.5676639676094055\n",
      "Step: 5800  \tTraining loss: 0.43671444058418274\n",
      "Step: 5800  \tTraining accuracy: 0.7789599895477295\n",
      "Step: 5800  \tValid loss: 0.5675325989723206\n",
      "Step: 5900  \tTraining loss: 0.43664389848709106\n",
      "Step: 5900  \tTraining accuracy: 0.7789937257766724\n",
      "Step: 5900  \tValid loss: 0.5674010515213013\n",
      "Step: 6000  \tTraining loss: 0.4365752339363098\n",
      "Step: 6000  \tTraining accuracy: 0.7790263295173645\n",
      "Step: 6000  \tValid loss: 0.5672796368598938\n",
      "Step: 6100  \tTraining loss: 0.4365086555480957\n",
      "Step: 6100  \tTraining accuracy: 0.7790466547012329\n",
      "Step: 6100  \tValid loss: 0.5671882033348083\n",
      "Step: 6200  \tTraining loss: 0.4364430904388428\n",
      "Step: 6200  \tTraining accuracy: 0.7790883183479309\n",
      "Step: 6200  \tValid loss: 0.5670749545097351\n",
      "Step: 6300  \tTraining loss: 0.436379075050354\n",
      "Step: 6300  \tTraining accuracy: 0.7791286110877991\n",
      "Step: 6300  \tValid loss: 0.5669677257537842\n",
      "Step: 6400  \tTraining loss: 0.436316579580307\n",
      "Step: 6400  \tTraining accuracy: 0.7791677117347717\n",
      "Step: 6400  \tValid loss: 0.5668726563453674\n",
      "Step: 6500  \tTraining loss: 0.4362563192844391\n",
      "Step: 6500  \tTraining accuracy: 0.7792055606842041\n",
      "Step: 6500  \tValid loss: 0.566744327545166\n",
      "Step: 6600  \tTraining loss: 0.43619710206985474\n",
      "Step: 6600  \tTraining accuracy: 0.779242217540741\n",
      "Step: 6600  \tValid loss: 0.5665997862815857\n",
      "Step: 6700  \tTraining loss: 0.4361409842967987\n",
      "Step: 6700  \tTraining accuracy: 0.7792778015136719\n",
      "Step: 6700  \tValid loss: 0.56639564037323\n",
      "Step: 6800  \tTraining loss: 0.43608689308166504\n",
      "Step: 6800  \tTraining accuracy: 0.7793123722076416\n",
      "Step: 6800  \tValid loss: 0.5662252902984619\n",
      "Step: 6900  \tTraining loss: 0.4360346794128418\n",
      "Step: 6900  \tTraining accuracy: 0.7793458700180054\n",
      "Step: 6900  \tValid loss: 0.5660291314125061\n",
      "Step: 7000  \tTraining loss: 0.4359842836856842\n",
      "Step: 7000  \tTraining accuracy: 0.7793784141540527\n",
      "Step: 7000  \tValid loss: 0.5658732056617737\n",
      "Step: 7100  \tTraining loss: 0.4359375834465027\n",
      "Step: 7100  \tTraining accuracy: 0.7794100642204285\n",
      "Step: 7100  \tValid loss: 0.5657290816307068\n",
      "Step: 7200  \tTraining loss: 0.43588992953300476\n",
      "Step: 7200  \tTraining accuracy: 0.7794408202171326\n",
      "Step: 7200  \tValid loss: 0.565599262714386\n",
      "Step: 7300  \tTraining loss: 0.43584558367729187\n",
      "Step: 7300  \tTraining accuracy: 0.7794707417488098\n",
      "Step: 7300  \tValid loss: 0.5655226707458496\n",
      "Step: 7400  \tTraining loss: 0.43580251932144165\n",
      "Step: 7400  \tTraining accuracy: 0.7794998288154602\n",
      "Step: 7400  \tValid loss: 0.5654269456863403\n",
      "Step: 7500  \tTraining loss: 0.43575993180274963\n",
      "Step: 7500  \tTraining accuracy: 0.7795281410217285\n",
      "Step: 7500  \tValid loss: 0.5653780102729797\n",
      "Step: 7600  \tTraining loss: 0.4357195496559143\n",
      "Step: 7600  \tTraining accuracy: 0.7795556783676147\n",
      "Step: 7600  \tValid loss: 0.5653259754180908\n",
      "Step: 7700  \tTraining loss: 0.43568024039268494\n",
      "Step: 7700  \tTraining accuracy: 0.7795825600624084\n",
      "Step: 7700  \tValid loss: 0.5652326941490173\n",
      "Step: 7800  \tTraining loss: 0.43564239144325256\n",
      "Step: 7800  \tTraining accuracy: 0.7796086668968201\n",
      "Step: 7800  \tValid loss: 0.5651518106460571\n",
      "Step: 7900  \tTraining loss: 0.4356056749820709\n",
      "Step: 7900  \tTraining accuracy: 0.7796341776847839\n",
      "Step: 7900  \tValid loss: 0.5651149749755859\n",
      "Step: 8000  \tTraining loss: 0.4355703890323639\n",
      "Step: 8000  \tTraining accuracy: 0.7796589732170105\n",
      "Step: 8000  \tValid loss: 0.5650652050971985\n",
      "Step: 8100  \tTraining loss: 0.43553662300109863\n",
      "Step: 8100  \tTraining accuracy: 0.7796832323074341\n",
      "Step: 8100  \tValid loss: 0.5649338960647583\n",
      "Step: 8200  \tTraining loss: 0.435503214597702\n",
      "Step: 8200  \tTraining accuracy: 0.7797068357467651\n",
      "Step: 8200  \tValid loss: 0.5648311376571655\n",
      "Step: 8300  \tTraining loss: 0.43547144532203674\n",
      "Step: 8300  \tTraining accuracy: 0.7797299027442932\n",
      "Step: 8300  \tValid loss: 0.5647222399711609\n",
      "Step: 8400  \tTraining loss: 0.4354408383369446\n",
      "Step: 8400  \tTraining accuracy: 0.7797524333000183\n",
      "Step: 8400  \tValid loss: 0.5646127462387085\n",
      "Step: 8500  \tTraining loss: 0.43541157245635986\n",
      "Step: 8500  \tTraining accuracy: 0.7797743678092957\n",
      "Step: 8500  \tValid loss: 0.5645166635513306\n",
      "Step: 8600  \tTraining loss: 0.4353832006454468\n",
      "Step: 8600  \tTraining accuracy: 0.7797958254814148\n",
      "Step: 8600  \tValid loss: 0.5644445419311523\n",
      "Step: 8700  \tTraining loss: 0.43535539507865906\n",
      "Step: 8700  \tTraining accuracy: 0.7798168063163757\n",
      "Step: 8700  \tValid loss: 0.5643506050109863\n",
      "Step: 8800  \tTraining loss: 0.43532904982566833\n",
      "Step: 8800  \tTraining accuracy: 0.7798373103141785\n",
      "Step: 8800  \tValid loss: 0.5643069744110107\n",
      "Step: 8900  \tTraining loss: 0.43530282378196716\n",
      "Step: 8900  \tTraining accuracy: 0.7798572778701782\n",
      "Step: 8900  \tValid loss: 0.5642067193984985\n",
      "Step: 9000  \tTraining loss: 0.4352775514125824\n",
      "Step: 9000  \tTraining accuracy: 0.7798768877983093\n",
      "Step: 9000  \tValid loss: 0.5641791820526123\n",
      "Step: 9100  \tTraining loss: 0.4352530837059021\n",
      "Step: 9100  \tTraining accuracy: 0.7798960208892822\n",
      "Step: 9100  \tValid loss: 0.5641094446182251\n",
      "Step: 9200  \tTraining loss: 0.43522951006889343\n",
      "Step: 9200  \tTraining accuracy: 0.7799147367477417\n",
      "Step: 9200  \tValid loss: 0.5640037059783936\n",
      "Step: 9300  \tTraining loss: 0.4352059066295624\n",
      "Step: 9300  \tTraining accuracy: 0.7799330353736877\n",
      "Step: 9300  \tValid loss: 0.5639064908027649\n",
      "Step: 9400  \tTraining loss: 0.43518370389938354\n",
      "Step: 9400  \tTraining accuracy: 0.7799509763717651\n",
      "Step: 9400  \tValid loss: 0.5638353824615479\n",
      "Step: 9500  \tTraining loss: 0.43516111373901367\n",
      "Step: 9500  \tTraining accuracy: 0.7799685001373291\n",
      "Step: 9500  \tValid loss: 0.5637483596801758\n",
      "Step: 9600  \tTraining loss: 0.43514007329940796\n",
      "Step: 9600  \tTraining accuracy: 0.7799857258796692\n",
      "Step: 9600  \tValid loss: 0.5636754631996155\n",
      "Step: 9700  \tTraining loss: 0.435118705034256\n",
      "Step: 9700  \tTraining accuracy: 0.7800025343894958\n",
      "Step: 9700  \tValid loss: 0.5636615753173828\n",
      "Step: 9800  \tTraining loss: 0.4350982904434204\n",
      "Step: 9800  \tTraining accuracy: 0.7800189852714539\n",
      "Step: 9800  \tValid loss: 0.563651978969574\n",
      "Step: 9900  \tTraining loss: 0.43507781624794006\n",
      "Step: 9900  \tTraining accuracy: 0.780035138130188\n",
      "Step: 9900  \tValid loss: 0.5636596083641052\n",
      "Step: 10000  \tTraining loss: 0.4350579082965851\n",
      "Step: 10000  \tTraining accuracy: 0.7800509333610535\n",
      "Step: 10000  \tValid loss: 0.5636669397354126\n",
      "Step: 10100  \tTraining loss: 0.4350390136241913\n",
      "Step: 10100  \tTraining accuracy: 0.7800664901733398\n",
      "Step: 10100  \tValid loss: 0.5636395812034607\n",
      "Step: 10200  \tTraining loss: 0.43501925468444824\n",
      "Step: 10200  \tTraining accuracy: 0.7800816893577576\n",
      "Step: 10200  \tValid loss: 0.5636706948280334\n",
      "Step: 10300  \tTraining loss: 0.43500015139579773\n",
      "Step: 10300  \tTraining accuracy: 0.7800965905189514\n",
      "Step: 10300  \tValid loss: 0.5636760592460632\n",
      "Step: 10400  \tTraining loss: 0.4349801540374756\n",
      "Step: 10400  \tTraining accuracy: 0.7801111936569214\n",
      "Step: 10400  \tValid loss: 0.5637033581733704\n",
      "Step: 10500  \tTraining loss: 0.43496039509773254\n",
      "Step: 10500  \tTraining accuracy: 0.7801254987716675\n",
      "Step: 10500  \tValid loss: 0.5636513829231262\n",
      "Step: 10600  \tTraining loss: 0.4349404573440552\n",
      "Step: 10600  \tTraining accuracy: 0.7801395654678345\n",
      "Step: 10600  \tValid loss: 0.5635324716567993\n",
      "Step: 10700  \tTraining loss: 0.43492212891578674\n",
      "Step: 10700  \tTraining accuracy: 0.7801533937454224\n",
      "Step: 10700  \tValid loss: 0.5633849501609802\n",
      "Step: 10800  \tTraining loss: 0.43490269780158997\n",
      "Step: 10800  \tTraining accuracy: 0.7801669239997864\n",
      "Step: 10800  \tValid loss: 0.5632932782173157\n",
      "Step: 10900  \tTraining loss: 0.43488460779190063\n",
      "Step: 10900  \tTraining accuracy: 0.7801802158355713\n",
      "Step: 10900  \tValid loss: 0.563218355178833\n",
      "Step: 11000  \tTraining loss: 0.43486443161964417\n",
      "Step: 11000  \tTraining accuracy: 0.7801932692527771\n",
      "Step: 11000  \tValid loss: 0.5632302761077881\n",
      "Step: 11100  \tTraining loss: 0.4348445534706116\n",
      "Step: 11100  \tTraining accuracy: 0.7802060842514038\n",
      "Step: 11100  \tValid loss: 0.5630893111228943\n",
      "Step: 11200  \tTraining loss: 0.4348239600658417\n",
      "Step: 11200  \tTraining accuracy: 0.7802186608314514\n",
      "Step: 11200  \tValid loss: 0.5631082057952881\n",
      "Step: 11300  \tTraining loss: 0.4348030686378479\n",
      "Step: 11300  \tTraining accuracy: 0.7802309989929199\n",
      "Step: 11300  \tValid loss: 0.5630220174789429\n",
      "Step: 11400  \tTraining loss: 0.4347803294658661\n",
      "Step: 11400  \tTraining accuracy: 0.7802431583404541\n",
      "Step: 11400  \tValid loss: 0.5629637241363525\n",
      "Step: 11500  \tTraining loss: 0.43475690484046936\n",
      "Step: 11500  \tTraining accuracy: 0.7802550792694092\n",
      "Step: 11500  \tValid loss: 0.5629370212554932\n",
      "Step: 11600  \tTraining loss: 0.43473246693611145\n",
      "Step: 11600  \tTraining accuracy: 0.7802668213844299\n",
      "Step: 11600  \tValid loss: 0.5629419088363647\n",
      "Step: 11700  \tTraining loss: 0.43470537662506104\n",
      "Step: 11700  \tTraining accuracy: 0.7802783250808716\n",
      "Step: 11700  \tValid loss: 0.5629510879516602\n",
      "Step: 11800  \tTraining loss: 0.4346771240234375\n",
      "Step: 11800  \tTraining accuracy: 0.7802896499633789\n",
      "Step: 11800  \tValid loss: 0.5629293322563171\n",
      "Step: 11900  \tTraining loss: 0.43464595079421997\n",
      "Step: 11900  \tTraining accuracy: 0.7803007960319519\n",
      "Step: 11900  \tValid loss: 0.5629432201385498\n",
      "Step: 12000  \tTraining loss: 0.43461236357688904\n",
      "Step: 12000  \tTraining accuracy: 0.7803117036819458\n",
      "Step: 12000  \tValid loss: 0.562934160232544\n",
      "Step: 12100  \tTraining loss: 0.43457475304603577\n",
      "Step: 12100  \tTraining accuracy: 0.7803224921226501\n",
      "Step: 12100  \tValid loss: 0.5629215836524963\n",
      "Step: 12200  \tTraining loss: 0.43453437089920044\n",
      "Step: 12200  \tTraining accuracy: 0.7803331017494202\n",
      "Step: 12200  \tValid loss: 0.5629332065582275\n",
      "Step: 12300  \tTraining loss: 0.4344898462295532\n",
      "Step: 12300  \tTraining accuracy: 0.7803435325622559\n",
      "Step: 12300  \tValid loss: 0.5629589557647705\n",
      "Step: 12400  \tTraining loss: 0.43444183468818665\n",
      "Step: 12400  \tTraining accuracy: 0.7803537249565125\n",
      "Step: 12400  \tValid loss: 0.5629605650901794\n",
      "Step: 12500  \tTraining loss: 0.4343893229961395\n",
      "Step: 12500  \tTraining accuracy: 0.7803638577461243\n",
      "Step: 12500  \tValid loss: 0.5629585981369019\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.78037375\n",
      "Precision: 0.82881004\n",
      "Recall: 0.8802661\n",
      "F1 score: 0.8038689\n",
      "AUC: 0.8048195\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0  0.780374    0.82881  0.880266  0.803869  0.80482  0.434344      0.780214   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.562881       0.780328   0.546846      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  12581.0  \n",
      "10\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_106586/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(957, 3)\n",
      "(957, 1)\n",
      "(528, 3)\n",
      "(528, 1)\n",
      "(429, 3)\n",
      "(429, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6405860781669617\n",
      "Step: 100  \tTraining accuracy: 0.6102403402328491\n",
      "Step: 100  \tValid loss: 0.6539904475212097\n",
      "Step: 200  \tTraining loss: 0.5961015820503235\n",
      "Step: 200  \tTraining accuracy: 0.6168582439422607\n",
      "Step: 200  \tValid loss: 0.6021854281425476\n",
      "Step: 300  \tTraining loss: 0.5344306826591492\n",
      "Step: 300  \tTraining accuracy: 0.6480668783187866\n",
      "Step: 300  \tValid loss: 0.5266157388687134\n",
      "Step: 400  \tTraining loss: 0.4866168797016144\n",
      "Step: 400  \tTraining accuracy: 0.6790565848350525\n",
      "Step: 400  \tValid loss: 0.46242615580558777\n",
      "Step: 500  \tTraining loss: 0.46200138330459595\n",
      "Step: 500  \tTraining accuracy: 0.7033553719520569\n",
      "Step: 500  \tValid loss: 0.424397349357605\n",
      "Step: 600  \tTraining loss: 0.4518030285835266\n",
      "Step: 600  \tTraining accuracy: 0.7188182473182678\n",
      "Step: 600  \tValid loss: 0.4051732122898102\n",
      "Step: 700  \tTraining loss: 0.4480094909667969\n",
      "Step: 700  \tTraining accuracy: 0.731452465057373\n",
      "Step: 700  \tValid loss: 0.3957380950450897\n",
      "Step: 800  \tTraining loss: 0.4463891386985779\n",
      "Step: 800  \tTraining accuracy: 0.7413444519042969\n",
      "Step: 800  \tValid loss: 0.3907701373100281\n",
      "Step: 900  \tTraining loss: 0.44253215193748474\n",
      "Step: 900  \tTraining accuracy: 0.7489089965820312\n",
      "Step: 900  \tValid loss: 0.3906688392162323\n",
      "Step: 1000  \tTraining loss: 0.44092077016830444\n",
      "Step: 1000  \tTraining accuracy: 0.7548809051513672\n",
      "Step: 1000  \tValid loss: 0.3867439031600952\n",
      "Step: 1100  \tTraining loss: 0.4397897720336914\n",
      "Step: 1100  \tTraining accuracy: 0.7599143981933594\n",
      "Step: 1100  \tValid loss: 0.3844717741012573\n",
      "Step: 1200  \tTraining loss: 0.4389362633228302\n",
      "Step: 1200  \tTraining accuracy: 0.7640725374221802\n",
      "Step: 1200  \tValid loss: 0.3830012083053589\n",
      "Step: 1300  \tTraining loss: 0.43811726570129395\n",
      "Step: 1300  \tTraining accuracy: 0.7675653100013733\n",
      "Step: 1300  \tValid loss: 0.38188743591308594\n",
      "Step: 1400  \tTraining loss: 0.4373285472393036\n",
      "Step: 1400  \tTraining accuracy: 0.7706954479217529\n",
      "Step: 1400  \tValid loss: 0.3810844421386719\n",
      "Step: 1500  \tTraining loss: 0.43660444021224976\n",
      "Step: 1500  \tTraining accuracy: 0.7734659314155579\n",
      "Step: 1500  \tValid loss: 0.3800933063030243\n",
      "Step: 1600  \tTraining loss: 0.43589460849761963\n",
      "Step: 1600  \tTraining accuracy: 0.77587890625\n",
      "Step: 1600  \tValid loss: 0.3794294595718384\n",
      "Step: 1700  \tTraining loss: 0.4351975917816162\n",
      "Step: 1700  \tTraining accuracy: 0.7779994010925293\n",
      "Step: 1700  \tValid loss: 0.37874844670295715\n",
      "Step: 1800  \tTraining loss: 0.4344540238380432\n",
      "Step: 1800  \tTraining accuracy: 0.7798776030540466\n",
      "Step: 1800  \tValid loss: 0.3780844211578369\n",
      "Step: 1900  \tTraining loss: 0.43374499678611755\n",
      "Step: 1900  \tTraining accuracy: 0.7815527319908142\n",
      "Step: 1900  \tValid loss: 0.37747716903686523\n",
      "Step: 2000  \tTraining loss: 0.4330412447452545\n",
      "Step: 2000  \tTraining accuracy: 0.7830560207366943\n",
      "Step: 2000  \tValid loss: 0.3768875300884247\n",
      "Step: 2100  \tTraining loss: 0.4323393404483795\n",
      "Step: 2100  \tTraining accuracy: 0.784412682056427\n",
      "Step: 2100  \tValid loss: 0.3764241337776184\n",
      "Step: 2200  \tTraining loss: 0.43163102865219116\n",
      "Step: 2200  \tTraining accuracy: 0.7857889533042908\n",
      "Step: 2200  \tValid loss: 0.3759711980819702\n",
      "Step: 2300  \tTraining loss: 0.43091970682144165\n",
      "Step: 2300  \tTraining accuracy: 0.7870660424232483\n",
      "Step: 2300  \tValid loss: 0.37555578351020813\n",
      "Step: 2400  \tTraining loss: 0.4302120804786682\n",
      "Step: 2400  \tTraining accuracy: 0.788234531879425\n",
      "Step: 2400  \tValid loss: 0.37516605854034424\n",
      "Step: 2500  \tTraining loss: 0.42951539158821106\n",
      "Step: 2500  \tTraining accuracy: 0.789456844329834\n",
      "Step: 2500  \tValid loss: 0.37486669421195984\n",
      "Step: 2600  \tTraining loss: 0.4288448691368103\n",
      "Step: 2600  \tTraining accuracy: 0.7904193997383118\n",
      "Step: 2600  \tValid loss: 0.37466683983802795\n",
      "Step: 2700  \tTraining loss: 0.4282299280166626\n",
      "Step: 2700  \tTraining accuracy: 0.7914078831672668\n",
      "Step: 2700  \tValid loss: 0.3745962083339691\n",
      "Step: 2800  \tTraining loss: 0.4276689887046814\n",
      "Step: 2800  \tTraining accuracy: 0.7923244833946228\n",
      "Step: 2800  \tValid loss: 0.37453582882881165\n",
      "Step: 2900  \tTraining loss: 0.4271705746650696\n",
      "Step: 2900  \tTraining accuracy: 0.7931767702102661\n",
      "Step: 2900  \tValid loss: 0.3745535910129547\n",
      "Step: 3000  \tTraining loss: 0.4267241656780243\n",
      "Step: 3000  \tTraining accuracy: 0.7940067052841187\n",
      "Step: 3000  \tValid loss: 0.37470173835754395\n",
      "Step: 3100  \tTraining loss: 0.42632895708084106\n",
      "Step: 3100  \tTraining accuracy: 0.7947822213172913\n",
      "Step: 3100  \tValid loss: 0.3748333752155304\n",
      "Step: 3200  \tTraining loss: 0.42598649859428406\n",
      "Step: 3200  \tTraining accuracy: 0.7955084443092346\n",
      "Step: 3200  \tValid loss: 0.3748621642589569\n",
      "Step: 3300  \tTraining loss: 0.425710529088974\n",
      "Step: 3300  \tTraining accuracy: 0.7961900234222412\n",
      "Step: 3300  \tValid loss: 0.37483295798301697\n",
      "Step: 3400  \tTraining loss: 0.4254510700702667\n",
      "Step: 3400  \tTraining accuracy: 0.7968308925628662\n",
      "Step: 3400  \tValid loss: 0.3748683035373688\n",
      "Step: 3500  \tTraining loss: 0.4252188801765442\n",
      "Step: 3500  \tTraining accuracy: 0.7974346280097961\n",
      "Step: 3500  \tValid loss: 0.3748919665813446\n",
      "Step: 3600  \tTraining loss: 0.42502978444099426\n",
      "Step: 3600  \tTraining accuracy: 0.7980043292045593\n",
      "Step: 3600  \tValid loss: 0.37492451071739197\n",
      "Step: 3700  \tTraining loss: 0.4248552918434143\n",
      "Step: 3700  \tTraining accuracy: 0.7985427975654602\n",
      "Step: 3700  \tValid loss: 0.37495502829551697\n",
      "Step: 3800  \tTraining loss: 0.4246633052825928\n",
      "Step: 3800  \tTraining accuracy: 0.7990525960922241\n",
      "Step: 3800  \tValid loss: 0.3749513030052185\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7995359\n",
      "Precision: 0.83361346\n",
      "Recall: 0.84931505\n",
      "F1 score: 0.80699545\n",
      "AUC: 0.79194975\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0  0.799536   0.833613  0.849315  0.806995  0.79195  0.424634      0.799469   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.374485       0.799139   0.418174      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3818.0  \n",
      "11\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_7765/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(754, 3)\n",
      "(754, 1)\n",
      "(416, 3)\n",
      "(416, 1)\n",
      "(338, 3)\n",
      "(338, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5531019568443298\n",
      "Step: 100  \tTraining accuracy: 0.7692307829856873\n",
      "Step: 100  \tValid loss: 0.5324629545211792\n",
      "Step: 200  \tTraining loss: 0.4638490676879883\n",
      "Step: 200  \tTraining accuracy: 0.778514564037323\n",
      "Step: 200  \tValid loss: 0.4422569274902344\n",
      "Step: 300  \tTraining loss: 0.41715991497039795\n",
      "Step: 300  \tTraining accuracy: 0.780371367931366\n",
      "Step: 300  \tValid loss: 0.39454299211502075\n",
      "Step: 400  \tTraining loss: 0.394682914018631\n",
      "Step: 400  \tTraining accuracy: 0.7811670899391174\n",
      "Step: 400  \tValid loss: 0.37145599722862244\n",
      "Step: 500  \tTraining loss: 0.38214483857154846\n",
      "Step: 500  \tTraining accuracy: 0.7816091775894165\n",
      "Step: 500  \tValid loss: 0.35888707637786865\n",
      "Step: 600  \tTraining loss: 0.37397801876068115\n",
      "Step: 600  \tTraining accuracy: 0.7867133021354675\n",
      "Step: 600  \tValid loss: 0.3514552414417267\n",
      "Step: 700  \tTraining loss: 0.3680685758590698\n",
      "Step: 700  \tTraining accuracy: 0.7910630702972412\n",
      "Step: 700  \tValid loss: 0.3465927541255951\n",
      "Step: 800  \tTraining loss: 0.3634425401687622\n",
      "Step: 800  \tTraining accuracy: 0.79372239112854\n",
      "Step: 800  \tValid loss: 0.343569278717041\n",
      "Step: 900  \tTraining loss: 0.35939887166023254\n",
      "Step: 900  \tTraining accuracy: 0.7959120273590088\n",
      "Step: 900  \tValid loss: 0.3418964743614197\n",
      "Step: 1000  \tTraining loss: 0.35585108399391174\n",
      "Step: 1000  \tTraining accuracy: 0.7975010275840759\n",
      "Step: 1000  \tValid loss: 0.33952462673187256\n",
      "Step: 1100  \tTraining loss: 0.35255226492881775\n",
      "Step: 1100  \tTraining accuracy: 0.7989137172698975\n",
      "Step: 1100  \tValid loss: 0.33733412623405457\n",
      "Step: 1200  \tTraining loss: 0.34923431277275085\n",
      "Step: 1200  \tTraining accuracy: 0.8003113865852356\n",
      "Step: 1200  \tValid loss: 0.33495837450027466\n",
      "Step: 1300  \tTraining loss: 0.34612712264060974\n",
      "Step: 1300  \tTraining accuracy: 0.8014323711395264\n",
      "Step: 1300  \tValid loss: 0.3329000174999237\n",
      "Step: 1400  \tTraining loss: 0.34310629963874817\n",
      "Step: 1400  \tTraining accuracy: 0.8025346398353577\n",
      "Step: 1400  \tValid loss: 0.3307948410511017\n",
      "Step: 1500  \tTraining loss: 0.34019988775253296\n",
      "Step: 1500  \tTraining accuracy: 0.8035763502120972\n",
      "Step: 1500  \tValid loss: 0.3288653790950775\n",
      "Step: 1600  \tTraining loss: 0.33738794922828674\n",
      "Step: 1600  \tTraining accuracy: 0.8044835925102234\n",
      "Step: 1600  \tValid loss: 0.3268423080444336\n",
      "Step: 1700  \tTraining loss: 0.3332775831222534\n",
      "Step: 1700  \tTraining accuracy: 0.8051603436470032\n",
      "Step: 1700  \tValid loss: 0.3251534700393677\n",
      "Step: 1800  \tTraining loss: 0.33091896772384644\n",
      "Step: 1800  \tTraining accuracy: 0.8057976365089417\n",
      "Step: 1800  \tValid loss: 0.32355257868766785\n",
      "Step: 1900  \tTraining loss: 0.32852408289909363\n",
      "Step: 1900  \tTraining accuracy: 0.8064377307891846\n",
      "Step: 1900  \tValid loss: 0.32281333208084106\n",
      "Step: 2000  \tTraining loss: 0.3270709216594696\n",
      "Step: 2000  \tTraining accuracy: 0.8069441318511963\n",
      "Step: 2000  \tValid loss: 0.3219180405139923\n",
      "Step: 2100  \tTraining loss: 0.325855016708374\n",
      "Step: 2100  \tTraining accuracy: 0.8074658513069153\n",
      "Step: 2100  \tValid loss: 0.32109561562538147\n",
      "Step: 2200  \tTraining loss: 0.3247739374637604\n",
      "Step: 2200  \tTraining accuracy: 0.807815670967102\n",
      "Step: 2200  \tValid loss: 0.32009851932525635\n",
      "Step: 2300  \tTraining loss: 0.3238937556743622\n",
      "Step: 2300  \tTraining accuracy: 0.8081933259963989\n",
      "Step: 2300  \tValid loss: 0.31958770751953125\n",
      "Step: 2400  \tTraining loss: 0.3231143653392792\n",
      "Step: 2400  \tTraining accuracy: 0.8085388541221619\n",
      "Step: 2400  \tValid loss: 0.3188791871070862\n",
      "Step: 2500  \tTraining loss: 0.3224482536315918\n",
      "Step: 2500  \tTraining accuracy: 0.8089103102684021\n",
      "Step: 2500  \tValid loss: 0.3181273639202118\n",
      "Step: 2600  \tTraining loss: 0.3218272626399994\n",
      "Step: 2600  \tTraining accuracy: 0.8092266321182251\n",
      "Step: 2600  \tValid loss: 0.31720930337905884\n",
      "Step: 2700  \tTraining loss: 0.32139384746551514\n",
      "Step: 2700  \tTraining accuracy: 0.8096691966056824\n",
      "Step: 2700  \tValid loss: 0.3168220818042755\n",
      "Step: 2800  \tTraining loss: 0.32104307413101196\n",
      "Step: 2800  \tTraining accuracy: 0.8101519346237183\n",
      "Step: 2800  \tValid loss: 0.3164725601673126\n",
      "Step: 2900  \tTraining loss: 0.32074835896492004\n",
      "Step: 2900  \tTraining accuracy: 0.810600757598877\n",
      "Step: 2900  \tValid loss: 0.31623655557632446\n",
      "Step: 3000  \tTraining loss: 0.3204991817474365\n",
      "Step: 3000  \tTraining accuracy: 0.8110191822052002\n",
      "Step: 3000  \tValid loss: 0.3161836862564087\n",
      "Step: 3100  \tTraining loss: 0.3202870488166809\n",
      "Step: 3100  \tTraining accuracy: 0.8114101886749268\n",
      "Step: 3100  \tValid loss: 0.31600528955459595\n",
      "Step: 3200  \tTraining loss: 0.3201175630092621\n",
      "Step: 3200  \tTraining accuracy: 0.811776340007782\n",
      "Step: 3200  \tValid loss: 0.31577789783477783\n",
      "Step: 3300  \tTraining loss: 0.3199712932109833\n",
      "Step: 3300  \tTraining accuracy: 0.8120995759963989\n",
      "Step: 3300  \tValid loss: 0.3156450688838959\n",
      "Step: 3400  \tTraining loss: 0.31986159086227417\n",
      "Step: 3400  \tTraining accuracy: 0.8124035000801086\n",
      "Step: 3400  \tValid loss: 0.3155199885368347\n",
      "Step: 3500  \tTraining loss: 0.3197598159313202\n",
      "Step: 3500  \tTraining accuracy: 0.8126897811889648\n",
      "Step: 3500  \tValid loss: 0.31545931100845337\n",
      "Step: 3600  \tTraining loss: 0.31967347860336304\n",
      "Step: 3600  \tTraining accuracy: 0.8129786849021912\n",
      "Step: 3600  \tValid loss: 0.3154149353504181\n",
      "Step: 3700  \tTraining loss: 0.3196195363998413\n",
      "Step: 3700  \tTraining accuracy: 0.8133062124252319\n",
      "Step: 3700  \tValid loss: 0.3154190480709076\n",
      "Step: 3800  \tTraining loss: 0.3195610046386719\n",
      "Step: 3800  \tTraining accuracy: 0.8136162757873535\n",
      "Step: 3800  \tValid loss: 0.31543397903442383\n",
      "Step: 3900  \tTraining loss: 0.3195159137248993\n",
      "Step: 3900  \tTraining accuracy: 0.8139102458953857\n",
      "Step: 3900  \tValid loss: 0.3154461681842804\n",
      "Step: 4000  \tTraining loss: 0.3194822371006012\n",
      "Step: 4000  \tTraining accuracy: 0.8141893148422241\n",
      "Step: 4000  \tValid loss: 0.3154824376106262\n",
      "Step: 4100  \tTraining loss: 0.3194426894187927\n",
      "Step: 4100  \tTraining accuracy: 0.8144546151161194\n",
      "Step: 4100  \tValid loss: 0.31546661257743835\n",
      "Step: 4200  \tTraining loss: 0.31941521167755127\n",
      "Step: 4200  \tTraining accuracy: 0.8147071003913879\n",
      "Step: 4200  \tValid loss: 0.31547239422798157\n",
      "Step: 4300  \tTraining loss: 0.3193935751914978\n",
      "Step: 4300  \tTraining accuracy: 0.8149789571762085\n",
      "Step: 4300  \tValid loss: 0.31548601388931274\n",
      "Step: 4400  \tTraining loss: 0.319374680519104\n",
      "Step: 4400  \tTraining accuracy: 0.8152687549591064\n",
      "Step: 4400  \tValid loss: 0.31550395488739014\n",
      "Step: 4500  \tTraining loss: 0.319354772567749\n",
      "Step: 4500  \tTraining accuracy: 0.8155455589294434\n",
      "Step: 4500  \tValid loss: 0.3154756724834442\n",
      "Step: 4600  \tTraining loss: 0.31933829188346863\n",
      "Step: 4600  \tTraining accuracy: 0.8158102035522461\n",
      "Step: 4600  \tValid loss: 0.3154730498790741\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8160634\n",
      "Precision: 0.6477987\n",
      "Recall: 0.59195405\n",
      "F1 score: 0.77930385\n",
      "AUC: 0.74770117\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.816063   0.647799  0.591954  0.779304  0.747701  0.319331      0.815895   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.315367        0.81593   0.298809      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  4659.0  \n",
      "12\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_17942/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(754, 3)\n",
      "(754, 1)\n",
      "(400, 3)\n",
      "(400, 1)\n",
      "(325, 3)\n",
      "(325, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5444535613059998\n",
      "Step: 100  \tTraining accuracy: 0.7334217429161072\n",
      "Step: 100  \tValid loss: 0.5632800459861755\n",
      "Step: 200  \tTraining loss: 0.5049623847007751\n",
      "Step: 200  \tTraining accuracy: 0.7313031554222107\n",
      "Step: 200  \tValid loss: 0.5360503196716309\n",
      "Step: 300  \tTraining loss: 0.4835911691188812\n",
      "Step: 300  \tTraining accuracy: 0.7308728694915771\n",
      "Step: 300  \tValid loss: 0.516322135925293\n",
      "Step: 400  \tTraining loss: 0.4656681418418884\n",
      "Step: 400  \tTraining accuracy: 0.7306877374649048\n",
      "Step: 400  \tValid loss: 0.49888449907302856\n",
      "Step: 500  \tTraining loss: 0.45048433542251587\n",
      "Step: 500  \tTraining accuracy: 0.7347826361656189\n",
      "Step: 500  \tValid loss: 0.4828830063343048\n",
      "Step: 600  \tTraining loss: 0.4393225908279419\n",
      "Step: 600  \tTraining accuracy: 0.7438949346542358\n",
      "Step: 600  \tValid loss: 0.47108638286590576\n",
      "Step: 700  \tTraining loss: 0.43123167753219604\n",
      "Step: 700  \tTraining accuracy: 0.7520772814750671\n",
      "Step: 700  \tValid loss: 0.46234330534935\n",
      "Step: 800  \tTraining loss: 0.42028573155403137\n",
      "Step: 800  \tTraining accuracy: 0.7583506107330322\n",
      "Step: 800  \tValid loss: 0.4491550922393799\n",
      "Step: 900  \tTraining loss: 0.4124247431755066\n",
      "Step: 900  \tTraining accuracy: 0.7633084654808044\n",
      "Step: 900  \tValid loss: 0.44023048877716064\n",
      "Step: 1000  \tTraining loss: 0.4015049338340759\n",
      "Step: 1000  \tTraining accuracy: 0.7670813798904419\n",
      "Step: 1000  \tValid loss: 0.4232945740222931\n",
      "Step: 1100  \tTraining loss: 0.39799654483795166\n",
      "Step: 1100  \tTraining accuracy: 0.7706510424613953\n",
      "Step: 1100  \tValid loss: 0.42420074343681335\n",
      "Step: 1200  \tTraining loss: 0.39622166752815247\n",
      "Step: 1200  \tTraining accuracy: 0.7732479572296143\n",
      "Step: 1200  \tValid loss: 0.4233514368534088\n",
      "Step: 1300  \tTraining loss: 0.3950296938419342\n",
      "Step: 1300  \tTraining accuracy: 0.7754296660423279\n",
      "Step: 1300  \tValid loss: 0.4218899607658386\n",
      "Step: 1400  \tTraining loss: 0.39419999718666077\n",
      "Step: 1400  \tTraining accuracy: 0.7777388691902161\n",
      "Step: 1400  \tValid loss: 0.4211260974407196\n",
      "Step: 1500  \tTraining loss: 0.39357081055641174\n",
      "Step: 1500  \tTraining accuracy: 0.7798694968223572\n",
      "Step: 1500  \tValid loss: 0.4206693768501282\n",
      "Step: 1600  \tTraining loss: 0.3930659592151642\n",
      "Step: 1600  \tTraining accuracy: 0.7817690372467041\n",
      "Step: 1600  \tValid loss: 0.4203528165817261\n",
      "Step: 1700  \tTraining loss: 0.39265185594558716\n",
      "Step: 1700  \tTraining accuracy: 0.7834793925285339\n",
      "Step: 1700  \tValid loss: 0.42011773586273193\n",
      "Step: 1800  \tTraining loss: 0.39230433106422424\n",
      "Step: 1800  \tTraining accuracy: 0.7849944233894348\n",
      "Step: 1800  \tValid loss: 0.4199085235595703\n",
      "Step: 1900  \tTraining loss: 0.3919963836669922\n",
      "Step: 1900  \tTraining accuracy: 0.7863091826438904\n",
      "Step: 1900  \tValid loss: 0.41981247067451477\n",
      "Step: 2000  \tTraining loss: 0.39170634746551514\n",
      "Step: 2000  \tTraining accuracy: 0.7874891757965088\n",
      "Step: 2000  \tValid loss: 0.4196241796016693\n",
      "Step: 2100  \tTraining loss: 0.3914738595485687\n",
      "Step: 2100  \tTraining accuracy: 0.7885540723800659\n",
      "Step: 2100  \tValid loss: 0.4192637503147125\n",
      "Step: 2200  \tTraining loss: 0.39127427339553833\n",
      "Step: 2200  \tTraining accuracy: 0.7895200252532959\n",
      "Step: 2200  \tValid loss: 0.41900748014450073\n",
      "Step: 2300  \tTraining loss: 0.39109155535697937\n",
      "Step: 2300  \tTraining accuracy: 0.7904000878334045\n",
      "Step: 2300  \tValid loss: 0.4188043177127838\n",
      "Step: 2400  \tTraining loss: 0.39092448353767395\n",
      "Step: 2400  \tTraining accuracy: 0.7912052869796753\n",
      "Step: 2400  \tValid loss: 0.4186120331287384\n",
      "Step: 2500  \tTraining loss: 0.390778511762619\n",
      "Step: 2500  \tTraining accuracy: 0.7919448018074036\n",
      "Step: 2500  \tValid loss: 0.41844308376312256\n",
      "Step: 2600  \tTraining loss: 0.390645831823349\n",
      "Step: 2600  \tTraining accuracy: 0.7926263809204102\n",
      "Step: 2600  \tValid loss: 0.41830185055732727\n",
      "Step: 2700  \tTraining loss: 0.3905191719532013\n",
      "Step: 2700  \tTraining accuracy: 0.7932819724082947\n",
      "Step: 2700  \tValid loss: 0.41818854212760925\n",
      "Step: 2800  \tTraining loss: 0.3903981149196625\n",
      "Step: 2800  \tTraining accuracy: 0.7939144968986511\n",
      "Step: 2800  \tValid loss: 0.4180881381034851\n",
      "Step: 2900  \tTraining loss: 0.39028942584991455\n",
      "Step: 2900  \tTraining accuracy: 0.7945026755332947\n",
      "Step: 2900  \tValid loss: 0.4179578721523285\n",
      "Step: 3000  \tTraining loss: 0.3901861906051636\n",
      "Step: 3000  \tTraining accuracy: 0.7950509786605835\n",
      "Step: 3000  \tValid loss: 0.4178503453731537\n",
      "Step: 3100  \tTraining loss: 0.39008811116218567\n",
      "Step: 3100  \tTraining accuracy: 0.7955633401870728\n",
      "Step: 3100  \tValid loss: 0.417755663394928\n",
      "Step: 3200  \tTraining loss: 0.38999614119529724\n",
      "Step: 3200  \tTraining accuracy: 0.7960431575775146\n",
      "Step: 3200  \tValid loss: 0.4176683723926544\n",
      "Step: 3300  \tTraining loss: 0.38990241289138794\n",
      "Step: 3300  \tTraining accuracy: 0.7964934706687927\n",
      "Step: 3300  \tValid loss: 0.41758328676223755\n",
      "Step: 3400  \tTraining loss: 0.3898158669471741\n",
      "Step: 3400  \tTraining accuracy: 0.7969169020652771\n",
      "Step: 3400  \tValid loss: 0.4174593687057495\n",
      "Step: 3500  \tTraining loss: 0.3897300660610199\n",
      "Step: 3500  \tTraining accuracy: 0.7973158359527588\n",
      "Step: 3500  \tValid loss: 0.41738471388816833\n",
      "Step: 3600  \tTraining loss: 0.38963884115219116\n",
      "Step: 3600  \tTraining accuracy: 0.7976922392845154\n",
      "Step: 3600  \tValid loss: 0.4173929691314697\n",
      "Step: 3700  \tTraining loss: 0.38956376910209656\n",
      "Step: 3700  \tTraining accuracy: 0.7980480790138245\n",
      "Step: 3700  \tValid loss: 0.41733819246292114\n",
      "Step: 3800  \tTraining loss: 0.38949236273765564\n",
      "Step: 3800  \tTraining accuracy: 0.7983849048614502\n",
      "Step: 3800  \tValid loss: 0.41731566190719604\n",
      "Step: 3900  \tTraining loss: 0.3894276022911072\n",
      "Step: 3900  \tTraining accuracy: 0.7987042665481567\n",
      "Step: 3900  \tValid loss: 0.4172898828983307\n",
      "Step: 4000  \tTraining loss: 0.3893684446811676\n",
      "Step: 4000  \tTraining accuracy: 0.7990074157714844\n",
      "Step: 4000  \tValid loss: 0.41724318265914917\n",
      "Step: 4100  \tTraining loss: 0.38931071758270264\n",
      "Step: 4100  \tTraining accuracy: 0.7992956638336182\n",
      "Step: 4100  \tValid loss: 0.417236864566803\n",
      "Step: 4200  \tTraining loss: 0.38925793766975403\n",
      "Step: 4200  \tTraining accuracy: 0.7995699644088745\n",
      "Step: 4200  \tValid loss: 0.4172040522098541\n",
      "Step: 4300  \tTraining loss: 0.3892088830471039\n",
      "Step: 4300  \tTraining accuracy: 0.7998313903808594\n",
      "Step: 4300  \tValid loss: 0.41720280051231384\n",
      "Step: 4400  \tTraining loss: 0.38916444778442383\n",
      "Step: 4400  \tTraining accuracy: 0.8000808358192444\n",
      "Step: 4400  \tValid loss: 0.4171425700187683\n",
      "Step: 4500  \tTraining loss: 0.389119952917099\n",
      "Step: 4500  \tTraining accuracy: 0.8003190159797668\n",
      "Step: 4500  \tValid loss: 0.4171331822872162\n",
      "Step: 4600  \tTraining loss: 0.3890785574913025\n",
      "Step: 4600  \tTraining accuracy: 0.8005467057228088\n",
      "Step: 4600  \tValid loss: 0.4171050190925598\n",
      "Step: 4700  \tTraining loss: 0.389038622379303\n",
      "Step: 4700  \tTraining accuracy: 0.8007646799087524\n",
      "Step: 4700  \tValid loss: 0.41710129380226135\n",
      "Step: 4800  \tTraining loss: 0.38899898529052734\n",
      "Step: 4800  \tTraining accuracy: 0.8009734153747559\n",
      "Step: 4800  \tValid loss: 0.4171803593635559\n",
      "Step: 4900  \tTraining loss: 0.3889623284339905\n",
      "Step: 4900  \tTraining accuracy: 0.8011735677719116\n",
      "Step: 4900  \tValid loss: 0.41710782051086426\n",
      "Step: 5000  \tTraining loss: 0.3889257311820984\n",
      "Step: 5000  \tTraining accuracy: 0.8013656735420227\n",
      "Step: 5000  \tValid loss: 0.4171452224254608\n",
      "Step: 5100  \tTraining loss: 0.3888918459415436\n",
      "Step: 5100  \tTraining accuracy: 0.8015500903129578\n",
      "Step: 5100  \tValid loss: 0.4171641170978546\n",
      "Step: 5200  \tTraining loss: 0.388856440782547\n",
      "Step: 5200  \tTraining accuracy: 0.8017274141311646\n",
      "Step: 5200  \tValid loss: 0.41712307929992676\n",
      "Step: 5300  \tTraining loss: 0.3888244330883026\n",
      "Step: 5300  \tTraining accuracy: 0.8018979430198669\n",
      "Step: 5300  \tValid loss: 0.4171063303947449\n",
      "Step: 5400  \tTraining loss: 0.38879305124282837\n",
      "Step: 5400  \tTraining accuracy: 0.8020874261856079\n",
      "Step: 5400  \tValid loss: 0.41712096333503723\n",
      "Step: 5500  \tTraining loss: 0.38876140117645264\n",
      "Step: 5500  \tTraining accuracy: 0.8023195266723633\n",
      "Step: 5500  \tValid loss: 0.4170769155025482\n",
      "Step: 5600  \tTraining loss: 0.3887234330177307\n",
      "Step: 5600  \tTraining accuracy: 0.8025432825088501\n",
      "Step: 5600  \tValid loss: 0.41706129908561707\n",
      "Step: 5700  \tTraining loss: 0.38869303464889526\n",
      "Step: 5700  \tTraining accuracy: 0.8027710914611816\n",
      "Step: 5700  \tValid loss: 0.4169900119304657\n",
      "Step: 5800  \tTraining loss: 0.3886622488498688\n",
      "Step: 5800  \tTraining accuracy: 0.8029791712760925\n",
      "Step: 5800  \tValid loss: 0.4170868694782257\n",
      "Step: 5900  \tTraining loss: 0.3886304199695587\n",
      "Step: 5900  \tTraining accuracy: 0.8031917214393616\n",
      "Step: 5900  \tValid loss: 0.4170703887939453\n",
      "Step: 6000  \tTraining loss: 0.38860079646110535\n",
      "Step: 6000  \tTraining accuracy: 0.8033857941627502\n",
      "Step: 6000  \tValid loss: 0.41713201999664307\n",
      "Step: 6100  \tTraining loss: 0.388572096824646\n",
      "Step: 6100  \tTraining accuracy: 0.8035845756530762\n",
      "Step: 6100  \tValid loss: 0.41714903712272644\n",
      "Step: 6200  \tTraining loss: 0.3885461091995239\n",
      "Step: 6200  \tTraining accuracy: 0.8037769198417664\n",
      "Step: 6200  \tValid loss: 0.41703590750694275\n",
      "Step: 6300  \tTraining loss: 0.38851743936538696\n",
      "Step: 6300  \tTraining accuracy: 0.8039631247520447\n",
      "Step: 6300  \tValid loss: 0.4171837568283081\n",
      "Step: 6400  \tTraining loss: 0.38849055767059326\n",
      "Step: 6400  \tTraining accuracy: 0.804143488407135\n",
      "Step: 6400  \tValid loss: 0.4171749949455261\n",
      "Step: 6500  \tTraining loss: 0.38846468925476074\n",
      "Step: 6500  \tTraining accuracy: 0.8043181896209717\n",
      "Step: 6500  \tValid loss: 0.4171134829521179\n",
      "Step: 6600  \tTraining loss: 0.38844388723373413\n",
      "Step: 6600  \tTraining accuracy: 0.8044875860214233\n",
      "Step: 6600  \tValid loss: 0.41710636019706726\n",
      "Step: 6700  \tTraining loss: 0.388418585062027\n",
      "Step: 6700  \tTraining accuracy: 0.8046519160270691\n",
      "Step: 6700  \tValid loss: 0.417225182056427\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.80481136\n",
      "Precision: 0.8573883\n",
      "Recall: 0.90235084\n",
      "F1 score: 0.82537484\n",
      "AUC: 0.7447078\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.804811   0.857388  0.902351  0.825375  0.744708  0.388406      0.804709   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.416964       0.804728   0.403931      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  6752.0  \n",
      "13\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_91327/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(957, 3)\n",
      "(957, 1)\n",
      "(528, 3)\n",
      "(528, 1)\n",
      "(429, 3)\n",
      "(429, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6121940612792969\n",
      "Step: 100  \tTraining accuracy: 0.6823406219482422\n",
      "Step: 100  \tValid loss: 0.6349525451660156\n",
      "Step: 200  \tTraining loss: 0.5007437467575073\n",
      "Step: 200  \tTraining accuracy: 0.7049808502197266\n",
      "Step: 200  \tValid loss: 0.5260787606239319\n",
      "Step: 300  \tTraining loss: 0.4027055501937866\n",
      "Step: 300  \tTraining accuracy: 0.7425287365913391\n",
      "Step: 300  \tValid loss: 0.42341744899749756\n",
      "Step: 400  \tTraining loss: 0.3578245937824249\n",
      "Step: 400  \tTraining accuracy: 0.7716076970100403\n",
      "Step: 400  \tValid loss: 0.37327441573143005\n",
      "Step: 500  \tTraining loss: 0.34047412872314453\n",
      "Step: 500  \tTraining accuracy: 0.7902008295059204\n",
      "Step: 500  \tValid loss: 0.3514937162399292\n",
      "Step: 600  \tTraining loss: 0.333550363779068\n",
      "Step: 600  \tTraining accuracy: 0.803172767162323\n",
      "Step: 600  \tValid loss: 0.3408200740814209\n",
      "Step: 700  \tTraining loss: 0.33042511343955994\n",
      "Step: 700  \tTraining accuracy: 0.8121533393859863\n",
      "Step: 700  \tValid loss: 0.3351297974586487\n",
      "Step: 800  \tTraining loss: 0.32888245582580566\n",
      "Step: 800  \tTraining accuracy: 0.819157063961029\n",
      "Step: 800  \tValid loss: 0.33176252245903015\n",
      "Step: 900  \tTraining loss: 0.3281621038913727\n",
      "Step: 900  \tTraining accuracy: 0.8243899345397949\n",
      "Step: 900  \tValid loss: 0.3298567235469818\n",
      "Step: 1000  \tTraining loss: 0.3278035819530487\n",
      "Step: 1000  \tTraining accuracy: 0.8285211324691772\n",
      "Step: 1000  \tValid loss: 0.32877424359321594\n",
      "Step: 1100  \tTraining loss: 0.3275863230228424\n",
      "Step: 1100  \tTraining accuracy: 0.8318654298782349\n",
      "Step: 1100  \tValid loss: 0.32812148332595825\n",
      "Step: 1200  \tTraining loss: 0.3273879885673523\n",
      "Step: 1200  \tTraining accuracy: 0.834628164768219\n",
      "Step: 1200  \tValid loss: 0.3276400566101074\n",
      "Step: 1300  \tTraining loss: 0.3272380828857422\n",
      "Step: 1300  \tTraining accuracy: 0.836948812007904\n",
      "Step: 1300  \tValid loss: 0.327283650636673\n",
      "Step: 1400  \tTraining loss: 0.3271065056324005\n",
      "Step: 1400  \tTraining accuracy: 0.8389256596565247\n",
      "Step: 1400  \tValid loss: 0.32700371742248535\n",
      "Step: 1500  \tTraining loss: 0.32698747515678406\n",
      "Step: 1500  \tTraining accuracy: 0.8406298160552979\n",
      "Step: 1500  \tValid loss: 0.32677537202835083\n",
      "Step: 1600  \tTraining loss: 0.32686713337898254\n",
      "Step: 1600  \tTraining accuracy: 0.8421141505241394\n",
      "Step: 1600  \tValid loss: 0.32669273018836975\n",
      "Step: 1700  \tTraining loss: 0.326761931180954\n",
      "Step: 1700  \tTraining accuracy: 0.8433868288993835\n",
      "Step: 1700  \tValid loss: 0.3265073299407959\n",
      "Step: 1800  \tTraining loss: 0.3266621232032776\n",
      "Step: 1800  \tTraining accuracy: 0.8445737957954407\n",
      "Step: 1800  \tValid loss: 0.32637909054756165\n",
      "Step: 1900  \tTraining loss: 0.32656916975975037\n",
      "Step: 1900  \tTraining accuracy: 0.8456324934959412\n",
      "Step: 1900  \tValid loss: 0.32625284790992737\n",
      "Step: 2000  \tTraining loss: 0.326482355594635\n",
      "Step: 2000  \tTraining accuracy: 0.8465825319290161\n",
      "Step: 2000  \tValid loss: 0.32612040638923645\n",
      "Step: 2100  \tTraining loss: 0.3263999819755554\n",
      "Step: 2100  \tTraining accuracy: 0.8474399447441101\n",
      "Step: 2100  \tValid loss: 0.3260268568992615\n",
      "Step: 2200  \tTraining loss: 0.3263232111930847\n",
      "Step: 2200  \tTraining accuracy: 0.8482175469398499\n",
      "Step: 2200  \tValid loss: 0.32592710852622986\n",
      "Step: 2300  \tTraining loss: 0.3262515068054199\n",
      "Step: 2300  \tTraining accuracy: 0.8489260673522949\n",
      "Step: 2300  \tValid loss: 0.3258495330810547\n",
      "Step: 2400  \tTraining loss: 0.326181024312973\n",
      "Step: 2400  \tTraining accuracy: 0.8495742678642273\n",
      "Step: 2400  \tValid loss: 0.3257432281970978\n",
      "Step: 2500  \tTraining loss: 0.32611286640167236\n",
      "Step: 2500  \tTraining accuracy: 0.8501695394515991\n",
      "Step: 2500  \tValid loss: 0.325701504945755\n",
      "Step: 2600  \tTraining loss: 0.32604900002479553\n",
      "Step: 2600  \tTraining accuracy: 0.8507181406021118\n",
      "Step: 2600  \tValid loss: 0.32564792037010193\n",
      "Step: 2700  \tTraining loss: 0.3259856700897217\n",
      "Step: 2700  \tTraining accuracy: 0.8512253165245056\n",
      "Step: 2700  \tValid loss: 0.32560795545578003\n",
      "Step: 2800  \tTraining loss: 0.3259253203868866\n",
      "Step: 2800  \tTraining accuracy: 0.8516956567764282\n",
      "Step: 2800  \tValid loss: 0.3255910575389862\n",
      "Step: 2900  \tTraining loss: 0.3258662819862366\n",
      "Step: 2900  \tTraining accuracy: 0.852224588394165\n",
      "Step: 2900  \tValid loss: 0.325517863035202\n",
      "Step: 3000  \tTraining loss: 0.32580700516700745\n",
      "Step: 3000  \tTraining accuracy: 0.8528239727020264\n",
      "Step: 3000  \tValid loss: 0.32552260160446167\n",
      "Step: 3100  \tTraining loss: 0.3257511854171753\n",
      "Step: 3100  \tTraining accuracy: 0.8533840179443359\n",
      "Step: 3100  \tValid loss: 0.3254857063293457\n",
      "Step: 3200  \tTraining loss: 0.3256969749927521\n",
      "Step: 3200  \tTraining accuracy: 0.8539085388183594\n",
      "Step: 3200  \tValid loss: 0.3254234194755554\n",
      "Step: 3300  \tTraining loss: 0.32564350962638855\n",
      "Step: 3300  \tTraining accuracy: 0.8544007539749146\n",
      "Step: 3300  \tValid loss: 0.32540982961654663\n",
      "Step: 3400  \tTraining loss: 0.3255898356437683\n",
      "Step: 3400  \tTraining accuracy: 0.8548635840415955\n",
      "Step: 3400  \tValid loss: 0.3254055976867676\n",
      "Step: 3500  \tTraining loss: 0.3255378007888794\n",
      "Step: 3500  \tTraining accuracy: 0.8552996516227722\n",
      "Step: 3500  \tValid loss: 0.32535529136657715\n",
      "Step: 3600  \tTraining loss: 0.3254876136779785\n",
      "Step: 3600  \tTraining accuracy: 0.855711042881012\n",
      "Step: 3600  \tValid loss: 0.32532477378845215\n",
      "Step: 3700  \tTraining loss: 0.32539254426956177\n",
      "Step: 3700  \tTraining accuracy: 0.8560999631881714\n",
      "Step: 3700  \tValid loss: 0.32540014386177063\n",
      "Step: 3800  \tTraining loss: 0.32531026005744934\n",
      "Step: 3800  \tTraining accuracy: 0.856468141078949\n",
      "Step: 3800  \tValid loss: 0.3254321217536926\n",
      "Step: 3900  \tTraining loss: 0.32520490884780884\n",
      "Step: 3900  \tTraining accuracy: 0.8568171858787537\n",
      "Step: 3900  \tValid loss: 0.3253965675830841\n",
      "Step: 4000  \tTraining loss: 0.32513129711151123\n",
      "Step: 4000  \tTraining accuracy: 0.8571485280990601\n",
      "Step: 4000  \tValid loss: 0.32529616355895996\n",
      "Step: 4100  \tTraining loss: 0.32507312297821045\n",
      "Step: 4100  \tTraining accuracy: 0.857463538646698\n",
      "Step: 4100  \tValid loss: 0.3252057135105133\n",
      "Step: 4200  \tTraining loss: 0.3250140845775604\n",
      "Step: 4200  \tTraining accuracy: 0.8577633500099182\n",
      "Step: 4200  \tValid loss: 0.32516780495643616\n",
      "Step: 4300  \tTraining loss: 0.32495591044425964\n",
      "Step: 4300  \tTraining accuracy: 0.8580490350723267\n",
      "Step: 4300  \tValid loss: 0.3251590430736542\n",
      "Step: 4400  \tTraining loss: 0.3248986303806305\n",
      "Step: 4400  \tTraining accuracy: 0.8583216071128845\n",
      "Step: 4400  \tValid loss: 0.3251502513885498\n",
      "Step: 4500  \tTraining loss: 0.3248416483402252\n",
      "Step: 4500  \tTraining accuracy: 0.8585819602012634\n",
      "Step: 4500  \tValid loss: 0.32514432072639465\n",
      "Step: 4600  \tTraining loss: 0.3247827887535095\n",
      "Step: 4600  \tTraining accuracy: 0.8588308095932007\n",
      "Step: 4600  \tValid loss: 0.3250980079174042\n",
      "Step: 4700  \tTraining loss: 0.32472485303878784\n",
      "Step: 4700  \tTraining accuracy: 0.8590689897537231\n",
      "Step: 4700  \tValid loss: 0.3251047134399414\n",
      "Step: 4800  \tTraining loss: 0.3246675729751587\n",
      "Step: 4800  \tTraining accuracy: 0.8592971563339233\n",
      "Step: 4800  \tValid loss: 0.3250614404678345\n",
      "Step: 4900  \tTraining loss: 0.32460805773735046\n",
      "Step: 4900  \tTraining accuracy: 0.859515905380249\n",
      "Step: 4900  \tValid loss: 0.3250485360622406\n",
      "Step: 5000  \tTraining loss: 0.3245488107204437\n",
      "Step: 5000  \tTraining accuracy: 0.8597257733345032\n",
      "Step: 5000  \tValid loss: 0.32506027817726135\n",
      "Step: 5100  \tTraining loss: 0.3244892358779907\n",
      "Step: 5100  \tTraining accuracy: 0.8599273562431335\n",
      "Step: 5100  \tValid loss: 0.32502686977386475\n",
      "Step: 5200  \tTraining loss: 0.32442936301231384\n",
      "Step: 5200  \tTraining accuracy: 0.8601211309432983\n",
      "Step: 5200  \tValid loss: 0.32498404383659363\n",
      "Step: 5300  \tTraining loss: 0.3243708908557892\n",
      "Step: 5300  \tTraining accuracy: 0.860307514667511\n",
      "Step: 5300  \tValid loss: 0.32499703764915466\n",
      "Step: 5400  \tTraining loss: 0.3243090808391571\n",
      "Step: 5400  \tTraining accuracy: 0.8604869246482849\n",
      "Step: 5400  \tValid loss: 0.32498109340667725\n",
      "Step: 5500  \tTraining loss: 0.32425013184547424\n",
      "Step: 5500  \tTraining accuracy: 0.8606597185134888\n",
      "Step: 5500  \tValid loss: 0.32494744658470154\n",
      "Step: 5600  \tTraining loss: 0.3241899609565735\n",
      "Step: 5600  \tTraining accuracy: 0.8608263731002808\n",
      "Step: 5600  \tValid loss: 0.32496118545532227\n",
      "Step: 5700  \tTraining loss: 0.32412999868392944\n",
      "Step: 5700  \tTraining accuracy: 0.8609870672225952\n",
      "Step: 5700  \tValid loss: 0.32493826746940613\n",
      "Step: 5800  \tTraining loss: 0.32407212257385254\n",
      "Step: 5800  \tTraining accuracy: 0.8611421585083008\n",
      "Step: 5800  \tValid loss: 0.32495173811912537\n",
      "Step: 5900  \tTraining loss: 0.3240145146846771\n",
      "Step: 5900  \tTraining accuracy: 0.8612919449806213\n",
      "Step: 5900  \tValid loss: 0.32492563128471375\n",
      "Step: 6000  \tTraining loss: 0.32395678758621216\n",
      "Step: 6000  \tTraining accuracy: 0.8614367246627808\n",
      "Step: 6000  \tValid loss: 0.32491442561149597\n",
      "Step: 6100  \tTraining loss: 0.32389917969703674\n",
      "Step: 6100  \tTraining accuracy: 0.8615767359733582\n",
      "Step: 6100  \tValid loss: 0.32492321729660034\n",
      "Step: 6200  \tTraining loss: 0.32384154200553894\n",
      "Step: 6200  \tTraining accuracy: 0.8617121577262878\n",
      "Step: 6200  \tValid loss: 0.3249370753765106\n",
      "Step: 6300  \tTraining loss: 0.3237842917442322\n",
      "Step: 6300  \tTraining accuracy: 0.8618432879447937\n",
      "Step: 6300  \tValid loss: 0.32489633560180664\n",
      "Step: 6400  \tTraining loss: 0.3237265646457672\n",
      "Step: 6400  \tTraining accuracy: 0.8619702458381653\n",
      "Step: 6400  \tValid loss: 0.32491758465766907\n",
      "Step: 6500  \tTraining loss: 0.3236670196056366\n",
      "Step: 6500  \tTraining accuracy: 0.8620932698249817\n",
      "Step: 6500  \tValid loss: 0.3249419629573822\n",
      "Step: 6600  \tTraining loss: 0.32360905408859253\n",
      "Step: 6600  \tTraining accuracy: 0.8622125387191772\n",
      "Step: 6600  \tValid loss: 0.3249497413635254\n",
      "Step: 6700  \tTraining loss: 0.32352539896965027\n",
      "Step: 6700  \tTraining accuracy: 0.8623282313346863\n",
      "Step: 6700  \tValid loss: 0.32499417662620544\n",
      "Step: 6800  \tTraining loss: 0.3234524428844452\n",
      "Step: 6800  \tTraining accuracy: 0.8624405264854431\n",
      "Step: 6800  \tValid loss: 0.324978232383728\n",
      "Step: 6900  \tTraining loss: 0.3233970105648041\n",
      "Step: 6900  \tTraining accuracy: 0.8625494837760925\n",
      "Step: 6900  \tValid loss: 0.3249590992927551\n",
      "Step: 7000  \tTraining loss: 0.3233434557914734\n",
      "Step: 7000  \tTraining accuracy: 0.8626553416252136\n",
      "Step: 7000  \tValid loss: 0.3249351978302002\n",
      "Step: 7100  \tTraining loss: 0.3232914209365845\n",
      "Step: 7100  \tTraining accuracy: 0.8627581596374512\n",
      "Step: 7100  \tValid loss: 0.3249645233154297\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8628581\n",
      "Precision: 0.88988096\n",
      "Recall: 0.91577333\n",
      "F1 score: 0.8752312\n",
      "AUC: 0.83617616\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.862858   0.889881  0.915773  0.875231  0.836176  0.323243      0.862842   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.324869       0.862839   0.289185      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  7198.0  \n",
      "14\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_35160/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(1798, 3)\n",
      "(1798, 1)\n",
      "(992, 3)\n",
      "(992, 1)\n",
      "(806, 3)\n",
      "(806, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6288299560546875\n",
      "Step: 100  \tTraining accuracy: 0.6646273732185364\n",
      "Step: 100  \tValid loss: 0.634850025177002\n",
      "Step: 200  \tTraining loss: 0.5592341423034668\n",
      "Step: 200  \tTraining accuracy: 0.6750092506408691\n",
      "Step: 200  \tValid loss: 0.5885993242263794\n",
      "Step: 300  \tTraining loss: 0.5160087943077087\n",
      "Step: 300  \tTraining accuracy: 0.7008898854255676\n",
      "Step: 300  \tValid loss: 0.5531308650970459\n",
      "Step: 400  \tTraining loss: 0.47592779994010925\n",
      "Step: 400  \tTraining accuracy: 0.7194501757621765\n",
      "Step: 400  \tValid loss: 0.5147565007209778\n",
      "Step: 500  \tTraining loss: 0.4460107982158661\n",
      "Step: 500  \tTraining accuracy: 0.7336546778678894\n",
      "Step: 500  \tValid loss: 0.48771020770072937\n",
      "Step: 600  \tTraining loss: 0.4275222718715668\n",
      "Step: 600  \tTraining accuracy: 0.7444129586219788\n",
      "Step: 600  \tValid loss: 0.4727899432182312\n",
      "Step: 700  \tTraining loss: 0.41680678725242615\n",
      "Step: 700  \tTraining accuracy: 0.7525455355644226\n",
      "Step: 700  \tValid loss: 0.46444764733314514\n",
      "Step: 800  \tTraining loss: 0.4110364317893982\n",
      "Step: 800  \tTraining accuracy: 0.7588061094284058\n",
      "Step: 800  \tValid loss: 0.46034422516822815\n",
      "Step: 900  \tTraining loss: 0.407962441444397\n",
      "Step: 900  \tTraining accuracy: 0.7642478346824646\n",
      "Step: 900  \tValid loss: 0.4583596885204315\n",
      "Step: 1000  \tTraining loss: 0.406281977891922\n",
      "Step: 1000  \tTraining accuracy: 0.7688660025596619\n",
      "Step: 1000  \tValid loss: 0.4573121666908264\n",
      "Step: 1100  \tTraining loss: 0.40522170066833496\n",
      "Step: 1100  \tTraining accuracy: 0.772472083568573\n",
      "Step: 1100  \tValid loss: 0.45674195885658264\n",
      "Step: 1200  \tTraining loss: 0.4044165015220642\n",
      "Step: 1200  \tTraining accuracy: 0.7754510045051575\n",
      "Step: 1200  \tValid loss: 0.4559575319290161\n",
      "Step: 1300  \tTraining loss: 0.4037788212299347\n",
      "Step: 1300  \tTraining accuracy: 0.7779088020324707\n",
      "Step: 1300  \tValid loss: 0.4551975727081299\n",
      "Step: 1400  \tTraining loss: 0.4032209515571594\n",
      "Step: 1400  \tTraining accuracy: 0.7800230979919434\n",
      "Step: 1400  \tValid loss: 0.45440909266471863\n",
      "Step: 1500  \tTraining loss: 0.4026859700679779\n",
      "Step: 1500  \tTraining accuracy: 0.78184574842453\n",
      "Step: 1500  \tValid loss: 0.45345041155815125\n",
      "Step: 1600  \tTraining loss: 0.40216264128685\n",
      "Step: 1600  \tTraining accuracy: 0.783433198928833\n",
      "Step: 1600  \tValid loss: 0.4525502026081085\n",
      "Step: 1700  \tTraining loss: 0.40161362290382385\n",
      "Step: 1700  \tTraining accuracy: 0.784828245639801\n",
      "Step: 1700  \tValid loss: 0.45181453227996826\n",
      "Step: 1800  \tTraining loss: 0.4010413587093353\n",
      "Step: 1800  \tTraining accuracy: 0.7860639095306396\n",
      "Step: 1800  \tValid loss: 0.45094195008277893\n",
      "Step: 1900  \tTraining loss: 0.4005078673362732\n",
      "Step: 1900  \tTraining accuracy: 0.7871659398078918\n",
      "Step: 1900  \tValid loss: 0.45019376277923584\n",
      "Step: 2000  \tTraining loss: 0.39992913603782654\n",
      "Step: 2000  \tTraining accuracy: 0.7881549596786499\n",
      "Step: 2000  \tValid loss: 0.4495784044265747\n",
      "Step: 2100  \tTraining loss: 0.3993290066719055\n",
      "Step: 2100  \tTraining accuracy: 0.7890745997428894\n",
      "Step: 2100  \tValid loss: 0.44894587993621826\n",
      "Step: 2200  \tTraining loss: 0.39872559905052185\n",
      "Step: 2200  \tTraining accuracy: 0.7899087071418762\n",
      "Step: 2200  \tValid loss: 0.4481261372566223\n",
      "Step: 2300  \tTraining loss: 0.39811962842941284\n",
      "Step: 2300  \tTraining accuracy: 0.7906686663627625\n",
      "Step: 2300  \tValid loss: 0.4473351240158081\n",
      "Step: 2400  \tTraining loss: 0.39752018451690674\n",
      "Step: 2400  \tTraining accuracy: 0.7913639545440674\n",
      "Step: 2400  \tValid loss: 0.4465121626853943\n",
      "Step: 2500  \tTraining loss: 0.39693015813827515\n",
      "Step: 2500  \tTraining accuracy: 0.7920024394989014\n",
      "Step: 2500  \tValid loss: 0.4458012282848358\n",
      "Step: 2600  \tTraining loss: 0.396345853805542\n",
      "Step: 2600  \tTraining accuracy: 0.7925909161567688\n",
      "Step: 2600  \tValid loss: 0.4451406002044678\n",
      "Step: 2700  \tTraining loss: 0.39578789472579956\n",
      "Step: 2700  \tTraining accuracy: 0.7931349277496338\n",
      "Step: 2700  \tValid loss: 0.4443954825401306\n",
      "Step: 2800  \tTraining loss: 0.39525237679481506\n",
      "Step: 2800  \tTraining accuracy: 0.7936899662017822\n",
      "Step: 2800  \tValid loss: 0.4436308443546295\n",
      "Step: 2900  \tTraining loss: 0.3947451412677765\n",
      "Step: 2900  \tTraining accuracy: 0.7942352890968323\n",
      "Step: 2900  \tValid loss: 0.443003386259079\n",
      "Step: 3000  \tTraining loss: 0.3942564129829407\n",
      "Step: 3000  \tTraining accuracy: 0.7948756814002991\n",
      "Step: 3000  \tValid loss: 0.44240817427635193\n",
      "Step: 3100  \tTraining loss: 0.3938015103340149\n",
      "Step: 3100  \tTraining accuracy: 0.795455813407898\n",
      "Step: 3100  \tValid loss: 0.4416806399822235\n",
      "Step: 3200  \tTraining loss: 0.3933677673339844\n",
      "Step: 3200  \tTraining accuracy: 0.7959991097450256\n",
      "Step: 3200  \tValid loss: 0.441101998090744\n",
      "Step: 3300  \tTraining loss: 0.39295852184295654\n",
      "Step: 3300  \tTraining accuracy: 0.7965089678764343\n",
      "Step: 3300  \tValid loss: 0.4406420886516571\n",
      "Step: 3400  \tTraining loss: 0.3925682604312897\n",
      "Step: 3400  \tTraining accuracy: 0.7969883680343628\n",
      "Step: 3400  \tValid loss: 0.4401538074016571\n",
      "Step: 3500  \tTraining loss: 0.39219653606414795\n",
      "Step: 3500  \tTraining accuracy: 0.7974399924278259\n",
      "Step: 3500  \tValid loss: 0.43972083926200867\n",
      "Step: 3600  \tTraining loss: 0.39184054732322693\n",
      "Step: 3600  \tTraining accuracy: 0.79786616563797\n",
      "Step: 3600  \tValid loss: 0.4393197298049927\n",
      "Step: 3700  \tTraining loss: 0.3915000855922699\n",
      "Step: 3700  \tTraining accuracy: 0.7982690334320068\n",
      "Step: 3700  \tValid loss: 0.4389020800590515\n",
      "Step: 3800  \tTraining loss: 0.39117708802223206\n",
      "Step: 3800  \tTraining accuracy: 0.7986280918121338\n",
      "Step: 3800  \tValid loss: 0.4383796453475952\n",
      "Step: 3900  \tTraining loss: 0.39086395502090454\n",
      "Step: 3900  \tTraining accuracy: 0.798961341381073\n",
      "Step: 3900  \tValid loss: 0.43802711367607117\n",
      "Step: 4000  \tTraining loss: 0.39055079221725464\n",
      "Step: 4000  \tTraining accuracy: 0.799277663230896\n",
      "Step: 4000  \tValid loss: 0.43776118755340576\n",
      "Step: 4100  \tTraining loss: 0.3902391195297241\n",
      "Step: 4100  \tTraining accuracy: 0.7995784282684326\n",
      "Step: 4100  \tValid loss: 0.4373585283756256\n",
      "Step: 4200  \tTraining loss: 0.3899189233779907\n",
      "Step: 4200  \tTraining accuracy: 0.799864649772644\n",
      "Step: 4200  \tValid loss: 0.43697234988212585\n",
      "Step: 4300  \tTraining loss: 0.389571875333786\n",
      "Step: 4300  \tTraining accuracy: 0.8001374006271362\n",
      "Step: 4300  \tValid loss: 0.4366900622844696\n",
      "Step: 4400  \tTraining loss: 0.38919344544410706\n",
      "Step: 4400  \tTraining accuracy: 0.8003976345062256\n",
      "Step: 4400  \tValid loss: 0.43647846579551697\n",
      "Step: 4500  \tTraining loss: 0.38877975940704346\n",
      "Step: 4500  \tTraining accuracy: 0.800646185874939\n",
      "Step: 4500  \tValid loss: 0.436116486787796\n",
      "Step: 4600  \tTraining loss: 0.3883323669433594\n",
      "Step: 4600  \tTraining accuracy: 0.8008837699890137\n",
      "Step: 4600  \tValid loss: 0.43578407168388367\n",
      "Step: 4700  \tTraining loss: 0.38780006766319275\n",
      "Step: 4700  \tTraining accuracy: 0.8011111617088318\n",
      "Step: 4700  \tValid loss: 0.43564045429229736\n",
      "Step: 4800  \tTraining loss: 0.3872548043727875\n",
      "Step: 4800  \tTraining accuracy: 0.8013289570808411\n",
      "Step: 4800  \tValid loss: 0.43525415658950806\n",
      "Step: 4900  \tTraining loss: 0.3867153823375702\n",
      "Step: 4900  \tTraining accuracy: 0.801537811756134\n",
      "Step: 4900  \tValid loss: 0.4348013997077942\n",
      "Step: 5000  \tTraining loss: 0.38618403673171997\n",
      "Step: 5000  \tTraining accuracy: 0.8017382025718689\n",
      "Step: 5000  \tValid loss: 0.43438634276390076\n",
      "Step: 5100  \tTraining loss: 0.3856178820133209\n",
      "Step: 5100  \tTraining accuracy: 0.8019306659698486\n",
      "Step: 5100  \tValid loss: 0.4340859651565552\n",
      "Step: 5200  \tTraining loss: 0.38507211208343506\n",
      "Step: 5200  \tTraining accuracy: 0.8021318316459656\n",
      "Step: 5200  \tValid loss: 0.43363693356513977\n",
      "Step: 5300  \tTraining loss: 0.384573757648468\n",
      "Step: 5300  \tTraining accuracy: 0.8023306131362915\n",
      "Step: 5300  \tValid loss: 0.43332037329673767\n",
      "Step: 5400  \tTraining loss: 0.38411957025527954\n",
      "Step: 5400  \tTraining accuracy: 0.8025220036506653\n",
      "Step: 5400  \tValid loss: 0.4329833984375\n",
      "Step: 5500  \tTraining loss: 0.3837350308895111\n",
      "Step: 5500  \tTraining accuracy: 0.8027063608169556\n",
      "Step: 5500  \tValid loss: 0.4325431287288666\n",
      "Step: 5600  \tTraining loss: 0.383417010307312\n",
      "Step: 5600  \tTraining accuracy: 0.8028740882873535\n",
      "Step: 5600  \tValid loss: 0.43214210867881775\n",
      "Step: 5700  \tTraining loss: 0.38313597440719604\n",
      "Step: 5700  \tTraining accuracy: 0.8030357956886292\n",
      "Step: 5700  \tValid loss: 0.43183350563049316\n",
      "Step: 5800  \tTraining loss: 0.38289016485214233\n",
      "Step: 5800  \tTraining accuracy: 0.8031919598579407\n",
      "Step: 5800  \tValid loss: 0.4315953552722931\n",
      "Step: 5900  \tTraining loss: 0.38266754150390625\n",
      "Step: 5900  \tTraining accuracy: 0.8033522367477417\n",
      "Step: 5900  \tValid loss: 0.43137866258621216\n",
      "Step: 6000  \tTraining loss: 0.38246896862983704\n",
      "Step: 6000  \tTraining accuracy: 0.8035025000572205\n",
      "Step: 6000  \tValid loss: 0.4311969578266144\n",
      "Step: 6100  \tTraining loss: 0.3822799623012543\n",
      "Step: 6100  \tTraining accuracy: 0.8036477565765381\n",
      "Step: 6100  \tValid loss: 0.43113985657691956\n",
      "Step: 6200  \tTraining loss: 0.38211527466773987\n",
      "Step: 6200  \tTraining accuracy: 0.8037883043289185\n",
      "Step: 6200  \tValid loss: 0.43112877011299133\n",
      "Step: 6300  \tTraining loss: 0.3819756805896759\n",
      "Step: 6300  \tTraining accuracy: 0.8039243817329407\n",
      "Step: 6300  \tValid loss: 0.4312626123428345\n",
      "Step: 6400  \tTraining loss: 0.38184958696365356\n",
      "Step: 6400  \tTraining accuracy: 0.8040561079978943\n",
      "Step: 6400  \tValid loss: 0.43131181597709656\n",
      "Step: 6500  \tTraining loss: 0.38173404335975647\n",
      "Step: 6500  \tTraining accuracy: 0.8041837811470032\n",
      "Step: 6500  \tValid loss: 0.431365430355072\n",
      "Step: 6600  \tTraining loss: 0.38162490725517273\n",
      "Step: 6600  \tTraining accuracy: 0.8043075799942017\n",
      "Step: 6600  \tValid loss: 0.4313459098339081\n",
      "Step: 6700  \tTraining loss: 0.3815244436264038\n",
      "Step: 6700  \tTraining accuracy: 0.8044276237487793\n",
      "Step: 6700  \tValid loss: 0.43133997917175293\n",
      "Step: 6800  \tTraining loss: 0.38142913579940796\n",
      "Step: 6800  \tTraining accuracy: 0.8045359253883362\n",
      "Step: 6800  \tValid loss: 0.43132254481315613\n",
      "Step: 6900  \tTraining loss: 0.3812917470932007\n",
      "Step: 6900  \tTraining accuracy: 0.8046653866767883\n",
      "Step: 6900  \tValid loss: 0.4312290549278259\n",
      "Step: 7000  \tTraining loss: 0.38109931349754333\n",
      "Step: 7000  \tTraining accuracy: 0.8047751188278198\n",
      "Step: 7000  \tValid loss: 0.4311663806438446\n",
      "Step: 7100  \tTraining loss: 0.38094362616539\n",
      "Step: 7100  \tTraining accuracy: 0.8048816919326782\n",
      "Step: 7100  \tValid loss: 0.43123939633369446\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.80498534\n",
      "Precision: 0.8565121\n",
      "Recall: 0.8272921\n",
      "F1 score: 0.7910159\n",
      "AUC: 0.8380646\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.804985   0.856512  0.827292  0.791016  0.838065  0.380865      0.804756   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.431026       0.804749   0.550671      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  7179.0  \n",
      "15\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_15763/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(870, 3)\n",
      "(870, 1)\n",
      "(480, 3)\n",
      "(480, 1)\n",
      "(390, 3)\n",
      "(390, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5607544183731079\n",
      "Step: 100  \tTraining accuracy: 0.7103448510169983\n",
      "Step: 100  \tValid loss: 0.5667826533317566\n",
      "Step: 200  \tTraining loss: 0.536148726940155\n",
      "Step: 200  \tTraining accuracy: 0.7249042391777039\n",
      "Step: 200  \tValid loss: 0.5539131164550781\n",
      "Step: 300  \tTraining loss: 0.5266628265380859\n",
      "Step: 300  \tTraining accuracy: 0.7310344576835632\n",
      "Step: 300  \tValid loss: 0.549669623374939\n",
      "Step: 400  \tTraining loss: 0.5184365510940552\n",
      "Step: 400  \tTraining accuracy: 0.7303776741027832\n",
      "Step: 400  \tValid loss: 0.5417149066925049\n",
      "Step: 500  \tTraining loss: 0.509619414806366\n",
      "Step: 500  \tTraining accuracy: 0.7312899231910706\n",
      "Step: 500  \tValid loss: 0.5323346853256226\n",
      "Step: 600  \tTraining loss: 0.5001532435417175\n",
      "Step: 600  \tTraining accuracy: 0.7327063679695129\n",
      "Step: 600  \tValid loss: 0.5216257572174072\n",
      "Step: 700  \tTraining loss: 0.4895622730255127\n",
      "Step: 700  \tTraining accuracy: 0.7365163564682007\n",
      "Step: 700  \tValid loss: 0.5092377662658691\n",
      "Step: 800  \tTraining loss: 0.47761979699134827\n",
      "Step: 800  \tTraining accuracy: 0.7401532530784607\n",
      "Step: 800  \tValid loss: 0.49541646242141724\n",
      "Step: 900  \tTraining loss: 0.4676898419857025\n",
      "Step: 900  \tTraining accuracy: 0.7434753179550171\n",
      "Step: 900  \tValid loss: 0.4836041331291199\n",
      "Step: 1000  \tTraining loss: 0.4594774842262268\n",
      "Step: 1000  \tTraining accuracy: 0.7467634677886963\n",
      "Step: 1000  \tValid loss: 0.47354942560195923\n",
      "Step: 1100  \tTraining loss: 0.45308253169059753\n",
      "Step: 1100  \tTraining accuracy: 0.7493157982826233\n",
      "Step: 1100  \tValid loss: 0.4654856324195862\n",
      "Step: 1200  \tTraining loss: 0.4482066333293915\n",
      "Step: 1200  \tTraining accuracy: 0.7514742612838745\n",
      "Step: 1200  \tValid loss: 0.4593471586704254\n",
      "Step: 1300  \tTraining loss: 0.44459494948387146\n",
      "Step: 1300  \tTraining accuracy: 0.7534253001213074\n",
      "Step: 1300  \tValid loss: 0.4548601508140564\n",
      "Step: 1400  \tTraining loss: 0.44150808453559875\n",
      "Step: 1400  \tTraining accuracy: 0.7550872564315796\n",
      "Step: 1400  \tValid loss: 0.4512275755405426\n",
      "Step: 1500  \tTraining loss: 0.4379633963108063\n",
      "Step: 1500  \tTraining accuracy: 0.7564803957939148\n",
      "Step: 1500  \tValid loss: 0.447261780500412\n",
      "Step: 1600  \tTraining loss: 0.434646338224411\n",
      "Step: 1600  \tTraining accuracy: 0.7576937079429626\n",
      "Step: 1600  \tValid loss: 0.44409504532814026\n",
      "Step: 1700  \tTraining loss: 0.4319346249103546\n",
      "Step: 1700  \tTraining accuracy: 0.7588645219802856\n",
      "Step: 1700  \tValid loss: 0.44191399216651917\n",
      "Step: 1800  \tTraining loss: 0.42969265580177307\n",
      "Step: 1800  \tTraining accuracy: 0.7599671483039856\n",
      "Step: 1800  \tValid loss: 0.4403468370437622\n",
      "Step: 1900  \tTraining loss: 0.4278133809566498\n",
      "Step: 1900  \tTraining accuracy: 0.7610748410224915\n",
      "Step: 1900  \tValid loss: 0.4391539394855499\n",
      "Step: 2000  \tTraining loss: 0.42619675397872925\n",
      "Step: 2000  \tTraining accuracy: 0.7620984315872192\n",
      "Step: 2000  \tValid loss: 0.4382757246494293\n",
      "Step: 2100  \tTraining loss: 0.4247874319553375\n",
      "Step: 2100  \tTraining accuracy: 0.7630782127380371\n",
      "Step: 2100  \tValid loss: 0.4376208186149597\n",
      "Step: 2200  \tTraining loss: 0.42356017231941223\n",
      "Step: 2200  \tTraining accuracy: 0.7640203237533569\n",
      "Step: 2200  \tValid loss: 0.43712928891181946\n",
      "Step: 2300  \tTraining loss: 0.422467976808548\n",
      "Step: 2300  \tTraining accuracy: 0.7648786902427673\n",
      "Step: 2300  \tValid loss: 0.4367997348308563\n",
      "Step: 2400  \tTraining loss: 0.42149782180786133\n",
      "Step: 2400  \tTraining accuracy: 0.7657373547554016\n",
      "Step: 2400  \tValid loss: 0.4366034269332886\n",
      "Step: 2500  \tTraining loss: 0.42063936591148376\n",
      "Step: 2500  \tTraining accuracy: 0.7665493488311768\n",
      "Step: 2500  \tValid loss: 0.43648532032966614\n",
      "Step: 2600  \tTraining loss: 0.4198666512966156\n",
      "Step: 2600  \tTraining accuracy: 0.7672752141952515\n",
      "Step: 2600  \tValid loss: 0.4365222156047821\n",
      "Step: 2700  \tTraining loss: 0.41916948556900024\n",
      "Step: 2700  \tTraining accuracy: 0.7679462432861328\n",
      "Step: 2700  \tValid loss: 0.4365895986557007\n",
      "Step: 2800  \tTraining loss: 0.4185485541820526\n",
      "Step: 2800  \tTraining accuracy: 0.7685684561729431\n",
      "Step: 2800  \tValid loss: 0.43667563796043396\n",
      "Step: 2900  \tTraining loss: 0.41799017786979675\n",
      "Step: 2900  \tTraining accuracy: 0.7691469788551331\n",
      "Step: 2900  \tValid loss: 0.4367589056491852\n",
      "Step: 3000  \tTraining loss: 0.4174441695213318\n",
      "Step: 3000  \tTraining accuracy: 0.7696863412857056\n",
      "Step: 3000  \tValid loss: 0.4371704161167145\n",
      "Step: 3100  \tTraining loss: 0.4169604778289795\n",
      "Step: 3100  \tTraining accuracy: 0.7701902985572815\n",
      "Step: 3100  \tValid loss: 0.43728822469711304\n",
      "Step: 3200  \tTraining loss: 0.41651651263237\n",
      "Step: 3200  \tTraining accuracy: 0.7706987857818604\n",
      "Step: 3200  \tValid loss: 0.43744727969169617\n",
      "Step: 3300  \tTraining loss: 0.4161134660243988\n",
      "Step: 3300  \tTraining accuracy: 0.7711759209632874\n",
      "Step: 3300  \tValid loss: 0.43743225932121277\n",
      "Step: 3400  \tTraining loss: 0.41578373312950134\n",
      "Step: 3400  \tTraining accuracy: 0.7716246247291565\n",
      "Step: 3400  \tValid loss: 0.4372805655002594\n",
      "Step: 3500  \tTraining loss: 0.4155066907405853\n",
      "Step: 3500  \tTraining accuracy: 0.7720472812652588\n",
      "Step: 3500  \tValid loss: 0.43720319867134094\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.77244616\n",
      "Precision: 0.83885545\n",
      "Recall: 0.90129447\n",
      "F1 score: 0.80016255\n",
      "AUC: 0.7383456\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.772446   0.838855  0.901294  0.800163  0.738346  0.415498      0.771954   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.436467       0.771963   0.492789      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3503.0  \n",
      "16\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_101678/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(1624, 3)\n",
      "(1624, 1)\n",
      "(880, 3)\n",
      "(880, 1)\n",
      "(715, 3)\n",
      "(715, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6584020256996155\n",
      "Step: 100  \tTraining accuracy: 0.6607142686843872\n",
      "Step: 100  \tValid loss: 0.6558172702789307\n",
      "Step: 200  \tTraining loss: 0.594546914100647\n",
      "Step: 200  \tTraining accuracy: 0.6811893582344055\n",
      "Step: 200  \tValid loss: 0.5882478356361389\n",
      "Step: 300  \tTraining loss: 0.521439790725708\n",
      "Step: 300  \tTraining accuracy: 0.711237907409668\n",
      "Step: 300  \tValid loss: 0.5118559002876282\n",
      "Step: 400  \tTraining loss: 0.4590287506580353\n",
      "Step: 400  \tTraining accuracy: 0.740537166595459\n",
      "Step: 400  \tValid loss: 0.4470256567001343\n",
      "Step: 500  \tTraining loss: 0.41819143295288086\n",
      "Step: 500  \tTraining accuracy: 0.7637931108474731\n",
      "Step: 500  \tValid loss: 0.40365070104599\n",
      "Step: 600  \tTraining loss: 0.3943924903869629\n",
      "Step: 600  \tTraining accuracy: 0.7791071534156799\n",
      "Step: 600  \tValid loss: 0.37748152017593384\n",
      "Step: 700  \tTraining loss: 0.38089725375175476\n",
      "Step: 700  \tTraining accuracy: 0.7895691990852356\n",
      "Step: 700  \tValid loss: 0.3619317412376404\n",
      "Step: 800  \tTraining loss: 0.3731016218662262\n",
      "Step: 800  \tTraining accuracy: 0.797325849533081\n",
      "Step: 800  \tValid loss: 0.3524554371833801\n",
      "Step: 900  \tTraining loss: 0.36837294697761536\n",
      "Step: 900  \tTraining accuracy: 0.8032582998275757\n",
      "Step: 900  \tValid loss: 0.34642043709754944\n",
      "Step: 1000  \tTraining loss: 0.3652382791042328\n",
      "Step: 1000  \tTraining accuracy: 0.8079424500465393\n",
      "Step: 1000  \tValid loss: 0.3423510193824768\n",
      "Step: 1100  \tTraining loss: 0.36287301778793335\n",
      "Step: 1100  \tTraining accuracy: 0.8117347955703735\n",
      "Step: 1100  \tValid loss: 0.33936795592308044\n",
      "Step: 1200  \tTraining loss: 0.36082735657691956\n",
      "Step: 1200  \tTraining accuracy: 0.8148677945137024\n",
      "Step: 1200  \tValid loss: 0.3369852304458618\n",
      "Step: 1300  \tTraining loss: 0.35887259244918823\n",
      "Step: 1300  \tTraining accuracy: 0.8177978992462158\n",
      "Step: 1300  \tValid loss: 0.33490225672721863\n",
      "Step: 1400  \tTraining loss: 0.3568924367427826\n",
      "Step: 1400  \tTraining accuracy: 0.8205930590629578\n",
      "Step: 1400  \tValid loss: 0.3329145908355713\n",
      "Step: 1500  \tTraining loss: 0.3549294173717499\n",
      "Step: 1500  \tTraining accuracy: 0.8230027556419373\n",
      "Step: 1500  \tValid loss: 0.331040620803833\n",
      "Step: 1600  \tTraining loss: 0.35300061106681824\n",
      "Step: 1600  \tTraining accuracy: 0.8251016736030579\n",
      "Step: 1600  \tValid loss: 0.3293232321739197\n",
      "Step: 1700  \tTraining loss: 0.3511274456977844\n",
      "Step: 1700  \tTraining accuracy: 0.8269462585449219\n",
      "Step: 1700  \tValid loss: 0.32783326506614685\n",
      "Step: 1800  \tTraining loss: 0.3495153784751892\n",
      "Step: 1800  \tTraining accuracy: 0.8284913301467896\n",
      "Step: 1800  \tValid loss: 0.3265934884548187\n",
      "Step: 1900  \tTraining loss: 0.3481517732143402\n",
      "Step: 1900  \tTraining accuracy: 0.8299869298934937\n",
      "Step: 1900  \tValid loss: 0.3253386616706848\n",
      "Step: 2000  \tTraining loss: 0.34698569774627686\n",
      "Step: 2000  \tTraining accuracy: 0.8313291668891907\n",
      "Step: 2000  \tValid loss: 0.3242056965827942\n",
      "Step: 2100  \tTraining loss: 0.3459983170032501\n",
      "Step: 2100  \tTraining accuracy: 0.8325404524803162\n",
      "Step: 2100  \tValid loss: 0.32324302196502686\n",
      "Step: 2200  \tTraining loss: 0.34515801072120667\n",
      "Step: 2200  \tTraining accuracy: 0.8336390852928162\n",
      "Step: 2200  \tValid loss: 0.32242903113365173\n",
      "Step: 2300  \tTraining loss: 0.3444555401802063\n",
      "Step: 2300  \tTraining accuracy: 0.8346401453018188\n",
      "Step: 2300  \tValid loss: 0.32171306014060974\n",
      "Step: 2400  \tTraining loss: 0.34387245774269104\n",
      "Step: 2400  \tTraining accuracy: 0.8355559706687927\n",
      "Step: 2400  \tValid loss: 0.32110705971717834\n",
      "Step: 2500  \tTraining loss: 0.3433866798877716\n",
      "Step: 2500  \tTraining accuracy: 0.8363590240478516\n",
      "Step: 2500  \tValid loss: 0.3205934762954712\n",
      "Step: 2600  \tTraining loss: 0.3429811894893646\n",
      "Step: 2600  \tTraining accuracy: 0.8371112942695618\n",
      "Step: 2600  \tValid loss: 0.3201533854007721\n",
      "Step: 2700  \tTraining loss: 0.3426421284675598\n",
      "Step: 2700  \tTraining accuracy: 0.8378068208694458\n",
      "Step: 2700  \tValid loss: 0.31978076696395874\n",
      "Step: 2800  \tTraining loss: 0.34235844016075134\n",
      "Step: 2800  \tTraining accuracy: 0.8384517431259155\n",
      "Step: 2800  \tValid loss: 0.31946292519569397\n",
      "Step: 2900  \tTraining loss: 0.3421146273612976\n",
      "Step: 2900  \tTraining accuracy: 0.8390514254570007\n",
      "Step: 2900  \tValid loss: 0.3192013204097748\n",
      "Step: 3000  \tTraining loss: 0.34190812706947327\n",
      "Step: 3000  \tTraining accuracy: 0.8396103978157043\n",
      "Step: 3000  \tValid loss: 0.3189796507358551\n",
      "Step: 3100  \tTraining loss: 0.34173354506492615\n",
      "Step: 3100  \tTraining accuracy: 0.8401327729225159\n",
      "Step: 3100  \tValid loss: 0.31879252195358276\n",
      "Step: 3200  \tTraining loss: 0.34158560633659363\n",
      "Step: 3200  \tTraining accuracy: 0.8406220078468323\n",
      "Step: 3200  \tValid loss: 0.31863248348236084\n",
      "Step: 3300  \tTraining loss: 0.3414593040943146\n",
      "Step: 3300  \tTraining accuracy: 0.8410811424255371\n",
      "Step: 3300  \tValid loss: 0.3185001015663147\n",
      "Step: 3400  \tTraining loss: 0.34135109186172485\n",
      "Step: 3400  \tTraining accuracy: 0.8415127992630005\n",
      "Step: 3400  \tValid loss: 0.31838783621788025\n",
      "Step: 3500  \tTraining loss: 0.34125816822052\n",
      "Step: 3500  \tTraining accuracy: 0.841919481754303\n",
      "Step: 3500  \tValid loss: 0.31829550862312317\n",
      "Step: 3600  \tTraining loss: 0.34117719531059265\n",
      "Step: 3600  \tTraining accuracy: 0.8423032760620117\n",
      "Step: 3600  \tValid loss: 0.3182198405265808\n",
      "Step: 3700  \tTraining loss: 0.3411073088645935\n",
      "Step: 3700  \tTraining accuracy: 0.8426660299301147\n",
      "Step: 3700  \tValid loss: 0.3181553781032562\n",
      "Step: 3800  \tTraining loss: 0.34104636311531067\n",
      "Step: 3800  \tTraining accuracy: 0.8429845571517944\n",
      "Step: 3800  \tValid loss: 0.31810736656188965\n",
      "Step: 3900  \tTraining loss: 0.3409924805164337\n",
      "Step: 3900  \tTraining accuracy: 0.8432785272598267\n",
      "Step: 3900  \tValid loss: 0.31807005405426025\n",
      "Step: 4000  \tTraining loss: 0.34094396233558655\n",
      "Step: 4000  \tTraining accuracy: 0.843557596206665\n",
      "Step: 4000  \tValid loss: 0.3180405795574188\n",
      "Step: 4100  \tTraining loss: 0.3409006595611572\n",
      "Step: 4100  \tTraining accuracy: 0.8438228368759155\n",
      "Step: 4100  \tValid loss: 0.3180168867111206\n",
      "Step: 4200  \tTraining loss: 0.3408621847629547\n",
      "Step: 4200  \tTraining accuracy: 0.8440753817558289\n",
      "Step: 4200  \tValid loss: 0.31799566745758057\n",
      "Step: 4300  \tTraining loss: 0.340827077627182\n",
      "Step: 4300  \tTraining accuracy: 0.8443159461021423\n",
      "Step: 4300  \tValid loss: 0.3179819881916046\n",
      "Step: 4400  \tTraining loss: 0.3407949209213257\n",
      "Step: 4400  \tTraining accuracy: 0.8445455431938171\n",
      "Step: 4400  \tValid loss: 0.31797853112220764\n",
      "Step: 4500  \tTraining loss: 0.3407649099826813\n",
      "Step: 4500  \tTraining accuracy: 0.844764769077301\n",
      "Step: 4500  \tValid loss: 0.31797605752944946\n",
      "Step: 4600  \tTraining loss: 0.340737521648407\n",
      "Step: 4600  \tTraining accuracy: 0.8449743390083313\n",
      "Step: 4600  \tValid loss: 0.31798025965690613\n",
      "Step: 4700  \tTraining loss: 0.340711385011673\n",
      "Step: 4700  \tTraining accuracy: 0.8451749682426453\n",
      "Step: 4700  \tValid loss: 0.31798771023750305\n",
      "Step: 4800  \tTraining loss: 0.34068673849105835\n",
      "Step: 4800  \tTraining accuracy: 0.8453670740127563\n",
      "Step: 4800  \tValid loss: 0.31799933314323425\n",
      "Step: 4900  \tTraining loss: 0.34066328406333923\n",
      "Step: 4900  \tTraining accuracy: 0.8455513119697571\n",
      "Step: 4900  \tValid loss: 0.31801268458366394\n",
      "Step: 5000  \tTraining loss: 0.3406408429145813\n",
      "Step: 5000  \tTraining accuracy: 0.8457280993461609\n",
      "Step: 5000  \tValid loss: 0.31802937388420105\n",
      "Step: 5100  \tTraining loss: 0.3406192362308502\n",
      "Step: 5100  \tTraining accuracy: 0.8458978533744812\n",
      "Step: 5100  \tValid loss: 0.3180485665798187\n",
      "Step: 5200  \tTraining loss: 0.34059810638427734\n",
      "Step: 5200  \tTraining accuracy: 0.8460610508918762\n",
      "Step: 5200  \tValid loss: 0.3180669844150543\n",
      "Step: 5300  \tTraining loss: 0.34057778120040894\n",
      "Step: 5300  \tTraining accuracy: 0.8462180495262146\n",
      "Step: 5300  \tValid loss: 0.31808754801750183\n",
      "Step: 5400  \tTraining loss: 0.3405577838420868\n",
      "Step: 5400  \tTraining accuracy: 0.8463691473007202\n",
      "Step: 5400  \tValid loss: 0.31810954213142395\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8465147\n",
      "Precision: 0.8488211\n",
      "Recall: 0.827027\n",
      "F1 score: 0.83550596\n",
      "AUC: 0.85186195\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.846515   0.848821  0.827027  0.835506  0.851862  0.340538      0.846444   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.317973       0.846345   0.346881      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  5477.0  \n",
      "17\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_306/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(783, 3)\n",
      "(783, 1)\n",
      "(432, 3)\n",
      "(432, 1)\n",
      "(351, 3)\n",
      "(351, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5040006637573242\n",
      "Step: 100  \tTraining accuracy: 0.8339719176292419\n",
      "Step: 100  \tValid loss: 0.5583534836769104\n",
      "Step: 200  \tTraining loss: 0.4140520989894867\n",
      "Step: 200  \tTraining accuracy: 0.8152405023574829\n",
      "Step: 200  \tValid loss: 0.539211630821228\n",
      "Step: 300  \tTraining loss: 0.40732574462890625\n",
      "Step: 300  \tTraining accuracy: 0.8114942312240601\n",
      "Step: 300  \tValid loss: 0.5553092360496521\n",
      "Step: 400  \tTraining loss: 0.4029274880886078\n",
      "Step: 400  \tTraining accuracy: 0.8098887205123901\n",
      "Step: 400  \tValid loss: 0.5528565049171448\n",
      "Step: 500  \tTraining loss: 0.4009365141391754\n",
      "Step: 500  \tTraining accuracy: 0.8089967370033264\n",
      "Step: 500  \tValid loss: 0.5546063184738159\n",
      "Step: 600  \tTraining loss: 0.4002712070941925\n",
      "Step: 600  \tTraining accuracy: 0.8084291219711304\n",
      "Step: 600  \tValid loss: 0.5559859871864319\n",
      "Step: 700  \tTraining loss: 0.3997015357017517\n",
      "Step: 700  \tTraining accuracy: 0.8080361485481262\n",
      "Step: 700  \tValid loss: 0.5561764240264893\n",
      "Step: 800  \tTraining loss: 0.39912039041519165\n",
      "Step: 800  \tTraining accuracy: 0.8077479600906372\n",
      "Step: 800  \tValid loss: 0.5560306906700134\n",
      "Step: 900  \tTraining loss: 0.3985048532485962\n",
      "Step: 900  \tTraining accuracy: 0.8075276017189026\n",
      "Step: 900  \tValid loss: 0.5556772351264954\n",
      "Step: 1000  \tTraining loss: 0.3978601098060608\n",
      "Step: 1000  \tTraining accuracy: 0.8073536157608032\n",
      "Step: 1000  \tValid loss: 0.5552661418914795\n",
      "Step: 1100  \tTraining loss: 0.3971894681453705\n",
      "Step: 1100  \tTraining accuracy: 0.807212769985199\n",
      "Step: 1100  \tValid loss: 0.5548105835914612\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.80731857\n",
      "Precision: 0.83912486\n",
      "Recall: 0.9984686\n",
      "F1 score: 0.8773238\n",
      "AUC: 0.51846504\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.807319   0.839125  0.998469  0.877324  0.518465  0.396805      0.805875   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.530896       0.807074   0.464488      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  1155.0  \n",
      "18\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_33209/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(1189, 3)\n",
      "(1189, 1)\n",
      "(640, 3)\n",
      "(640, 1)\n",
      "(520, 3)\n",
      "(520, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5496081113815308\n",
      "Step: 100  \tTraining accuracy: 0.7846930027008057\n",
      "Step: 100  \tValid loss: 0.6092116236686707\n",
      "Step: 200  \tTraining loss: 0.5150425434112549\n",
      "Step: 200  \tTraining accuracy: 0.7685132622718811\n",
      "Step: 200  \tValid loss: 0.5733779072761536\n",
      "Step: 300  \tTraining loss: 0.4968261122703552\n",
      "Step: 300  \tTraining accuracy: 0.7652454376220703\n",
      "Step: 300  \tValid loss: 0.545332133769989\n",
      "Step: 400  \tTraining loss: 0.49126744270324707\n",
      "Step: 400  \tTraining accuracy: 0.7638416886329651\n",
      "Step: 400  \tValid loss: 0.5314279198646545\n",
      "Step: 500  \tTraining loss: 0.4899213910102844\n",
      "Step: 500  \tTraining accuracy: 0.7630609273910522\n",
      "Step: 500  \tValid loss: 0.5261315107345581\n",
      "Step: 600  \tTraining loss: 0.48921889066696167\n",
      "Step: 600  \tTraining accuracy: 0.7625637650489807\n",
      "Step: 600  \tValid loss: 0.523485004901886\n",
      "Step: 700  \tTraining loss: 0.48866158723831177\n",
      "Step: 700  \tTraining accuracy: 0.7622194886207581\n",
      "Step: 700  \tValid loss: 0.5221971273422241\n",
      "Step: 800  \tTraining loss: 0.4881596863269806\n",
      "Step: 800  \tTraining accuracy: 0.7619668841362\n",
      "Step: 800  \tValid loss: 0.5215876698493958\n",
      "Step: 900  \tTraining loss: 0.48760324716567993\n",
      "Step: 900  \tTraining accuracy: 0.7617737054824829\n",
      "Step: 900  \tValid loss: 0.5213438272476196\n",
      "Step: 1000  \tTraining loss: 0.4868670105934143\n",
      "Step: 1000  \tTraining accuracy: 0.7616211175918579\n",
      "Step: 1000  \tValid loss: 0.5206059217453003\n",
      "Step: 1100  \tTraining loss: 0.4859619140625\n",
      "Step: 1100  \tTraining accuracy: 0.7614976167678833\n",
      "Step: 1100  \tValid loss: 0.5190473794937134\n",
      "Step: 1200  \tTraining loss: 0.48529431223869324\n",
      "Step: 1200  \tTraining accuracy: 0.7613955736160278\n",
      "Step: 1200  \tValid loss: 0.5182223320007324\n",
      "Step: 1300  \tTraining loss: 0.4848406910896301\n",
      "Step: 1300  \tTraining accuracy: 0.7613098621368408\n",
      "Step: 1300  \tValid loss: 0.5173243284225464\n",
      "Step: 1400  \tTraining loss: 0.4844830632209778\n",
      "Step: 1400  \tTraining accuracy: 0.761236846446991\n",
      "Step: 1400  \tValid loss: 0.5167064666748047\n",
      "Step: 1500  \tTraining loss: 0.4841977059841156\n",
      "Step: 1500  \tTraining accuracy: 0.7611739039421082\n",
      "Step: 1500  \tValid loss: 0.5164326429367065\n",
      "Step: 1600  \tTraining loss: 0.483977735042572\n",
      "Step: 1600  \tTraining accuracy: 0.7611190676689148\n",
      "Step: 1600  \tValid loss: 0.5161157250404358\n",
      "Step: 1700  \tTraining loss: 0.4838220775127411\n",
      "Step: 1700  \tTraining accuracy: 0.7610708475112915\n",
      "Step: 1700  \tValid loss: 0.5159732103347778\n",
      "Step: 1800  \tTraining loss: 0.4837131202220917\n",
      "Step: 1800  \tTraining accuracy: 0.7610281705856323\n",
      "Step: 1800  \tValid loss: 0.515886664390564\n",
      "Step: 1900  \tTraining loss: 0.4836346209049225\n",
      "Step: 1900  \tTraining accuracy: 0.7609900832176208\n",
      "Step: 1900  \tValid loss: 0.515878438949585\n",
      "Step: 2000  \tTraining loss: 0.4835765063762665\n",
      "Step: 2000  \tTraining accuracy: 0.7609559297561646\n",
      "Step: 2000  \tValid loss: 0.5158994197845459\n",
      "Step: 2100  \tTraining loss: 0.48353224992752075\n",
      "Step: 2100  \tTraining accuracy: 0.7609250545501709\n",
      "Step: 2100  \tValid loss: 0.515990674495697\n",
      "Step: 2200  \tTraining loss: 0.4834941327571869\n",
      "Step: 2200  \tTraining accuracy: 0.7608970999717712\n",
      "Step: 2200  \tValid loss: 0.5160361528396606\n",
      "Step: 2300  \tTraining loss: 0.4834640324115753\n",
      "Step: 2300  \tTraining accuracy: 0.7608716487884521\n",
      "Step: 2300  \tValid loss: 0.5160127282142639\n",
      "Step: 2400  \tTraining loss: 0.48344042897224426\n",
      "Step: 2400  \tTraining accuracy: 0.7608482837677002\n",
      "Step: 2400  \tValid loss: 0.5159127712249756\n",
      "Step: 2500  \tTraining loss: 0.4834176301956177\n",
      "Step: 2500  \tTraining accuracy: 0.7608268857002258\n",
      "Step: 2500  \tValid loss: 0.5158559679985046\n",
      "Step: 2600  \tTraining loss: 0.48339489102363586\n",
      "Step: 2600  \tTraining accuracy: 0.7608071565628052\n",
      "Step: 2600  \tValid loss: 0.5158049464225769\n",
      "Step: 2700  \tTraining loss: 0.48337024450302124\n",
      "Step: 2700  \tTraining accuracy: 0.7607889175415039\n",
      "Step: 2700  \tValid loss: 0.5158499479293823\n",
      "Step: 2800  \tTraining loss: 0.48334401845932007\n",
      "Step: 2800  \tTraining accuracy: 0.7607719898223877\n",
      "Step: 2800  \tValid loss: 0.5158756971359253\n",
      "Step: 2900  \tTraining loss: 0.4833025336265564\n",
      "Step: 2900  \tTraining accuracy: 0.760756254196167\n",
      "Step: 2900  \tValid loss: 0.5158365964889526\n",
      "Step: 3000  \tTraining loss: 0.4832516312599182\n",
      "Step: 3000  \tTraining accuracy: 0.7607415914535522\n",
      "Step: 3000  \tValid loss: 0.5159193277359009\n",
      "Step: 3100  \tTraining loss: 0.48319458961486816\n",
      "Step: 3100  \tTraining accuracy: 0.7607278823852539\n",
      "Step: 3100  \tValid loss: 0.5159810781478882\n",
      "Step: 3200  \tTraining loss: 0.48313581943511963\n",
      "Step: 3200  \tTraining accuracy: 0.7607150673866272\n",
      "Step: 3200  \tValid loss: 0.5160661935806274\n",
      "Step: 3300  \tTraining loss: 0.4830784797668457\n",
      "Step: 3300  \tTraining accuracy: 0.7607030272483826\n",
      "Step: 3300  \tValid loss: 0.5161236524581909\n",
      "Step: 3400  \tTraining loss: 0.4830169379711151\n",
      "Step: 3400  \tTraining accuracy: 0.7606917023658752\n",
      "Step: 3400  \tValid loss: 0.5161752104759216\n",
      "Step: 3500  \tTraining loss: 0.4829515218734741\n",
      "Step: 3500  \tTraining accuracy: 0.7606810331344604\n",
      "Step: 3500  \tValid loss: 0.5162585973739624\n",
      "Step: 3600  \tTraining loss: 0.4828823208808899\n",
      "Step: 3600  \tTraining accuracy: 0.7606709599494934\n",
      "Step: 3600  \tValid loss: 0.5163370370864868\n",
      "Step: 3700  \tTraining loss: 0.4828105568885803\n",
      "Step: 3700  \tTraining accuracy: 0.7606614232063293\n",
      "Step: 3700  \tValid loss: 0.5164099931716919\n",
      "Step: 3800  \tTraining loss: 0.48274239897727966\n",
      "Step: 3800  \tTraining accuracy: 0.7606524229049683\n",
      "Step: 3800  \tValid loss: 0.5165044069290161\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7606439\n",
      "Precision: 0.784693\n",
      "Recall: 1.0\n",
      "F1 score: 0.85240865\n",
      "AUC: 0.5\n",
      "   accuracy  precision  recall  f1_score  auc      loss  accuracy_val  \\\n",
      "0  0.760644   0.784693     1.0  0.852409  0.5  0.482706      0.760324   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.515666       0.760511   0.521224      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3859.0  \n",
      "19\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_8625/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(899, 3)\n",
      "(899, 1)\n",
      "(480, 3)\n",
      "(480, 1)\n",
      "(390, 3)\n",
      "(390, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4618247449398041\n",
      "Step: 100  \tTraining accuracy: 0.8264738321304321\n",
      "Step: 100  \tValid loss: 0.46710771322250366\n",
      "Step: 200  \tTraining loss: 0.44202595949172974\n",
      "Step: 200  \tTraining accuracy: 0.8294602632522583\n",
      "Step: 200  \tValid loss: 0.4548798203468323\n",
      "Step: 300  \tTraining loss: 0.4417420029640198\n",
      "Step: 300  \tTraining accuracy: 0.8300653696060181\n",
      "Step: 300  \tValid loss: 0.4547151029109955\n",
      "Step: 400  \tTraining loss: 0.4414370357990265\n",
      "Step: 400  \tTraining accuracy: 0.8303254842758179\n",
      "Step: 400  \tValid loss: 0.45444455742836\n",
      "Step: 500  \tTraining loss: 0.441106915473938\n",
      "Step: 500  \tTraining accuracy: 0.8304702043533325\n",
      "Step: 500  \tValid loss: 0.4541378617286682\n",
      "Step: 600  \tTraining loss: 0.44074559211730957\n",
      "Step: 600  \tTraining accuracy: 0.8305624127388\n",
      "Step: 600  \tValid loss: 0.4537914991378784\n",
      "Step: 700  \tTraining loss: 0.4403492212295532\n",
      "Step: 700  \tTraining accuracy: 0.8306262493133545\n",
      "Step: 700  \tValid loss: 0.45340633392333984\n",
      "Step: 800  \tTraining loss: 0.43991753458976746\n",
      "Step: 800  \tTraining accuracy: 0.830673098564148\n",
      "Step: 800  \tValid loss: 0.45298606157302856\n",
      "Step: 900  \tTraining loss: 0.4394526481628418\n",
      "Step: 900  \tTraining accuracy: 0.830708920955658\n",
      "Step: 900  \tValid loss: 0.45253658294677734\n",
      "Step: 1000  \tTraining loss: 0.43895766139030457\n",
      "Step: 1000  \tTraining accuracy: 0.8307372331619263\n",
      "Step: 1000  \tValid loss: 0.4520646333694458\n",
      "Step: 1100  \tTraining loss: 0.4384373426437378\n",
      "Step: 1100  \tTraining accuracy: 0.83076012134552\n",
      "Step: 1100  \tValid loss: 0.4515776038169861\n",
      "Step: 1200  \tTraining loss: 0.4378971755504608\n",
      "Step: 1200  \tTraining accuracy: 0.8307790756225586\n",
      "Step: 1200  \tValid loss: 0.45108336210250854\n",
      "Step: 1300  \tTraining loss: 0.43734243512153625\n",
      "Step: 1300  \tTraining accuracy: 0.8307949304580688\n",
      "Step: 1300  \tValid loss: 0.4505883455276489\n",
      "Step: 1400  \tTraining loss: 0.43677935004234314\n",
      "Step: 1400  \tTraining accuracy: 0.8308085203170776\n",
      "Step: 1400  \tValid loss: 0.45010024309158325\n",
      "Step: 1500  \tTraining loss: 0.43621352314949036\n",
      "Step: 1500  \tTraining accuracy: 0.8308202028274536\n",
      "Step: 1500  \tValid loss: 0.4496254622936249\n",
      "Step: 1600  \tTraining loss: 0.435649573802948\n",
      "Step: 1600  \tTraining accuracy: 0.8308303356170654\n",
      "Step: 1600  \tValid loss: 0.44917038083076477\n",
      "Step: 1700  \tTraining loss: 0.4350925385951996\n",
      "Step: 1700  \tTraining accuracy: 0.8308392763137817\n",
      "Step: 1700  \tValid loss: 0.4487399458885193\n",
      "Step: 1800  \tTraining loss: 0.4345463514328003\n",
      "Step: 1800  \tTraining accuracy: 0.8308472037315369\n",
      "Step: 1800  \tValid loss: 0.44833889603614807\n",
      "Step: 1900  \tTraining loss: 0.4340141713619232\n",
      "Step: 1900  \tTraining accuracy: 0.8308542966842651\n",
      "Step: 1900  \tValid loss: 0.4479707181453705\n",
      "Step: 2000  \tTraining loss: 0.4334983229637146\n",
      "Step: 2000  \tTraining accuracy: 0.8308606147766113\n",
      "Step: 2000  \tValid loss: 0.4476390779018402\n",
      "Step: 2100  \tTraining loss: 0.43300119042396545\n",
      "Step: 2100  \tTraining accuracy: 0.8308663368225098\n",
      "Step: 2100  \tValid loss: 0.44734612107276917\n",
      "Step: 2200  \tTraining loss: 0.43252018094062805\n",
      "Step: 2200  \tTraining accuracy: 0.8308715224266052\n",
      "Step: 2200  \tValid loss: 0.44711339473724365\n",
      "Step: 2300  \tTraining loss: 0.4320555627346039\n",
      "Step: 2300  \tTraining accuracy: 0.8308762311935425\n",
      "Step: 2300  \tValid loss: 0.44692501425743103\n",
      "Step: 2400  \tTraining loss: 0.43160513043403625\n",
      "Step: 2400  \tTraining accuracy: 0.8308805823326111\n",
      "Step: 2400  \tValid loss: 0.44678395986557007\n",
      "Step: 2500  \tTraining loss: 0.4311525225639343\n",
      "Step: 2500  \tTraining accuracy: 0.830884575843811\n",
      "Step: 2500  \tValid loss: 0.4467262327671051\n",
      "Step: 2600  \tTraining loss: 0.4306614398956299\n",
      "Step: 2600  \tTraining accuracy: 0.8308882117271423\n",
      "Step: 2600  \tValid loss: 0.44683271646499634\n",
      "Step: 2700  \tTraining loss: 0.43010222911834717\n",
      "Step: 2700  \tTraining accuracy: 0.8309555649757385\n",
      "Step: 2700  \tValid loss: 0.4472557306289673\n",
      "Step: 2800  \tTraining loss: 0.42951327562332153\n",
      "Step: 2800  \tTraining accuracy: 0.8311413526535034\n",
      "Step: 2800  \tValid loss: 0.448026567697525\n",
      "Step: 2900  \tTraining loss: 0.4289485216140747\n",
      "Step: 2900  \tTraining accuracy: 0.8313140869140625\n",
      "Step: 2900  \tValid loss: 0.4489413797855377\n",
      "Step: 3000  \tTraining loss: 0.4284417927265167\n",
      "Step: 3000  \tTraining accuracy: 0.8314750790596008\n",
      "Step: 3000  \tValid loss: 0.4496214687824249\n",
      "Step: 3100  \tTraining loss: 0.4280088245868683\n",
      "Step: 3100  \tTraining accuracy: 0.8316255807876587\n",
      "Step: 3100  \tValid loss: 0.45046553015708923\n",
      "Step: 3200  \tTraining loss: 0.4276328980922699\n",
      "Step: 3200  \tTraining accuracy: 0.8317664861679077\n",
      "Step: 3200  \tValid loss: 0.4508349597454071\n",
      "Step: 3300  \tTraining loss: 0.427293986082077\n",
      "Step: 3300  \tTraining accuracy: 0.8318987488746643\n",
      "Step: 3300  \tValid loss: 0.45121556520462036\n",
      "Step: 3400  \tTraining loss: 0.4269889295101166\n",
      "Step: 3400  \tTraining accuracy: 0.8320230841636658\n",
      "Step: 3400  \tValid loss: 0.451572984457016\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8321402\n",
      "Precision: 0.8307175\n",
      "Recall: 0.9973082\n",
      "F1 score: 0.9079744\n",
      "AUC: 0.51467973\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0   0.83214   0.830718  0.997308  0.907974  0.51468  0.426718      0.832175   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.446726       0.832233   0.403779      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3498.0  \n",
      "20\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_114587/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(899, 3)\n",
      "(899, 1)\n",
      "(496, 3)\n",
      "(496, 1)\n",
      "(403, 3)\n",
      "(403, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.644153892993927\n",
      "Step: 100  \tTraining accuracy: 0.6017797589302063\n",
      "Step: 100  \tValid loss: 0.6360429525375366\n",
      "Step: 200  \tTraining loss: 0.5842534899711609\n",
      "Step: 200  \tTraining accuracy: 0.6258805990219116\n",
      "Step: 200  \tValid loss: 0.579853892326355\n",
      "Step: 300  \tTraining loss: 0.5321004390716553\n",
      "Step: 300  \tTraining accuracy: 0.6560623049736023\n",
      "Step: 300  \tValid loss: 0.5319522619247437\n",
      "Step: 400  \tTraining loss: 0.4850659668445587\n",
      "Step: 400  \tTraining accuracy: 0.6805974841117859\n",
      "Step: 400  \tValid loss: 0.493415892124176\n",
      "Step: 500  \tTraining loss: 0.4612659513950348\n",
      "Step: 500  \tTraining accuracy: 0.697441577911377\n",
      "Step: 500  \tValid loss: 0.48003682494163513\n",
      "Step: 600  \tTraining loss: 0.45430806279182434\n",
      "Step: 600  \tTraining accuracy: 0.7081605792045593\n",
      "Step: 600  \tValid loss: 0.4804283380508423\n",
      "Step: 700  \tTraining loss: 0.4526602625846863\n",
      "Step: 700  \tTraining accuracy: 0.716437041759491\n",
      "Step: 700  \tValid loss: 0.48219531774520874\n",
      "Step: 800  \tTraining loss: 0.4519241154193878\n",
      "Step: 800  \tTraining accuracy: 0.7223581671714783\n",
      "Step: 800  \tValid loss: 0.48262232542037964\n",
      "Step: 900  \tTraining loss: 0.451198548078537\n",
      "Step: 900  \tTraining accuracy: 0.7268860936164856\n",
      "Step: 900  \tValid loss: 0.4822288751602173\n",
      "Step: 1000  \tTraining loss: 0.4504408538341522\n",
      "Step: 1000  \tTraining accuracy: 0.7304607629776001\n",
      "Step: 1000  \tValid loss: 0.48151347041130066\n",
      "Step: 1100  \tTraining loss: 0.44958382844924927\n",
      "Step: 1100  \tTraining accuracy: 0.7333545088768005\n",
      "Step: 1100  \tValid loss: 0.480712354183197\n",
      "Step: 1200  \tTraining loss: 0.44817841053009033\n",
      "Step: 1200  \tTraining accuracy: 0.7355031967163086\n",
      "Step: 1200  \tValid loss: 0.47949090600013733\n",
      "Step: 1300  \tTraining loss: 0.4467144012451172\n",
      "Step: 1300  \tTraining accuracy: 0.7374861240386963\n",
      "Step: 1300  \tValid loss: 0.47807613015174866\n",
      "Step: 1400  \tTraining loss: 0.44558799266815186\n",
      "Step: 1400  \tTraining accuracy: 0.7390928268432617\n",
      "Step: 1400  \tValid loss: 0.47705572843551636\n",
      "Step: 1500  \tTraining loss: 0.4448305368423462\n",
      "Step: 1500  \tTraining accuracy: 0.7404779195785522\n",
      "Step: 1500  \tValid loss: 0.4760749638080597\n",
      "Step: 1600  \tTraining loss: 0.444134920835495\n",
      "Step: 1600  \tTraining accuracy: 0.7417919635772705\n",
      "Step: 1600  \tValid loss: 0.4751850962638855\n",
      "Step: 1700  \tTraining loss: 0.4430980682373047\n",
      "Step: 1700  \tTraining accuracy: 0.7429466843605042\n",
      "Step: 1700  \tValid loss: 0.473781019449234\n",
      "Step: 1800  \tTraining loss: 0.4413149654865265\n",
      "Step: 1800  \tTraining accuracy: 0.744128406047821\n",
      "Step: 1800  \tValid loss: 0.47164732217788696\n",
      "Step: 1900  \tTraining loss: 0.4390212297439575\n",
      "Step: 1900  \tTraining accuracy: 0.7455731630325317\n",
      "Step: 1900  \tValid loss: 0.4675860106945038\n",
      "Step: 2000  \tTraining loss: 0.4378766715526581\n",
      "Step: 2000  \tTraining accuracy: 0.7468411922454834\n",
      "Step: 2000  \tValid loss: 0.4668947160243988\n",
      "Step: 2100  \tTraining loss: 0.4366227686405182\n",
      "Step: 2100  \tTraining accuracy: 0.7482026219367981\n",
      "Step: 2100  \tValid loss: 0.4664298892021179\n",
      "Step: 2200  \tTraining loss: 0.43571215867996216\n",
      "Step: 2200  \tTraining accuracy: 0.7493855953216553\n",
      "Step: 2200  \tValid loss: 0.46565353870391846\n",
      "Step: 2300  \tTraining loss: 0.43491265177726746\n",
      "Step: 2300  \tTraining accuracy: 0.7504634857177734\n",
      "Step: 2300  \tValid loss: 0.4651144742965698\n",
      "Step: 2400  \tTraining loss: 0.43427565693855286\n",
      "Step: 2400  \tTraining accuracy: 0.7514969110488892\n",
      "Step: 2400  \tValid loss: 0.46472468972206116\n",
      "Step: 2500  \tTraining loss: 0.43375271558761597\n",
      "Step: 2500  \tTraining accuracy: 0.7524233460426331\n",
      "Step: 2500  \tValid loss: 0.46451741456985474\n",
      "Step: 2600  \tTraining loss: 0.4333137273788452\n",
      "Step: 2600  \tTraining accuracy: 0.753277063369751\n",
      "Step: 2600  \tValid loss: 0.464318186044693\n",
      "Step: 2700  \tTraining loss: 0.4329424798488617\n",
      "Step: 2700  \tTraining accuracy: 0.7540243864059448\n",
      "Step: 2700  \tValid loss: 0.46424293518066406\n",
      "Step: 2800  \tTraining loss: 0.432628870010376\n",
      "Step: 2800  \tTraining accuracy: 0.754757821559906\n",
      "Step: 2800  \tValid loss: 0.46425074338912964\n",
      "Step: 2900  \tTraining loss: 0.432361364364624\n",
      "Step: 2900  \tTraining accuracy: 0.7554397583007812\n",
      "Step: 2900  \tValid loss: 0.46419766545295715\n",
      "Step: 3000  \tTraining loss: 0.4321344196796417\n",
      "Step: 3000  \tTraining accuracy: 0.7560755014419556\n",
      "Step: 3000  \tValid loss: 0.46425944566726685\n",
      "Step: 3100  \tTraining loss: 0.4319395422935486\n",
      "Step: 3100  \tTraining accuracy: 0.7567059993743896\n",
      "Step: 3100  \tValid loss: 0.4642086327075958\n",
      "Step: 3200  \tTraining loss: 0.43177077174186707\n",
      "Step: 3200  \tTraining accuracy: 0.7572611570358276\n",
      "Step: 3200  \tValid loss: 0.464150607585907\n",
      "Step: 3300  \tTraining loss: 0.43162423372268677\n",
      "Step: 3300  \tTraining accuracy: 0.7577821612358093\n",
      "Step: 3300  \tValid loss: 0.46410071849823\n",
      "Step: 3400  \tTraining loss: 0.4314965605735779\n",
      "Step: 3400  \tTraining accuracy: 0.7582720518112183\n",
      "Step: 3400  \tValid loss: 0.4639824330806732\n",
      "Step: 3500  \tTraining loss: 0.43138420581817627\n",
      "Step: 3500  \tTraining accuracy: 0.7587335109710693\n",
      "Step: 3500  \tValid loss: 0.4640061557292938\n",
      "Step: 3600  \tTraining loss: 0.43128493428230286\n",
      "Step: 3600  \tTraining accuracy: 0.7591690421104431\n",
      "Step: 3600  \tValid loss: 0.4640882909297943\n",
      "Step: 3700  \tTraining loss: 0.431194543838501\n",
      "Step: 3700  \tTraining accuracy: 0.759580671787262\n",
      "Step: 3700  \tValid loss: 0.46390631794929504\n",
      "Step: 3800  \tTraining loss: 0.43111342191696167\n",
      "Step: 3800  \tTraining accuracy: 0.7599703669548035\n",
      "Step: 3800  \tValid loss: 0.46400943398475647\n",
      "Step: 3900  \tTraining loss: 0.4310377836227417\n",
      "Step: 3900  \tTraining accuracy: 0.7603397965431213\n",
      "Step: 3900  \tValid loss: 0.4639776051044464\n",
      "Step: 4000  \tTraining loss: 0.43096670508384705\n",
      "Step: 4000  \tTraining accuracy: 0.7606905102729797\n",
      "Step: 4000  \tValid loss: 0.4639844000339508\n",
      "Step: 4100  \tTraining loss: 0.43090057373046875\n",
      "Step: 4100  \tTraining accuracy: 0.7610238790512085\n",
      "Step: 4100  \tValid loss: 0.4640701413154602\n",
      "Step: 4200  \tTraining loss: 0.430837482213974\n",
      "Step: 4200  \tTraining accuracy: 0.7613412737846375\n",
      "Step: 4200  \tValid loss: 0.46425190567970276\n",
      "Step: 4300  \tTraining loss: 0.43077215552330017\n",
      "Step: 4300  \tTraining accuracy: 0.761643648147583\n",
      "Step: 4300  \tValid loss: 0.46424761414527893\n",
      "Step: 4400  \tTraining loss: 0.4307069778442383\n",
      "Step: 4400  \tTraining accuracy: 0.7619321346282959\n",
      "Step: 4400  \tValid loss: 0.4640793204307556\n",
      "Step: 4500  \tTraining loss: 0.43064361810684204\n",
      "Step: 4500  \tTraining accuracy: 0.7622076869010925\n",
      "Step: 4500  \tValid loss: 0.46418890357017517\n",
      "Step: 4600  \tTraining loss: 0.4305802285671234\n",
      "Step: 4600  \tTraining accuracy: 0.7624711394309998\n",
      "Step: 4600  \tValid loss: 0.4642186462879181\n",
      "Step: 4700  \tTraining loss: 0.43051376938819885\n",
      "Step: 4700  \tTraining accuracy: 0.7627232074737549\n",
      "Step: 4700  \tValid loss: 0.4642084836959839\n",
      "Step: 4800  \tTraining loss: 0.430448055267334\n",
      "Step: 4800  \tTraining accuracy: 0.7629647254943848\n",
      "Step: 4800  \tValid loss: 0.4642440974712372\n",
      "Step: 4900  \tTraining loss: 0.4303794801235199\n",
      "Step: 4900  \tTraining accuracy: 0.7631962299346924\n",
      "Step: 4900  \tValid loss: 0.4641200006008148\n",
      "Step: 5000  \tTraining loss: 0.43031036853790283\n",
      "Step: 5000  \tTraining accuracy: 0.7634183764457703\n",
      "Step: 5000  \tValid loss: 0.46411409974098206\n",
      "Step: 5100  \tTraining loss: 0.4302404224872589\n",
      "Step: 5100  \tTraining accuracy: 0.7636317610740662\n",
      "Step: 5100  \tValid loss: 0.4642862379550934\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.76383686\n",
      "Precision: 0.8607306\n",
      "Recall: 0.752495\n",
      "F1 score: 0.71258897\n",
      "AUC: 0.79961437\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.763837   0.860731  0.752495  0.712589  0.799614  0.430236      0.763539   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.463872       0.763624   0.530466      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  5104.0  \n",
      "21\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_73762/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(957, 3)\n",
      "(957, 1)\n",
      "(528, 3)\n",
      "(528, 1)\n",
      "(429, 3)\n",
      "(429, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6378507614135742\n",
      "Step: 100  \tTraining accuracy: 0.7105538249015808\n",
      "Step: 100  \tValid loss: 0.6448243260383606\n",
      "Step: 200  \tTraining loss: 0.5618859529495239\n",
      "Step: 200  \tTraining accuracy: 0.7161267995834351\n",
      "Step: 200  \tValid loss: 0.5771991610527039\n",
      "Step: 300  \tTraining loss: 0.4870474934577942\n",
      "Step: 300  \tTraining accuracy: 0.7337512969970703\n",
      "Step: 300  \tValid loss: 0.5026041269302368\n",
      "Step: 400  \tTraining loss: 0.41893935203552246\n",
      "Step: 400  \tTraining accuracy: 0.7584714293479919\n",
      "Step: 400  \tValid loss: 0.43275031447410583\n",
      "Step: 500  \tTraining loss: 0.3663571774959564\n",
      "Step: 500  \tTraining accuracy: 0.7805642485618591\n",
      "Step: 500  \tValid loss: 0.3782972991466522\n",
      "Step: 600  \tTraining loss: 0.32967090606689453\n",
      "Step: 600  \tTraining accuracy: 0.7992780208587646\n",
      "Step: 600  \tValid loss: 0.3400101065635681\n",
      "Step: 700  \tTraining loss: 0.3048425018787384\n",
      "Step: 700  \tTraining accuracy: 0.8136805891990662\n",
      "Step: 700  \tValid loss: 0.31383728981018066\n",
      "Step: 800  \tTraining loss: 0.2879684567451477\n",
      "Step: 800  \tTraining accuracy: 0.8248693943023682\n",
      "Step: 800  \tValid loss: 0.29584959149360657\n",
      "Step: 900  \tTraining loss: 0.2763136327266693\n",
      "Step: 900  \tTraining accuracy: 0.8334255218505859\n",
      "Step: 900  \tValid loss: 0.28326642513275146\n",
      "Step: 1000  \tTraining loss: 0.2681006193161011\n",
      "Step: 1000  \tTraining accuracy: 0.8401803970336914\n",
      "Step: 1000  \tValid loss: 0.2742921710014343\n",
      "Step: 1100  \tTraining loss: 0.2622067630290985\n",
      "Step: 1100  \tTraining accuracy: 0.845399796962738\n",
      "Step: 1100  \tValid loss: 0.26776179671287537\n",
      "Step: 1200  \tTraining loss: 0.2579044699668884\n",
      "Step: 1200  \tTraining accuracy: 0.8495298027992249\n",
      "Step: 1200  \tValid loss: 0.262920081615448\n",
      "Step: 1300  \tTraining loss: 0.2547110617160797\n",
      "Step: 1300  \tTraining accuracy: 0.8529989719390869\n",
      "Step: 1300  \tValid loss: 0.25926342606544495\n",
      "Step: 1400  \tTraining loss: 0.2522987723350525\n",
      "Step: 1400  \tTraining accuracy: 0.8559541702270508\n",
      "Step: 1400  \tValid loss: 0.2564496099948883\n",
      "Step: 1500  \tTraining loss: 0.2504412531852722\n",
      "Step: 1500  \tTraining accuracy: 0.8585017919540405\n",
      "Step: 1500  \tValid loss: 0.25423765182495117\n",
      "Step: 1600  \tTraining loss: 0.24897867441177368\n",
      "Step: 1600  \tTraining accuracy: 0.8608555197715759\n",
      "Step: 1600  \tValid loss: 0.2524597644805908\n",
      "Step: 1700  \tTraining loss: 0.24779696762561798\n",
      "Step: 1700  \tTraining accuracy: 0.862987220287323\n",
      "Step: 1700  \tValid loss: 0.25099101662635803\n",
      "Step: 1800  \tTraining loss: 0.24681252241134644\n",
      "Step: 1800  \tTraining accuracy: 0.8648753762245178\n",
      "Step: 1800  \tValid loss: 0.24974173307418823\n",
      "Step: 1900  \tTraining loss: 0.24596519768238068\n",
      "Step: 1900  \tTraining accuracy: 0.8665593266487122\n",
      "Step: 1900  \tValid loss: 0.24864526093006134\n",
      "Step: 2000  \tTraining loss: 0.24521246552467346\n",
      "Step: 2000  \tTraining accuracy: 0.8680706024169922\n",
      "Step: 2000  \tValid loss: 0.24765324592590332\n",
      "Step: 2100  \tTraining loss: 0.24452657997608185\n",
      "Step: 2100  \tTraining accuracy: 0.8694344758987427\n",
      "Step: 2100  \tValid loss: 0.2467327117919922\n",
      "Step: 2200  \tTraining loss: 0.24389179050922394\n",
      "Step: 2200  \tTraining accuracy: 0.8706714510917664\n",
      "Step: 2200  \tValid loss: 0.24586443603038788\n",
      "Step: 2300  \tTraining loss: 0.24330277740955353\n",
      "Step: 2300  \tTraining accuracy: 0.8717984557151794\n",
      "Step: 2300  \tValid loss: 0.2450413852930069\n",
      "Step: 2400  \tTraining loss: 0.24276162683963776\n",
      "Step: 2400  \tTraining accuracy: 0.8728295564651489\n",
      "Step: 2400  \tValid loss: 0.2442658394575119\n",
      "Step: 2500  \tTraining loss: 0.24227307736873627\n",
      "Step: 2500  \tTraining accuracy: 0.8737764954566956\n",
      "Step: 2500  \tValid loss: 0.24354501068592072\n",
      "Step: 2600  \tTraining loss: 0.24184074997901917\n",
      "Step: 2600  \tTraining accuracy: 0.8746491074562073\n",
      "Step: 2600  \tValid loss: 0.2428855299949646\n",
      "Step: 2700  \tTraining loss: 0.2414652407169342\n",
      "Step: 2700  \tTraining accuracy: 0.8755545020103455\n",
      "Step: 2700  \tValid loss: 0.24229145050048828\n",
      "Step: 2800  \tTraining loss: 0.24114346504211426\n",
      "Step: 2800  \tTraining accuracy: 0.8764700293540955\n",
      "Step: 2800  \tValid loss: 0.2417619377374649\n",
      "Step: 2900  \tTraining loss: 0.2408706396818161\n",
      "Step: 2900  \tTraining accuracy: 0.8773213028907776\n",
      "Step: 2900  \tValid loss: 0.2412940114736557\n",
      "Step: 3000  \tTraining loss: 0.24064065515995026\n",
      "Step: 3000  \tTraining accuracy: 0.8781148791313171\n",
      "Step: 3000  \tValid loss: 0.24088241159915924\n",
      "Step: 3100  \tTraining loss: 0.240447998046875\n",
      "Step: 3100  \tTraining accuracy: 0.8788392543792725\n",
      "Step: 3100  \tValid loss: 0.24052242934703827\n",
      "Step: 3200  \tTraining loss: 0.2402871996164322\n",
      "Step: 3200  \tTraining accuracy: 0.8796005845069885\n",
      "Step: 3200  \tValid loss: 0.24020816385746002\n",
      "Step: 3300  \tTraining loss: 0.24015334248542786\n",
      "Step: 3300  \tTraining accuracy: 0.8803150653839111\n",
      "Step: 3300  \tValid loss: 0.23993411660194397\n",
      "Step: 3400  \tTraining loss: 0.24004220962524414\n",
      "Step: 3400  \tTraining accuracy: 0.8809869289398193\n",
      "Step: 3400  \tValid loss: 0.23969583213329315\n",
      "Step: 3500  \tTraining loss: 0.2399500161409378\n",
      "Step: 3500  \tTraining accuracy: 0.8816198110580444\n",
      "Step: 3500  \tValid loss: 0.23948854207992554\n",
      "Step: 3600  \tTraining loss: 0.23987363278865814\n",
      "Step: 3600  \tTraining accuracy: 0.8822170495986938\n",
      "Step: 3600  \tValid loss: 0.23930862545967102\n",
      "Step: 3700  \tTraining loss: 0.2398104965686798\n",
      "Step: 3700  \tTraining accuracy: 0.8827815055847168\n",
      "Step: 3700  \tValid loss: 0.23915255069732666\n",
      "Step: 3800  \tTraining loss: 0.2397581934928894\n",
      "Step: 3800  \tTraining accuracy: 0.883315920829773\n",
      "Step: 3800  \tValid loss: 0.23901666700839996\n",
      "Step: 3900  \tTraining loss: 0.2397148609161377\n",
      "Step: 3900  \tTraining accuracy: 0.8838225603103638\n",
      "Step: 3900  \tValid loss: 0.23889894783496857\n",
      "Step: 4000  \tTraining loss: 0.23967906832695007\n",
      "Step: 4000  \tTraining accuracy: 0.8843035101890564\n",
      "Step: 4000  \tValid loss: 0.23879648745059967\n",
      "Step: 4100  \tTraining loss: 0.23964928090572357\n",
      "Step: 4100  \tTraining accuracy: 0.8847607374191284\n",
      "Step: 4100  \tValid loss: 0.23870731890201569\n",
      "Step: 4200  \tTraining loss: 0.23962467908859253\n",
      "Step: 4200  \tTraining accuracy: 0.8852337002754211\n",
      "Step: 4200  \tValid loss: 0.23862974345684052\n",
      "Step: 4300  \tTraining loss: 0.2396041303873062\n",
      "Step: 4300  \tTraining accuracy: 0.8857458829879761\n",
      "Step: 4300  \tValid loss: 0.23856185376644135\n",
      "Step: 4400  \tTraining loss: 0.23958703875541687\n",
      "Step: 4400  \tTraining accuracy: 0.8862345218658447\n",
      "Step: 4400  \tValid loss: 0.2385026216506958\n",
      "Step: 4500  \tTraining loss: 0.23957261443138123\n",
      "Step: 4500  \tTraining accuracy: 0.8867011666297913\n",
      "Step: 4500  \tValid loss: 0.2384507805109024\n",
      "Step: 4600  \tTraining loss: 0.2395605593919754\n",
      "Step: 4600  \tTraining accuracy: 0.8871473073959351\n",
      "Step: 4600  \tValid loss: 0.23840519785881042\n",
      "Step: 4700  \tTraining loss: 0.23955048620700836\n",
      "Step: 4700  \tTraining accuracy: 0.887574315071106\n",
      "Step: 4700  \tValid loss: 0.2383653223514557\n",
      "Step: 4800  \tTraining loss: 0.23954185843467712\n",
      "Step: 4800  \tTraining accuracy: 0.8879832625389099\n",
      "Step: 4800  \tValid loss: 0.23833014070987701\n",
      "Step: 4900  \tTraining loss: 0.23953454196453094\n",
      "Step: 4900  \tTraining accuracy: 0.8883754014968872\n",
      "Step: 4900  \tValid loss: 0.2382989525794983\n",
      "Step: 5000  \tTraining loss: 0.23952822387218475\n",
      "Step: 5000  \tTraining accuracy: 0.8887516856193542\n",
      "Step: 5000  \tValid loss: 0.23827148973941803\n",
      "Step: 5100  \tTraining loss: 0.23952290415763855\n",
      "Step: 5100  \tTraining accuracy: 0.8891130685806274\n",
      "Step: 5100  \tValid loss: 0.23824703693389893\n",
      "Step: 5200  \tTraining loss: 0.23951832950115204\n",
      "Step: 5200  \tTraining accuracy: 0.8894603848457336\n",
      "Step: 5200  \tValid loss: 0.23822546005249023\n",
      "Step: 5300  \tTraining loss: 0.23951418697834015\n",
      "Step: 5300  \tTraining accuracy: 0.8897944688796997\n",
      "Step: 5300  \tValid loss: 0.23820625245571136\n",
      "Step: 5400  \tTraining loss: 0.239510640501976\n",
      "Step: 5400  \tTraining accuracy: 0.8901160955429077\n",
      "Step: 5400  \tValid loss: 0.2381889373064041\n",
      "Step: 5500  \tTraining loss: 0.23950743675231934\n",
      "Step: 5500  \tTraining accuracy: 0.8904259204864502\n",
      "Step: 5500  \tValid loss: 0.23817351460456848\n",
      "Step: 5600  \tTraining loss: 0.23950454592704773\n",
      "Step: 5600  \tTraining accuracy: 0.8907245993614197\n",
      "Step: 5600  \tValid loss: 0.238159641623497\n",
      "Step: 5700  \tTraining loss: 0.23950202763080597\n",
      "Step: 5700  \tTraining accuracy: 0.8910126686096191\n",
      "Step: 5700  \tValid loss: 0.23814718425273895\n",
      "Step: 5800  \tTraining loss: 0.23949959874153137\n",
      "Step: 5800  \tTraining accuracy: 0.8912907242774963\n",
      "Step: 5800  \tValid loss: 0.2381359338760376\n",
      "Step: 5900  \tTraining loss: 0.23949745297431946\n",
      "Step: 5900  \tTraining accuracy: 0.8915592432022095\n",
      "Step: 5900  \tValid loss: 0.23812569677829742\n",
      "Step: 6000  \tTraining loss: 0.23949547111988068\n",
      "Step: 6000  \tTraining accuracy: 0.8918188214302063\n",
      "Step: 6000  \tValid loss: 0.23811635375022888\n",
      "Step: 6100  \tTraining loss: 0.23949357867240906\n",
      "Step: 6100  \tTraining accuracy: 0.8920697569847107\n",
      "Step: 6100  \tValid loss: 0.23810799419879913\n",
      "Step: 6200  \tTraining loss: 0.239491805434227\n",
      "Step: 6200  \tTraining accuracy: 0.8923125267028809\n",
      "Step: 6200  \tValid loss: 0.2381003051996231\n",
      "Step: 6300  \tTraining loss: 0.23949021100997925\n",
      "Step: 6300  \tTraining accuracy: 0.8925475478172302\n",
      "Step: 6300  \tValid loss: 0.23809319734573364\n",
      "Step: 6400  \tTraining loss: 0.23948855698108673\n",
      "Step: 6400  \tTraining accuracy: 0.8927751779556274\n",
      "Step: 6400  \tValid loss: 0.2380867302417755\n",
      "Step: 6500  \tTraining loss: 0.23948705196380615\n",
      "Step: 6500  \tTraining accuracy: 0.8929957151412964\n",
      "Step: 6500  \tValid loss: 0.23808075487613678\n",
      "Step: 6600  \tTraining loss: 0.23948557674884796\n",
      "Step: 6600  \tTraining accuracy: 0.8932095170021057\n",
      "Step: 6600  \tValid loss: 0.23807522654533386\n",
      "Step: 6700  \tTraining loss: 0.23948416113853455\n",
      "Step: 6700  \tTraining accuracy: 0.8934169411659241\n",
      "Step: 6700  \tValid loss: 0.23807024955749512\n",
      "Step: 6800  \tTraining loss: 0.23948273062705994\n",
      "Step: 6800  \tTraining accuracy: 0.8936181664466858\n",
      "Step: 6800  \tValid loss: 0.2380654364824295\n",
      "Step: 6900  \tTraining loss: 0.2394813895225525\n",
      "Step: 6900  \tTraining accuracy: 0.8938135504722595\n",
      "Step: 6900  \tValid loss: 0.23806098103523254\n",
      "Step: 7000  \tTraining loss: 0.23948003351688385\n",
      "Step: 7000  \tTraining accuracy: 0.8940032720565796\n",
      "Step: 7000  \tValid loss: 0.23805686831474304\n",
      "Step: 7100  \tTraining loss: 0.23947881162166595\n",
      "Step: 7100  \tTraining accuracy: 0.8941876292228699\n",
      "Step: 7100  \tValid loss: 0.23805297911167145\n",
      "Step: 7200  \tTraining loss: 0.23947744071483612\n",
      "Step: 7200  \tTraining accuracy: 0.8943668603897095\n",
      "Step: 7200  \tValid loss: 0.23804935812950134\n",
      "Step: 7300  \tTraining loss: 0.23947621881961823\n",
      "Step: 7300  \tTraining accuracy: 0.8945411443710327\n",
      "Step: 7300  \tValid loss: 0.23804599046707153\n",
      "Step: 7400  \tTraining loss: 0.23947499692440033\n",
      "Step: 7400  \tTraining accuracy: 0.8947106599807739\n",
      "Step: 7400  \tValid loss: 0.23804278671741486\n",
      "Step: 7500  \tTraining loss: 0.23947377502918243\n",
      "Step: 7500  \tTraining accuracy: 0.8948756456375122\n",
      "Step: 7500  \tValid loss: 0.23803973197937012\n",
      "Step: 7600  \tTraining loss: 0.23947259783744812\n",
      "Step: 7600  \tTraining accuracy: 0.8950362205505371\n",
      "Step: 7600  \tValid loss: 0.23803682625293732\n",
      "Step: 7700  \tTraining loss: 0.2394714206457138\n",
      "Step: 7700  \tTraining accuracy: 0.8951926231384277\n",
      "Step: 7700  \tValid loss: 0.23803412914276123\n",
      "Step: 7800  \tTraining loss: 0.2394702285528183\n",
      "Step: 7800  \tTraining accuracy: 0.8953449726104736\n",
      "Step: 7800  \tValid loss: 0.23803147673606873\n",
      "Step: 7900  \tTraining loss: 0.2394690364599228\n",
      "Step: 7900  \tTraining accuracy: 0.8954935073852539\n",
      "Step: 7900  \tValid loss: 0.238028883934021\n",
      "Step: 8000  \tTraining loss: 0.2394677996635437\n",
      "Step: 8000  \tTraining accuracy: 0.8956382274627686\n",
      "Step: 8000  \tValid loss: 0.2380264550447464\n",
      "Step: 8100  \tTraining loss: 0.23946665227413177\n",
      "Step: 8100  \tTraining accuracy: 0.8957793712615967\n",
      "Step: 8100  \tValid loss: 0.2380242794752121\n",
      "Step: 8200  \tTraining loss: 0.23946547508239746\n",
      "Step: 8200  \tTraining accuracy: 0.8959170579910278\n",
      "Step: 8200  \tValid loss: 0.2380220890045166\n",
      "Step: 8300  \tTraining loss: 0.23946432769298553\n",
      "Step: 8300  \tTraining accuracy: 0.8960514068603516\n",
      "Step: 8300  \tValid loss: 0.23802003264427185\n",
      "Step: 8400  \tTraining loss: 0.23946315050125122\n",
      "Step: 8400  \tTraining accuracy: 0.8961825370788574\n",
      "Step: 8400  \tValid loss: 0.2380179464817047\n",
      "Step: 8500  \tTraining loss: 0.23946206271648407\n",
      "Step: 8500  \tTraining accuracy: 0.896310567855835\n",
      "Step: 8500  \tValid loss: 0.23801594972610474\n",
      "Step: 8600  \tTraining loss: 0.23946096003055573\n",
      "Step: 8600  \tTraining accuracy: 0.8964356184005737\n",
      "Step: 8600  \tValid loss: 0.23801398277282715\n",
      "Step: 8700  \tTraining loss: 0.2394598424434662\n",
      "Step: 8700  \tTraining accuracy: 0.8965577483177185\n",
      "Step: 8700  \tValid loss: 0.23801210522651672\n",
      "Step: 8800  \tTraining loss: 0.23945868015289307\n",
      "Step: 8800  \tTraining accuracy: 0.8966771364212036\n",
      "Step: 8800  \tValid loss: 0.2380102425813675\n",
      "Step: 8900  \tTraining loss: 0.23945759236812592\n",
      "Step: 8900  \tTraining accuracy: 0.896793782711029\n",
      "Step: 8900  \tValid loss: 0.2380084991455078\n",
      "Step: 9000  \tTraining loss: 0.23945650458335876\n",
      "Step: 9000  \tTraining accuracy: 0.8969078063964844\n",
      "Step: 9000  \tValid loss: 0.2380068153142929\n",
      "Step: 9100  \tTraining loss: 0.23945537209510803\n",
      "Step: 9100  \tTraining accuracy: 0.8970193266868591\n",
      "Step: 9100  \tValid loss: 0.2380051612854004\n",
      "Step: 9200  \tTraining loss: 0.23945428431034088\n",
      "Step: 9200  \tTraining accuracy: 0.8971284627914429\n",
      "Step: 9200  \tValid loss: 0.23800352215766907\n",
      "Step: 9300  \tTraining loss: 0.23945319652557373\n",
      "Step: 9300  \tTraining accuracy: 0.8972351551055908\n",
      "Step: 9300  \tValid loss: 0.23800186812877655\n",
      "Step: 9400  \tTraining loss: 0.23945213854312897\n",
      "Step: 9400  \tTraining accuracy: 0.8973396420478821\n",
      "Step: 9400  \tValid loss: 0.23800022900104523\n",
      "Step: 9500  \tTraining loss: 0.23945097625255585\n",
      "Step: 9500  \tTraining accuracy: 0.8974418640136719\n",
      "Step: 9500  \tValid loss: 0.23799864947795868\n",
      "Step: 9600  \tTraining loss: 0.23944996297359467\n",
      "Step: 9600  \tTraining accuracy: 0.8975419402122498\n",
      "Step: 9600  \tValid loss: 0.23799718916416168\n",
      "Step: 9700  \tTraining loss: 0.2394489347934723\n",
      "Step: 9700  \tTraining accuracy: 0.8976399898529053\n",
      "Step: 9700  \tValid loss: 0.23799578845500946\n",
      "Step: 9800  \tTraining loss: 0.23944784700870514\n",
      "Step: 9800  \tTraining accuracy: 0.8977359533309937\n",
      "Step: 9800  \tValid loss: 0.23799434304237366\n",
      "Step: 9900  \tTraining loss: 0.23944677412509918\n",
      "Step: 9900  \tTraining accuracy: 0.897830069065094\n",
      "Step: 9900  \tValid loss: 0.23799297213554382\n",
      "Step: 10000  \tTraining loss: 0.23944570124149323\n",
      "Step: 10000  \tTraining accuracy: 0.8979222178459167\n",
      "Step: 10000  \tValid loss: 0.2379915416240692\n",
      "Step: 10100  \tTraining loss: 0.23944464325904846\n",
      "Step: 10100  \tTraining accuracy: 0.8980125784873962\n",
      "Step: 10100  \tValid loss: 0.23799023032188416\n",
      "Step: 10200  \tTraining loss: 0.2394435852766037\n",
      "Step: 10200  \tTraining accuracy: 0.8981010913848877\n",
      "Step: 10200  \tValid loss: 0.23798894882202148\n",
      "Step: 10300  \tTraining loss: 0.23944251239299774\n",
      "Step: 10300  \tTraining accuracy: 0.8981879353523254\n",
      "Step: 10300  \tValid loss: 0.2379876971244812\n",
      "Step: 10400  \tTraining loss: 0.23944146931171417\n",
      "Step: 10400  \tTraining accuracy: 0.8982731103897095\n",
      "Step: 10400  \tValid loss: 0.23798641562461853\n",
      "Step: 10500  \tTraining loss: 0.23944051563739777\n",
      "Step: 10500  \tTraining accuracy: 0.8983566164970398\n",
      "Step: 10500  \tValid loss: 0.2379852533340454\n",
      "Step: 10600  \tTraining loss: 0.23943939805030823\n",
      "Step: 10600  \tTraining accuracy: 0.898438572883606\n",
      "Step: 10600  \tValid loss: 0.2379840612411499\n",
      "Step: 10700  \tTraining loss: 0.23943831026554108\n",
      "Step: 10700  \tTraining accuracy: 0.8985189199447632\n",
      "Step: 10700  \tValid loss: 0.23798279464244843\n",
      "Step: 10800  \tTraining loss: 0.2394372969865799\n",
      "Step: 10800  \tTraining accuracy: 0.8985978364944458\n",
      "Step: 10800  \tValid loss: 0.2379816621541977\n",
      "Step: 10900  \tTraining loss: 0.2394362986087799\n",
      "Step: 10900  \tTraining accuracy: 0.8986753225326538\n",
      "Step: 10900  \tValid loss: 0.23798051476478577\n",
      "Step: 11000  \tTraining loss: 0.2394353300333023\n",
      "Step: 11000  \tTraining accuracy: 0.8987513184547424\n",
      "Step: 11000  \tValid loss: 0.23797941207885742\n",
      "Step: 11100  \tTraining loss: 0.23943427205085754\n",
      "Step: 11100  \tTraining accuracy: 0.898826003074646\n",
      "Step: 11100  \tValid loss: 0.23797829449176788\n",
      "Step: 11200  \tTraining loss: 0.23943322896957397\n",
      "Step: 11200  \tTraining accuracy: 0.8988993167877197\n",
      "Step: 11200  \tValid loss: 0.23797714710235596\n",
      "Step: 11300  \tTraining loss: 0.23943227529525757\n",
      "Step: 11300  \tTraining accuracy: 0.8989713191986084\n",
      "Step: 11300  \tValid loss: 0.23797611892223358\n",
      "Step: 11400  \tTraining loss: 0.23943129181861877\n",
      "Step: 11400  \tTraining accuracy: 0.8990420699119568\n",
      "Step: 11400  \tValid loss: 0.2379750907421112\n",
      "Step: 11500  \tTraining loss: 0.23943018913269043\n",
      "Step: 11500  \tTraining accuracy: 0.8991115689277649\n",
      "Step: 11500  \tValid loss: 0.23797404766082764\n",
      "Step: 11600  \tTraining loss: 0.23942922055721283\n",
      "Step: 11600  \tTraining accuracy: 0.8991798758506775\n",
      "Step: 11600  \tValid loss: 0.23797297477722168\n",
      "Step: 11700  \tTraining loss: 0.23942819237709045\n",
      "Step: 11700  \tTraining accuracy: 0.8992470502853394\n",
      "Step: 11700  \tValid loss: 0.2379719465970993\n",
      "Step: 11800  \tTraining loss: 0.23942722380161285\n",
      "Step: 11800  \tTraining accuracy: 0.8993130326271057\n",
      "Step: 11800  \tValid loss: 0.23797088861465454\n",
      "Step: 11900  \tTraining loss: 0.2394261360168457\n",
      "Step: 11900  \tTraining accuracy: 0.8993778824806213\n",
      "Step: 11900  \tValid loss: 0.23796997964382172\n",
      "Step: 12000  \tTraining loss: 0.2394251525402069\n",
      "Step: 12000  \tTraining accuracy: 0.899441659450531\n",
      "Step: 12000  \tValid loss: 0.2379690259695053\n",
      "Step: 12100  \tTraining loss: 0.23942415416240692\n",
      "Step: 12100  \tTraining accuracy: 0.8995044231414795\n",
      "Step: 12100  \tValid loss: 0.23796795308589935\n",
      "Step: 12200  \tTraining loss: 0.2394232600927353\n",
      "Step: 12200  \tTraining accuracy: 0.899566113948822\n",
      "Step: 12200  \tValid loss: 0.23796699941158295\n",
      "Step: 12300  \tTraining loss: 0.23942218720912933\n",
      "Step: 12300  \tTraining accuracy: 0.8996267914772034\n",
      "Step: 12300  \tValid loss: 0.23796600103378296\n",
      "Step: 12400  \tTraining loss: 0.23942123353481293\n",
      "Step: 12400  \tTraining accuracy: 0.8996865153312683\n",
      "Step: 12400  \tValid loss: 0.23796506226062775\n",
      "Step: 12500  \tTraining loss: 0.23942022025585175\n",
      "Step: 12500  \tTraining accuracy: 0.8997452855110168\n",
      "Step: 12500  \tValid loss: 0.23796410858631134\n",
      "Step: 12600  \tTraining loss: 0.23941929638385773\n",
      "Step: 12600  \tTraining accuracy: 0.899803102016449\n",
      "Step: 12600  \tValid loss: 0.2379632145166397\n",
      "Step: 12700  \tTraining loss: 0.23941828310489655\n",
      "Step: 12700  \tTraining accuracy: 0.8998599648475647\n",
      "Step: 12700  \tValid loss: 0.2379622608423233\n",
      "Step: 12800  \tTraining loss: 0.23941732943058014\n",
      "Step: 12800  \tTraining accuracy: 0.8999159932136536\n",
      "Step: 12800  \tValid loss: 0.23796136677265167\n",
      "Step: 12900  \tTraining loss: 0.23941631615161896\n",
      "Step: 12900  \tTraining accuracy: 0.8999711275100708\n",
      "Step: 12900  \tValid loss: 0.23796047270298004\n",
      "Step: 13000  \tTraining loss: 0.23941534757614136\n",
      "Step: 13000  \tTraining accuracy: 0.9000254273414612\n",
      "Step: 13000  \tValid loss: 0.2379595786333084\n",
      "Step: 13100  \tTraining loss: 0.23941433429718018\n",
      "Step: 13100  \tTraining accuracy: 0.9000788927078247\n",
      "Step: 13100  \tValid loss: 0.23795875906944275\n",
      "Step: 13200  \tTraining loss: 0.23941345512866974\n",
      "Step: 13200  \tTraining accuracy: 0.9001315236091614\n",
      "Step: 13200  \tValid loss: 0.2379579246044159\n",
      "Step: 13300  \tTraining loss: 0.23941250145435333\n",
      "Step: 13300  \tTraining accuracy: 0.900183379650116\n",
      "Step: 13300  \tValid loss: 0.23795711994171143\n",
      "Step: 13400  \tTraining loss: 0.23941147327423096\n",
      "Step: 13400  \tTraining accuracy: 0.9002344012260437\n",
      "Step: 13400  \tValid loss: 0.23795634508132935\n",
      "Step: 13500  \tTraining loss: 0.23941053450107574\n",
      "Step: 13500  \tTraining accuracy: 0.9002847075462341\n",
      "Step: 13500  \tValid loss: 0.23795555531978607\n",
      "Step: 13600  \tTraining loss: 0.23940962553024292\n",
      "Step: 13600  \tTraining accuracy: 0.9003342986106873\n",
      "Step: 13600  \tValid loss: 0.23795484006404877\n",
      "Step: 13700  \tTraining loss: 0.23940861225128174\n",
      "Step: 13700  \tTraining accuracy: 0.9003831148147583\n",
      "Step: 13700  \tValid loss: 0.23795413970947266\n",
      "Step: 13800  \tTraining loss: 0.23940759897232056\n",
      "Step: 13800  \tTraining accuracy: 0.9004312753677368\n",
      "Step: 13800  \tValid loss: 0.23795339465141296\n",
      "Step: 13900  \tTraining loss: 0.23940663039684296\n",
      "Step: 13900  \tTraining accuracy: 0.900478720664978\n",
      "Step: 13900  \tValid loss: 0.23795273900032043\n",
      "Step: 14000  \tTraining loss: 0.23940569162368774\n",
      "Step: 14000  \tTraining accuracy: 0.9005254507064819\n",
      "Step: 14000  \tValid loss: 0.2379520684480667\n",
      "Step: 14100  \tTraining loss: 0.23940476775169373\n",
      "Step: 14100  \tTraining accuracy: 0.9005715250968933\n",
      "Step: 14100  \tValid loss: 0.23795142769813538\n",
      "Step: 14200  \tTraining loss: 0.2394038289785385\n",
      "Step: 14200  \tTraining accuracy: 0.9006170034408569\n",
      "Step: 14200  \tValid loss: 0.2379506230354309\n",
      "Step: 14300  \tTraining loss: 0.23940278589725494\n",
      "Step: 14300  \tTraining accuracy: 0.9006617665290833\n",
      "Step: 14300  \tValid loss: 0.2379499226808548\n",
      "Step: 14400  \tTraining loss: 0.2394018918275833\n",
      "Step: 14400  \tTraining accuracy: 0.9007059931755066\n",
      "Step: 14400  \tValid loss: 0.2379492223262787\n",
      "Step: 14500  \tTraining loss: 0.23940089344978333\n",
      "Step: 14500  \tTraining accuracy: 0.9007495045661926\n",
      "Step: 14500  \tValid loss: 0.23794861137866974\n",
      "Step: 14600  \tTraining loss: 0.2393999695777893\n",
      "Step: 14600  \tTraining accuracy: 0.9007924795150757\n",
      "Step: 14600  \tValid loss: 0.23794786632061005\n",
      "Step: 14700  \tTraining loss: 0.2393990457057953\n",
      "Step: 14700  \tTraining accuracy: 0.900834858417511\n",
      "Step: 14700  \tValid loss: 0.23794721066951752\n",
      "Step: 14800  \tTraining loss: 0.23939812183380127\n",
      "Step: 14800  \tTraining accuracy: 0.9008767008781433\n",
      "Step: 14800  \tValid loss: 0.23794656991958618\n",
      "Step: 14900  \tTraining loss: 0.23939700424671173\n",
      "Step: 14900  \tTraining accuracy: 0.9009179472923279\n",
      "Step: 14900  \tValid loss: 0.23794583976268768\n",
      "Step: 15000  \tTraining loss: 0.23939621448516846\n",
      "Step: 15000  \tTraining accuracy: 0.9009585976600647\n",
      "Step: 15000  \tValid loss: 0.23794522881507874\n",
      "Step: 15100  \tTraining loss: 0.23939532041549683\n",
      "Step: 15100  \tTraining accuracy: 0.9009987711906433\n",
      "Step: 15100  \tValid loss: 0.23794463276863098\n",
      "Step: 15200  \tTraining loss: 0.23939430713653564\n",
      "Step: 15200  \tTraining accuracy: 0.901038408279419\n",
      "Step: 15200  \tValid loss: 0.23794400691986084\n",
      "Step: 15300  \tTraining loss: 0.2393934428691864\n",
      "Step: 15300  \tTraining accuracy: 0.9010775089263916\n",
      "Step: 15300  \tValid loss: 0.2379433959722519\n",
      "Step: 15400  \tTraining loss: 0.2393924742937088\n",
      "Step: 15400  \tTraining accuracy: 0.9011160731315613\n",
      "Step: 15400  \tValid loss: 0.23794281482696533\n",
      "Step: 15500  \tTraining loss: 0.2393915355205536\n",
      "Step: 15500  \tTraining accuracy: 0.9011541604995728\n",
      "Step: 15500  \tValid loss: 0.237942174077034\n",
      "Step: 15600  \tTraining loss: 0.23939058184623718\n",
      "Step: 15600  \tTraining accuracy: 0.901191771030426\n",
      "Step: 15600  \tValid loss: 0.23794162273406982\n",
      "Step: 15700  \tTraining loss: 0.2393895536661148\n",
      "Step: 15700  \tTraining accuracy: 0.9012289047241211\n",
      "Step: 15700  \tValid loss: 0.23794099688529968\n",
      "Step: 15800  \tTraining loss: 0.23938867449760437\n",
      "Step: 15800  \tTraining accuracy: 0.9012655019760132\n",
      "Step: 15800  \tValid loss: 0.2379404604434967\n",
      "Step: 15900  \tTraining loss: 0.23938779532909393\n",
      "Step: 15900  \tTraining accuracy: 0.9013017416000366\n",
      "Step: 15900  \tValid loss: 0.2379399538040161\n",
      "Step: 16000  \tTraining loss: 0.23938684165477753\n",
      "Step: 16000  \tTraining accuracy: 0.9013374447822571\n",
      "Step: 16000  \tValid loss: 0.23793938755989075\n",
      "Step: 16100  \tTraining loss: 0.23938579857349396\n",
      "Step: 16100  \tTraining accuracy: 0.9013727307319641\n",
      "Step: 16100  \tValid loss: 0.23793883621692657\n",
      "Step: 16200  \tTraining loss: 0.23938491940498352\n",
      "Step: 16200  \tTraining accuracy: 0.9014075994491577\n",
      "Step: 16200  \tValid loss: 0.23793832957744598\n",
      "Step: 16300  \tTraining loss: 0.2393839955329895\n",
      "Step: 16300  \tTraining accuracy: 0.9014419913291931\n",
      "Step: 16300  \tValid loss: 0.23793774843215942\n",
      "Step: 16400  \tTraining loss: 0.23938310146331787\n",
      "Step: 16400  \tTraining accuracy: 0.9014760255813599\n",
      "Step: 16400  \tValid loss: 0.23793728649616241\n",
      "Step: 16500  \tTraining loss: 0.23938220739364624\n",
      "Step: 16500  \tTraining accuracy: 0.9015095829963684\n",
      "Step: 16500  \tValid loss: 0.2379368096590042\n",
      "Step: 16600  \tTraining loss: 0.23938122391700745\n",
      "Step: 16600  \tTraining accuracy: 0.9015427827835083\n",
      "Step: 16600  \tValid loss: 0.237936332821846\n",
      "Step: 16700  \tTraining loss: 0.23938028514385223\n",
      "Step: 16700  \tTraining accuracy: 0.9015755653381348\n",
      "Step: 16700  \tValid loss: 0.23793581128120422\n",
      "Step: 16800  \tTraining loss: 0.2393794059753418\n",
      "Step: 16800  \tTraining accuracy: 0.9016079306602478\n",
      "Step: 16800  \tValid loss: 0.23793531954288483\n",
      "Step: 16900  \tTraining loss: 0.23937851190567017\n",
      "Step: 16900  \tTraining accuracy: 0.9016399383544922\n",
      "Step: 16900  \tValid loss: 0.23793485760688782\n",
      "Step: 17000  \tTraining loss: 0.23937751352787018\n",
      "Step: 17000  \tTraining accuracy: 0.9016715884208679\n",
      "Step: 17000  \tValid loss: 0.2379344403743744\n",
      "Step: 17100  \tTraining loss: 0.23937654495239258\n",
      "Step: 17100  \tTraining accuracy: 0.9017028212547302\n",
      "Step: 17100  \tValid loss: 0.23793397843837738\n",
      "Step: 17200  \tTraining loss: 0.23937557637691498\n",
      "Step: 17200  \tTraining accuracy: 0.9017337560653687\n",
      "Step: 17200  \tValid loss: 0.2379334717988968\n",
      "Step: 17300  \tTraining loss: 0.23937466740608215\n",
      "Step: 17300  \tTraining accuracy: 0.9017642736434937\n",
      "Step: 17300  \tValid loss: 0.23793305456638336\n",
      "Step: 17400  \tTraining loss: 0.23937377333641052\n",
      "Step: 17400  \tTraining accuracy: 0.90179443359375\n",
      "Step: 17400  \tValid loss: 0.23793260753154755\n",
      "Step: 17500  \tTraining loss: 0.2393728345632553\n",
      "Step: 17500  \tTraining accuracy: 0.9018242955207825\n",
      "Step: 17500  \tValid loss: 0.23793216049671173\n",
      "Step: 17600  \tTraining loss: 0.2393719106912613\n",
      "Step: 17600  \tTraining accuracy: 0.9018537998199463\n",
      "Step: 17600  \tValid loss: 0.23793171346187592\n",
      "Step: 17700  \tTraining loss: 0.23937095701694489\n",
      "Step: 17700  \tTraining accuracy: 0.9018829464912415\n",
      "Step: 17700  \tValid loss: 0.23793131113052368\n",
      "Step: 17800  \tTraining loss: 0.23937009274959564\n",
      "Step: 17800  \tTraining accuracy: 0.9019117951393127\n",
      "Step: 17800  \tValid loss: 0.23793084919452667\n",
      "Step: 17900  \tTraining loss: 0.23936907947063446\n",
      "Step: 17900  \tTraining accuracy: 0.9019402861595154\n",
      "Step: 17900  \tValid loss: 0.23793040215969086\n",
      "Step: 18000  \tTraining loss: 0.23936817049980164\n",
      "Step: 18000  \tTraining accuracy: 0.9019684791564941\n",
      "Step: 18000  \tValid loss: 0.23792999982833862\n",
      "Step: 18100  \tTraining loss: 0.23936727643013\n",
      "Step: 18100  \tTraining accuracy: 0.901996374130249\n",
      "Step: 18100  \tValid loss: 0.23792962729930878\n",
      "Step: 18200  \tTraining loss: 0.2393663078546524\n",
      "Step: 18200  \tTraining accuracy: 0.90202397108078\n",
      "Step: 18200  \tValid loss: 0.23792922496795654\n",
      "Step: 18300  \tTraining loss: 0.2393653690814972\n",
      "Step: 18300  \tTraining accuracy: 0.9020512104034424\n",
      "Step: 18300  \tValid loss: 0.23792876303195953\n",
      "Step: 18400  \tTraining loss: 0.23936451971530914\n",
      "Step: 18400  \tTraining accuracy: 0.9020782113075256\n",
      "Step: 18400  \tValid loss: 0.2379283905029297\n",
      "Step: 18500  \tTraining loss: 0.23936346173286438\n",
      "Step: 18500  \tTraining accuracy: 0.9021048545837402\n",
      "Step: 18500  \tValid loss: 0.23792801797389984\n",
      "Step: 18600  \tTraining loss: 0.23936256766319275\n",
      "Step: 18600  \tTraining accuracy: 0.9021312594413757\n",
      "Step: 18600  \tValid loss: 0.2379276156425476\n",
      "Step: 18700  \tTraining loss: 0.23936165869235992\n",
      "Step: 18700  \tTraining accuracy: 0.9021573662757874\n",
      "Step: 18700  \tValid loss: 0.23792722821235657\n",
      "Step: 18800  \tTraining loss: 0.2393607348203659\n",
      "Step: 18800  \tTraining accuracy: 0.9021832346916199\n",
      "Step: 18800  \tValid loss: 0.23792679607868195\n",
      "Step: 18900  \tTraining loss: 0.23935967683792114\n",
      "Step: 18900  \tTraining accuracy: 0.9022087454795837\n",
      "Step: 18900  \tValid loss: 0.23792648315429688\n",
      "Step: 19000  \tTraining loss: 0.23935885727405548\n",
      "Step: 19000  \tTraining accuracy: 0.9022340774536133\n",
      "Step: 19000  \tValid loss: 0.23792599141597748\n",
      "Step: 19100  \tTraining loss: 0.2393578737974167\n",
      "Step: 19100  \tTraining accuracy: 0.902259111404419\n",
      "Step: 19100  \tValid loss: 0.2379254400730133\n",
      "Step: 19200  \tTraining loss: 0.23935696482658386\n",
      "Step: 19200  \tTraining accuracy: 0.9022838473320007\n",
      "Step: 19200  \tValid loss: 0.2379249781370163\n",
      "Step: 19300  \tTraining loss: 0.23935599625110626\n",
      "Step: 19300  \tTraining accuracy: 0.9023083448410034\n",
      "Step: 19300  \tValid loss: 0.23792462050914764\n",
      "Step: 19400  \tTraining loss: 0.2393551468849182\n",
      "Step: 19400  \tTraining accuracy: 0.902332603931427\n",
      "Step: 19400  \tValid loss: 0.23792432248592377\n",
      "Step: 19500  \tTraining loss: 0.2393541932106018\n",
      "Step: 19500  \tTraining accuracy: 0.9023566246032715\n",
      "Step: 19500  \tValid loss: 0.23792394995689392\n",
      "Step: 19600  \tTraining loss: 0.2393532246351242\n",
      "Step: 19600  \tTraining accuracy: 0.9023803472518921\n",
      "Step: 19600  \tValid loss: 0.23792362213134766\n",
      "Step: 19700  \tTraining loss: 0.2393523007631302\n",
      "Step: 19700  \tTraining accuracy: 0.9024038910865784\n",
      "Step: 19700  \tValid loss: 0.2379232794046402\n",
      "Step: 19800  \tTraining loss: 0.23935136198997498\n",
      "Step: 19800  \tTraining accuracy: 0.9024271368980408\n",
      "Step: 19800  \tValid loss: 0.2379230260848999\n",
      "Step: 19900  \tTraining loss: 0.23935043811798096\n",
      "Step: 19900  \tTraining accuracy: 0.9024502038955688\n",
      "Step: 19900  \tValid loss: 0.2379227578639984\n",
      "Step: 20000  \tTraining loss: 0.23934954404830933\n",
      "Step: 20000  \tTraining accuracy: 0.9024730324745178\n",
      "Step: 20000  \tValid loss: 0.2379225492477417\n",
      "Step: 20100  \tTraining loss: 0.23934856057167053\n",
      "Step: 20100  \tTraining accuracy: 0.9024955630302429\n",
      "Step: 20100  \tValid loss: 0.2379222810268402\n",
      "Step: 20200  \tTraining loss: 0.23934762179851532\n",
      "Step: 20200  \tTraining accuracy: 0.9025179743766785\n",
      "Step: 20200  \tValid loss: 0.23792202770709991\n",
      "Step: 20300  \tTraining loss: 0.2393466681241989\n",
      "Step: 20300  \tTraining accuracy: 0.9025400876998901\n",
      "Step: 20300  \tValid loss: 0.2379218339920044\n",
      "Step: 20400  \tTraining loss: 0.2393457144498825\n",
      "Step: 20400  \tTraining accuracy: 0.9025620222091675\n",
      "Step: 20400  \tValid loss: 0.2379215657711029\n",
      "Step: 20500  \tTraining loss: 0.2393447756767273\n",
      "Step: 20500  \tTraining accuracy: 0.9025837182998657\n",
      "Step: 20500  \tValid loss: 0.23792140185832977\n",
      "Step: 20600  \tTraining loss: 0.23934386670589447\n",
      "Step: 20600  \tTraining accuracy: 0.9026052355766296\n",
      "Step: 20600  \tValid loss: 0.23792125284671783\n",
      "Step: 20700  \tTraining loss: 0.23934289813041687\n",
      "Step: 20700  \tTraining accuracy: 0.9026265144348145\n",
      "Step: 20700  \tValid loss: 0.23792092502117157\n",
      "Step: 20800  \tTraining loss: 0.23934192955493927\n",
      "Step: 20800  \tTraining accuracy: 0.9026475548744202\n",
      "Step: 20800  \tValid loss: 0.23792080581188202\n",
      "Step: 20900  \tTraining loss: 0.23934094607830048\n",
      "Step: 20900  \tTraining accuracy: 0.9026684761047363\n",
      "Step: 20900  \tValid loss: 0.2379206418991089\n",
      "Step: 21000  \tTraining loss: 0.23933997750282288\n",
      "Step: 21000  \tTraining accuracy: 0.9026891589164734\n",
      "Step: 21000  \tValid loss: 0.2379205822944641\n",
      "Step: 21100  \tTraining loss: 0.23933906853199005\n",
      "Step: 21100  \tTraining accuracy: 0.9027096033096313\n",
      "Step: 21100  \tValid loss: 0.23792041838169098\n",
      "Step: 21200  \tTraining loss: 0.23933809995651245\n",
      "Step: 21200  \tTraining accuracy: 0.9027299284934998\n",
      "Step: 21200  \tValid loss: 0.23792019486427307\n",
      "Step: 21300  \tTraining loss: 0.23933714628219604\n",
      "Step: 21300  \tTraining accuracy: 0.9027500152587891\n",
      "Step: 21300  \tValid loss: 0.23792009055614471\n",
      "Step: 21400  \tTraining loss: 0.23933617770671844\n",
      "Step: 21400  \tTraining accuracy: 0.902769923210144\n",
      "Step: 21400  \tValid loss: 0.23791998624801636\n",
      "Step: 21500  \tTraining loss: 0.23933528363704681\n",
      "Step: 21500  \tTraining accuracy: 0.9027896523475647\n",
      "Step: 21500  \tValid loss: 0.23791983723640442\n",
      "Step: 21600  \tTraining loss: 0.23933424055576324\n",
      "Step: 21600  \tTraining accuracy: 0.902809202671051\n",
      "Step: 21600  \tValid loss: 0.23791968822479248\n",
      "Step: 21700  \tTraining loss: 0.23933343589305878\n",
      "Step: 21700  \tTraining accuracy: 0.902828574180603\n",
      "Step: 21700  \tValid loss: 0.23791953921318054\n",
      "Step: 21800  \tTraining loss: 0.23933227360248566\n",
      "Step: 21800  \tTraining accuracy: 0.9028477668762207\n",
      "Step: 21800  \tValid loss: 0.23791946470737457\n",
      "Step: 21900  \tTraining loss: 0.23933137953281403\n",
      "Step: 21900  \tTraining accuracy: 0.9028667211532593\n",
      "Step: 21900  \tValid loss: 0.2379194051027298\n",
      "Step: 22000  \tTraining loss: 0.23933038115501404\n",
      "Step: 22000  \tTraining accuracy: 0.9028855562210083\n",
      "Step: 22000  \tValid loss: 0.23791924118995667\n",
      "Step: 22100  \tTraining loss: 0.23932945728302002\n",
      "Step: 22100  \tTraining accuracy: 0.9029042720794678\n",
      "Step: 22100  \tValid loss: 0.23791922628879547\n",
      "Step: 22200  \tTraining loss: 0.23932842910289764\n",
      "Step: 22200  \tTraining accuracy: 0.9029227495193481\n",
      "Step: 22200  \tValid loss: 0.23791906237602234\n",
      "Step: 22300  \tTraining loss: 0.23932746052742004\n",
      "Step: 22300  \tTraining accuracy: 0.9029410481452942\n",
      "Step: 22300  \tValid loss: 0.23791900277137756\n",
      "Step: 22400  \tTraining loss: 0.23932652175426483\n",
      "Step: 22400  \tTraining accuracy: 0.9029592275619507\n",
      "Step: 22400  \tValid loss: 0.2379189133644104\n",
      "Step: 22500  \tTraining loss: 0.23932550847530365\n",
      "Step: 22500  \tTraining accuracy: 0.9029772281646729\n",
      "Step: 22500  \tValid loss: 0.23791873455047607\n",
      "Step: 22600  \tTraining loss: 0.23932455480098724\n",
      "Step: 22600  \tTraining accuracy: 0.9029951095581055\n",
      "Step: 22600  \tValid loss: 0.2379186451435089\n",
      "Step: 22700  \tTraining loss: 0.23932354152202606\n",
      "Step: 22700  \tTraining accuracy: 0.903012752532959\n",
      "Step: 22700  \tValid loss: 0.23791860044002533\n",
      "Step: 22800  \tTraining loss: 0.23932252824306488\n",
      "Step: 22800  \tTraining accuracy: 0.903030276298523\n",
      "Step: 22800  \tValid loss: 0.2379184514284134\n",
      "Step: 22900  \tTraining loss: 0.23932163417339325\n",
      "Step: 22900  \tTraining accuracy: 0.9030476808547974\n",
      "Step: 22900  \tValid loss: 0.23791833221912384\n",
      "Step: 23000  \tTraining loss: 0.23932050168514252\n",
      "Step: 23000  \tTraining accuracy: 0.9030649065971375\n",
      "Step: 23000  \tValid loss: 0.23791822791099548\n",
      "Step: 23100  \tTraining loss: 0.2393196076154709\n",
      "Step: 23100  \tTraining accuracy: 0.9030819535255432\n",
      "Step: 23100  \tValid loss: 0.23791813850402832\n",
      "Step: 23200  \tTraining loss: 0.23931851983070374\n",
      "Step: 23200  \tTraining accuracy: 0.9030989408493042\n",
      "Step: 23200  \tValid loss: 0.237917959690094\n",
      "Step: 23300  \tTraining loss: 0.23931753635406494\n",
      "Step: 23300  \tTraining accuracy: 0.9031156897544861\n",
      "Step: 23300  \tValid loss: 0.23791798949241638\n",
      "Step: 23400  \tTraining loss: 0.2393164485692978\n",
      "Step: 23400  \tTraining accuracy: 0.9031323194503784\n",
      "Step: 23400  \tValid loss: 0.23791778087615967\n",
      "Step: 23500  \tTraining loss: 0.23931550979614258\n",
      "Step: 23500  \tTraining accuracy: 0.9031488299369812\n",
      "Step: 23500  \tValid loss: 0.23791776597499847\n",
      "Step: 23600  \tTraining loss: 0.2393144965171814\n",
      "Step: 23600  \tTraining accuracy: 0.9031651616096497\n",
      "Step: 23600  \tValid loss: 0.2379176914691925\n",
      "Step: 23700  \tTraining loss: 0.2393135130405426\n",
      "Step: 23700  \tTraining accuracy: 0.9031814336776733\n",
      "Step: 23700  \tValid loss: 0.23791764676570892\n",
      "Step: 23800  \tTraining loss: 0.2393125295639038\n",
      "Step: 23800  \tTraining accuracy: 0.9031974673271179\n",
      "Step: 23800  \tValid loss: 0.23791760206222534\n",
      "Step: 23900  \tTraining loss: 0.23931145668029785\n",
      "Step: 23900  \tTraining accuracy: 0.9032134413719177\n",
      "Step: 23900  \tValid loss: 0.23791752755641937\n",
      "Step: 24000  \tTraining loss: 0.23931045830249786\n",
      "Step: 24000  \tTraining accuracy: 0.9032292366027832\n",
      "Step: 24000  \tValid loss: 0.23791749775409698\n",
      "Step: 24100  \tTraining loss: 0.23930935561656952\n",
      "Step: 24100  \tTraining accuracy: 0.9032449126243591\n",
      "Step: 24100  \tValid loss: 0.23791739344596863\n",
      "Step: 24200  \tTraining loss: 0.23930829763412476\n",
      "Step: 24200  \tTraining accuracy: 0.9032604694366455\n",
      "Step: 24200  \tValid loss: 0.2379174530506134\n",
      "Step: 24300  \tTraining loss: 0.23930735886096954\n",
      "Step: 24300  \tTraining accuracy: 0.9032759070396423\n",
      "Step: 24300  \tValid loss: 0.23791755735874176\n",
      "Step: 24400  \tTraining loss: 0.23930630087852478\n",
      "Step: 24400  \tTraining accuracy: 0.9032912254333496\n",
      "Step: 24400  \tValid loss: 0.2379174679517746\n",
      "Step: 24500  \tTraining loss: 0.2393052577972412\n",
      "Step: 24500  \tTraining accuracy: 0.9033063650131226\n",
      "Step: 24500  \tValid loss: 0.23791752755641937\n",
      "Step: 24600  \tTraining loss: 0.23930424451828003\n",
      "Step: 24600  \tTraining accuracy: 0.9033214449882507\n",
      "Step: 24600  \tValid loss: 0.23791760206222534\n",
      "Step: 24700  \tTraining loss: 0.23930314183235168\n",
      "Step: 24700  \tTraining accuracy: 0.9033363461494446\n",
      "Step: 24700  \tValid loss: 0.23791761696338654\n",
      "Step: 24800  \tTraining loss: 0.23930217325687408\n",
      "Step: 24800  \tTraining accuracy: 0.9033511877059937\n",
      "Step: 24800  \tValid loss: 0.23791766166687012\n",
      "Step: 24900  \tTraining loss: 0.2393011450767517\n",
      "Step: 24900  \tTraining accuracy: 0.9033658504486084\n",
      "Step: 24900  \tValid loss: 0.2379176765680313\n",
      "Step: 25000  \tTraining loss: 0.23929999768733978\n",
      "Step: 25000  \tTraining accuracy: 0.9033804535865784\n",
      "Step: 25000  \tValid loss: 0.23791766166687012\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.9033949\n",
      "Precision: 0.9006928\n",
      "Recall: 0.8924485\n",
      "F1 score: 0.8992413\n",
      "AUC: 0.9048781\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.903395   0.900693  0.892448  0.899241  0.904878  0.239299       0.90339   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.237917       0.903396   0.286958      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  25097.0  \n",
      "22\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_2711/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(1015, 3)\n",
      "(1015, 1)\n",
      "(560, 3)\n",
      "(560, 1)\n",
      "(455, 3)\n",
      "(455, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.634225070476532\n",
      "Step: 100  \tTraining accuracy: 0.7310344576835632\n",
      "Step: 100  \tValid loss: 0.648997962474823\n",
      "Step: 200  \tTraining loss: 0.5879334211349487\n",
      "Step: 200  \tTraining accuracy: 0.7310344576835632\n",
      "Step: 200  \tValid loss: 0.6249690651893616\n",
      "Step: 300  \tTraining loss: 0.5762500762939453\n",
      "Step: 300  \tTraining accuracy: 0.7310344576835632\n",
      "Step: 300  \tValid loss: 0.6220211982727051\n",
      "Step: 400  \tTraining loss: 0.566739559173584\n",
      "Step: 400  \tTraining accuracy: 0.7310344576835632\n",
      "Step: 400  \tValid loss: 0.6152169108390808\n",
      "Step: 500  \tTraining loss: 0.5565309524536133\n",
      "Step: 500  \tTraining accuracy: 0.7315818071365356\n",
      "Step: 500  \tValid loss: 0.6061601638793945\n",
      "Step: 600  \tTraining loss: 0.5454038381576538\n",
      "Step: 600  \tTraining accuracy: 0.7332736253738403\n",
      "Step: 600  \tValid loss: 0.5951712727546692\n",
      "Step: 700  \tTraining loss: 0.5340761542320251\n",
      "Step: 700  \tTraining accuracy: 0.7360363602638245\n",
      "Step: 700  \tValid loss: 0.583171010017395\n",
      "Step: 800  \tTraining loss: 0.5229774713516235\n",
      "Step: 800  \tTraining accuracy: 0.738916277885437\n",
      "Step: 800  \tValid loss: 0.5706174373626709\n",
      "Step: 900  \tTraining loss: 0.5124308466911316\n",
      "Step: 900  \tTraining accuracy: 0.7411184906959534\n",
      "Step: 900  \tValid loss: 0.5581849813461304\n",
      "Step: 1000  \tTraining loss: 0.5028112530708313\n",
      "Step: 1000  \tTraining accuracy: 0.7428571581840515\n",
      "Step: 1000  \tValid loss: 0.5464747548103333\n",
      "Step: 1100  \tTraining loss: 0.49450623989105225\n",
      "Step: 1100  \tTraining accuracy: 0.7442646026611328\n",
      "Step: 1100  \tValid loss: 0.5361846089363098\n",
      "Step: 1200  \tTraining loss: 0.4878174960613251\n",
      "Step: 1200  \tTraining accuracy: 0.7454273104667664\n",
      "Step: 1200  \tValid loss: 0.5278338193893433\n",
      "Step: 1300  \tTraining loss: 0.48276427388191223\n",
      "Step: 1300  \tTraining accuracy: 0.7464827299118042\n",
      "Step: 1300  \tValid loss: 0.5215442776679993\n",
      "Step: 1400  \tTraining loss: 0.47911325097084045\n",
      "Step: 1400  \tTraining accuracy: 0.7474183440208435\n",
      "Step: 1400  \tValid loss: 0.5170726180076599\n",
      "Step: 1500  \tTraining loss: 0.4765285551548004\n",
      "Step: 1500  \tTraining accuracy: 0.7481229901313782\n",
      "Step: 1500  \tValid loss: 0.5140088796615601\n",
      "Step: 1600  \tTraining loss: 0.4746984541416168\n",
      "Step: 1600  \tTraining accuracy: 0.7488955855369568\n",
      "Step: 1600  \tValid loss: 0.5119507908821106\n",
      "Step: 1700  \tTraining loss: 0.4733864665031433\n",
      "Step: 1700  \tTraining accuracy: 0.7497537136077881\n",
      "Step: 1700  \tValid loss: 0.5105816721916199\n",
      "Step: 1800  \tTraining loss: 0.4724278748035431\n",
      "Step: 1800  \tTraining accuracy: 0.7504292726516724\n",
      "Step: 1800  \tValid loss: 0.5096753835678101\n",
      "Step: 1900  \tTraining loss: 0.47171255946159363\n",
      "Step: 1900  \tTraining accuracy: 0.751164972782135\n",
      "Step: 1900  \tValid loss: 0.5090798139572144\n",
      "Step: 2000  \tTraining loss: 0.47116759419441223\n",
      "Step: 2000  \tTraining accuracy: 0.7517999410629272\n",
      "Step: 2000  \tValid loss: 0.5086925625801086\n",
      "Step: 2100  \tTraining loss: 0.4707443416118622\n",
      "Step: 2100  \tTraining accuracy: 0.7524450421333313\n",
      "Step: 2100  \tValid loss: 0.5084460377693176\n",
      "Step: 2200  \tTraining loss: 0.4704083800315857\n",
      "Step: 2200  \tTraining accuracy: 0.7529613971710205\n",
      "Step: 2200  \tValid loss: 0.5082945823669434\n",
      "Step: 2300  \tTraining loss: 0.47013628482818604\n",
      "Step: 2300  \tTraining accuracy: 0.7534318566322327\n",
      "Step: 2300  \tValid loss: 0.5082071423530579\n",
      "Step: 2400  \tTraining loss: 0.4699113368988037\n",
      "Step: 2400  \tTraining accuracy: 0.7538203597068787\n",
      "Step: 2400  \tValid loss: 0.508162796497345\n",
      "Step: 2500  \tTraining loss: 0.46972155570983887\n",
      "Step: 2500  \tTraining accuracy: 0.7541570067405701\n",
      "Step: 2500  \tValid loss: 0.5081472396850586\n",
      "Step: 2600  \tTraining loss: 0.46955764293670654\n",
      "Step: 2600  \tTraining accuracy: 0.7544673085212708\n",
      "Step: 2600  \tValid loss: 0.5081501603126526\n",
      "Step: 2700  \tTraining loss: 0.46941378712654114\n",
      "Step: 2700  \tTraining accuracy: 0.7546983957290649\n",
      "Step: 2700  \tValid loss: 0.5081650614738464\n",
      "Step: 2800  \tTraining loss: 0.4692849814891815\n",
      "Step: 2800  \tTraining accuracy: 0.7549664378166199\n",
      "Step: 2800  \tValid loss: 0.5081869959831238\n",
      "Step: 2900  \tTraining loss: 0.4691673517227173\n",
      "Step: 2900  \tTraining accuracy: 0.7552156448364258\n",
      "Step: 2900  \tValid loss: 0.5082127451896667\n",
      "Step: 3000  \tTraining loss: 0.4690588414669037\n",
      "Step: 3000  \tTraining accuracy: 0.7554479241371155\n",
      "Step: 3000  \tValid loss: 0.5082401037216187\n",
      "Step: 3100  \tTraining loss: 0.4689568877220154\n",
      "Step: 3100  \tTraining accuracy: 0.7556650042533875\n",
      "Step: 3100  \tValid loss: 0.5082677602767944\n",
      "Step: 3200  \tTraining loss: 0.46886056661605835\n",
      "Step: 3200  \tTraining accuracy: 0.7558683156967163\n",
      "Step: 3200  \tValid loss: 0.5082948207855225\n",
      "Step: 3300  \tTraining loss: 0.46876832842826843\n",
      "Step: 3300  \tTraining accuracy: 0.7560591101646423\n",
      "Step: 3300  \tValid loss: 0.508320689201355\n",
      "Step: 3400  \tTraining loss: 0.46867990493774414\n",
      "Step: 3400  \tTraining accuracy: 0.7562385201454163\n",
      "Step: 3400  \tValid loss: 0.508344829082489\n",
      "Step: 3500  \tTraining loss: 0.4685947895050049\n",
      "Step: 3500  \tTraining accuracy: 0.7564074993133545\n",
      "Step: 3500  \tValid loss: 0.5083674192428589\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.756567\n",
      "Precision: 0.7368421\n",
      "Recall: 0.7171492\n",
      "F1 score: 0.74632007\n",
      "AUC: 0.7569846\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.756567   0.736842  0.717149   0.74632  0.756985  0.468569      0.756495   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.508146       0.756604   0.427594      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3529.0  \n",
      "23\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_29431/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(841, 3)\n",
      "(841, 1)\n",
      "(464, 3)\n",
      "(464, 1)\n",
      "(377, 3)\n",
      "(377, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5807345509529114\n",
      "Step: 100  \tTraining accuracy: 0.7241379022598267\n",
      "Step: 100  \tValid loss: 0.6056783199310303\n",
      "Step: 200  \tTraining loss: 0.570778489112854\n",
      "Step: 200  \tTraining accuracy: 0.7213634848594666\n",
      "Step: 200  \tValid loss: 0.5919041037559509\n",
      "Step: 300  \tTraining loss: 0.5568526983261108\n",
      "Step: 300  \tTraining accuracy: 0.72342449426651\n",
      "Step: 300  \tValid loss: 0.5715407729148865\n",
      "Step: 400  \tTraining loss: 0.5421047210693359\n",
      "Step: 400  \tTraining accuracy: 0.7273653745651245\n",
      "Step: 400  \tValid loss: 0.549526035785675\n",
      "Step: 500  \tTraining loss: 0.5302990674972534\n",
      "Step: 500  \tTraining accuracy: 0.7310080528259277\n",
      "Step: 500  \tValid loss: 0.5320587754249573\n",
      "Step: 600  \tTraining loss: 0.5217487215995789\n",
      "Step: 600  \tTraining accuracy: 0.7339746952056885\n",
      "Step: 600  \tValid loss: 0.5203699469566345\n",
      "Step: 700  \tTraining loss: 0.5151448249816895\n",
      "Step: 700  \tTraining accuracy: 0.737492024898529\n",
      "Step: 700  \tValid loss: 0.5125700235366821\n",
      "Step: 800  \tTraining loss: 0.5089004039764404\n",
      "Step: 800  \tTraining accuracy: 0.7407054901123047\n",
      "Step: 800  \tValid loss: 0.505999743938446\n",
      "Step: 900  \tTraining loss: 0.5019040703773499\n",
      "Step: 900  \tTraining accuracy: 0.7443519830703735\n",
      "Step: 900  \tValid loss: 0.4987194538116455\n",
      "Step: 1000  \tTraining loss: 0.494430810213089\n",
      "Step: 1000  \tTraining accuracy: 0.7474810481071472\n",
      "Step: 1000  \tValid loss: 0.4905461370944977\n",
      "Step: 1100  \tTraining loss: 0.4882586896419525\n",
      "Step: 1100  \tTraining accuracy: 0.7504104971885681\n",
      "Step: 1100  \tValid loss: 0.4832719564437866\n",
      "Step: 1200  \tTraining loss: 0.4846600592136383\n",
      "Step: 1200  \tTraining accuracy: 0.7533474564552307\n",
      "Step: 1200  \tValid loss: 0.4786994457244873\n",
      "Step: 1300  \tTraining loss: 0.4829670190811157\n",
      "Step: 1300  \tTraining accuracy: 0.7563852667808533\n",
      "Step: 1300  \tValid loss: 0.4767303168773651\n",
      "Step: 1400  \tTraining loss: 0.48204436898231506\n",
      "Step: 1400  \tTraining accuracy: 0.7591051459312439\n",
      "Step: 1400  \tValid loss: 0.4762721359729767\n",
      "Step: 1500  \tTraining loss: 0.48135432600975037\n",
      "Step: 1500  \tTraining accuracy: 0.7614908218383789\n",
      "Step: 1500  \tValid loss: 0.47643402218818665\n",
      "Step: 1600  \tTraining loss: 0.4807242751121521\n",
      "Step: 1600  \tTraining accuracy: 0.7635303735733032\n",
      "Step: 1600  \tValid loss: 0.4767288863658905\n",
      "Step: 1700  \tTraining loss: 0.4800901710987091\n",
      "Step: 1700  \tTraining accuracy: 0.7653226852416992\n",
      "Step: 1700  \tValid loss: 0.4769764542579651\n",
      "Step: 1800  \tTraining loss: 0.4794244170188904\n",
      "Step: 1800  \tTraining accuracy: 0.7669101357460022\n",
      "Step: 1800  \tValid loss: 0.4771530032157898\n",
      "Step: 1900  \tTraining loss: 0.47871387004852295\n",
      "Step: 1900  \tTraining accuracy: 0.7683259844779968\n",
      "Step: 1900  \tValid loss: 0.47728651762008667\n",
      "Step: 2000  \tTraining loss: 0.4779532849788666\n",
      "Step: 2000  \tTraining accuracy: 0.7695966362953186\n",
      "Step: 2000  \tValid loss: 0.47741416096687317\n",
      "Step: 2100  \tTraining loss: 0.4771413505077362\n",
      "Step: 2100  \tTraining accuracy: 0.7707433104515076\n",
      "Step: 2100  \tValid loss: 0.4775681793689728\n",
      "Step: 2200  \tTraining loss: 0.4762791395187378\n",
      "Step: 2200  \tTraining accuracy: 0.7717832922935486\n",
      "Step: 2200  \tValid loss: 0.47777050733566284\n",
      "Step: 2300  \tTraining loss: 0.4753686785697937\n",
      "Step: 2300  \tTraining accuracy: 0.7727308869361877\n",
      "Step: 2300  \tValid loss: 0.47803443670272827\n",
      "Step: 2400  \tTraining loss: 0.4744105339050293\n",
      "Step: 2400  \tTraining accuracy: 0.773597776889801\n",
      "Step: 2400  \tValid loss: 0.47837257385253906\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7743939\n",
      "Precision: 0.8055556\n",
      "Recall: 0.95238096\n",
      "F1 score: 0.8390724\n",
      "AUC: 0.6744663\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.774394   0.805556  0.952381  0.839072  0.674466  0.474351       0.77388   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.476271       0.773707   0.488129      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  2405.0  \n",
      "24\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_26611/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(1189, 3)\n",
      "(1189, 1)\n",
      "(640, 3)\n",
      "(640, 1)\n",
      "(520, 3)\n",
      "(520, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6663133502006531\n",
      "Step: 100  \tTraining accuracy: 0.6089150309562683\n",
      "Step: 100  \tValid loss: 0.6649953722953796\n",
      "Step: 200  \tTraining loss: 0.6574113965034485\n",
      "Step: 200  \tTraining accuracy: 0.6325607895851135\n",
      "Step: 200  \tValid loss: 0.6573034524917603\n",
      "Step: 300  \tTraining loss: 0.6464498043060303\n",
      "Step: 300  \tTraining accuracy: 0.6395447850227356\n",
      "Step: 300  \tValid loss: 0.6452576518058777\n",
      "Step: 400  \tTraining loss: 0.6377673745155334\n",
      "Step: 400  \tTraining accuracy: 0.644123375415802\n",
      "Step: 400  \tValid loss: 0.6346976161003113\n",
      "Step: 500  \tTraining loss: 0.6331494450569153\n",
      "Step: 500  \tTraining accuracy: 0.6475200653076172\n",
      "Step: 500  \tValid loss: 0.628813624382019\n",
      "Step: 600  \tTraining loss: 0.6314569711685181\n",
      "Step: 600  \tTraining accuracy: 0.6496829986572266\n",
      "Step: 600  \tValid loss: 0.6263654828071594\n",
      "Step: 700  \tTraining loss: 0.6308979392051697\n",
      "Step: 700  \tTraining accuracy: 0.6517044901847839\n",
      "Step: 700  \tValid loss: 0.6256132125854492\n",
      "Step: 800  \tTraining loss: 0.630668044090271\n",
      "Step: 800  \tTraining accuracy: 0.6534709334373474\n",
      "Step: 800  \tValid loss: 0.6254029273986816\n",
      "Step: 900  \tTraining loss: 0.6305127739906311\n",
      "Step: 900  \tTraining accuracy: 0.6551724076271057\n",
      "Step: 900  \tValid loss: 0.6253095865249634\n",
      "Step: 1000  \tTraining loss: 0.6303715109825134\n",
      "Step: 1000  \tTraining accuracy: 0.6564263105392456\n",
      "Step: 1000  \tValid loss: 0.6252261996269226\n",
      "Step: 1100  \tTraining loss: 0.630224883556366\n",
      "Step: 1100  \tTraining accuracy: 0.6573604941368103\n",
      "Step: 1100  \tValid loss: 0.625128448009491\n",
      "Step: 1200  \tTraining loss: 0.6300644874572754\n",
      "Step: 1200  \tTraining accuracy: 0.6581693291664124\n",
      "Step: 1200  \tValid loss: 0.6250050067901611\n",
      "Step: 1300  \tTraining loss: 0.6298826932907104\n",
      "Step: 1300  \tTraining accuracy: 0.658848762512207\n",
      "Step: 1300  \tValid loss: 0.6248448491096497\n",
      "Step: 1400  \tTraining loss: 0.6296758651733398\n",
      "Step: 1400  \tTraining accuracy: 0.6594275832176208\n",
      "Step: 1400  \tValid loss: 0.6246962547302246\n",
      "Step: 1500  \tTraining loss: 0.6294296979904175\n",
      "Step: 1500  \tTraining accuracy: 0.6599266529083252\n",
      "Step: 1500  \tValid loss: 0.6244285702705383\n",
      "Step: 1600  \tTraining loss: 0.6291178464889526\n",
      "Step: 1600  \tTraining accuracy: 0.6603612899780273\n",
      "Step: 1600  \tValid loss: 0.6239780187606812\n",
      "Step: 1700  \tTraining loss: 0.6286418437957764\n",
      "Step: 1700  \tTraining accuracy: 0.6607432961463928\n",
      "Step: 1700  \tValid loss: 0.6232256293296814\n",
      "Step: 1800  \tTraining loss: 0.6281285881996155\n",
      "Step: 1800  \tTraining accuracy: 0.6610816717147827\n",
      "Step: 1800  \tValid loss: 0.6224182844161987\n",
      "Step: 1900  \tTraining loss: 0.627440333366394\n",
      "Step: 1900  \tTraining accuracy: 0.6613834500312805\n",
      "Step: 1900  \tValid loss: 0.6218213438987732\n",
      "Step: 2000  \tTraining loss: 0.6269739270210266\n",
      "Step: 2000  \tTraining accuracy: 0.6616542935371399\n",
      "Step: 2000  \tValid loss: 0.6212595701217651\n",
      "Step: 2100  \tTraining loss: 0.6266544461250305\n",
      "Step: 2100  \tTraining accuracy: 0.6618779897689819\n",
      "Step: 2100  \tValid loss: 0.6208620667457581\n",
      "Step: 2200  \tTraining loss: 0.6264277100563049\n",
      "Step: 2200  \tTraining accuracy: 0.662100613117218\n",
      "Step: 2200  \tValid loss: 0.62052983045578\n",
      "Step: 2300  \tTraining loss: 0.6262621283531189\n",
      "Step: 2300  \tTraining accuracy: 0.6623602509498596\n",
      "Step: 2300  \tValid loss: 0.6202350854873657\n",
      "Step: 2400  \tTraining loss: 0.6261448264122009\n",
      "Step: 2400  \tTraining accuracy: 0.6625253558158875\n",
      "Step: 2400  \tValid loss: 0.6200291514396667\n",
      "Step: 2500  \tTraining loss: 0.626068115234375\n",
      "Step: 2500  \tTraining accuracy: 0.6625727415084839\n",
      "Step: 2500  \tValid loss: 0.6198561787605286\n",
      "Step: 2600  \tTraining loss: 0.6260001063346863\n",
      "Step: 2600  \tTraining accuracy: 0.6625997424125671\n",
      "Step: 2600  \tValid loss: 0.6196908354759216\n",
      "Step: 2700  \tTraining loss: 0.6259482502937317\n",
      "Step: 2700  \tTraining accuracy: 0.6626246571540833\n",
      "Step: 2700  \tValid loss: 0.6195652484893799\n",
      "Step: 2800  \tTraining loss: 0.6258982419967651\n",
      "Step: 2800  \tTraining accuracy: 0.6626477837562561\n",
      "Step: 2800  \tValid loss: 0.6194323897361755\n",
      "Step: 2900  \tTraining loss: 0.6258484721183777\n",
      "Step: 2900  \tTraining accuracy: 0.66266930103302\n",
      "Step: 2900  \tValid loss: 0.6193021535873413\n",
      "Step: 3000  \tTraining loss: 0.6258012056350708\n",
      "Step: 3000  \tTraining accuracy: 0.6626893877983093\n",
      "Step: 3000  \tValid loss: 0.6191878318786621\n",
      "Step: 3100  \tTraining loss: 0.6257489919662476\n",
      "Step: 3100  \tTraining accuracy: 0.6627081036567688\n",
      "Step: 3100  \tValid loss: 0.6190503835678101\n",
      "Step: 3200  \tTraining loss: 0.625699520111084\n",
      "Step: 3200  \tTraining accuracy: 0.6627256274223328\n",
      "Step: 3200  \tValid loss: 0.6189081072807312\n",
      "Step: 3300  \tTraining loss: 0.6256553530693054\n",
      "Step: 3300  \tTraining accuracy: 0.6627421379089355\n",
      "Step: 3300  \tValid loss: 0.6187757253646851\n",
      "Step: 3400  \tTraining loss: 0.6256063580513\n",
      "Step: 3400  \tTraining accuracy: 0.6627575755119324\n",
      "Step: 3400  \tValid loss: 0.6186364889144897\n",
      "Step: 3500  \tTraining loss: 0.625562310218811\n",
      "Step: 3500  \tTraining accuracy: 0.6627721786499023\n",
      "Step: 3500  \tValid loss: 0.6184693574905396\n",
      "Step: 3600  \tTraining loss: 0.6255216002464294\n",
      "Step: 3600  \tTraining accuracy: 0.6627859473228455\n",
      "Step: 3600  \tValid loss: 0.6183153390884399\n",
      "Step: 3700  \tTraining loss: 0.6254825592041016\n",
      "Step: 3700  \tTraining accuracy: 0.6627989411354065\n",
      "Step: 3700  \tValid loss: 0.6181799173355103\n",
      "Step: 3800  \tTraining loss: 0.6254473924636841\n",
      "Step: 3800  \tTraining accuracy: 0.662811279296875\n",
      "Step: 3800  \tValid loss: 0.6180466413497925\n",
      "Step: 3900  \tTraining loss: 0.6254125237464905\n",
      "Step: 3900  \tTraining accuracy: 0.662822961807251\n",
      "Step: 3900  \tValid loss: 0.6178992390632629\n",
      "Step: 4000  \tTraining loss: 0.6253817677497864\n",
      "Step: 4000  \tTraining accuracy: 0.6628340482711792\n",
      "Step: 4000  \tValid loss: 0.6177815198898315\n",
      "Step: 4100  \tTraining loss: 0.6253504157066345\n",
      "Step: 4100  \tTraining accuracy: 0.6628445982933044\n",
      "Step: 4100  \tValid loss: 0.617652952671051\n",
      "Step: 4200  \tTraining loss: 0.6253224611282349\n",
      "Step: 4200  \tTraining accuracy: 0.6628546118736267\n",
      "Step: 4200  \tValid loss: 0.6175562143325806\n",
      "Step: 4300  \tTraining loss: 0.6252985000610352\n",
      "Step: 4300  \tTraining accuracy: 0.6628642082214355\n",
      "Step: 4300  \tValid loss: 0.6174575090408325\n",
      "Step: 4400  \tTraining loss: 0.6252714991569519\n",
      "Step: 4400  \tTraining accuracy: 0.6628537178039551\n",
      "Step: 4400  \tValid loss: 0.6173831820487976\n",
      "Step: 4500  \tTraining loss: 0.6252498030662537\n",
      "Step: 4500  \tTraining accuracy: 0.6628341674804688\n",
      "Step: 4500  \tValid loss: 0.617316722869873\n",
      "Step: 4600  \tTraining loss: 0.6252259016036987\n",
      "Step: 4600  \tTraining accuracy: 0.662815511226654\n",
      "Step: 4600  \tValid loss: 0.6171969175338745\n",
      "Step: 4700  \tTraining loss: 0.6252023577690125\n",
      "Step: 4700  \tTraining accuracy: 0.6627976298332214\n",
      "Step: 4700  \tValid loss: 0.6170886754989624\n",
      "Step: 4800  \tTraining loss: 0.6251871585845947\n",
      "Step: 4800  \tTraining accuracy: 0.6627804636955261\n",
      "Step: 4800  \tValid loss: 0.6171401739120483\n",
      "Step: 4900  \tTraining loss: 0.6251609921455383\n",
      "Step: 4900  \tTraining accuracy: 0.6627640724182129\n",
      "Step: 4900  \tValid loss: 0.6170400381088257\n",
      "Step: 5000  \tTraining loss: 0.6251319050788879\n",
      "Step: 5000  \tTraining accuracy: 0.6627482771873474\n",
      "Step: 5000  \tValid loss: 0.6170431971549988\n",
      "Step: 5100  \tTraining loss: 0.6251081824302673\n",
      "Step: 5100  \tTraining accuracy: 0.6627331376075745\n",
      "Step: 5100  \tValid loss: 0.6169851422309875\n",
      "Step: 5200  \tTraining loss: 0.6250813007354736\n",
      "Step: 5200  \tTraining accuracy: 0.6627185940742493\n",
      "Step: 5200  \tValid loss: 0.6169124245643616\n",
      "Step: 5300  \tTraining loss: 0.6250550150871277\n",
      "Step: 5300  \tTraining accuracy: 0.6627046465873718\n",
      "Step: 5300  \tValid loss: 0.6168903112411499\n",
      "Step: 5400  \tTraining loss: 0.6250318288803101\n",
      "Step: 5400  \tTraining accuracy: 0.6626911759376526\n",
      "Step: 5400  \tValid loss: 0.6168661117553711\n",
      "Step: 5500  \tTraining loss: 0.6250085830688477\n",
      "Step: 5500  \tTraining accuracy: 0.6626781821250916\n",
      "Step: 5500  \tValid loss: 0.6168567538261414\n",
      "Step: 5600  \tTraining loss: 0.6249855160713196\n",
      "Step: 5600  \tTraining accuracy: 0.6626656651496887\n",
      "Step: 5600  \tValid loss: 0.6168216466903687\n",
      "Step: 5700  \tTraining loss: 0.6249625086784363\n",
      "Step: 5700  \tTraining accuracy: 0.6626536250114441\n",
      "Step: 5700  \tValid loss: 0.6168245077133179\n",
      "Step: 5800  \tTraining loss: 0.6249390244483948\n",
      "Step: 5800  \tTraining accuracy: 0.6626419425010681\n",
      "Step: 5800  \tValid loss: 0.6167908906936646\n",
      "Step: 5900  \tTraining loss: 0.6249154806137085\n",
      "Step: 5900  \tTraining accuracy: 0.6626306772232056\n",
      "Step: 5900  \tValid loss: 0.6167968511581421\n",
      "Step: 6000  \tTraining loss: 0.6248936653137207\n",
      "Step: 6000  \tTraining accuracy: 0.6626198291778564\n",
      "Step: 6000  \tValid loss: 0.6167640089988708\n",
      "Step: 6100  \tTraining loss: 0.6248688697814941\n",
      "Step: 6100  \tTraining accuracy: 0.662609338760376\n",
      "Step: 6100  \tValid loss: 0.6167625188827515\n",
      "Step: 6200  \tTraining loss: 0.6248461604118347\n",
      "Step: 6200  \tTraining accuracy: 0.6625991463661194\n",
      "Step: 6200  \tValid loss: 0.6167382597923279\n",
      "Step: 6300  \tTraining loss: 0.6248230934143066\n",
      "Step: 6300  \tTraining accuracy: 0.6625893115997314\n",
      "Step: 6300  \tValid loss: 0.6167327761650085\n",
      "Step: 6400  \tTraining loss: 0.624797523021698\n",
      "Step: 6400  \tTraining accuracy: 0.6625797748565674\n",
      "Step: 6400  \tValid loss: 0.616716742515564\n",
      "Step: 6500  \tTraining loss: 0.6247749328613281\n",
      "Step: 6500  \tTraining accuracy: 0.6625705361366272\n",
      "Step: 6500  \tValid loss: 0.6166749000549316\n",
      "Step: 6600  \tTraining loss: 0.6247564554214478\n",
      "Step: 6600  \tTraining accuracy: 0.6625615954399109\n",
      "Step: 6600  \tValid loss: 0.6166930198669434\n",
      "Step: 6700  \tTraining loss: 0.6247404217720032\n",
      "Step: 6700  \tTraining accuracy: 0.6625528931617737\n",
      "Step: 6700  \tValid loss: 0.6166635751724243\n",
      "Step: 6800  \tTraining loss: 0.6247299909591675\n",
      "Step: 6800  \tTraining accuracy: 0.6625633835792542\n",
      "Step: 6800  \tValid loss: 0.6166812181472778\n",
      "Step: 6900  \tTraining loss: 0.6247168183326721\n",
      "Step: 6900  \tTraining accuracy: 0.6625238656997681\n",
      "Step: 6900  \tValid loss: 0.6166485548019409\n",
      "Step: 7000  \tTraining loss: 0.6247028708457947\n",
      "Step: 7000  \tTraining accuracy: 0.6624854803085327\n",
      "Step: 7000  \tValid loss: 0.6166225671768188\n",
      "Step: 7100  \tTraining loss: 0.6246930360794067\n",
      "Step: 7100  \tTraining accuracy: 0.6624300479888916\n",
      "Step: 7100  \tValid loss: 0.6165909767150879\n",
      "Step: 7200  \tTraining loss: 0.6246805787086487\n",
      "Step: 7200  \tTraining accuracy: 0.6624416708946228\n",
      "Step: 7200  \tValid loss: 0.6165952682495117\n",
      "Step: 7300  \tTraining loss: 0.6246713399887085\n",
      "Step: 7300  \tTraining accuracy: 0.6624059677124023\n",
      "Step: 7300  \tValid loss: 0.6165758371353149\n",
      "Step: 7400  \tTraining loss: 0.6246587634086609\n",
      "Step: 7400  \tTraining accuracy: 0.6623712778091431\n",
      "Step: 7400  \tValid loss: 0.6165614724159241\n",
      "Step: 7500  \tTraining loss: 0.6246463060379028\n",
      "Step: 7500  \tTraining accuracy: 0.6623375415802002\n",
      "Step: 7500  \tValid loss: 0.6165293455123901\n",
      "Step: 7600  \tTraining loss: 0.6246339678764343\n",
      "Step: 7600  \tTraining accuracy: 0.6623046398162842\n",
      "Step: 7600  \tValid loss: 0.6165183186531067\n",
      "Step: 7700  \tTraining loss: 0.6246244311332703\n",
      "Step: 7700  \tTraining accuracy: 0.6622726321220398\n",
      "Step: 7700  \tValid loss: 0.6165138483047485\n",
      "Step: 7800  \tTraining loss: 0.6246112585067749\n",
      "Step: 7800  \tTraining accuracy: 0.6622249484062195\n",
      "Step: 7800  \tValid loss: 0.616479218006134\n",
      "Step: 7900  \tTraining loss: 0.6246044635772705\n",
      "Step: 7900  \tTraining accuracy: 0.6622219085693359\n",
      "Step: 7900  \tValid loss: 0.616482138633728\n",
      "Step: 8000  \tTraining loss: 0.624593198299408\n",
      "Step: 8000  \tTraining accuracy: 0.6621921062469482\n",
      "Step: 8000  \tValid loss: 0.6164911985397339\n",
      "Step: 8100  \tTraining loss: 0.624578058719635\n",
      "Step: 8100  \tTraining accuracy: 0.6621630787849426\n",
      "Step: 8100  \tValid loss: 0.6164411902427673\n",
      "Step: 8200  \tTraining loss: 0.6245691180229187\n",
      "Step: 8200  \tTraining accuracy: 0.6621347665786743\n",
      "Step: 8200  \tValid loss: 0.6164275407791138\n",
      "Step: 8300  \tTraining loss: 0.6245620250701904\n",
      "Step: 8300  \tTraining accuracy: 0.6621071696281433\n",
      "Step: 8300  \tValid loss: 0.6164330244064331\n",
      "Step: 8400  \tTraining loss: 0.6245495676994324\n",
      "Step: 8400  \tTraining accuracy: 0.6620801687240601\n",
      "Step: 8400  \tValid loss: 0.6164297461509705\n",
      "Step: 8500  \tTraining loss: 0.6245408058166504\n",
      "Step: 8500  \tTraining accuracy: 0.6620538234710693\n",
      "Step: 8500  \tValid loss: 0.6164067387580872\n",
      "Step: 8600  \tTraining loss: 0.6245311498641968\n",
      "Step: 8600  \tTraining accuracy: 0.6620281338691711\n",
      "Step: 8600  \tValid loss: 0.6163860559463501\n",
      "Step: 8700  \tTraining loss: 0.624527096748352\n",
      "Step: 8700  \tTraining accuracy: 0.6620030403137207\n",
      "Step: 8700  \tValid loss: 0.616371750831604\n",
      "Step: 8800  \tTraining loss: 0.6245167255401611\n",
      "Step: 8800  \tTraining accuracy: 0.6619784832000732\n",
      "Step: 8800  \tValid loss: 0.6163921356201172\n",
      "Step: 8900  \tTraining loss: 0.6245104670524597\n",
      "Step: 8900  \tTraining accuracy: 0.6619544625282288\n",
      "Step: 8900  \tValid loss: 0.6163956522941589\n",
      "Step: 9000  \tTraining loss: 0.6244987845420837\n",
      "Step: 9000  \tTraining accuracy: 0.661931037902832\n",
      "Step: 9000  \tValid loss: 0.6163891553878784\n",
      "Step: 9100  \tTraining loss: 0.6244927644729614\n",
      "Step: 9100  \tTraining accuracy: 0.6619080901145935\n",
      "Step: 9100  \tValid loss: 0.6163855791091919\n",
      "Step: 9200  \tTraining loss: 0.6244819164276123\n",
      "Step: 9200  \tTraining accuracy: 0.661885678768158\n",
      "Step: 9200  \tValid loss: 0.6163617372512817\n",
      "Step: 9300  \tTraining loss: 0.6244735717773438\n",
      "Step: 9300  \tTraining accuracy: 0.6618637442588806\n",
      "Step: 9300  \tValid loss: 0.6163684129714966\n",
      "Step: 9400  \tTraining loss: 0.6244677901268005\n",
      "Step: 9400  \tTraining accuracy: 0.6618422269821167\n",
      "Step: 9400  \tValid loss: 0.6163519620895386\n",
      "Step: 9500  \tTraining loss: 0.6244661211967468\n",
      "Step: 9500  \tTraining accuracy: 0.661821186542511\n",
      "Step: 9500  \tValid loss: 0.6164069175720215\n",
      "Step: 9600  \tTraining loss: 0.6244494915008545\n",
      "Step: 9600  \tTraining accuracy: 0.6618006229400635\n",
      "Step: 9600  \tValid loss: 0.6163579225540161\n",
      "Step: 9700  \tTraining loss: 0.6244419813156128\n",
      "Step: 9700  \tTraining accuracy: 0.6617804765701294\n",
      "Step: 9700  \tValid loss: 0.6163433790206909\n",
      "Step: 9800  \tTraining loss: 0.624433696269989\n",
      "Step: 9800  \tTraining accuracy: 0.6617607474327087\n",
      "Step: 9800  \tValid loss: 0.6163493394851685\n",
      "Step: 9900  \tTraining loss: 0.6244338154792786\n",
      "Step: 9900  \tTraining accuracy: 0.6617457270622253\n",
      "Step: 9900  \tValid loss: 0.6163777112960815\n",
      "Step: 10000  \tTraining loss: 0.6244192719459534\n",
      "Step: 10000  \tTraining accuracy: 0.6617224216461182\n",
      "Step: 10000  \tValid loss: 0.6163430213928223\n",
      "Step: 10100  \tTraining loss: 0.6244097948074341\n",
      "Step: 10100  \tTraining accuracy: 0.6616995930671692\n",
      "Step: 10100  \tValid loss: 0.6163163185119629\n",
      "Step: 10200  \tTraining loss: 0.6244030594825745\n",
      "Step: 10200  \tTraining accuracy: 0.6616772413253784\n",
      "Step: 10200  \tValid loss: 0.6163181066513062\n",
      "Step: 10300  \tTraining loss: 0.624396026134491\n",
      "Step: 10300  \tTraining accuracy: 0.6616553068161011\n",
      "Step: 10300  \tValid loss: 0.6163395047187805\n",
      "Step: 10400  \tTraining loss: 0.624389111995697\n",
      "Step: 10400  \tTraining accuracy: 0.6616337895393372\n",
      "Step: 10400  \tValid loss: 0.6163322925567627\n",
      "Step: 10500  \tTraining loss: 0.6243894696235657\n",
      "Step: 10500  \tTraining accuracy: 0.6616127490997314\n",
      "Step: 10500  \tValid loss: 0.616365909576416\n",
      "Step: 10600  \tTraining loss: 0.624371349811554\n",
      "Step: 10600  \tTraining accuracy: 0.6615920066833496\n",
      "Step: 10600  \tValid loss: 0.6163118481636047\n",
      "Step: 10700  \tTraining loss: 0.6243664026260376\n",
      "Step: 10700  \tTraining accuracy: 0.661571741104126\n",
      "Step: 10700  \tValid loss: 0.6163470149040222\n",
      "Step: 10800  \tTraining loss: 0.6243565678596497\n",
      "Step: 10800  \tTraining accuracy: 0.6615517735481262\n",
      "Step: 10800  \tValid loss: 0.6163390874862671\n",
      "Step: 10900  \tTraining loss: 0.6243501901626587\n",
      "Step: 10900  \tTraining accuracy: 0.6615322232246399\n",
      "Step: 10900  \tValid loss: 0.6163436770439148\n",
      "Step: 11000  \tTraining loss: 0.6243435144424438\n",
      "Step: 11000  \tTraining accuracy: 0.6615130305290222\n",
      "Step: 11000  \tValid loss: 0.6163283586502075\n",
      "Step: 11100  \tTraining loss: 0.6243337392807007\n",
      "Step: 11100  \tTraining accuracy: 0.6614941954612732\n",
      "Step: 11100  \tValid loss: 0.6163190603256226\n",
      "Step: 11200  \tTraining loss: 0.6243273615837097\n",
      "Step: 11200  \tTraining accuracy: 0.661475658416748\n",
      "Step: 11200  \tValid loss: 0.6163476705551147\n",
      "Step: 11300  \tTraining loss: 0.6243252158164978\n",
      "Step: 11300  \tTraining accuracy: 0.6614574790000916\n",
      "Step: 11300  \tValid loss: 0.6163443326950073\n",
      "Step: 11400  \tTraining loss: 0.6243115663528442\n",
      "Step: 11400  \tTraining accuracy: 0.6614395976066589\n",
      "Step: 11400  \tValid loss: 0.6163209080696106\n",
      "Step: 11500  \tTraining loss: 0.6243074536323547\n",
      "Step: 11500  \tTraining accuracy: 0.661422073841095\n",
      "Step: 11500  \tValid loss: 0.6163002252578735\n",
      "Step: 11600  \tTraining loss: 0.6242964863777161\n",
      "Step: 11600  \tTraining accuracy: 0.6614047884941101\n",
      "Step: 11600  \tValid loss: 0.6163333058357239\n",
      "Step: 11700  \tTraining loss: 0.6242904663085938\n",
      "Step: 11700  \tTraining accuracy: 0.6613878607749939\n",
      "Step: 11700  \tValid loss: 0.6163226962089539\n",
      "Step: 11800  \tTraining loss: 0.6242819428443909\n",
      "Step: 11800  \tTraining accuracy: 0.6613711714744568\n",
      "Step: 11800  \tValid loss: 0.6163085103034973\n",
      "Step: 11900  \tTraining loss: 0.6242761015892029\n",
      "Step: 11900  \tTraining accuracy: 0.6613548398017883\n",
      "Step: 11900  \tValid loss: 0.6163225173950195\n",
      "Step: 12000  \tTraining loss: 0.6242710947990417\n",
      "Step: 12000  \tTraining accuracy: 0.6613386869430542\n",
      "Step: 12000  \tValid loss: 0.6163319945335388\n",
      "Step: 12100  \tTraining loss: 0.6242622137069702\n",
      "Step: 12100  \tTraining accuracy: 0.661322832107544\n",
      "Step: 12100  \tValid loss: 0.6163007616996765\n",
      "Step: 12200  \tTraining loss: 0.6242543458938599\n",
      "Step: 12200  \tTraining accuracy: 0.6613072752952576\n",
      "Step: 12200  \tValid loss: 0.6163188219070435\n",
      "Step: 12300  \tTraining loss: 0.6242454051971436\n",
      "Step: 12300  \tTraining accuracy: 0.6612919569015503\n",
      "Step: 12300  \tValid loss: 0.6163302659988403\n",
      "Step: 12400  \tTraining loss: 0.6242392659187317\n",
      "Step: 12400  \tTraining accuracy: 0.6612768769264221\n",
      "Step: 12400  \tValid loss: 0.6162988543510437\n",
      "Step: 12500  \tTraining loss: 0.6242341995239258\n",
      "Step: 12500  \tTraining accuracy: 0.661262035369873\n",
      "Step: 12500  \tValid loss: 0.6163147687911987\n",
      "Step: 12600  \tTraining loss: 0.6242266893386841\n",
      "Step: 12600  \tTraining accuracy: 0.6612474322319031\n",
      "Step: 12600  \tValid loss: 0.6162964105606079\n",
      "Step: 12700  \tTraining loss: 0.6242155432701111\n",
      "Step: 12700  \tTraining accuracy: 0.6612330675125122\n",
      "Step: 12700  \tValid loss: 0.6163045167922974\n",
      "Step: 12800  \tTraining loss: 0.624210774898529\n",
      "Step: 12800  \tTraining accuracy: 0.6612189412117004\n",
      "Step: 12800  \tValid loss: 0.6163044571876526\n",
      "Step: 12900  \tTraining loss: 0.6242052316665649\n",
      "Step: 12900  \tTraining accuracy: 0.661204993724823\n",
      "Step: 12900  \tValid loss: 0.6163234710693359\n",
      "Step: 13000  \tTraining loss: 0.6242014169692993\n",
      "Step: 13000  \tTraining accuracy: 0.6611912846565247\n",
      "Step: 13000  \tValid loss: 0.6163071393966675\n",
      "Step: 13100  \tTraining loss: 0.6241897344589233\n",
      "Step: 13100  \tTraining accuracy: 0.6611777544021606\n",
      "Step: 13100  \tValid loss: 0.6163098216056824\n",
      "Step: 13200  \tTraining loss: 0.6241830587387085\n",
      "Step: 13200  \tTraining accuracy: 0.6611644625663757\n",
      "Step: 13200  \tValid loss: 0.6162714958190918\n",
      "Step: 13300  \tTraining loss: 0.6241734623908997\n",
      "Step: 13300  \tTraining accuracy: 0.6611514091491699\n",
      "Step: 13300  \tValid loss: 0.61628258228302\n",
      "Step: 13400  \tTraining loss: 0.6241680979728699\n",
      "Step: 13400  \tTraining accuracy: 0.6611384749412537\n",
      "Step: 13400  \tValid loss: 0.6163002848625183\n",
      "Step: 13500  \tTraining loss: 0.6241615414619446\n",
      "Step: 13500  \tTraining accuracy: 0.6611257791519165\n",
      "Step: 13500  \tValid loss: 0.6163042783737183\n",
      "Step: 13600  \tTraining loss: 0.6241557598114014\n",
      "Step: 13600  \tTraining accuracy: 0.6611132621765137\n",
      "Step: 13600  \tValid loss: 0.6162862181663513\n",
      "Step: 13700  \tTraining loss: 0.6241496801376343\n",
      "Step: 13700  \tTraining accuracy: 0.6611009240150452\n",
      "Step: 13700  \tValid loss: 0.6163036227226257\n",
      "Step: 13800  \tTraining loss: 0.6241396069526672\n",
      "Step: 13800  \tTraining accuracy: 0.661088764667511\n",
      "Step: 13800  \tValid loss: 0.6162797212600708\n",
      "Step: 13900  \tTraining loss: 0.6241365671157837\n",
      "Step: 13900  \tTraining accuracy: 0.6610767841339111\n",
      "Step: 13900  \tValid loss: 0.6162947416305542\n",
      "Step: 14000  \tTraining loss: 0.6241273283958435\n",
      "Step: 14000  \tTraining accuracy: 0.6610649824142456\n",
      "Step: 14000  \tValid loss: 0.616277813911438\n",
      "Step: 14100  \tTraining loss: 0.6241198182106018\n",
      "Step: 14100  \tTraining accuracy: 0.6610533595085144\n",
      "Step: 14100  \tValid loss: 0.6162881255149841\n",
      "Step: 14200  \tTraining loss: 0.6241152286529541\n",
      "Step: 14200  \tTraining accuracy: 0.6610418558120728\n",
      "Step: 14200  \tValid loss: 0.6162782907485962\n",
      "Step: 14300  \tTraining loss: 0.6241074800491333\n",
      "Step: 14300  \tTraining accuracy: 0.6610305905342102\n",
      "Step: 14300  \tValid loss: 0.6162948608398438\n",
      "Step: 14400  \tTraining loss: 0.6241016983985901\n",
      "Step: 14400  \tTraining accuracy: 0.6610193848609924\n",
      "Step: 14400  \tValid loss: 0.6162819266319275\n",
      "Step: 14500  \tTraining loss: 0.6240946650505066\n",
      "Step: 14500  \tTraining accuracy: 0.6610084176063538\n",
      "Step: 14500  \tValid loss: 0.6162737607955933\n",
      "Step: 14600  \tTraining loss: 0.6240884065628052\n",
      "Step: 14600  \tTraining accuracy: 0.6609975695610046\n",
      "Step: 14600  \tValid loss: 0.6162762641906738\n",
      "Step: 14700  \tTraining loss: 0.6240828037261963\n",
      "Step: 14700  \tTraining accuracy: 0.6609868407249451\n",
      "Step: 14700  \tValid loss: 0.6162775754928589\n",
      "Step: 14800  \tTraining loss: 0.6240732073783875\n",
      "Step: 14800  \tTraining accuracy: 0.6609762907028198\n",
      "Step: 14800  \tValid loss: 0.6162639260292053\n",
      "Step: 14900  \tTraining loss: 0.6240724325180054\n",
      "Step: 14900  \tTraining accuracy: 0.6609658598899841\n",
      "Step: 14900  \tValid loss: 0.6162623167037964\n",
      "Step: 15000  \tTraining loss: 0.6240627765655518\n",
      "Step: 15000  \tTraining accuracy: 0.6609556078910828\n",
      "Step: 15000  \tValid loss: 0.6162697076797485\n",
      "Step: 15100  \tTraining loss: 0.6240530610084534\n",
      "Step: 15100  \tTraining accuracy: 0.660945475101471\n",
      "Step: 15100  \tValid loss: 0.6162643432617188\n",
      "Step: 15200  \tTraining loss: 0.6240488886833191\n",
      "Step: 15200  \tTraining accuracy: 0.6609354615211487\n",
      "Step: 15200  \tValid loss: 0.616280734539032\n",
      "Step: 15300  \tTraining loss: 0.624043345451355\n",
      "Step: 15300  \tTraining accuracy: 0.660925567150116\n",
      "Step: 15300  \tValid loss: 0.6162598729133606\n",
      "Step: 15400  \tTraining loss: 0.6240362524986267\n",
      "Step: 15400  \tTraining accuracy: 0.6609158515930176\n",
      "Step: 15400  \tValid loss: 0.6162614226341248\n",
      "Step: 15500  \tTraining loss: 0.6240350604057312\n",
      "Step: 15500  \tTraining accuracy: 0.660906195640564\n",
      "Step: 15500  \tValid loss: 0.6162797212600708\n",
      "Step: 15600  \tTraining loss: 0.624023973941803\n",
      "Step: 15600  \tTraining accuracy: 0.6608967185020447\n",
      "Step: 15600  \tValid loss: 0.6162484884262085\n",
      "Step: 15700  \tTraining loss: 0.6240167021751404\n",
      "Step: 15700  \tTraining accuracy: 0.6608873605728149\n",
      "Step: 15700  \tValid loss: 0.6162434816360474\n",
      "Step: 15800  \tTraining loss: 0.6240078210830688\n",
      "Step: 15800  \tTraining accuracy: 0.6608781218528748\n",
      "Step: 15800  \tValid loss: 0.6162359118461609\n",
      "Step: 15900  \tTraining loss: 0.6240030527114868\n",
      "Step: 15900  \tTraining accuracy: 0.6608689427375793\n",
      "Step: 15900  \tValid loss: 0.6162501573562622\n",
      "Step: 16000  \tTraining loss: 0.623995304107666\n",
      "Step: 16000  \tTraining accuracy: 0.6608599424362183\n",
      "Step: 16000  \tValid loss: 0.6162415742874146\n",
      "Step: 16100  \tTraining loss: 0.6239885091781616\n",
      "Step: 16100  \tTraining accuracy: 0.660851001739502\n",
      "Step: 16100  \tValid loss: 0.6162436008453369\n",
      "Step: 16200  \tTraining loss: 0.6239858865737915\n",
      "Step: 16200  \tTraining accuracy: 0.66084223985672\n",
      "Step: 16200  \tValid loss: 0.6162450909614563\n",
      "Step: 16300  \tTraining loss: 0.6239774227142334\n",
      "Step: 16300  \tTraining accuracy: 0.6608335375785828\n",
      "Step: 16300  \tValid loss: 0.6162585020065308\n",
      "Step: 16400  \tTraining loss: 0.6239707469940186\n",
      "Step: 16400  \tTraining accuracy: 0.6608249545097351\n",
      "Step: 16400  \tValid loss: 0.6162444949150085\n",
      "Step: 16500  \tTraining loss: 0.6239643096923828\n",
      "Step: 16500  \tTraining accuracy: 0.6608164310455322\n",
      "Step: 16500  \tValid loss: 0.6162624955177307\n",
      "Step: 16600  \tTraining loss: 0.6239599585533142\n",
      "Step: 16600  \tTraining accuracy: 0.6608080863952637\n",
      "Step: 16600  \tValid loss: 0.6162480115890503\n",
      "Step: 16700  \tTraining loss: 0.623950719833374\n",
      "Step: 16700  \tTraining accuracy: 0.6607998013496399\n",
      "Step: 16700  \tValid loss: 0.6162309646606445\n",
      "Step: 16800  \tTraining loss: 0.6239463090896606\n",
      "Step: 16800  \tTraining accuracy: 0.6607916355133057\n",
      "Step: 16800  \tValid loss: 0.6162511110305786\n",
      "Step: 16900  \tTraining loss: 0.6239393949508667\n",
      "Step: 16900  \tTraining accuracy: 0.6607835292816162\n",
      "Step: 16900  \tValid loss: 0.6162516474723816\n",
      "Step: 17000  \tTraining loss: 0.6239376664161682\n",
      "Step: 17000  \tTraining accuracy: 0.6607755422592163\n",
      "Step: 17000  \tValid loss: 0.6162682771682739\n",
      "Step: 17100  \tTraining loss: 0.6239270567893982\n",
      "Step: 17100  \tTraining accuracy: 0.660767674446106\n",
      "Step: 17100  \tValid loss: 0.6162341833114624\n",
      "Step: 17200  \tTraining loss: 0.6239219903945923\n",
      "Step: 17200  \tTraining accuracy: 0.6607598662376404\n",
      "Step: 17200  \tValid loss: 0.6162446737289429\n",
      "Step: 17300  \tTraining loss: 0.6239157319068909\n",
      "Step: 17300  \tTraining accuracy: 0.6607521176338196\n",
      "Step: 17300  \tValid loss: 0.6162289381027222\n",
      "Step: 17400  \tTraining loss: 0.6239108443260193\n",
      "Step: 17400  \tTraining accuracy: 0.6607444882392883\n",
      "Step: 17400  \tValid loss: 0.6162439584732056\n",
      "Step: 17500  \tTraining loss: 0.6239026188850403\n",
      "Step: 17500  \tTraining accuracy: 0.6607369780540466\n",
      "Step: 17500  \tValid loss: 0.6162435412406921\n",
      "Step: 17600  \tTraining loss: 0.6238930225372314\n",
      "Step: 17600  \tTraining accuracy: 0.6607295274734497\n",
      "Step: 17600  \tValid loss: 0.6162410974502563\n",
      "Step: 17700  \tTraining loss: 0.6238877177238464\n",
      "Step: 17700  \tTraining accuracy: 0.6607221961021423\n",
      "Step: 17700  \tValid loss: 0.6162473559379578\n",
      "Step: 17800  \tTraining loss: 0.6238818168640137\n",
      "Step: 17800  \tTraining accuracy: 0.660714864730835\n",
      "Step: 17800  \tValid loss: 0.6162318587303162\n",
      "Step: 17900  \tTraining loss: 0.6238790154457092\n",
      "Step: 17900  \tTraining accuracy: 0.6607077121734619\n",
      "Step: 17900  \tValid loss: 0.6162353157997131\n",
      "Step: 18000  \tTraining loss: 0.6238686442375183\n",
      "Step: 18000  \tTraining accuracy: 0.6607005596160889\n",
      "Step: 18000  \tValid loss: 0.6162334084510803\n",
      "Step: 18100  \tTraining loss: 0.6238657832145691\n",
      "Step: 18100  \tTraining accuracy: 0.6606935262680054\n",
      "Step: 18100  \tValid loss: 0.6162394285202026\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.66068655\n",
      "Precision: 0.664488\n",
      "Recall: 0.84254146\n",
      "F1 score: 0.73874575\n",
      "AUC: 0.5900879\n",
      "   accuracy  precision    recall  f1_score       auc     loss  accuracy_val  \\\n",
      "0  0.660687   0.664488  0.842541  0.738746  0.590088  0.62386       0.66073   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.616205       0.660737   0.597584      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  18197.0  \n",
      "25\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_62753/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(1421, 3)\n",
      "(1421, 1)\n",
      "(768, 3)\n",
      "(768, 1)\n",
      "(624, 3)\n",
      "(624, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6692529320716858\n",
      "Step: 100  \tTraining accuracy: 0.6038001179695129\n",
      "Step: 100  \tValid loss: 0.6667056083679199\n",
      "Step: 200  \tTraining loss: 0.6279618144035339\n",
      "Step: 200  \tTraining accuracy: 0.6341521143913269\n",
      "Step: 200  \tValid loss: 0.6195464134216309\n",
      "Step: 300  \tTraining loss: 0.5994198322296143\n",
      "Step: 300  \tTraining accuracy: 0.6577267050743103\n",
      "Step: 300  \tValid loss: 0.5904949903488159\n",
      "Step: 400  \tTraining loss: 0.5887945294380188\n",
      "Step: 400  \tTraining accuracy: 0.6680527329444885\n",
      "Step: 400  \tValid loss: 0.5823854804039001\n",
      "Step: 500  \tTraining loss: 0.5843077898025513\n",
      "Step: 500  \tTraining accuracy: 0.6752939224243164\n",
      "Step: 500  \tValid loss: 0.5809277296066284\n",
      "Step: 600  \tTraining loss: 0.5821376442909241\n",
      "Step: 600  \tTraining accuracy: 0.6804210543632507\n",
      "Step: 600  \tValid loss: 0.5813007950782776\n",
      "Step: 700  \tTraining loss: 0.5811437964439392\n",
      "Step: 700  \tTraining accuracy: 0.6840264201164246\n",
      "Step: 700  \tValid loss: 0.5821642875671387\n",
      "Step: 800  \tTraining loss: 0.580695629119873\n",
      "Step: 800  \tTraining accuracy: 0.6865289807319641\n",
      "Step: 800  \tValid loss: 0.5829678177833557\n",
      "Step: 900  \tTraining loss: 0.5804684162139893\n",
      "Step: 900  \tTraining accuracy: 0.6886102557182312\n",
      "Step: 900  \tValid loss: 0.5835393071174622\n",
      "Step: 1000  \tTraining loss: 0.5803197026252747\n",
      "Step: 1000  \tTraining accuracy: 0.6901787519454956\n",
      "Step: 1000  \tValid loss: 0.5838727951049805\n",
      "Step: 1100  \tTraining loss: 0.580190896987915\n",
      "Step: 1100  \tTraining accuracy: 0.6914486885070801\n",
      "Step: 1100  \tValid loss: 0.5840441584587097\n",
      "Step: 1200  \tTraining loss: 0.5800598859786987\n",
      "Step: 1200  \tTraining accuracy: 0.6925596594810486\n",
      "Step: 1200  \tValid loss: 0.5840960144996643\n",
      "Step: 1300  \tTraining loss: 0.5799168348312378\n",
      "Step: 1300  \tTraining accuracy: 0.6933223605155945\n",
      "Step: 1300  \tValid loss: 0.584082841873169\n",
      "Step: 1400  \tTraining loss: 0.5797596573829651\n",
      "Step: 1400  \tTraining accuracy: 0.693972110748291\n",
      "Step: 1400  \tValid loss: 0.5840408205986023\n",
      "Step: 1500  \tTraining loss: 0.5795928239822388\n",
      "Step: 1500  \tTraining accuracy: 0.6945077776908875\n",
      "Step: 1500  \tValid loss: 0.583987295627594\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.6949743\n",
      "Precision: 0.69585985\n",
      "Recall: 0.6551724\n",
      "F1 score: 0.6740446\n",
      "AUC: 0.70092833\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.694974    0.69586  0.655172  0.674045  0.700928  0.579571      0.694679   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.580913       0.694161   0.614977      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  1512.0  \n",
      "26\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_76337/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(870, 3)\n",
      "(870, 1)\n",
      "(464, 3)\n",
      "(464, 1)\n",
      "(377, 3)\n",
      "(377, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6702373623847961\n",
      "Step: 100  \tTraining accuracy: 0.5701149702072144\n",
      "Step: 100  \tValid loss: 0.6699866056442261\n",
      "Step: 200  \tTraining loss: 0.6331918835639954\n",
      "Step: 200  \tTraining accuracy: 0.5943433046340942\n",
      "Step: 200  \tValid loss: 0.6286188960075378\n",
      "Step: 300  \tTraining loss: 0.5876951813697815\n",
      "Step: 300  \tTraining accuracy: 0.6246505379676819\n",
      "Step: 300  \tValid loss: 0.5766358375549316\n",
      "Step: 400  \tTraining loss: 0.5714324712753296\n",
      "Step: 400  \tTraining accuracy: 0.6453440189361572\n",
      "Step: 400  \tValid loss: 0.5578733682632446\n",
      "Step: 500  \tTraining loss: 0.5643590688705444\n",
      "Step: 500  \tTraining accuracy: 0.6598392724990845\n",
      "Step: 500  \tValid loss: 0.5483345985412598\n",
      "Step: 600  \tTraining loss: 0.5600984692573547\n",
      "Step: 600  \tTraining accuracy: 0.6700265407562256\n",
      "Step: 600  \tValid loss: 0.542121171951294\n",
      "Step: 700  \tTraining loss: 0.5573581457138062\n",
      "Step: 700  \tTraining accuracy: 0.6770833134651184\n",
      "Step: 700  \tValid loss: 0.5385563969612122\n",
      "Step: 800  \tTraining loss: 0.5554580092430115\n",
      "Step: 800  \tTraining accuracy: 0.6831166744232178\n",
      "Step: 800  \tValid loss: 0.5366230010986328\n",
      "Step: 900  \tTraining loss: 0.5540863275527954\n",
      "Step: 900  \tTraining accuracy: 0.6877318024635315\n",
      "Step: 900  \tValid loss: 0.5356488227844238\n",
      "Step: 1000  \tTraining loss: 0.5530602931976318\n",
      "Step: 1000  \tTraining accuracy: 0.6910688877105713\n",
      "Step: 1000  \tValid loss: 0.5352297425270081\n",
      "Step: 1100  \tTraining loss: 0.5521839261054993\n",
      "Step: 1100  \tTraining accuracy: 0.6943826675415039\n",
      "Step: 1100  \tValid loss: 0.535134494304657\n",
      "Step: 1200  \tTraining loss: 0.5514108538627625\n",
      "Step: 1200  \tTraining accuracy: 0.696561872959137\n",
      "Step: 1200  \tValid loss: 0.5350798964500427\n",
      "Step: 1300  \tTraining loss: 0.5506493449211121\n",
      "Step: 1300  \tTraining accuracy: 0.6983926892280579\n",
      "Step: 1300  \tValid loss: 0.5349932312965393\n",
      "Step: 1400  \tTraining loss: 0.5498536825180054\n",
      "Step: 1400  \tTraining accuracy: 0.6999524235725403\n",
      "Step: 1400  \tValid loss: 0.5348896384239197\n",
      "Step: 1500  \tTraining loss: 0.5472854375839233\n",
      "Step: 1500  \tTraining accuracy: 0.701901376247406\n",
      "Step: 1500  \tValid loss: 0.5356676578521729\n",
      "Step: 1600  \tTraining loss: 0.5463424324989319\n",
      "Step: 1600  \tTraining accuracy: 0.7035990357398987\n",
      "Step: 1600  \tValid loss: 0.5358321666717529\n",
      "Step: 1700  \tTraining loss: 0.5456415414810181\n",
      "Step: 1700  \tTraining accuracy: 0.7048077583312988\n",
      "Step: 1700  \tValid loss: 0.5360737442970276\n",
      "Step: 1800  \tTraining loss: 0.5450242757797241\n",
      "Step: 1800  \tTraining accuracy: 0.705845057964325\n",
      "Step: 1800  \tValid loss: 0.5363119840621948\n",
      "Step: 1900  \tTraining loss: 0.5444660186767578\n",
      "Step: 1900  \tTraining accuracy: 0.7067702412605286\n",
      "Step: 1900  \tValid loss: 0.5365573167800903\n",
      "Step: 2000  \tTraining loss: 0.5439584851264954\n",
      "Step: 2000  \tTraining accuracy: 0.7076005935668945\n",
      "Step: 2000  \tValid loss: 0.5368146896362305\n",
      "Step: 2100  \tTraining loss: 0.5435013175010681\n",
      "Step: 2100  \tTraining accuracy: 0.7083499431610107\n",
      "Step: 2100  \tValid loss: 0.537080705165863\n",
      "Step: 2200  \tTraining loss: 0.5430944561958313\n",
      "Step: 2200  \tTraining accuracy: 0.7090296745300293\n",
      "Step: 2200  \tValid loss: 0.5373520851135254\n",
      "Step: 2300  \tTraining loss: 0.5427290201187134\n",
      "Step: 2300  \tTraining accuracy: 0.7096489667892456\n",
      "Step: 2300  \tValid loss: 0.5376270413398743\n",
      "Step: 2400  \tTraining loss: 0.542396068572998\n",
      "Step: 2400  \tTraining accuracy: 0.7102155685424805\n",
      "Step: 2400  \tValid loss: 0.5379213690757751\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7107359\n",
      "Precision: 0.68157893\n",
      "Recall: 0.69251335\n",
      "F1 score: 0.7163916\n",
      "AUC: 0.72428083\n",
      "   accuracy  precision    recall  f1_score       auc     loss  accuracy_val  \\\n",
      "0  0.710736   0.681579  0.692513  0.716392  0.724281  0.54224      0.710355   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.534664       0.710074   0.579115      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  2450.0  \n",
      "27\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_38996/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(870, 3)\n",
      "(870, 1)\n",
      "(480, 3)\n",
      "(480, 1)\n",
      "(390, 3)\n",
      "(390, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.654058575630188\n",
      "Step: 100  \tTraining accuracy: 0.5954023003578186\n",
      "Step: 100  \tValid loss: 0.6619473695755005\n",
      "Step: 200  \tTraining loss: 0.6282951235771179\n",
      "Step: 200  \tTraining accuracy: 0.6160919666290283\n",
      "Step: 200  \tValid loss: 0.6199515461921692\n",
      "Step: 300  \tTraining loss: 0.5947872996330261\n",
      "Step: 300  \tTraining accuracy: 0.6482758522033691\n",
      "Step: 300  \tValid loss: 0.5741285681724548\n",
      "Step: 400  \tTraining loss: 0.5685598254203796\n",
      "Step: 400  \tTraining accuracy: 0.6696223020553589\n",
      "Step: 400  \tValid loss: 0.5366942286491394\n",
      "Step: 500  \tTraining loss: 0.5586387515068054\n",
      "Step: 500  \tTraining accuracy: 0.6830140352249146\n",
      "Step: 500  \tValid loss: 0.517639696598053\n",
      "Step: 600  \tTraining loss: 0.5562877058982849\n",
      "Step: 600  \tTraining accuracy: 0.6935214400291443\n",
      "Step: 600  \tValid loss: 0.5099526047706604\n",
      "Step: 700  \tTraining loss: 0.5556585192680359\n",
      "Step: 700  \tTraining accuracy: 0.7007073163986206\n",
      "Step: 700  \tValid loss: 0.5067715644836426\n",
      "Step: 800  \tTraining loss: 0.5553156137466431\n",
      "Step: 800  \tTraining accuracy: 0.7062835097312927\n",
      "Step: 800  \tValid loss: 0.5052803754806519\n",
      "Step: 900  \tTraining loss: 0.5550130009651184\n",
      "Step: 900  \tTraining accuracy: 0.7106828689575195\n",
      "Step: 900  \tValid loss: 0.5044586658477783\n",
      "Step: 1000  \tTraining loss: 0.5547157526016235\n",
      "Step: 1000  \tTraining accuracy: 0.7141560912132263\n",
      "Step: 1000  \tValid loss: 0.5039139986038208\n",
      "Step: 1100  \tTraining loss: 0.5544216632843018\n",
      "Step: 1100  \tTraining accuracy: 0.7169677019119263\n",
      "Step: 1100  \tValid loss: 0.5034987926483154\n",
      "Step: 1200  \tTraining loss: 0.5541333556175232\n",
      "Step: 1200  \tTraining accuracy: 0.7192903757095337\n",
      "Step: 1200  \tValid loss: 0.5031603574752808\n",
      "Step: 1300  \tTraining loss: 0.5538538694381714\n",
      "Step: 1300  \tTraining accuracy: 0.7212413549423218\n",
      "Step: 1300  \tValid loss: 0.5028771162033081\n",
      "Step: 1400  \tTraining loss: 0.5535843968391418\n",
      "Step: 1400  \tTraining accuracy: 0.7229033708572388\n",
      "Step: 1400  \tValid loss: 0.5026401281356812\n",
      "Step: 1500  \tTraining loss: 0.5533251166343689\n",
      "Step: 1500  \tTraining accuracy: 0.7243360877037048\n",
      "Step: 1500  \tValid loss: 0.5024446249008179\n",
      "Step: 1600  \tTraining loss: 0.553072988986969\n",
      "Step: 1600  \tTraining accuracy: 0.7255839705467224\n",
      "Step: 1600  \tValid loss: 0.502277135848999\n",
      "Step: 1700  \tTraining loss: 0.5528244376182556\n",
      "Step: 1700  \tTraining accuracy: 0.7266805768013\n",
      "Step: 1700  \tValid loss: 0.5021241307258606\n",
      "Step: 1800  \tTraining loss: 0.5525744557380676\n",
      "Step: 1800  \tTraining accuracy: 0.7276518940925598\n",
      "Step: 1800  \tValid loss: 0.5019651055335999\n",
      "Step: 1900  \tTraining loss: 0.5523169040679932\n",
      "Step: 1900  \tTraining accuracy: 0.7285181879997253\n",
      "Step: 1900  \tValid loss: 0.5017784833908081\n",
      "Step: 2000  \tTraining loss: 0.5520433187484741\n",
      "Step: 2000  \tTraining accuracy: 0.7294429540634155\n",
      "Step: 2000  \tValid loss: 0.5015363097190857\n",
      "Step: 2100  \tTraining loss: 0.5517427921295166\n",
      "Step: 2100  \tTraining accuracy: 0.7302775382995605\n",
      "Step: 2100  \tValid loss: 0.5012107491493225\n",
      "Step: 2200  \tTraining loss: 0.5514008402824402\n",
      "Step: 2200  \tTraining accuracy: 0.7310344576835632\n",
      "Step: 2200  \tValid loss: 0.5007836222648621\n",
      "Step: 2300  \tTraining loss: 0.5509994029998779\n",
      "Step: 2300  \tTraining accuracy: 0.7317241430282593\n",
      "Step: 2300  \tValid loss: 0.5002720952033997\n",
      "Step: 2400  \tTraining loss: 0.5505185127258301\n",
      "Step: 2400  \tTraining accuracy: 0.7323551177978516\n",
      "Step: 2400  \tValid loss: 0.4997081458568573\n",
      "Step: 2500  \tTraining loss: 0.5499480962753296\n",
      "Step: 2500  \tTraining accuracy: 0.7329345345497131\n",
      "Step: 2500  \tValid loss: 0.49909207224845886\n",
      "Step: 2600  \tTraining loss: 0.5492915511131287\n",
      "Step: 2600  \tTraining accuracy: 0.7334685325622559\n",
      "Step: 2600  \tValid loss: 0.49841123819351196\n",
      "Step: 2700  \tTraining loss: 0.5485528111457825\n",
      "Step: 2700  \tTraining accuracy: 0.7339622378349304\n",
      "Step: 2700  \tValid loss: 0.4976639747619629\n",
      "Step: 2800  \tTraining loss: 0.5477333068847656\n",
      "Step: 2800  \tTraining accuracy: 0.7343155741691589\n",
      "Step: 2800  \tValid loss: 0.4968481659889221\n",
      "Step: 2900  \tTraining loss: 0.5468324422836304\n",
      "Step: 2900  \tTraining accuracy: 0.7346440553665161\n",
      "Step: 2900  \tValid loss: 0.4959673583507538\n",
      "Step: 3000  \tTraining loss: 0.5458588004112244\n",
      "Step: 3000  \tTraining accuracy: 0.7349697947502136\n",
      "Step: 3000  \tValid loss: 0.49511080980300903\n",
      "Step: 3100  \tTraining loss: 0.5448142886161804\n",
      "Step: 3100  \tTraining accuracy: 0.7353495359420776\n",
      "Step: 3100  \tValid loss: 0.4942370057106018\n",
      "Step: 3200  \tTraining loss: 0.543701708316803\n",
      "Step: 3200  \tTraining accuracy: 0.7357781529426575\n",
      "Step: 3200  \tValid loss: 0.49330464005470276\n",
      "Step: 3300  \tTraining loss: 0.5425297021865845\n",
      "Step: 3300  \tTraining accuracy: 0.7361803650856018\n",
      "Step: 3300  \tValid loss: 0.49232596158981323\n",
      "Step: 3400  \tTraining loss: 0.5413159728050232\n",
      "Step: 3400  \tTraining accuracy: 0.7365586161613464\n",
      "Step: 3400  \tValid loss: 0.49131032824516296\n",
      "Step: 3500  \tTraining loss: 0.5400823354721069\n",
      "Step: 3500  \tTraining accuracy: 0.7369481921195984\n",
      "Step: 3500  \tValid loss: 0.49028024077415466\n",
      "Step: 3600  \tTraining loss: 0.538857638835907\n",
      "Step: 3600  \tTraining accuracy: 0.7374129891395569\n",
      "Step: 3600  \tValid loss: 0.4892638027667999\n",
      "Step: 3700  \tTraining loss: 0.5376700162887573\n",
      "Step: 3700  \tTraining accuracy: 0.7379310131072998\n",
      "Step: 3700  \tValid loss: 0.48827841877937317\n",
      "Step: 3800  \tTraining loss: 0.536544919013977\n",
      "Step: 3800  \tTraining accuracy: 0.738452136516571\n",
      "Step: 3800  \tValid loss: 0.48734763264656067\n",
      "Step: 3900  \tTraining loss: 0.5354976654052734\n",
      "Step: 3900  \tTraining accuracy: 0.7389461398124695\n",
      "Step: 3900  \tValid loss: 0.48649877309799194\n",
      "Step: 4000  \tTraining loss: 0.5345333814620972\n",
      "Step: 4000  \tTraining accuracy: 0.7394587397575378\n",
      "Step: 4000  \tValid loss: 0.48573705554008484\n",
      "Step: 4100  \tTraining loss: 0.5336480736732483\n",
      "Step: 4100  \tTraining accuracy: 0.7399035096168518\n",
      "Step: 4100  \tValid loss: 0.48503854870796204\n",
      "Step: 4200  \tTraining loss: 0.5328307747840881\n",
      "Step: 4200  \tTraining accuracy: 0.7403129935264587\n",
      "Step: 4200  \tValid loss: 0.4844154119491577\n",
      "Step: 4300  \tTraining loss: 0.5320706963539124\n",
      "Step: 4300  \tTraining accuracy: 0.7407572865486145\n",
      "Step: 4300  \tValid loss: 0.48385384678840637\n",
      "Step: 4400  \tTraining loss: 0.5313600301742554\n",
      "Step: 4400  \tTraining accuracy: 0.7411811351776123\n",
      "Step: 4400  \tValid loss: 0.48336634039878845\n",
      "Step: 4500  \tTraining loss: 0.5306934118270874\n",
      "Step: 4500  \tTraining accuracy: 0.7415859699249268\n",
      "Step: 4500  \tValid loss: 0.4829496443271637\n",
      "Step: 4600  \tTraining loss: 0.5300678610801697\n",
      "Step: 4600  \tTraining accuracy: 0.7419729828834534\n",
      "Step: 4600  \tValid loss: 0.482604056596756\n",
      "Step: 4700  \tTraining loss: 0.5294812321662903\n",
      "Step: 4700  \tTraining accuracy: 0.742405116558075\n",
      "Step: 4700  \tValid loss: 0.48231348395347595\n",
      "Step: 4800  \tTraining loss: 0.5289322733879089\n",
      "Step: 4800  \tTraining accuracy: 0.7428796291351318\n",
      "Step: 4800  \tValid loss: 0.48207131028175354\n",
      "Step: 4900  \tTraining loss: 0.5284225940704346\n",
      "Step: 4900  \tTraining accuracy: 0.7434293031692505\n",
      "Step: 4900  \tValid loss: 0.48186275362968445\n",
      "Step: 5000  \tTraining loss: 0.5279496312141418\n",
      "Step: 5000  \tTraining accuracy: 0.7439568042755127\n",
      "Step: 5000  \tValid loss: 0.48171868920326233\n",
      "Step: 5100  \tTraining loss: 0.5275123119354248\n",
      "Step: 5100  \tTraining accuracy: 0.7444633841514587\n",
      "Step: 5100  \tValid loss: 0.48161280155181885\n",
      "Step: 5200  \tTraining loss: 0.5271084308624268\n",
      "Step: 5200  \tTraining accuracy: 0.7450061440467834\n",
      "Step: 5200  \tValid loss: 0.48153772950172424\n",
      "Step: 5300  \tTraining loss: 0.5267348885536194\n",
      "Step: 5300  \tTraining accuracy: 0.7455172538757324\n",
      "Step: 5300  \tValid loss: 0.48150375485420227\n",
      "Step: 5400  \tTraining loss: 0.5263887047767639\n",
      "Step: 5400  \tTraining accuracy: 0.7460092306137085\n",
      "Step: 5400  \tValid loss: 0.4814814329147339\n",
      "Step: 5500  \tTraining loss: 0.5260668992996216\n",
      "Step: 5500  \tTraining accuracy: 0.7464832067489624\n",
      "Step: 5500  \tValid loss: 0.4814947843551636\n",
      "Step: 5600  \tTraining loss: 0.5257658362388611\n",
      "Step: 5600  \tTraining accuracy: 0.746940016746521\n",
      "Step: 5600  \tValid loss: 0.4815077781677246\n",
      "Step: 5700  \tTraining loss: 0.5254833698272705\n",
      "Step: 5700  \tTraining accuracy: 0.7473807334899902\n",
      "Step: 5700  \tValid loss: 0.4815414547920227\n",
      "Step: 5800  \tTraining loss: 0.5252161026000977\n",
      "Step: 5800  \tTraining accuracy: 0.7478060722351074\n",
      "Step: 5800  \tValid loss: 0.48157936334609985\n",
      "Step: 5900  \tTraining loss: 0.5249646306037903\n",
      "Step: 5900  \tTraining accuracy: 0.7482169270515442\n",
      "Step: 5900  \tValid loss: 0.4815579056739807\n",
      "Step: 6000  \tTraining loss: 0.5247278213500977\n",
      "Step: 6000  \tTraining accuracy: 0.7486139535903931\n",
      "Step: 6000  \tValid loss: 0.4815429747104645\n",
      "Step: 6100  \tTraining loss: 0.5245051383972168\n",
      "Step: 6100  \tTraining accuracy: 0.7489978075027466\n",
      "Step: 6100  \tValid loss: 0.4815470576286316\n",
      "Step: 6200  \tTraining loss: 0.5242958068847656\n",
      "Step: 6200  \tTraining accuracy: 0.749369204044342\n",
      "Step: 6200  \tValid loss: 0.48154717683792114\n",
      "Step: 6300  \tTraining loss: 0.5241004228591919\n",
      "Step: 6300  \tTraining accuracy: 0.7497287392616272\n",
      "Step: 6300  \tValid loss: 0.4815567135810852\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.75007695\n",
      "Precision: 0.78610605\n",
      "Recall: 0.83011585\n",
      "F1 score: 0.77050155\n",
      "AUC: 0.7488647\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.750077   0.786106  0.830116  0.770502  0.748865  0.523941      0.749964   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.481468       0.749798   0.481828      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  6386.0  \n",
      "28\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_13543/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(870, 3)\n",
      "(870, 1)\n",
      "(464, 3)\n",
      "(464, 1)\n",
      "(377, 3)\n",
      "(377, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6819336414337158\n",
      "Step: 100  \tTraining accuracy: 0.5528735518455505\n",
      "Step: 100  \tValid loss: 0.6969336271286011\n",
      "Step: 200  \tTraining loss: 0.6629277467727661\n",
      "Step: 200  \tTraining accuracy: 0.5656722187995911\n",
      "Step: 200  \tValid loss: 0.6857178211212158\n",
      "Step: 300  \tTraining loss: 0.6354086399078369\n",
      "Step: 300  \tTraining accuracy: 0.5815470814704895\n",
      "Step: 300  \tValid loss: 0.6723607778549194\n",
      "Step: 400  \tTraining loss: 0.6180817484855652\n",
      "Step: 400  \tTraining accuracy: 0.5933699607849121\n",
      "Step: 400  \tValid loss: 0.6692561507225037\n",
      "Step: 500  \tTraining loss: 0.609956681728363\n",
      "Step: 500  \tTraining accuracy: 0.5994296073913574\n",
      "Step: 500  \tValid loss: 0.6718394756317139\n",
      "Step: 600  \tTraining loss: 0.604871392250061\n",
      "Step: 600  \tTraining accuracy: 0.6048806309700012\n",
      "Step: 600  \tValid loss: 0.6718196272850037\n",
      "Step: 700  \tTraining loss: 0.5981609225273132\n",
      "Step: 700  \tTraining accuracy: 0.609824001789093\n",
      "Step: 700  \tValid loss: 0.6681666970252991\n",
      "Step: 800  \tTraining loss: 0.5921913385391235\n",
      "Step: 800  \tTraining accuracy: 0.6133727431297302\n",
      "Step: 800  \tValid loss: 0.6640965938568115\n",
      "Step: 900  \tTraining loss: 0.5882932543754578\n",
      "Step: 900  \tTraining accuracy: 0.6167742609977722\n",
      "Step: 900  \tValid loss: 0.6610735058784485\n",
      "Step: 1000  \tTraining loss: 0.5857279300689697\n",
      "Step: 1000  \tTraining accuracy: 0.6203208565711975\n",
      "Step: 1000  \tValid loss: 0.6588549017906189\n",
      "Step: 1100  \tTraining loss: 0.5836997032165527\n",
      "Step: 1100  \tTraining accuracy: 0.623414933681488\n",
      "Step: 1100  \tValid loss: 0.6571622490882874\n",
      "Step: 1200  \tTraining loss: 0.5819188952445984\n",
      "Step: 1200  \tTraining accuracy: 0.6264283061027527\n",
      "Step: 1200  \tValid loss: 0.654839813709259\n",
      "Step: 1300  \tTraining loss: 0.580597460269928\n",
      "Step: 1300  \tTraining accuracy: 0.6288197636604309\n",
      "Step: 1300  \tValid loss: 0.6525159478187561\n",
      "Step: 1400  \tTraining loss: 0.5795698761940002\n",
      "Step: 1400  \tTraining accuracy: 0.6311166882514954\n",
      "Step: 1400  \tValid loss: 0.6507334113121033\n",
      "Step: 1500  \tTraining loss: 0.5788333415985107\n",
      "Step: 1500  \tTraining accuracy: 0.6332581639289856\n",
      "Step: 1500  \tValid loss: 0.6495881080627441\n",
      "Step: 1600  \tTraining loss: 0.578263521194458\n",
      "Step: 1600  \tTraining accuracy: 0.6352365016937256\n",
      "Step: 1600  \tValid loss: 0.6487522721290588\n",
      "Step: 1700  \tTraining loss: 0.5778214335441589\n",
      "Step: 1700  \tTraining accuracy: 0.6370459794998169\n",
      "Step: 1700  \tValid loss: 0.6478642225265503\n",
      "Step: 1800  \tTraining loss: 0.5774149894714355\n",
      "Step: 1800  \tTraining accuracy: 0.6385486125946045\n",
      "Step: 1800  \tValid loss: 0.6471095681190491\n",
      "Step: 1900  \tTraining loss: 0.5770362019538879\n",
      "Step: 1900  \tTraining accuracy: 0.6399520039558411\n",
      "Step: 1900  \tValid loss: 0.646371603012085\n",
      "Step: 2000  \tTraining loss: 0.5766725540161133\n",
      "Step: 2000  \tTraining accuracy: 0.6412115693092346\n",
      "Step: 2000  \tValid loss: 0.6457675695419312\n",
      "Step: 2100  \tTraining loss: 0.5759662985801697\n",
      "Step: 2100  \tTraining accuracy: 0.642291247844696\n",
      "Step: 2100  \tValid loss: 0.6447212100028992\n",
      "Step: 2200  \tTraining loss: 0.5755624771118164\n",
      "Step: 2200  \tTraining accuracy: 0.643379271030426\n",
      "Step: 2200  \tValid loss: 0.6440846920013428\n",
      "Step: 2300  \tTraining loss: 0.5751374959945679\n",
      "Step: 2300  \tTraining accuracy: 0.6443706154823303\n",
      "Step: 2300  \tValid loss: 0.6430708765983582\n",
      "Step: 2400  \tTraining loss: 0.5747056007385254\n",
      "Step: 2400  \tTraining accuracy: 0.6453521847724915\n",
      "Step: 2400  \tValid loss: 0.6421805024147034\n",
      "Step: 2500  \tTraining loss: 0.5742844939231873\n",
      "Step: 2500  \tTraining accuracy: 0.6462059617042542\n",
      "Step: 2500  \tValid loss: 0.6413536667823792\n",
      "Step: 2600  \tTraining loss: 0.5738889575004578\n",
      "Step: 2600  \tTraining accuracy: 0.6469928026199341\n",
      "Step: 2600  \tValid loss: 0.6407595872879028\n",
      "Step: 2700  \tTraining loss: 0.5735236406326294\n",
      "Step: 2700  \tTraining accuracy: 0.6477202773094177\n",
      "Step: 2700  \tValid loss: 0.6401498317718506\n",
      "Step: 2800  \tTraining loss: 0.573160707950592\n",
      "Step: 2800  \tTraining accuracy: 0.6484161019325256\n",
      "Step: 2800  \tValid loss: 0.6398311257362366\n",
      "Step: 2900  \tTraining loss: 0.572834849357605\n",
      "Step: 2900  \tTraining accuracy: 0.6491246223449707\n",
      "Step: 2900  \tValid loss: 0.639344334602356\n",
      "Step: 3000  \tTraining loss: 0.5725366473197937\n",
      "Step: 3000  \tTraining accuracy: 0.6497851014137268\n",
      "Step: 3000  \tValid loss: 0.6390302181243896\n",
      "Step: 3100  \tTraining loss: 0.5721312761306763\n",
      "Step: 3100  \tTraining accuracy: 0.650402307510376\n",
      "Step: 3100  \tValid loss: 0.6384711265563965\n",
      "Step: 3200  \tTraining loss: 0.5718458294868469\n",
      "Step: 3200  \tTraining accuracy: 0.6509802937507629\n",
      "Step: 3200  \tValid loss: 0.6382994651794434\n",
      "Step: 3300  \tTraining loss: 0.5715898871421814\n",
      "Step: 3300  \tTraining accuracy: 0.6515227556228638\n",
      "Step: 3300  \tValid loss: 0.6380720734596252\n",
      "Step: 3400  \tTraining loss: 0.571353018283844\n",
      "Step: 3400  \tTraining accuracy: 0.6520328521728516\n",
      "Step: 3400  \tValid loss: 0.6377521753311157\n",
      "Step: 3500  \tTraining loss: 0.5711337924003601\n",
      "Step: 3500  \tTraining accuracy: 0.6525133848190308\n",
      "Step: 3500  \tValid loss: 0.6373773813247681\n",
      "Step: 3600  \tTraining loss: 0.5709316730499268\n",
      "Step: 3600  \tTraining accuracy: 0.6529174447059631\n",
      "Step: 3600  \tValid loss: 0.636999785900116\n",
      "Step: 3700  \tTraining loss: 0.5707443356513977\n",
      "Step: 3700  \tTraining accuracy: 0.6533154249191284\n",
      "Step: 3700  \tValid loss: 0.6366174817085266\n",
      "Step: 3800  \tTraining loss: 0.5705589056015015\n",
      "Step: 3800  \tTraining accuracy: 0.6537388563156128\n",
      "Step: 3800  \tValid loss: 0.6363857388496399\n",
      "Step: 3900  \tTraining loss: 0.570360541343689\n",
      "Step: 3900  \tTraining accuracy: 0.6541100144386292\n",
      "Step: 3900  \tValid loss: 0.6361101269721985\n",
      "Step: 4000  \tTraining loss: 0.5702188611030579\n",
      "Step: 4000  \tTraining accuracy: 0.6544031500816345\n",
      "Step: 4000  \tValid loss: 0.6360488533973694\n",
      "Step: 4100  \tTraining loss: 0.5700919032096863\n",
      "Step: 4100  \tTraining accuracy: 0.6546818614006042\n",
      "Step: 4100  \tValid loss: 0.6358505487442017\n",
      "Step: 4200  \tTraining loss: 0.569973349571228\n",
      "Step: 4200  \tTraining accuracy: 0.6549471020698547\n",
      "Step: 4200  \tValid loss: 0.6356877088546753\n",
      "Step: 4300  \tTraining loss: 0.5698650479316711\n",
      "Step: 4300  \tTraining accuracy: 0.6551998853683472\n",
      "Step: 4300  \tValid loss: 0.6356322169303894\n",
      "Step: 4400  \tTraining loss: 0.5697623491287231\n",
      "Step: 4400  \tTraining accuracy: 0.6554411053657532\n",
      "Step: 4400  \tValid loss: 0.6355187892913818\n",
      "Step: 4500  \tTraining loss: 0.5696694254875183\n",
      "Step: 4500  \tTraining accuracy: 0.6556714177131653\n",
      "Step: 4500  \tValid loss: 0.6353546380996704\n",
      "Step: 4600  \tTraining loss: 0.5695841312408447\n",
      "Step: 4600  \tTraining accuracy: 0.6558915972709656\n",
      "Step: 4600  \tValid loss: 0.63520747423172\n",
      "Step: 4700  \tTraining loss: 0.5695056915283203\n",
      "Step: 4700  \tTraining accuracy: 0.6561023592948914\n",
      "Step: 4700  \tValid loss: 0.6351097822189331\n",
      "Step: 4800  \tTraining loss: 0.5694324970245361\n",
      "Step: 4800  \tTraining accuracy: 0.6563041806221008\n",
      "Step: 4800  \tValid loss: 0.6350619196891785\n",
      "Step: 4900  \tTraining loss: 0.5693628787994385\n",
      "Step: 4900  \tTraining accuracy: 0.6564977765083313\n",
      "Step: 4900  \tValid loss: 0.6349285840988159\n",
      "Step: 5000  \tTraining loss: 0.569297730922699\n",
      "Step: 5000  \tTraining accuracy: 0.6566834449768066\n",
      "Step: 5000  \tValid loss: 0.6348745226860046\n",
      "Step: 5100  \tTraining loss: 0.5692360997200012\n",
      "Step: 5100  \tTraining accuracy: 0.6568618416786194\n",
      "Step: 5100  \tValid loss: 0.6347749829292297\n",
      "Step: 5200  \tTraining loss: 0.5691860318183899\n",
      "Step: 5200  \tTraining accuracy: 0.6570332646369934\n",
      "Step: 5200  \tValid loss: 0.6351507306098938\n",
      "Step: 5300  \tTraining loss: 0.5691202878952026\n",
      "Step: 5300  \tTraining accuracy: 0.6571981906890869\n",
      "Step: 5300  \tValid loss: 0.6346330046653748\n",
      "Step: 5400  \tTraining loss: 0.5690651535987854\n",
      "Step: 5400  \tTraining accuracy: 0.6573569178581238\n",
      "Step: 5400  \tValid loss: 0.6346002817153931\n",
      "Step: 5500  \tTraining loss: 0.5690126419067383\n",
      "Step: 5500  \tTraining accuracy: 0.6575098633766174\n",
      "Step: 5500  \tValid loss: 0.6345680952072144\n",
      "Step: 5600  \tTraining loss: 0.5689616799354553\n",
      "Step: 5600  \tTraining accuracy: 0.657657265663147\n",
      "Step: 5600  \tValid loss: 0.634523868560791\n",
      "Step: 5700  \tTraining loss: 0.5689125061035156\n",
      "Step: 5700  \tTraining accuracy: 0.657799482345581\n",
      "Step: 5700  \tValid loss: 0.6345117688179016\n",
      "Step: 5800  \tTraining loss: 0.5688648223876953\n",
      "Step: 5800  \tTraining accuracy: 0.6579367518424988\n",
      "Step: 5800  \tValid loss: 0.6343948245048523\n",
      "Step: 5900  \tTraining loss: 0.5688180923461914\n",
      "Step: 5900  \tTraining accuracy: 0.6580693125724792\n",
      "Step: 5900  \tValid loss: 0.6343917846679688\n",
      "Step: 6000  \tTraining loss: 0.5687734484672546\n",
      "Step: 6000  \tTraining accuracy: 0.6581974029541016\n",
      "Step: 6000  \tValid loss: 0.6342726349830627\n",
      "Step: 6100  \tTraining loss: 0.5687285661697388\n",
      "Step: 6100  \tTraining accuracy: 0.6583212614059448\n",
      "Step: 6100  \tValid loss: 0.6342836618423462\n",
      "Step: 6200  \tTraining loss: 0.568684995174408\n",
      "Step: 6200  \tTraining accuracy: 0.6584411263465881\n",
      "Step: 6200  \tValid loss: 0.6342144012451172\n",
      "Step: 6300  \tTraining loss: 0.5686423182487488\n",
      "Step: 6300  \tTraining accuracy: 0.658557116985321\n",
      "Step: 6300  \tValid loss: 0.6341835260391235\n",
      "Step: 6400  \tTraining loss: 0.5686005353927612\n",
      "Step: 6400  \tTraining accuracy: 0.6586694717407227\n",
      "Step: 6400  \tValid loss: 0.6341540813446045\n",
      "Step: 6500  \tTraining loss: 0.5685594081878662\n",
      "Step: 6500  \tTraining accuracy: 0.6587783098220825\n",
      "Step: 6500  \tValid loss: 0.6341387033462524\n",
      "Step: 6600  \tTraining loss: 0.5685191750526428\n",
      "Step: 6600  \tTraining accuracy: 0.6588838696479797\n",
      "Step: 6600  \tValid loss: 0.6340594291687012\n",
      "Step: 6700  \tTraining loss: 0.5684797763824463\n",
      "Step: 6700  \tTraining accuracy: 0.6589862704277039\n",
      "Step: 6700  \tValid loss: 0.633982241153717\n",
      "Step: 6800  \tTraining loss: 0.5684406757354736\n",
      "Step: 6800  \tTraining accuracy: 0.6590855717658997\n",
      "Step: 6800  \tValid loss: 0.6339949369430542\n",
      "Step: 6900  \tTraining loss: 0.5684020519256592\n",
      "Step: 6900  \tTraining accuracy: 0.6591820120811462\n",
      "Step: 6900  \tValid loss: 0.6339827179908752\n",
      "Step: 7000  \tTraining loss: 0.5683640241622925\n",
      "Step: 7000  \tTraining accuracy: 0.6592757105827332\n",
      "Step: 7000  \tValid loss: 0.633971631526947\n",
      "Step: 7100  \tTraining loss: 0.5683267116546631\n",
      "Step: 7100  \tTraining accuracy: 0.6593667268753052\n",
      "Step: 7100  \tValid loss: 0.6338925957679749\n",
      "Step: 7200  \tTraining loss: 0.5682896375656128\n",
      "Step: 7200  \tTraining accuracy: 0.6594551801681519\n",
      "Step: 7200  \tValid loss: 0.6338945031166077\n",
      "Step: 7300  \tTraining loss: 0.5682532787322998\n",
      "Step: 7300  \tTraining accuracy: 0.6595411896705627\n",
      "Step: 7300  \tValid loss: 0.633876383304596\n",
      "Step: 7400  \tTraining loss: 0.568216860294342\n",
      "Step: 7400  \tTraining accuracy: 0.6596248745918274\n",
      "Step: 7400  \tValid loss: 0.6338199973106384\n",
      "Step: 7500  \tTraining loss: 0.5681811571121216\n",
      "Step: 7500  \tTraining accuracy: 0.6597062945365906\n",
      "Step: 7500  \tValid loss: 0.6337951421737671\n",
      "Step: 7600  \tTraining loss: 0.5681453347206116\n",
      "Step: 7600  \tTraining accuracy: 0.6597855687141418\n",
      "Step: 7600  \tValid loss: 0.6338070631027222\n",
      "Step: 7700  \tTraining loss: 0.5681101083755493\n",
      "Step: 7700  \tTraining accuracy: 0.6598628163337708\n",
      "Step: 7700  \tValid loss: 0.6337699890136719\n",
      "Step: 7800  \tTraining loss: 0.5680748820304871\n",
      "Step: 7800  \tTraining accuracy: 0.6599380373954773\n",
      "Step: 7800  \tValid loss: 0.6337428689002991\n",
      "Step: 7900  \tTraining loss: 0.5680396556854248\n",
      "Step: 7900  \tTraining accuracy: 0.6600112915039062\n",
      "Step: 7900  \tValid loss: 0.6337248682975769\n",
      "Step: 8000  \tTraining loss: 0.5680042505264282\n",
      "Step: 8000  \tTraining accuracy: 0.6600827574729919\n",
      "Step: 8000  \tValid loss: 0.6337172389030457\n",
      "Step: 8100  \tTraining loss: 0.5679688453674316\n",
      "Step: 8100  \tTraining accuracy: 0.6601161360740662\n",
      "Step: 8100  \tValid loss: 0.6337160468101501\n",
      "Step: 8200  \tTraining loss: 0.5679336190223694\n",
      "Step: 8200  \tTraining accuracy: 0.6601487398147583\n",
      "Step: 8200  \tValid loss: 0.6336756348609924\n",
      "Step: 8300  \tTraining loss: 0.5678975582122803\n",
      "Step: 8300  \tTraining accuracy: 0.6601805090904236\n",
      "Step: 8300  \tValid loss: 0.6336716413497925\n",
      "Step: 8400  \tTraining loss: 0.567861020565033\n",
      "Step: 8400  \tTraining accuracy: 0.6602115035057068\n",
      "Step: 8400  \tValid loss: 0.6336838006973267\n",
      "Step: 8500  \tTraining loss: 0.5678243637084961\n",
      "Step: 8500  \tTraining accuracy: 0.6602417826652527\n",
      "Step: 8500  \tValid loss: 0.6336871981620789\n",
      "Step: 8600  \tTraining loss: 0.5677862763404846\n",
      "Step: 8600  \tTraining accuracy: 0.6602713465690613\n",
      "Step: 8600  \tValid loss: 0.6337012648582458\n",
      "Step: 8700  \tTraining loss: 0.5677483081817627\n",
      "Step: 8700  \tTraining accuracy: 0.6603002548217773\n",
      "Step: 8700  \tValid loss: 0.6336816549301147\n",
      "Step: 8800  \tTraining loss: 0.5677094459533691\n",
      "Step: 8800  \tTraining accuracy: 0.6603284478187561\n",
      "Step: 8800  \tValid loss: 0.6337047219276428\n",
      "Step: 8900  \tTraining loss: 0.5676699280738831\n",
      "Step: 8900  \tTraining accuracy: 0.6603560447692871\n",
      "Step: 8900  \tValid loss: 0.6337326169013977\n",
      "Step: 9000  \tTraining loss: 0.5676286816596985\n",
      "Step: 9000  \tTraining accuracy: 0.6603830456733704\n",
      "Step: 9000  \tValid loss: 0.6337856650352478\n",
      "Step: 9100  \tTraining loss: 0.5675883889198303\n",
      "Step: 9100  \tTraining accuracy: 0.6604093909263611\n",
      "Step: 9100  \tValid loss: 0.6337971091270447\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.6604352\n",
      "Precision: 0.7005871\n",
      "Recall: 0.7442827\n",
      "F1 score: 0.68040806\n",
      "AUC: 0.6754832\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.660435   0.700587  0.744283  0.680408  0.675483  0.567583       0.66031   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0   0.63343       0.660416   0.607441      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  9113.0  \n",
      "29\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_22490/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(1798, 3)\n",
      "(1798, 1)\n",
      "(992, 3)\n",
      "(992, 1)\n",
      "(806, 3)\n",
      "(806, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.639711320400238\n",
      "Step: 100  \tTraining accuracy: 0.7519466280937195\n",
      "Step: 100  \tValid loss: 0.6413084864616394\n",
      "Step: 200  \tTraining loss: 0.5065968632698059\n",
      "Step: 200  \tTraining accuracy: 0.7586206793785095\n",
      "Step: 200  \tValid loss: 0.5181493163108826\n",
      "Step: 300  \tTraining loss: 0.47205695509910583\n",
      "Step: 300  \tTraining accuracy: 0.7763069868087769\n",
      "Step: 300  \tValid loss: 0.49348321557044983\n",
      "Step: 400  \tTraining loss: 0.45160776376724243\n",
      "Step: 400  \tTraining accuracy: 0.7873827815055847\n",
      "Step: 400  \tValid loss: 0.4756172299385071\n",
      "Step: 500  \tTraining loss: 0.42876386642456055\n",
      "Step: 500  \tTraining accuracy: 0.7927326560020447\n",
      "Step: 500  \tValid loss: 0.455047070980072\n",
      "Step: 600  \tTraining loss: 0.4059053361415863\n",
      "Step: 600  \tTraining accuracy: 0.7958843111991882\n",
      "Step: 600  \tValid loss: 0.4360814392566681\n",
      "Step: 700  \tTraining loss: 0.3865753412246704\n",
      "Step: 700  \tTraining accuracy: 0.7982801198959351\n",
      "Step: 700  \tValid loss: 0.42289528250694275\n",
      "Step: 800  \tTraining loss: 0.3724929988384247\n",
      "Step: 800  \tTraining accuracy: 0.8011865019798279\n",
      "Step: 800  \tValid loss: 0.41636356711387634\n",
      "Step: 900  \tTraining loss: 0.36265113949775696\n",
      "Step: 900  \tTraining accuracy: 0.8049139380455017\n",
      "Step: 900  \tValid loss: 0.4144707918167114\n",
      "Step: 1000  \tTraining loss: 0.35680314898490906\n",
      "Step: 1000  \tTraining accuracy: 0.8084713816642761\n",
      "Step: 1000  \tValid loss: 0.41622427105903625\n",
      "Step: 1100  \tTraining loss: 0.3537277281284332\n",
      "Step: 1100  \tTraining accuracy: 0.8114306926727295\n",
      "Step: 1100  \tValid loss: 0.4193757474422455\n",
      "Step: 1200  \tTraining loss: 0.35213303565979004\n",
      "Step: 1200  \tTraining accuracy: 0.8142622113227844\n",
      "Step: 1200  \tValid loss: 0.42248398065567017\n",
      "Step: 1300  \tTraining loss: 0.35097700357437134\n",
      "Step: 1300  \tTraining accuracy: 0.816729724407196\n",
      "Step: 1300  \tValid loss: 0.42483219504356384\n",
      "Step: 1400  \tTraining loss: 0.3498370945453644\n",
      "Step: 1400  \tTraining accuracy: 0.8188316226005554\n",
      "Step: 1400  \tValid loss: 0.42692500352859497\n",
      "Step: 1500  \tTraining loss: 0.34925755858421326\n",
      "Step: 1500  \tTraining accuracy: 0.8206436038017273\n",
      "Step: 1500  \tValid loss: 0.42888808250427246\n",
      "Step: 1600  \tTraining loss: 0.34879013895988464\n",
      "Step: 1600  \tTraining accuracy: 0.8220424056053162\n",
      "Step: 1600  \tValid loss: 0.43014630675315857\n",
      "Step: 1700  \tTraining loss: 0.3483821749687195\n",
      "Step: 1700  \tTraining accuracy: 0.8232885003089905\n",
      "Step: 1700  \tValid loss: 0.4309674799442291\n",
      "Step: 1800  \tTraining loss: 0.3480016887187958\n",
      "Step: 1800  \tTraining accuracy: 0.8244398832321167\n",
      "Step: 1800  \tValid loss: 0.43160971999168396\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.82546675\n",
      "Precision: 0.86427796\n",
      "Recall: 0.8468085\n",
      "F1 score: 0.8170391\n",
      "AUC: 0.8505604\n",
      "   accuracy  precision    recall  f1_score      auc      loss  accuracy_val  \\\n",
      "0  0.825467   0.864278  0.846808  0.817039  0.85056  0.347651      0.824774   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.414453       0.824699   0.341427      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  1890.0  \n",
      "30\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_26205/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(2378, 3)\n",
      "(2378, 1)\n",
      "(1312, 3)\n",
      "(1312, 1)\n",
      "(1066, 3)\n",
      "(1066, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6636736392974854\n",
      "Step: 100  \tTraining accuracy: 0.5984020233154297\n",
      "Step: 100  \tValid loss: 0.6552575826644897\n",
      "Step: 200  \tTraining loss: 0.6370200514793396\n",
      "Step: 200  \tTraining accuracy: 0.6246145367622375\n",
      "Step: 200  \tValid loss: 0.6224031448364258\n",
      "Step: 300  \tTraining loss: 0.60829758644104\n",
      "Step: 300  \tTraining accuracy: 0.6515559554100037\n",
      "Step: 300  \tValid loss: 0.5864648222923279\n",
      "Step: 400  \tTraining loss: 0.5904183387756348\n",
      "Step: 400  \tTraining accuracy: 0.6716929078102112\n",
      "Step: 400  \tValid loss: 0.5638437271118164\n",
      "Step: 500  \tTraining loss: 0.5811187624931335\n",
      "Step: 500  \tTraining accuracy: 0.6851696372032166\n",
      "Step: 500  \tValid loss: 0.5514017343521118\n",
      "Step: 600  \tTraining loss: 0.5761295557022095\n",
      "Step: 600  \tTraining accuracy: 0.6936309933662415\n",
      "Step: 600  \tValid loss: 0.5444653034210205\n",
      "Step: 700  \tTraining loss: 0.5733395218849182\n",
      "Step: 700  \tTraining accuracy: 0.6994888782501221\n",
      "Step: 700  \tValid loss: 0.54034024477005\n",
      "Step: 800  \tTraining loss: 0.5717006921768188\n",
      "Step: 800  \tTraining accuracy: 0.7036445140838623\n",
      "Step: 800  \tValid loss: 0.5376693606376648\n",
      "Step: 900  \tTraining loss: 0.570679247379303\n",
      "Step: 900  \tTraining accuracy: 0.706970751285553\n",
      "Step: 900  \tValid loss: 0.5358794331550598\n",
      "Step: 1000  \tTraining loss: 0.5700086951255798\n",
      "Step: 1000  \tTraining accuracy: 0.7099287509918213\n",
      "Step: 1000  \tValid loss: 0.5346441268920898\n",
      "Step: 1100  \tTraining loss: 0.569545328617096\n",
      "Step: 1100  \tTraining accuracy: 0.7125435471534729\n",
      "Step: 1100  \tValid loss: 0.533776581287384\n",
      "Step: 1200  \tTraining loss: 0.569198489189148\n",
      "Step: 1200  \tTraining accuracy: 0.714685320854187\n",
      "Step: 1200  \tValid loss: 0.5330810546875\n",
      "Step: 1300  \tTraining loss: 0.5689281821250916\n",
      "Step: 1300  \tTraining accuracy: 0.7164844274520874\n",
      "Step: 1300  \tValid loss: 0.5325326323509216\n",
      "Step: 1400  \tTraining loss: 0.5687076449394226\n",
      "Step: 1400  \tTraining accuracy: 0.7180169820785522\n",
      "Step: 1400  \tValid loss: 0.5320823192596436\n",
      "Step: 1500  \tTraining loss: 0.568523108959198\n",
      "Step: 1500  \tTraining accuracy: 0.7193381786346436\n",
      "Step: 1500  \tValid loss: 0.5316905379295349\n",
      "Step: 1600  \tTraining loss: 0.5683606863021851\n",
      "Step: 1600  \tTraining accuracy: 0.7204889059066772\n",
      "Step: 1600  \tValid loss: 0.531348466873169\n",
      "Step: 1700  \tTraining loss: 0.568213939666748\n",
      "Step: 1700  \tTraining accuracy: 0.7215000987052917\n",
      "Step: 1700  \tValid loss: 0.5310519337654114\n",
      "Step: 1800  \tTraining loss: 0.5680785775184631\n",
      "Step: 1800  \tTraining accuracy: 0.7223957777023315\n",
      "Step: 1800  \tValid loss: 0.530788242816925\n",
      "Step: 1900  \tTraining loss: 0.5679552555084229\n",
      "Step: 1900  \tTraining accuracy: 0.7231945991516113\n",
      "Step: 1900  \tValid loss: 0.5305545926094055\n",
      "Step: 2000  \tTraining loss: 0.5678433179855347\n",
      "Step: 2000  \tTraining accuracy: 0.7239546179771423\n",
      "Step: 2000  \tValid loss: 0.530370831489563\n",
      "Step: 2100  \tTraining loss: 0.5677493214607239\n",
      "Step: 2100  \tTraining accuracy: 0.7246302366256714\n",
      "Step: 2100  \tValid loss: 0.5302274823188782\n",
      "Step: 2200  \tTraining loss: 0.5676656365394592\n",
      "Step: 2200  \tTraining accuracy: 0.7252430319786072\n",
      "Step: 2200  \tValid loss: 0.5300926566123962\n",
      "Step: 2300  \tTraining loss: 0.5675889849662781\n",
      "Step: 2300  \tTraining accuracy: 0.7258013486862183\n",
      "Step: 2300  \tValid loss: 0.5299704670906067\n",
      "Step: 2400  \tTraining loss: 0.5675128698348999\n",
      "Step: 2400  \tTraining accuracy: 0.7263299822807312\n",
      "Step: 2400  \tValid loss: 0.5298161506652832\n",
      "Step: 2500  \tTraining loss: 0.5674353241920471\n",
      "Step: 2500  \tTraining accuracy: 0.7268155217170715\n",
      "Step: 2500  \tValid loss: 0.5296651124954224\n",
      "Step: 2600  \tTraining loss: 0.5673639178276062\n",
      "Step: 2600  \tTraining accuracy: 0.7272464632987976\n",
      "Step: 2600  \tValid loss: 0.5295629501342773\n",
      "Step: 2700  \tTraining loss: 0.5672988891601562\n",
      "Step: 2700  \tTraining accuracy: 0.7276449203491211\n",
      "Step: 2700  \tValid loss: 0.529499351978302\n",
      "Step: 2800  \tTraining loss: 0.5672365427017212\n",
      "Step: 2800  \tTraining accuracy: 0.7280067205429077\n",
      "Step: 2800  \tValid loss: 0.5294016599655151\n",
      "Step: 2900  \tTraining loss: 0.5671807527542114\n",
      "Step: 2900  \tTraining accuracy: 0.7284021377563477\n",
      "Step: 2900  \tValid loss: 0.5293160080909729\n",
      "Step: 3000  \tTraining loss: 0.567126989364624\n",
      "Step: 3000  \tTraining accuracy: 0.7287707924842834\n",
      "Step: 3000  \tValid loss: 0.5292245149612427\n",
      "Step: 3100  \tTraining loss: 0.5670739412307739\n",
      "Step: 3100  \tTraining accuracy: 0.7291152477264404\n",
      "Step: 3100  \tValid loss: 0.5291488766670227\n",
      "Step: 3200  \tTraining loss: 0.5670216083526611\n",
      "Step: 3200  \tTraining accuracy: 0.7294378280639648\n",
      "Step: 3200  \tValid loss: 0.5290676951408386\n",
      "Step: 3300  \tTraining loss: 0.5669722557067871\n",
      "Step: 3300  \tTraining accuracy: 0.729740560054779\n",
      "Step: 3300  \tValid loss: 0.5289739966392517\n",
      "Step: 3400  \tTraining loss: 0.5669261813163757\n",
      "Step: 3400  \tTraining accuracy: 0.7300252318382263\n",
      "Step: 3400  \tValid loss: 0.5289037227630615\n",
      "Step: 3500  \tTraining loss: 0.566879153251648\n",
      "Step: 3500  \tTraining accuracy: 0.7302933931350708\n",
      "Step: 3500  \tValid loss: 0.5288099050521851\n",
      "Step: 3600  \tTraining loss: 0.5668330788612366\n",
      "Step: 3600  \tTraining accuracy: 0.7305464148521423\n",
      "Step: 3600  \tValid loss: 0.528731107711792\n",
      "Step: 3700  \tTraining loss: 0.5667862296104431\n",
      "Step: 3700  \tTraining accuracy: 0.730785608291626\n",
      "Step: 3700  \tValid loss: 0.5286415219306946\n",
      "Step: 3800  \tTraining loss: 0.5667392015457153\n",
      "Step: 3800  \tTraining accuracy: 0.7310120463371277\n",
      "Step: 3800  \tValid loss: 0.5285515189170837\n",
      "Step: 3900  \tTraining loss: 0.5666918754577637\n",
      "Step: 3900  \tTraining accuracy: 0.7312267422676086\n",
      "Step: 3900  \tValid loss: 0.5284563899040222\n",
      "Step: 4000  \tTraining loss: 0.566643238067627\n",
      "Step: 4000  \tTraining accuracy: 0.7314305305480957\n",
      "Step: 4000  \tValid loss: 0.5283656716346741\n",
      "Step: 4100  \tTraining loss: 0.5665946006774902\n",
      "Step: 4100  \tTraining accuracy: 0.7316242456436157\n",
      "Step: 4100  \tValid loss: 0.5282778143882751\n",
      "Step: 4200  \tTraining loss: 0.5665455460548401\n",
      "Step: 4200  \tTraining accuracy: 0.7318086624145508\n",
      "Step: 4200  \tValid loss: 0.5281879901885986\n",
      "Step: 4300  \tTraining loss: 0.5664967894554138\n",
      "Step: 4300  \tTraining accuracy: 0.7319843769073486\n",
      "Step: 4300  \tValid loss: 0.5280982851982117\n",
      "Step: 4400  \tTraining loss: 0.5664458870887756\n",
      "Step: 4400  \tTraining accuracy: 0.7321568131446838\n",
      "Step: 4400  \tValid loss: 0.5280066132545471\n",
      "Step: 4500  \tTraining loss: 0.5663919448852539\n",
      "Step: 4500  \tTraining accuracy: 0.7323262691497803\n",
      "Step: 4500  \tValid loss: 0.5279152989387512\n",
      "Step: 4600  \tTraining loss: 0.566332995891571\n",
      "Step: 4600  \tTraining accuracy: 0.7324790358543396\n",
      "Step: 4600  \tValid loss: 0.5278152227401733\n",
      "Step: 4700  \tTraining loss: 0.5662691593170166\n",
      "Step: 4700  \tTraining accuracy: 0.7326252460479736\n",
      "Step: 4700  \tValid loss: 0.5276913642883301\n",
      "Step: 4800  \tTraining loss: 0.5661976337432861\n",
      "Step: 4800  \tTraining accuracy: 0.7327696681022644\n",
      "Step: 4800  \tValid loss: 0.5275724530220032\n",
      "Step: 4900  \tTraining loss: 0.5661167502403259\n",
      "Step: 4900  \tTraining accuracy: 0.732912540435791\n",
      "Step: 4900  \tValid loss: 0.5274373292922974\n",
      "Step: 5000  \tTraining loss: 0.5660291910171509\n",
      "Step: 5000  \tTraining accuracy: 0.7330411076545715\n",
      "Step: 5000  \tValid loss: 0.5272793769836426\n",
      "Step: 5100  \tTraining loss: 0.5659353137016296\n",
      "Step: 5100  \tTraining accuracy: 0.7331687211990356\n",
      "Step: 5100  \tValid loss: 0.5270951986312866\n",
      "Step: 5200  \tTraining loss: 0.5658408403396606\n",
      "Step: 5200  \tTraining accuracy: 0.7332995533943176\n",
      "Step: 5200  \tValid loss: 0.5268911123275757\n",
      "Step: 5300  \tTraining loss: 0.5657477974891663\n",
      "Step: 5300  \tTraining accuracy: 0.7334254384040833\n",
      "Step: 5300  \tValid loss: 0.5266880393028259\n",
      "Step: 5400  \tTraining loss: 0.5656588077545166\n",
      "Step: 5400  \tTraining accuracy: 0.7335466146469116\n",
      "Step: 5400  \tValid loss: 0.5264834761619568\n",
      "Step: 5500  \tTraining loss: 0.5655692219734192\n",
      "Step: 5500  \tTraining accuracy: 0.7336633205413818\n",
      "Step: 5500  \tValid loss: 0.5262912511825562\n",
      "Step: 5600  \tTraining loss: 0.5654935836791992\n",
      "Step: 5600  \tTraining accuracy: 0.7337758541107178\n",
      "Step: 5600  \tValid loss: 0.5261164307594299\n",
      "Step: 5700  \tTraining loss: 0.5654260516166687\n",
      "Step: 5700  \tTraining accuracy: 0.733884334564209\n",
      "Step: 5700  \tValid loss: 0.5259491801261902\n",
      "Step: 5800  \tTraining loss: 0.5653582215309143\n",
      "Step: 5800  \tTraining accuracy: 0.7339891195297241\n",
      "Step: 5800  \tValid loss: 0.5257624983787537\n",
      "Step: 5900  \tTraining loss: 0.5652964115142822\n",
      "Step: 5900  \tTraining accuracy: 0.734090268611908\n",
      "Step: 5900  \tValid loss: 0.5256071090698242\n",
      "Step: 6000  \tTraining loss: 0.5652386546134949\n",
      "Step: 6000  \tTraining accuracy: 0.7341880202293396\n",
      "Step: 6000  \tValid loss: 0.5254572629928589\n",
      "Step: 6100  \tTraining loss: 0.5651781558990479\n",
      "Step: 6100  \tTraining accuracy: 0.7342825531959534\n",
      "Step: 6100  \tValid loss: 0.5252822041511536\n",
      "Step: 6200  \tTraining loss: 0.5651070475578308\n",
      "Step: 6200  \tTraining accuracy: 0.7343740463256836\n",
      "Step: 6200  \tValid loss: 0.5251039862632751\n",
      "Step: 6300  \tTraining loss: 0.565011203289032\n",
      "Step: 6300  \tTraining accuracy: 0.7344558238983154\n",
      "Step: 6300  \tValid loss: 0.5249733924865723\n",
      "Step: 6400  \tTraining loss: 0.5649234652519226\n",
      "Step: 6400  \tTraining accuracy: 0.7345317602157593\n",
      "Step: 6400  \tValid loss: 0.5248494148254395\n",
      "Step: 6500  \tTraining loss: 0.5648586750030518\n",
      "Step: 6500  \tTraining accuracy: 0.7346053123474121\n",
      "Step: 6500  \tValid loss: 0.5247335433959961\n",
      "Step: 6600  \tTraining loss: 0.5647682547569275\n",
      "Step: 6600  \tTraining accuracy: 0.7346766591072083\n",
      "Step: 6600  \tValid loss: 0.5245311856269836\n",
      "Step: 6700  \tTraining loss: 0.5646271109580994\n",
      "Step: 6700  \tTraining accuracy: 0.7347458004951477\n",
      "Step: 6700  \tValid loss: 0.5243902802467346\n",
      "Step: 6800  \tTraining loss: 0.5645046234130859\n",
      "Step: 6800  \tTraining accuracy: 0.7348129749298096\n",
      "Step: 6800  \tValid loss: 0.5242347717285156\n",
      "Step: 6900  \tTraining loss: 0.5643957257270813\n",
      "Step: 6900  \tTraining accuracy: 0.7348873019218445\n",
      "Step: 6900  \tValid loss: 0.5240617990493774\n",
      "Step: 7000  \tTraining loss: 0.5642390251159668\n",
      "Step: 7000  \tTraining accuracy: 0.7349595427513123\n",
      "Step: 7000  \tValid loss: 0.5239260792732239\n",
      "Step: 7100  \tTraining loss: 0.5641492605209351\n",
      "Step: 7100  \tTraining accuracy: 0.7350297570228577\n",
      "Step: 7100  \tValid loss: 0.5238129496574402\n",
      "Step: 7200  \tTraining loss: 0.564065158367157\n",
      "Step: 7200  \tTraining accuracy: 0.7351097464561462\n",
      "Step: 7200  \tValid loss: 0.5237088203430176\n",
      "Step: 7300  \tTraining loss: 0.563988208770752\n",
      "Step: 7300  \tTraining accuracy: 0.735219419002533\n",
      "Step: 7300  \tValid loss: 0.5236039757728577\n",
      "Step: 7400  \tTraining loss: 0.5639233589172363\n",
      "Step: 7400  \tTraining accuracy: 0.7353203892707825\n",
      "Step: 7400  \tValid loss: 0.5235194563865662\n",
      "Step: 7500  \tTraining loss: 0.5638729929924011\n",
      "Step: 7500  \tTraining accuracy: 0.7354186177253723\n",
      "Step: 7500  \tValid loss: 0.5234144926071167\n",
      "Step: 7600  \tTraining loss: 0.5638269186019897\n",
      "Step: 7600  \tTraining accuracy: 0.7355198860168457\n",
      "Step: 7600  \tValid loss: 0.5233028531074524\n",
      "Step: 7700  \tTraining loss: 0.5637835264205933\n",
      "Step: 7700  \tTraining accuracy: 0.7356184124946594\n",
      "Step: 7700  \tValid loss: 0.5232023596763611\n",
      "Step: 7800  \tTraining loss: 0.5637406706809998\n",
      "Step: 7800  \tTraining accuracy: 0.7357144951820374\n",
      "Step: 7800  \tValid loss: 0.5231069326400757\n",
      "Step: 7900  \tTraining loss: 0.5636999011039734\n",
      "Step: 7900  \tTraining accuracy: 0.7358080744743347\n",
      "Step: 7900  \tValid loss: 0.5230283141136169\n",
      "Step: 8000  \tTraining loss: 0.5636578798294067\n",
      "Step: 8000  \tTraining accuracy: 0.7358993291854858\n",
      "Step: 8000  \tValid loss: 0.5229130387306213\n",
      "Step: 8100  \tTraining loss: 0.563618004322052\n",
      "Step: 8100  \tTraining accuracy: 0.7359856367111206\n",
      "Step: 8100  \tValid loss: 0.5228473544120789\n",
      "Step: 8200  \tTraining loss: 0.5635772347450256\n",
      "Step: 8200  \tTraining accuracy: 0.7360672950744629\n",
      "Step: 8200  \tValid loss: 0.5227441191673279\n",
      "Step: 8300  \tTraining loss: 0.5635380744934082\n",
      "Step: 8300  \tTraining accuracy: 0.7361469864845276\n",
      "Step: 8300  \tValid loss: 0.522667407989502\n",
      "Step: 8400  \tTraining loss: 0.563499391078949\n",
      "Step: 8400  \tTraining accuracy: 0.7362247705459595\n",
      "Step: 8400  \tValid loss: 0.5225915908813477\n",
      "Step: 8500  \tTraining loss: 0.5634605884552002\n",
      "Step: 8500  \tTraining accuracy: 0.7363007068634033\n",
      "Step: 8500  \tValid loss: 0.5225133299827576\n",
      "Step: 8600  \tTraining loss: 0.5634234547615051\n",
      "Step: 8600  \tTraining accuracy: 0.7363748550415039\n",
      "Step: 8600  \tValid loss: 0.5224408507347107\n",
      "Step: 8700  \tTraining loss: 0.5633850693702698\n",
      "Step: 8700  \tTraining accuracy: 0.736447274684906\n",
      "Step: 8700  \tValid loss: 0.5223429799079895\n",
      "Step: 8800  \tTraining loss: 0.5633484125137329\n",
      "Step: 8800  \tTraining accuracy: 0.7365180850028992\n",
      "Step: 8800  \tValid loss: 0.5222904086112976\n",
      "Step: 8900  \tTraining loss: 0.5633124709129333\n",
      "Step: 8900  \tTraining accuracy: 0.7365872859954834\n",
      "Step: 8900  \tValid loss: 0.5222179293632507\n",
      "Step: 9000  \tTraining loss: 0.5632769465446472\n",
      "Step: 9000  \tTraining accuracy: 0.7366548776626587\n",
      "Step: 9000  \tValid loss: 0.5221518278121948\n",
      "Step: 9100  \tTraining loss: 0.5632412433624268\n",
      "Step: 9100  \tTraining accuracy: 0.7367210388183594\n",
      "Step: 9100  \tValid loss: 0.5220842957496643\n",
      "Step: 9200  \tTraining loss: 0.5632065534591675\n",
      "Step: 9200  \tTraining accuracy: 0.7367857694625854\n",
      "Step: 9200  \tValid loss: 0.5220016837120056\n",
      "Step: 9300  \tTraining loss: 0.5631723403930664\n",
      "Step: 9300  \tTraining accuracy: 0.7368490695953369\n",
      "Step: 9300  \tValid loss: 0.521937370300293\n",
      "Step: 9400  \tTraining loss: 0.5631384253501892\n",
      "Step: 9400  \tTraining accuracy: 0.7369109988212585\n",
      "Step: 9400  \tValid loss: 0.5218680500984192\n",
      "Step: 9500  \tTraining loss: 0.5631055235862732\n",
      "Step: 9500  \tTraining accuracy: 0.7369716167449951\n",
      "Step: 9500  \tValid loss: 0.5218166708946228\n",
      "Step: 9600  \tTraining loss: 0.5630726218223572\n",
      "Step: 9600  \tTraining accuracy: 0.7370309829711914\n",
      "Step: 9600  \tValid loss: 0.5217288136482239\n",
      "Step: 9700  \tTraining loss: 0.5630419254302979\n",
      "Step: 9700  \tTraining accuracy: 0.7370890974998474\n",
      "Step: 9700  \tValid loss: 0.521683931350708\n",
      "Step: 9800  \tTraining loss: 0.5630099773406982\n",
      "Step: 9800  \tTraining accuracy: 0.7371460795402527\n",
      "Step: 9800  \tValid loss: 0.5216286778450012\n",
      "Step: 9900  \tTraining loss: 0.5629791617393494\n",
      "Step: 9900  \tTraining accuracy: 0.7372018694877625\n",
      "Step: 9900  \tValid loss: 0.5215710997581482\n",
      "Step: 10000  \tTraining loss: 0.5629488229751587\n",
      "Step: 10000  \tTraining accuracy: 0.7372565269470215\n",
      "Step: 10000  \tValid loss: 0.5215131044387817\n",
      "Step: 10100  \tTraining loss: 0.5629192590713501\n",
      "Step: 10100  \tTraining accuracy: 0.7373101115226746\n",
      "Step: 10100  \tValid loss: 0.5214427709579468\n",
      "Step: 10200  \tTraining loss: 0.5628906488418579\n",
      "Step: 10200  \tTraining accuracy: 0.7373626232147217\n",
      "Step: 10200  \tValid loss: 0.521402895450592\n",
      "Step: 10300  \tTraining loss: 0.562861979007721\n",
      "Step: 10300  \tTraining accuracy: 0.7374141216278076\n",
      "Step: 10300  \tValid loss: 0.5213334560394287\n",
      "Step: 10400  \tTraining loss: 0.562833309173584\n",
      "Step: 10400  \tTraining accuracy: 0.7374646067619324\n",
      "Step: 10400  \tValid loss: 0.521280825138092\n",
      "Step: 10500  \tTraining loss: 0.5628060102462769\n",
      "Step: 10500  \tTraining accuracy: 0.7375141382217407\n",
      "Step: 10500  \tValid loss: 0.5212298035621643\n",
      "Step: 10600  \tTraining loss: 0.5627800822257996\n",
      "Step: 10600  \tTraining accuracy: 0.7375627160072327\n",
      "Step: 10600  \tValid loss: 0.5211867690086365\n",
      "Step: 10700  \tTraining loss: 0.5626916885375977\n",
      "Step: 10700  \tTraining accuracy: 0.737610399723053\n",
      "Step: 10700  \tValid loss: 0.5210752487182617\n",
      "Step: 10800  \tTraining loss: 0.5626433491706848\n",
      "Step: 10800  \tTraining accuracy: 0.7376571893692017\n",
      "Step: 10800  \tValid loss: 0.5211288928985596\n",
      "Step: 10900  \tTraining loss: 0.5626027584075928\n",
      "Step: 10900  \tTraining accuracy: 0.7377031445503235\n",
      "Step: 10900  \tValid loss: 0.5211179852485657\n",
      "Step: 11000  \tTraining loss: 0.5625682473182678\n",
      "Step: 11000  \tTraining accuracy: 0.7377482056617737\n",
      "Step: 11000  \tValid loss: 0.521080493927002\n",
      "Step: 11100  \tTraining loss: 0.5625370144844055\n",
      "Step: 11100  \tTraining accuracy: 0.7377924919128418\n",
      "Step: 11100  \tValid loss: 0.521025538444519\n",
      "Step: 11200  \tTraining loss: 0.5625057816505432\n",
      "Step: 11200  \tTraining accuracy: 0.7378360033035278\n",
      "Step: 11200  \tValid loss: 0.5209753513336182\n",
      "Step: 11300  \tTraining loss: 0.5624756813049316\n",
      "Step: 11300  \tTraining accuracy: 0.737878680229187\n",
      "Step: 11300  \tValid loss: 0.5209327936172485\n",
      "Step: 11400  \tTraining loss: 0.5624452829360962\n",
      "Step: 11400  \tTraining accuracy: 0.7379206418991089\n",
      "Step: 11400  \tValid loss: 0.5208738446235657\n",
      "Step: 11500  \tTraining loss: 0.5624151229858398\n",
      "Step: 11500  \tTraining accuracy: 0.7379618883132935\n",
      "Step: 11500  \tValid loss: 0.5208317637443542\n",
      "Step: 11600  \tTraining loss: 0.5623866319656372\n",
      "Step: 11600  \tTraining accuracy: 0.7380024194717407\n",
      "Step: 11600  \tValid loss: 0.5207939743995667\n",
      "Step: 11700  \tTraining loss: 0.5623593330383301\n",
      "Step: 11700  \tTraining accuracy: 0.7380422353744507\n",
      "Step: 11700  \tValid loss: 0.5207535624504089\n",
      "Step: 11800  \tTraining loss: 0.5623301863670349\n",
      "Step: 11800  \tTraining accuracy: 0.7380777597427368\n",
      "Step: 11800  \tValid loss: 0.5206882357597351\n",
      "Step: 11900  \tTraining loss: 0.5623030066490173\n",
      "Step: 11900  \tTraining accuracy: 0.73811274766922\n",
      "Step: 11900  \tValid loss: 0.5206370949745178\n",
      "Step: 12000  \tTraining loss: 0.5622752904891968\n",
      "Step: 12000  \tTraining accuracy: 0.7381470799446106\n",
      "Step: 12000  \tValid loss: 0.5205997228622437\n",
      "Step: 12100  \tTraining loss: 0.5622497797012329\n",
      "Step: 12100  \tTraining accuracy: 0.7381808757781982\n",
      "Step: 12100  \tValid loss: 0.5205574035644531\n",
      "Step: 12200  \tTraining loss: 0.562224805355072\n",
      "Step: 12200  \tTraining accuracy: 0.7382141351699829\n",
      "Step: 12200  \tValid loss: 0.520514726638794\n",
      "Step: 12300  \tTraining loss: 0.5622003674507141\n",
      "Step: 12300  \tTraining accuracy: 0.7382468581199646\n",
      "Step: 12300  \tValid loss: 0.5204640626907349\n",
      "Step: 12400  \tTraining loss: 0.5621756911277771\n",
      "Step: 12400  \tTraining accuracy: 0.7382790446281433\n",
      "Step: 12400  \tValid loss: 0.5204144716262817\n",
      "Step: 12500  \tTraining loss: 0.5621522068977356\n",
      "Step: 12500  \tTraining accuracy: 0.738310694694519\n",
      "Step: 12500  \tValid loss: 0.5203849077224731\n",
      "Step: 12600  \tTraining loss: 0.5621281862258911\n",
      "Step: 12600  \tTraining accuracy: 0.7383418679237366\n",
      "Step: 12600  \tValid loss: 0.5203370451927185\n",
      "Step: 12700  \tTraining loss: 0.5621044039726257\n",
      "Step: 12700  \tTraining accuracy: 0.7383725047111511\n",
      "Step: 12700  \tValid loss: 0.520283043384552\n",
      "Step: 12800  \tTraining loss: 0.5620824098587036\n",
      "Step: 12800  \tTraining accuracy: 0.7384026646614075\n",
      "Step: 12800  \tValid loss: 0.5202392339706421\n",
      "Step: 12900  \tTraining loss: 0.562058687210083\n",
      "Step: 12900  \tTraining accuracy: 0.7384324073791504\n",
      "Step: 12900  \tValid loss: 0.5201877355575562\n",
      "Step: 13000  \tTraining loss: 0.5620366334915161\n",
      "Step: 13000  \tTraining accuracy: 0.7384616136550903\n",
      "Step: 13000  \tValid loss: 0.5201601386070251\n",
      "Step: 13100  \tTraining loss: 0.562014639377594\n",
      "Step: 13100  \tTraining accuracy: 0.7384904623031616\n",
      "Step: 13100  \tValid loss: 0.5201143622398376\n",
      "Step: 13200  \tTraining loss: 0.5619932413101196\n",
      "Step: 13200  \tTraining accuracy: 0.7385188341140747\n",
      "Step: 13200  \tValid loss: 0.5200613141059875\n",
      "Step: 13300  \tTraining loss: 0.561972975730896\n",
      "Step: 13300  \tTraining accuracy: 0.7385467290878296\n",
      "Step: 13300  \tValid loss: 0.5200235843658447\n",
      "Step: 13400  \tTraining loss: 0.5619526505470276\n",
      "Step: 13400  \tTraining accuracy: 0.7385742664337158\n",
      "Step: 13400  \tValid loss: 0.5199777483940125\n",
      "Step: 13500  \tTraining loss: 0.5619321465492249\n",
      "Step: 13500  \tTraining accuracy: 0.7386013865470886\n",
      "Step: 13500  \tValid loss: 0.5199453234672546\n",
      "Step: 13600  \tTraining loss: 0.5619120001792908\n",
      "Step: 13600  \tTraining accuracy: 0.738628089427948\n",
      "Step: 13600  \tValid loss: 0.5198969841003418\n",
      "Step: 13700  \tTraining loss: 0.5618920922279358\n",
      "Step: 13700  \tTraining accuracy: 0.738654375076294\n",
      "Step: 13700  \tValid loss: 0.519855797290802\n",
      "Step: 13800  \tTraining loss: 0.5618721842765808\n",
      "Step: 13800  \tTraining accuracy: 0.7386803030967712\n",
      "Step: 13800  \tValid loss: 0.5198314785957336\n",
      "Step: 13900  \tTraining loss: 0.5618524551391602\n",
      "Step: 13900  \tTraining accuracy: 0.7387058734893799\n",
      "Step: 13900  \tValid loss: 0.5197858810424805\n",
      "Step: 14000  \tTraining loss: 0.5618336200714111\n",
      "Step: 14000  \tTraining accuracy: 0.7387310862541199\n",
      "Step: 14000  \tValid loss: 0.5197466611862183\n",
      "Step: 14100  \tTraining loss: 0.5618150234222412\n",
      "Step: 14100  \tTraining accuracy: 0.7387559413909912\n",
      "Step: 14100  \tValid loss: 0.5197259783744812\n",
      "Step: 14200  \tTraining loss: 0.561796247959137\n",
      "Step: 14200  \tTraining accuracy: 0.7387833595275879\n",
      "Step: 14200  \tValid loss: 0.5196887850761414\n",
      "Step: 14300  \tTraining loss: 0.5617775917053223\n",
      "Step: 14300  \tTraining accuracy: 0.7388104200363159\n",
      "Step: 14300  \tValid loss: 0.5196449756622314\n",
      "Step: 14400  \tTraining loss: 0.5617590546607971\n",
      "Step: 14400  \tTraining accuracy: 0.7388371229171753\n",
      "Step: 14400  \tValid loss: 0.5196237564086914\n",
      "Step: 14500  \tTraining loss: 0.5617407560348511\n",
      "Step: 14500  \tTraining accuracy: 0.738863468170166\n",
      "Step: 14500  \tValid loss: 0.5195858478546143\n",
      "Step: 14600  \tTraining loss: 0.5617233514785767\n",
      "Step: 14600  \tTraining accuracy: 0.7388893961906433\n",
      "Step: 14600  \tValid loss: 0.5195671916007996\n",
      "Step: 14700  \tTraining loss: 0.5617049932479858\n",
      "Step: 14700  \tTraining accuracy: 0.7389150261878967\n",
      "Step: 14700  \tValid loss: 0.5195209383964539\n",
      "Step: 14800  \tTraining loss: 0.5616875290870667\n",
      "Step: 14800  \tTraining accuracy: 0.7389402985572815\n",
      "Step: 14800  \tValid loss: 0.5194990634918213\n",
      "Step: 14900  \tTraining loss: 0.5616702437400818\n",
      "Step: 14900  \tTraining accuracy: 0.7389652132987976\n",
      "Step: 14900  \tValid loss: 0.5194773077964783\n",
      "Step: 15000  \tTraining loss: 0.5616527199745178\n",
      "Step: 15000  \tTraining accuracy: 0.7389897704124451\n",
      "Step: 15000  \tValid loss: 0.5194311141967773\n",
      "Step: 15100  \tTraining loss: 0.5616358518600464\n",
      "Step: 15100  \tTraining accuracy: 0.7390140295028687\n",
      "Step: 15100  \tValid loss: 0.5194122195243835\n",
      "Step: 15200  \tTraining loss: 0.5616183280944824\n",
      "Step: 15200  \tTraining accuracy: 0.7390379905700684\n",
      "Step: 15200  \tValid loss: 0.5193755030632019\n",
      "Step: 15300  \tTraining loss: 0.5616015791893005\n",
      "Step: 15300  \tTraining accuracy: 0.7390615940093994\n",
      "Step: 15300  \tValid loss: 0.5193483233451843\n",
      "Step: 15400  \tTraining loss: 0.561585009098053\n",
      "Step: 15400  \tTraining accuracy: 0.7390849590301514\n",
      "Step: 15400  \tValid loss: 0.5193226337432861\n",
      "Step: 15500  \tTraining loss: 0.5615683197975159\n",
      "Step: 15500  \tTraining accuracy: 0.7391079664230347\n",
      "Step: 15500  \tValid loss: 0.5192855000495911\n",
      "Step: 15600  \tTraining loss: 0.5615519881248474\n",
      "Step: 15600  \tTraining accuracy: 0.7391306757926941\n",
      "Step: 15600  \tValid loss: 0.5192723870277405\n",
      "Step: 15700  \tTraining loss: 0.561535656452179\n",
      "Step: 15700  \tTraining accuracy: 0.7391530871391296\n",
      "Step: 15700  \tValid loss: 0.5192334055900574\n",
      "Step: 15800  \tTraining loss: 0.5615195631980896\n",
      "Step: 15800  \tTraining accuracy: 0.7391752600669861\n",
      "Step: 15800  \tValid loss: 0.5192183256149292\n",
      "Step: 15900  \tTraining loss: 0.5615034699440002\n",
      "Step: 15900  \tTraining accuracy: 0.7391971349716187\n",
      "Step: 15900  \tValid loss: 0.5191871523857117\n",
      "Step: 16000  \tTraining loss: 0.561488151550293\n",
      "Step: 16000  \tTraining accuracy: 0.7392187118530273\n",
      "Step: 16000  \tValid loss: 0.5191725492477417\n",
      "Step: 16100  \tTraining loss: 0.5614725947380066\n",
      "Step: 16100  \tTraining accuracy: 0.7392400503158569\n",
      "Step: 16100  \tValid loss: 0.5191390514373779\n",
      "Step: 16200  \tTraining loss: 0.5614565014839172\n",
      "Step: 16200  \tTraining accuracy: 0.7392610907554626\n",
      "Step: 16200  \tValid loss: 0.5191153287887573\n",
      "Step: 16300  \tTraining loss: 0.5614414811134338\n",
      "Step: 16300  \tTraining accuracy: 0.7392818927764893\n",
      "Step: 16300  \tValid loss: 0.5191036462783813\n",
      "Step: 16400  \tTraining loss: 0.5614252686500549\n",
      "Step: 16400  \tTraining accuracy: 0.739302396774292\n",
      "Step: 16400  \tValid loss: 0.5190666913986206\n",
      "Step: 16500  \tTraining loss: 0.5614112615585327\n",
      "Step: 16500  \tTraining accuracy: 0.7393227219581604\n",
      "Step: 16500  \tValid loss: 0.5190592408180237\n",
      "Step: 16600  \tTraining loss: 0.561396598815918\n",
      "Step: 16600  \tTraining accuracy: 0.7393427491188049\n",
      "Step: 16600  \tValid loss: 0.5190163850784302\n",
      "Step: 16700  \tTraining loss: 0.5613824725151062\n",
      "Step: 16700  \tTraining accuracy: 0.7393625974655151\n",
      "Step: 16700  \tValid loss: 0.5189864039421082\n",
      "Step: 16800  \tTraining loss: 0.5613694190979004\n",
      "Step: 16800  \tTraining accuracy: 0.7393821477890015\n",
      "Step: 16800  \tValid loss: 0.5189583897590637\n",
      "Step: 16900  \tTraining loss: 0.5613560080528259\n",
      "Step: 16900  \tTraining accuracy: 0.7394014596939087\n",
      "Step: 16900  \tValid loss: 0.5189409255981445\n",
      "Step: 17000  \tTraining loss: 0.5613426566123962\n",
      "Step: 17000  \tTraining accuracy: 0.7394205927848816\n",
      "Step: 17000  \tValid loss: 0.5189029574394226\n",
      "Step: 17100  \tTraining loss: 0.5613293051719666\n",
      "Step: 17100  \tTraining accuracy: 0.7394394874572754\n",
      "Step: 17100  \tValid loss: 0.5188811421394348\n",
      "Step: 17200  \tTraining loss: 0.5613167881965637\n",
      "Step: 17200  \tTraining accuracy: 0.7394581437110901\n",
      "Step: 17200  \tValid loss: 0.518869936466217\n",
      "Step: 17300  \tTraining loss: 0.5613042712211609\n",
      "Step: 17300  \tTraining accuracy: 0.7394766211509705\n",
      "Step: 17300  \tValid loss: 0.5188502073287964\n",
      "Step: 17400  \tTraining loss: 0.5612913370132446\n",
      "Step: 17400  \tTraining accuracy: 0.7394948601722717\n",
      "Step: 17400  \tValid loss: 0.5188338756561279\n",
      "Step: 17500  \tTraining loss: 0.5612785220146179\n",
      "Step: 17500  \tTraining accuracy: 0.7395128607749939\n",
      "Step: 17500  \tValid loss: 0.518803596496582\n",
      "Step: 17600  \tTraining loss: 0.5612664818763733\n",
      "Step: 17600  \tTraining accuracy: 0.7395306825637817\n",
      "Step: 17600  \tValid loss: 0.5187744498252869\n",
      "Step: 17700  \tTraining loss: 0.5612537860870361\n",
      "Step: 17700  \tTraining accuracy: 0.7395483255386353\n",
      "Step: 17700  \tValid loss: 0.5187567472457886\n",
      "Step: 17800  \tTraining loss: 0.5612418055534363\n",
      "Step: 17800  \tTraining accuracy: 0.7395657300949097\n",
      "Step: 17800  \tValid loss: 0.5187358856201172\n",
      "Step: 17900  \tTraining loss: 0.5612297654151917\n",
      "Step: 17900  \tTraining accuracy: 0.7395829558372498\n",
      "Step: 17900  \tValid loss: 0.5187077522277832\n",
      "Step: 18000  \tTraining loss: 0.5612184405326843\n",
      "Step: 18000  \tTraining accuracy: 0.7396000027656555\n",
      "Step: 18000  \tValid loss: 0.5186901092529297\n",
      "Step: 18100  \tTraining loss: 0.5612068176269531\n",
      "Step: 18100  \tTraining accuracy: 0.739616870880127\n",
      "Step: 18100  \tValid loss: 0.5186766386032104\n",
      "Step: 18200  \tTraining loss: 0.5611953139305115\n",
      "Step: 18200  \tTraining accuracy: 0.7396335005760193\n",
      "Step: 18200  \tValid loss: 0.5186594724655151\n",
      "Step: 18300  \tTraining loss: 0.5611845254898071\n",
      "Step: 18300  \tTraining accuracy: 0.7396500110626221\n",
      "Step: 18300  \tValid loss: 0.5186505913734436\n",
      "Step: 18400  \tTraining loss: 0.5611726641654968\n",
      "Step: 18400  \tTraining accuracy: 0.7396662831306458\n",
      "Step: 18400  \tValid loss: 0.518620491027832\n",
      "Step: 18500  \tTraining loss: 0.561161458492279\n",
      "Step: 18500  \tTraining accuracy: 0.7396824359893799\n",
      "Step: 18500  \tValid loss: 0.5186057090759277\n",
      "Step: 18600  \tTraining loss: 0.5611497163772583\n",
      "Step: 18600  \tTraining accuracy: 0.7396983504295349\n",
      "Step: 18600  \tValid loss: 0.5185744762420654\n",
      "Step: 18700  \tTraining loss: 0.5611388087272644\n",
      "Step: 18700  \tTraining accuracy: 0.7397141456604004\n",
      "Step: 18700  \tValid loss: 0.5185636878013611\n",
      "Step: 18800  \tTraining loss: 0.5611281394958496\n",
      "Step: 18800  \tTraining accuracy: 0.7397297620773315\n",
      "Step: 18800  \tValid loss: 0.5185325145721436\n",
      "Step: 18900  \tTraining loss: 0.5611176490783691\n",
      "Step: 18900  \tTraining accuracy: 0.7397451996803284\n",
      "Step: 18900  \tValid loss: 0.5185374617576599\n",
      "Step: 19000  \tTraining loss: 0.5611070990562439\n",
      "Step: 19000  \tTraining accuracy: 0.7397604584693909\n",
      "Step: 19000  \tValid loss: 0.5185206532478333\n",
      "Step: 19100  \tTraining loss: 0.56109619140625\n",
      "Step: 19100  \tTraining accuracy: 0.7397755980491638\n",
      "Step: 19100  \tValid loss: 0.5184901356697083\n",
      "Step: 19200  \tTraining loss: 0.5610854625701904\n",
      "Step: 19200  \tTraining accuracy: 0.7397905588150024\n",
      "Step: 19200  \tValid loss: 0.5184772610664368\n",
      "Step: 19300  \tTraining loss: 0.5610752105712891\n",
      "Step: 19300  \tTraining accuracy: 0.7398053407669067\n",
      "Step: 19300  \tValid loss: 0.518467903137207\n",
      "Step: 19400  \tTraining loss: 0.5610648989677429\n",
      "Step: 19400  \tTraining accuracy: 0.7398200035095215\n",
      "Step: 19400  \tValid loss: 0.5184483528137207\n",
      "Step: 19500  \tTraining loss: 0.5610545873641968\n",
      "Step: 19500  \tTraining accuracy: 0.7398344874382019\n",
      "Step: 19500  \tValid loss: 0.5184295177459717\n",
      "Step: 19600  \tTraining loss: 0.5610443949699402\n",
      "Step: 19600  \tTraining accuracy: 0.7398488521575928\n",
      "Step: 19600  \tValid loss: 0.5184167623519897\n",
      "Step: 19700  \tTraining loss: 0.5610343217849731\n",
      "Step: 19700  \tTraining accuracy: 0.7398630976676941\n",
      "Step: 19700  \tValid loss: 0.5184148550033569\n",
      "Step: 19800  \tTraining loss: 0.5610241889953613\n",
      "Step: 19800  \tTraining accuracy: 0.7398771643638611\n",
      "Step: 19800  \tValid loss: 0.518386960029602\n",
      "Step: 19900  \tTraining loss: 0.5610144734382629\n",
      "Step: 19900  \tTraining accuracy: 0.7398910522460938\n",
      "Step: 19900  \tValid loss: 0.5183700919151306\n",
      "Step: 20000  \tTraining loss: 0.561004638671875\n",
      "Step: 20000  \tTraining accuracy: 0.7399048209190369\n",
      "Step: 20000  \tValid loss: 0.5183526277542114\n",
      "Step: 20100  \tTraining loss: 0.5609941482543945\n",
      "Step: 20100  \tTraining accuracy: 0.7399184703826904\n",
      "Step: 20100  \tValid loss: 0.5183355212211609\n",
      "Step: 20200  \tTraining loss: 0.5609846711158752\n",
      "Step: 20200  \tTraining accuracy: 0.7399320006370544\n",
      "Step: 20200  \tValid loss: 0.518323540687561\n",
      "Step: 20300  \tTraining loss: 0.560974657535553\n",
      "Step: 20300  \tTraining accuracy: 0.7399454116821289\n",
      "Step: 20300  \tValid loss: 0.5182996392250061\n",
      "Step: 20400  \tTraining loss: 0.5609654188156128\n",
      "Step: 20400  \tTraining accuracy: 0.739958643913269\n",
      "Step: 20400  \tValid loss: 0.5182880163192749\n",
      "Step: 20500  \tTraining loss: 0.5609552264213562\n",
      "Step: 20500  \tTraining accuracy: 0.7399717569351196\n",
      "Step: 20500  \tValid loss: 0.5182772278785706\n",
      "Step: 20600  \tTraining loss: 0.5609460473060608\n",
      "Step: 20600  \tTraining accuracy: 0.7399847507476807\n",
      "Step: 20600  \tValid loss: 0.5182602405548096\n",
      "Step: 20700  \tTraining loss: 0.560936450958252\n",
      "Step: 20700  \tTraining accuracy: 0.7399976253509521\n",
      "Step: 20700  \tValid loss: 0.5182468891143799\n",
      "Step: 20800  \tTraining loss: 0.5609269738197327\n",
      "Step: 20800  \tTraining accuracy: 0.7400103211402893\n",
      "Step: 20800  \tValid loss: 0.5182356834411621\n",
      "Step: 20900  \tTraining loss: 0.5609177947044373\n",
      "Step: 20900  \tTraining accuracy: 0.7400229573249817\n",
      "Step: 20900  \tValid loss: 0.5182324051856995\n",
      "Step: 21000  \tTraining loss: 0.5609083771705627\n",
      "Step: 21000  \tTraining accuracy: 0.7400354743003845\n",
      "Step: 21000  \tValid loss: 0.5182235240936279\n",
      "Step: 21100  \tTraining loss: 0.5608994364738464\n",
      "Step: 21100  \tTraining accuracy: 0.740047812461853\n",
      "Step: 21100  \tValid loss: 0.5182213187217712\n",
      "Step: 21200  \tTraining loss: 0.5608896017074585\n",
      "Step: 21200  \tTraining accuracy: 0.7400600910186768\n",
      "Step: 21200  \tValid loss: 0.5181975364685059\n",
      "Step: 21300  \tTraining loss: 0.5608801245689392\n",
      "Step: 21300  \tTraining accuracy: 0.7400722503662109\n",
      "Step: 21300  \tValid loss: 0.5182002186775208\n",
      "Step: 21400  \tTraining loss: 0.5608707070350647\n",
      "Step: 21400  \tTraining accuracy: 0.7400842905044556\n",
      "Step: 21400  \tValid loss: 0.5181849598884583\n",
      "Step: 21500  \tTraining loss: 0.5608614683151245\n",
      "Step: 21500  \tTraining accuracy: 0.7400961518287659\n",
      "Step: 21500  \tValid loss: 0.5181806683540344\n",
      "Step: 21600  \tTraining loss: 0.5608522295951843\n",
      "Step: 21600  \tTraining accuracy: 0.7401080131530762\n",
      "Step: 21600  \tValid loss: 0.5181775689125061\n",
      "Step: 21700  \tTraining loss: 0.5608426332473755\n",
      "Step: 21700  \tTraining accuracy: 0.7401196956634521\n",
      "Step: 21700  \tValid loss: 0.518163800239563\n",
      "Step: 21800  \tTraining loss: 0.5608335137367249\n",
      "Step: 21800  \tTraining accuracy: 0.7401312589645386\n",
      "Step: 21800  \tValid loss: 0.5181449055671692\n",
      "Step: 21900  \tTraining loss: 0.5608243346214294\n",
      "Step: 21900  \tTraining accuracy: 0.7401427626609802\n",
      "Step: 21900  \tValid loss: 0.5181342363357544\n",
      "Step: 22000  \tTraining loss: 0.5608152747154236\n",
      "Step: 22000  \tTraining accuracy: 0.7401541471481323\n",
      "Step: 22000  \tValid loss: 0.5181316137313843\n",
      "Step: 22100  \tTraining loss: 0.5608059763908386\n",
      "Step: 22100  \tTraining accuracy: 0.7401654124259949\n",
      "Step: 22100  \tValid loss: 0.5181188583374023\n",
      "Step: 22200  \tTraining loss: 0.5607970952987671\n",
      "Step: 22200  \tTraining accuracy: 0.7401766180992126\n",
      "Step: 22200  \tValid loss: 0.5181059241294861\n",
      "Step: 22300  \tTraining loss: 0.560788631439209\n",
      "Step: 22300  \tTraining accuracy: 0.7401877045631409\n",
      "Step: 22300  \tValid loss: 0.5181096792221069\n",
      "Step: 22400  \tTraining loss: 0.5607791543006897\n",
      "Step: 22400  \tTraining accuracy: 0.7401986718177795\n",
      "Step: 22400  \tValid loss: 0.5180975794792175\n",
      "Step: 22500  \tTraining loss: 0.5607708096504211\n",
      "Step: 22500  \tTraining accuracy: 0.7402095198631287\n",
      "Step: 22500  \tValid loss: 0.5180855989456177\n",
      "Step: 22600  \tTraining loss: 0.5607608556747437\n",
      "Step: 22600  \tTraining accuracy: 0.740220308303833\n",
      "Step: 22600  \tValid loss: 0.5180613994598389\n",
      "Step: 22700  \tTraining loss: 0.5607523918151855\n",
      "Step: 22700  \tTraining accuracy: 0.7402309775352478\n",
      "Step: 22700  \tValid loss: 0.5180590152740479\n",
      "Step: 22800  \tTraining loss: 0.5607433319091797\n",
      "Step: 22800  \tTraining accuracy: 0.7402415871620178\n",
      "Step: 22800  \tValid loss: 0.5180511474609375\n",
      "Step: 22900  \tTraining loss: 0.5607353448867798\n",
      "Step: 22900  \tTraining accuracy: 0.7402520775794983\n",
      "Step: 22900  \tValid loss: 0.5180619955062866\n",
      "Step: 23000  \tTraining loss: 0.5607263445854187\n",
      "Step: 23000  \tTraining accuracy: 0.740262508392334\n",
      "Step: 23000  \tValid loss: 0.518035888671875\n",
      "Step: 23100  \tTraining loss: 0.5607175827026367\n",
      "Step: 23100  \tTraining accuracy: 0.7402728199958801\n",
      "Step: 23100  \tValid loss: 0.5180370211601257\n",
      "Step: 23200  \tTraining loss: 0.5607090592384338\n",
      "Step: 23200  \tTraining accuracy: 0.7402830719947815\n",
      "Step: 23200  \tValid loss: 0.518028736114502\n",
      "Step: 23300  \tTraining loss: 0.5606998205184937\n",
      "Step: 23300  \tTraining accuracy: 0.7402932047843933\n",
      "Step: 23300  \tValid loss: 0.5179930925369263\n",
      "Step: 23400  \tTraining loss: 0.5606914758682251\n",
      "Step: 23400  \tTraining accuracy: 0.7403032183647156\n",
      "Step: 23400  \tValid loss: 0.5179926753044128\n",
      "Step: 23500  \tTraining loss: 0.560683012008667\n",
      "Step: 23500  \tTraining accuracy: 0.7403132319450378\n",
      "Step: 23500  \tValid loss: 0.5179974436759949\n",
      "Step: 23600  \tTraining loss: 0.5606746673583984\n",
      "Step: 23600  \tTraining accuracy: 0.7403230667114258\n",
      "Step: 23600  \tValid loss: 0.5179868936538696\n",
      "Step: 23700  \tTraining loss: 0.5606656074523926\n",
      "Step: 23700  \tTraining accuracy: 0.7403329014778137\n",
      "Step: 23700  \tValid loss: 0.5179786086082458\n",
      "Step: 23800  \tTraining loss: 0.5606573820114136\n",
      "Step: 23800  \tTraining accuracy: 0.7403426170349121\n",
      "Step: 23800  \tValid loss: 0.5179710984230042\n",
      "Step: 23900  \tTraining loss: 0.5606495141983032\n",
      "Step: 23900  \tTraining accuracy: 0.7403522729873657\n",
      "Step: 23900  \tValid loss: 0.5179857611656189\n",
      "Step: 24000  \tTraining loss: 0.5606411099433899\n",
      "Step: 24000  \tTraining accuracy: 0.7403618097305298\n",
      "Step: 24000  \tValid loss: 0.5179730653762817\n",
      "Step: 24100  \tTraining loss: 0.5606323480606079\n",
      "Step: 24100  \tTraining accuracy: 0.7403712868690491\n",
      "Step: 24100  \tValid loss: 0.5179517269134521\n",
      "Step: 24200  \tTraining loss: 0.5606237649917603\n",
      "Step: 24200  \tTraining accuracy: 0.7403807044029236\n",
      "Step: 24200  \tValid loss: 0.5179484486579895\n",
      "Step: 24300  \tTraining loss: 0.5606155395507812\n",
      "Step: 24300  \tTraining accuracy: 0.7403900027275085\n",
      "Step: 24300  \tValid loss: 0.5179424285888672\n",
      "Step: 24400  \tTraining loss: 0.560607373714447\n",
      "Step: 24400  \tTraining accuracy: 0.7403992414474487\n",
      "Step: 24400  \tValid loss: 0.5179287791252136\n",
      "Step: 24500  \tTraining loss: 0.560599148273468\n",
      "Step: 24500  \tTraining accuracy: 0.7404084205627441\n",
      "Step: 24500  \tValid loss: 0.5179217457771301\n",
      "Step: 24600  \tTraining loss: 0.5605908036231995\n",
      "Step: 24600  \tTraining accuracy: 0.74041748046875\n",
      "Step: 24600  \tValid loss: 0.5179236531257629\n",
      "Step: 24700  \tTraining loss: 0.5605828166007996\n",
      "Step: 24700  \tTraining accuracy: 0.7404265403747559\n",
      "Step: 24700  \tValid loss: 0.5179136395454407\n",
      "Step: 24800  \tTraining loss: 0.560574471950531\n",
      "Step: 24800  \tTraining accuracy: 0.7404354810714722\n",
      "Step: 24800  \tValid loss: 0.5179014205932617\n",
      "Step: 24900  \tTraining loss: 0.560566246509552\n",
      "Step: 24900  \tTraining accuracy: 0.7404443621635437\n",
      "Step: 24900  \tValid loss: 0.5179027318954468\n",
      "Step: 25000  \tTraining loss: 0.5605579018592834\n",
      "Step: 25000  \tTraining accuracy: 0.7404531240463257\n",
      "Step: 25000  \tValid loss: 0.5178793668746948\n",
      "Step: 25100  \tTraining loss: 0.5605498552322388\n",
      "Step: 25100  \tTraining accuracy: 0.7404618859291077\n",
      "Step: 25100  \tValid loss: 0.517878532409668\n",
      "Step: 25200  \tTraining loss: 0.5605421662330627\n",
      "Step: 25200  \tTraining accuracy: 0.7404705286026001\n",
      "Step: 25200  \tValid loss: 0.517879843711853\n",
      "Step: 25300  \tTraining loss: 0.560534656047821\n",
      "Step: 25300  \tTraining accuracy: 0.7404791712760925\n",
      "Step: 25300  \tValid loss: 0.5178929567337036\n",
      "Step: 25400  \tTraining loss: 0.5605263710021973\n",
      "Step: 25400  \tTraining accuracy: 0.7404876947402954\n",
      "Step: 25400  \tValid loss: 0.5178743600845337\n",
      "Step: 25500  \tTraining loss: 0.5605178475379944\n",
      "Step: 25500  \tTraining accuracy: 0.7404961585998535\n",
      "Step: 25500  \tValid loss: 0.5178529620170593\n",
      "Step: 25600  \tTraining loss: 0.5605100393295288\n",
      "Step: 25600  \tTraining accuracy: 0.7405045032501221\n",
      "Step: 25600  \tValid loss: 0.5178562998771667\n",
      "Step: 25700  \tTraining loss: 0.5605019330978394\n",
      "Step: 25700  \tTraining accuracy: 0.7405128479003906\n",
      "Step: 25700  \tValid loss: 0.5178481340408325\n",
      "Step: 25800  \tTraining loss: 0.5604938268661499\n",
      "Step: 25800  \tTraining accuracy: 0.7405211329460144\n",
      "Step: 25800  \tValid loss: 0.5178358554840088\n",
      "Step: 25900  \tTraining loss: 0.5604864358901978\n",
      "Step: 25900  \tTraining accuracy: 0.7405292987823486\n",
      "Step: 25900  \tValid loss: 0.517837405204773\n",
      "Step: 26000  \tTraining loss: 0.5604783892631531\n",
      "Step: 26000  \tTraining accuracy: 0.7405374646186829\n",
      "Step: 26000  \tValid loss: 0.517836332321167\n",
      "Step: 26100  \tTraining loss: 0.5604705810546875\n",
      "Step: 26100  \tTraining accuracy: 0.7405455112457275\n",
      "Step: 26100  \tValid loss: 0.5178341865539551\n",
      "Step: 26200  \tTraining loss: 0.5604631304740906\n",
      "Step: 26200  \tTraining accuracy: 0.7405535578727722\n",
      "Step: 26200  \tValid loss: 0.5178293585777283\n",
      "Step: 26300  \tTraining loss: 0.5604550838470459\n",
      "Step: 26300  \tTraining accuracy: 0.7405614852905273\n",
      "Step: 26300  \tValid loss: 0.5178161263465881\n",
      "Step: 26400  \tTraining loss: 0.5604469776153564\n",
      "Step: 26400  \tTraining accuracy: 0.7405694127082825\n",
      "Step: 26400  \tValid loss: 0.5178095698356628\n",
      "Step: 26500  \tTraining loss: 0.5604397058486938\n",
      "Step: 26500  \tTraining accuracy: 0.740577220916748\n",
      "Step: 26500  \tValid loss: 0.5178160071372986\n",
      "Step: 26600  \tTraining loss: 0.5604313015937805\n",
      "Step: 26600  \tTraining accuracy: 0.7405849695205688\n",
      "Step: 26600  \tValid loss: 0.5177836418151855\n",
      "Step: 26700  \tTraining loss: 0.5604239702224731\n",
      "Step: 26700  \tTraining accuracy: 0.7405927181243896\n",
      "Step: 26700  \tValid loss: 0.5177950263023376\n",
      "Step: 26800  \tTraining loss: 0.5604164600372314\n",
      "Step: 26800  \tTraining accuracy: 0.7406003475189209\n",
      "Step: 26800  \tValid loss: 0.5178024768829346\n",
      "Step: 26900  \tTraining loss: 0.560408890247345\n",
      "Step: 26900  \tTraining accuracy: 0.7406079769134521\n",
      "Step: 26900  \tValid loss: 0.5178089141845703\n",
      "Step: 27000  \tTraining loss: 0.5604010224342346\n",
      "Step: 27000  \tTraining accuracy: 0.7406154870986938\n",
      "Step: 27000  \tValid loss: 0.517784595489502\n",
      "Step: 27100  \tTraining loss: 0.5603938698768616\n",
      "Step: 27100  \tTraining accuracy: 0.7406229972839355\n",
      "Step: 27100  \tValid loss: 0.5177892446517944\n",
      "Step: 27200  \tTraining loss: 0.5603860020637512\n",
      "Step: 27200  \tTraining accuracy: 0.7406304478645325\n",
      "Step: 27200  \tValid loss: 0.5177757740020752\n",
      "Step: 27300  \tTraining loss: 0.5603784918785095\n",
      "Step: 27300  \tTraining accuracy: 0.7406377792358398\n",
      "Step: 27300  \tValid loss: 0.5177714824676514\n",
      "Step: 27400  \tTraining loss: 0.5603707432746887\n",
      "Step: 27400  \tTraining accuracy: 0.7406451106071472\n",
      "Step: 27400  \tValid loss: 0.5177664160728455\n",
      "Step: 27500  \tTraining loss: 0.5603633522987366\n",
      "Step: 27500  \tTraining accuracy: 0.7406523823738098\n",
      "Step: 27500  \tValid loss: 0.5177723169326782\n",
      "Step: 27600  \tTraining loss: 0.5603556632995605\n",
      "Step: 27600  \tTraining accuracy: 0.7406595945358276\n",
      "Step: 27600  \tValid loss: 0.5177658796310425\n",
      "Step: 27700  \tTraining loss: 0.5603489279747009\n",
      "Step: 27700  \tTraining accuracy: 0.7406668066978455\n",
      "Step: 27700  \tValid loss: 0.5177887082099915\n",
      "Step: 27800  \tTraining loss: 0.5603413581848145\n",
      "Step: 27800  \tTraining accuracy: 0.7406738996505737\n",
      "Step: 27800  \tValid loss: 0.5177627801895142\n",
      "Step: 27900  \tTraining loss: 0.5603341460227966\n",
      "Step: 27900  \tTraining accuracy: 0.7406809329986572\n",
      "Step: 27900  \tValid loss: 0.5177668333053589\n",
      "Step: 28000  \tTraining loss: 0.5603265166282654\n",
      "Step: 28000  \tTraining accuracy: 0.7406879663467407\n",
      "Step: 28000  \tValid loss: 0.5177639126777649\n",
      "Step: 28100  \tTraining loss: 0.5603188276290894\n",
      "Step: 28100  \tTraining accuracy: 0.7406949400901794\n",
      "Step: 28100  \tValid loss: 0.5177469253540039\n",
      "Step: 28200  \tTraining loss: 0.5603116154670715\n",
      "Step: 28200  \tTraining accuracy: 0.7407018542289734\n",
      "Step: 28200  \tValid loss: 0.5177472829818726\n",
      "Step: 28300  \tTraining loss: 0.5603040456771851\n",
      "Step: 28300  \tTraining accuracy: 0.7407087087631226\n",
      "Step: 28300  \tValid loss: 0.5177303552627563\n",
      "Step: 28400  \tTraining loss: 0.560297429561615\n",
      "Step: 28400  \tTraining accuracy: 0.740715503692627\n",
      "Step: 28400  \tValid loss: 0.5177419185638428\n",
      "Step: 28500  \tTraining loss: 0.5602900981903076\n",
      "Step: 28500  \tTraining accuracy: 0.7407222986221313\n",
      "Step: 28500  \tValid loss: 0.517750084400177\n",
      "Step: 28600  \tTraining loss: 0.5602832436561584\n",
      "Step: 28600  \tTraining accuracy: 0.740729033946991\n",
      "Step: 28600  \tValid loss: 0.5177477598190308\n",
      "Step: 28700  \tTraining loss: 0.5602759122848511\n",
      "Step: 28700  \tTraining accuracy: 0.7407357096672058\n",
      "Step: 28700  \tValid loss: 0.517746090888977\n",
      "Step: 28800  \tTraining loss: 0.5602684020996094\n",
      "Step: 28800  \tTraining accuracy: 0.7407423257827759\n",
      "Step: 28800  \tValid loss: 0.5177311897277832\n",
      "Step: 28900  \tTraining loss: 0.5602612495422363\n",
      "Step: 28900  \tTraining accuracy: 0.7407488822937012\n",
      "Step: 28900  \tValid loss: 0.5177280306816101\n",
      "Step: 29000  \tTraining loss: 0.5602543354034424\n",
      "Step: 29000  \tTraining accuracy: 0.7407554388046265\n",
      "Step: 29000  \tValid loss: 0.5177227258682251\n",
      "Step: 29100  \tTraining loss: 0.5602465271949768\n",
      "Step: 29100  \tTraining accuracy: 0.740761935710907\n",
      "Step: 29100  \tValid loss: 0.5177199840545654\n",
      "Step: 29200  \tTraining loss: 0.5602400302886963\n",
      "Step: 29200  \tTraining accuracy: 0.7407683730125427\n",
      "Step: 29200  \tValid loss: 0.5177280306816101\n",
      "Step: 29300  \tTraining loss: 0.5602326989173889\n",
      "Step: 29300  \tTraining accuracy: 0.7407747507095337\n",
      "Step: 29300  \tValid loss: 0.5177177786827087\n",
      "Step: 29400  \tTraining loss: 0.5602256059646606\n",
      "Step: 29400  \tTraining accuracy: 0.7407811284065247\n",
      "Step: 29400  \tValid loss: 0.5177026987075806\n",
      "Step: 29500  \tTraining loss: 0.5602185726165771\n",
      "Step: 29500  \tTraining accuracy: 0.7407874464988708\n",
      "Step: 29500  \tValid loss: 0.517712414264679\n",
      "Step: 29600  \tTraining loss: 0.5602110028266907\n",
      "Step: 29600  \tTraining accuracy: 0.7407937049865723\n",
      "Step: 29600  \tValid loss: 0.5176918506622314\n",
      "Step: 29700  \tTraining loss: 0.5602038502693176\n",
      "Step: 29700  \tTraining accuracy: 0.7407999634742737\n",
      "Step: 29700  \tValid loss: 0.5176929235458374\n",
      "Step: 29800  \tTraining loss: 0.5601980090141296\n",
      "Step: 29800  \tTraining accuracy: 0.7408061027526855\n",
      "Step: 29800  \tValid loss: 0.5177090764045715\n",
      "Step: 29900  \tTraining loss: 0.5601904988288879\n",
      "Step: 29900  \tTraining accuracy: 0.7408123016357422\n",
      "Step: 29900  \tValid loss: 0.5176867842674255\n",
      "Step: 30000  \tTraining loss: 0.5601838827133179\n",
      "Step: 30000  \tTraining accuracy: 0.7408183813095093\n",
      "Step: 30000  \tValid loss: 0.5176871418952942\n",
      "Step: 30100  \tTraining loss: 0.5601770877838135\n",
      "Step: 30100  \tTraining accuracy: 0.7408244609832764\n",
      "Step: 30100  \tValid loss: 0.5176932215690613\n",
      "Step: 30200  \tTraining loss: 0.5601698160171509\n",
      "Step: 30200  \tTraining accuracy: 0.7408304810523987\n",
      "Step: 30200  \tValid loss: 0.5176920890808105\n",
      "Step: 30300  \tTraining loss: 0.5601629018783569\n",
      "Step: 30300  \tTraining accuracy: 0.7408364415168762\n",
      "Step: 30300  \tValid loss: 0.5176723003387451\n",
      "Step: 30400  \tTraining loss: 0.5601557493209839\n",
      "Step: 30400  \tTraining accuracy: 0.7408424019813538\n",
      "Step: 30400  \tValid loss: 0.5176671743392944\n",
      "Step: 30500  \tTraining loss: 0.5601497292518616\n",
      "Step: 30500  \tTraining accuracy: 0.7408483028411865\n",
      "Step: 30500  \tValid loss: 0.5176831483840942\n",
      "Step: 30600  \tTraining loss: 0.5601427555084229\n",
      "Step: 30600  \tTraining accuracy: 0.7408542037010193\n",
      "Step: 30600  \tValid loss: 0.5176796317100525\n",
      "Step: 30700  \tTraining loss: 0.560136079788208\n",
      "Step: 30700  \tTraining accuracy: 0.7408599853515625\n",
      "Step: 30700  \tValid loss: 0.5176759362220764\n",
      "Step: 30800  \tTraining loss: 0.560128927230835\n",
      "Step: 30800  \tTraining accuracy: 0.7408657670021057\n",
      "Step: 30800  \tValid loss: 0.5176615118980408\n",
      "Step: 30900  \tTraining loss: 0.5601224899291992\n",
      "Step: 30900  \tTraining accuracy: 0.7408715486526489\n",
      "Step: 30900  \tValid loss: 0.5176692008972168\n",
      "Step: 31000  \tTraining loss: 0.5601152181625366\n",
      "Step: 31000  \tTraining accuracy: 0.7408772706985474\n",
      "Step: 31000  \tValid loss: 0.5176622271537781\n",
      "Step: 31100  \tTraining loss: 0.5601082444190979\n",
      "Step: 31100  \tTraining accuracy: 0.740882933139801\n",
      "Step: 31100  \tValid loss: 0.5176441669464111\n",
      "Step: 31200  \tTraining loss: 0.5601019859313965\n",
      "Step: 31200  \tTraining accuracy: 0.7408885955810547\n",
      "Step: 31200  \tValid loss: 0.5176540613174438\n",
      "Step: 31300  \tTraining loss: 0.5600947141647339\n",
      "Step: 31300  \tTraining accuracy: 0.7408941984176636\n",
      "Step: 31300  \tValid loss: 0.517632246017456\n",
      "Step: 31400  \tTraining loss: 0.5600880980491638\n",
      "Step: 31400  \tTraining accuracy: 0.7408997416496277\n",
      "Step: 31400  \tValid loss: 0.517632007598877\n",
      "Step: 31500  \tTraining loss: 0.5600813627243042\n",
      "Step: 31500  \tTraining accuracy: 0.7409052848815918\n",
      "Step: 31500  \tValid loss: 0.5176305770874023\n",
      "Step: 31600  \tTraining loss: 0.5600755214691162\n",
      "Step: 31600  \tTraining accuracy: 0.7409108281135559\n",
      "Step: 31600  \tValid loss: 0.5176411271095276\n",
      "Step: 31700  \tTraining loss: 0.5600679516792297\n",
      "Step: 31700  \tTraining accuracy: 0.7409162521362305\n",
      "Step: 31700  \tValid loss: 0.5176343321800232\n",
      "Step: 31800  \tTraining loss: 0.5600616931915283\n",
      "Step: 31800  \tTraining accuracy: 0.740921676158905\n",
      "Step: 31800  \tValid loss: 0.5176420211791992\n",
      "Step: 31900  \tTraining loss: 0.5600547194480896\n",
      "Step: 31900  \tTraining accuracy: 0.7409271001815796\n",
      "Step: 31900  \tValid loss: 0.517634391784668\n",
      "Step: 32000  \tTraining loss: 0.5600478053092957\n",
      "Step: 32000  \tTraining accuracy: 0.7409324645996094\n",
      "Step: 32000  \tValid loss: 0.5176169872283936\n",
      "Step: 32100  \tTraining loss: 0.5600413680076599\n",
      "Step: 32100  \tTraining accuracy: 0.7409377694129944\n",
      "Step: 32100  \tValid loss: 0.5176191329956055\n",
      "Step: 32200  \tTraining loss: 0.5600348114967346\n",
      "Step: 32200  \tTraining accuracy: 0.7409430742263794\n",
      "Step: 32200  \tValid loss: 0.5176172256469727\n",
      "Step: 32300  \tTraining loss: 0.5600284337997437\n",
      "Step: 32300  \tTraining accuracy: 0.7409483790397644\n",
      "Step: 32300  \tValid loss: 0.5176101922988892\n",
      "Step: 32400  \tTraining loss: 0.5600219964981079\n",
      "Step: 32400  \tTraining accuracy: 0.7409535646438599\n",
      "Step: 32400  \tValid loss: 0.5176107287406921\n",
      "Step: 32500  \tTraining loss: 0.5600148439407349\n",
      "Step: 32500  \tTraining accuracy: 0.7409588098526001\n",
      "Step: 32500  \tValid loss: 0.5175960063934326\n",
      "Step: 32600  \tTraining loss: 0.5600088834762573\n",
      "Step: 32600  \tTraining accuracy: 0.7409639358520508\n",
      "Step: 32600  \tValid loss: 0.5176122784614563\n",
      "Step: 32700  \tTraining loss: 0.5600025653839111\n",
      "Step: 32700  \tTraining accuracy: 0.7409691214561462\n",
      "Step: 32700  \tValid loss: 0.5176118016242981\n",
      "Step: 32800  \tTraining loss: 0.5599958896636963\n",
      "Step: 32800  \tTraining accuracy: 0.7409741878509521\n",
      "Step: 32800  \tValid loss: 0.5176095962524414\n",
      "Step: 32900  \tTraining loss: 0.5599898099899292\n",
      "Step: 32900  \tTraining accuracy: 0.7409792542457581\n",
      "Step: 32900  \tValid loss: 0.5176133513450623\n",
      "Step: 33000  \tTraining loss: 0.55998295545578\n",
      "Step: 33000  \tTraining accuracy: 0.740984320640564\n",
      "Step: 33000  \tValid loss: 0.5175958871841431\n",
      "Step: 33100  \tTraining loss: 0.5599762797355652\n",
      "Step: 33100  \tTraining accuracy: 0.7409893274307251\n",
      "Step: 33100  \tValid loss: 0.5175917148590088\n",
      "Step: 33200  \tTraining loss: 0.559970498085022\n",
      "Step: 33200  \tTraining accuracy: 0.7409943342208862\n",
      "Step: 33200  \tValid loss: 0.5176042914390564\n",
      "Step: 33300  \tTraining loss: 0.559963583946228\n",
      "Step: 33300  \tTraining accuracy: 0.7409992814064026\n",
      "Step: 33300  \tValid loss: 0.5175886154174805\n",
      "Step: 33400  \tTraining loss: 0.5599575638771057\n",
      "Step: 33400  \tTraining accuracy: 0.7410041689872742\n",
      "Step: 33400  \tValid loss: 0.5175988078117371\n",
      "Step: 33500  \tTraining loss: 0.5599509477615356\n",
      "Step: 33500  \tTraining accuracy: 0.7410090565681458\n",
      "Step: 33500  \tValid loss: 0.5175888538360596\n",
      "Step: 33600  \tTraining loss: 0.5599443316459656\n",
      "Step: 33600  \tTraining accuracy: 0.7410139441490173\n",
      "Step: 33600  \tValid loss: 0.5175816416740417\n",
      "Step: 33700  \tTraining loss: 0.5599381923675537\n",
      "Step: 33700  \tTraining accuracy: 0.7410200238227844\n",
      "Step: 33700  \tValid loss: 0.5175847411155701\n",
      "Step: 33800  \tTraining loss: 0.559931755065918\n",
      "Step: 33800  \tTraining accuracy: 0.7410260438919067\n",
      "Step: 33800  \tValid loss: 0.517593502998352\n",
      "Step: 33900  \tTraining loss: 0.5599256157875061\n",
      "Step: 33900  \tTraining accuracy: 0.741032063961029\n",
      "Step: 33900  \tValid loss: 0.5175858736038208\n",
      "Step: 34000  \tTraining loss: 0.5599194169044495\n",
      "Step: 34000  \tTraining accuracy: 0.7410380840301514\n",
      "Step: 34000  \tValid loss: 0.5175865888595581\n",
      "Step: 34100  \tTraining loss: 0.5599133968353271\n",
      "Step: 34100  \tTraining accuracy: 0.7410439848899841\n",
      "Step: 34100  \tValid loss: 0.5175890326499939\n",
      "Step: 34200  \tTraining loss: 0.5599067211151123\n",
      "Step: 34200  \tTraining accuracy: 0.7410498857498169\n",
      "Step: 34200  \tValid loss: 0.5175824165344238\n",
      "Step: 34300  \tTraining loss: 0.5599004626274109\n",
      "Step: 34300  \tTraining accuracy: 0.7410557866096497\n",
      "Step: 34300  \tValid loss: 0.5175794959068298\n",
      "Step: 34400  \tTraining loss: 0.5598942041397095\n",
      "Step: 34400  \tTraining accuracy: 0.7410616278648376\n",
      "Step: 34400  \tValid loss: 0.5175767540931702\n",
      "Step: 34500  \tTraining loss: 0.5598883628845215\n",
      "Step: 34500  \tTraining accuracy: 0.7410674095153809\n",
      "Step: 34500  \tValid loss: 0.5175826549530029\n",
      "Step: 34600  \tTraining loss: 0.5598824620246887\n",
      "Step: 34600  \tTraining accuracy: 0.7410731911659241\n",
      "Step: 34600  \tValid loss: 0.5175760984420776\n",
      "Step: 34700  \tTraining loss: 0.5598756670951843\n",
      "Step: 34700  \tTraining accuracy: 0.7410789132118225\n",
      "Step: 34700  \tValid loss: 0.5175706148147583\n",
      "Step: 34800  \tTraining loss: 0.5598697662353516\n",
      "Step: 34800  \tTraining accuracy: 0.7410840392112732\n",
      "Step: 34800  \tValid loss: 0.5175826549530029\n",
      "Step: 34900  \tTraining loss: 0.5598634481430054\n",
      "Step: 34900  \tTraining accuracy: 0.7410879135131836\n",
      "Step: 34900  \tValid loss: 0.5175660252571106\n",
      "Step: 35000  \tTraining loss: 0.5598573684692383\n",
      "Step: 35000  \tTraining accuracy: 0.7410917282104492\n",
      "Step: 35000  \tValid loss: 0.5175668001174927\n",
      "Step: 35100  \tTraining loss: 0.5598515272140503\n",
      "Step: 35100  \tTraining accuracy: 0.7410955429077148\n",
      "Step: 35100  \tValid loss: 0.517581045627594\n",
      "Step: 35200  \tTraining loss: 0.5598452687263489\n",
      "Step: 35200  \tTraining accuracy: 0.7411023378372192\n",
      "Step: 35200  \tValid loss: 0.5175707936286926\n",
      "Step: 35300  \tTraining loss: 0.5598391890525818\n",
      "Step: 35300  \tTraining accuracy: 0.7411127090454102\n",
      "Step: 35300  \tValid loss: 0.5175701379776001\n",
      "Step: 35400  \tTraining loss: 0.5598331093788147\n",
      "Step: 35400  \tTraining accuracy: 0.7411229610443115\n",
      "Step: 35400  \tValid loss: 0.5175725221633911\n",
      "Step: 35500  \tTraining loss: 0.5598273277282715\n",
      "Step: 35500  \tTraining accuracy: 0.7411331534385681\n",
      "Step: 35500  \tValid loss: 0.5175707340240479\n",
      "Step: 35600  \tTraining loss: 0.5598212480545044\n",
      "Step: 35600  \tTraining accuracy: 0.7411433458328247\n",
      "Step: 35600  \tValid loss: 0.5175646543502808\n",
      "Step: 35700  \tTraining loss: 0.5598151683807373\n",
      "Step: 35700  \tTraining accuracy: 0.7411534190177917\n",
      "Step: 35700  \tValid loss: 0.5175673961639404\n",
      "Step: 35800  \tTraining loss: 0.5598094463348389\n",
      "Step: 35800  \tTraining accuracy: 0.741163432598114\n",
      "Step: 35800  \tValid loss: 0.517566978931427\n",
      "Step: 35900  \tTraining loss: 0.5598032474517822\n",
      "Step: 35900  \tTraining accuracy: 0.7411734461784363\n",
      "Step: 35900  \tValid loss: 0.5175676941871643\n",
      "Step: 36000  \tTraining loss: 0.5597972273826599\n",
      "Step: 36000  \tTraining accuracy: 0.7411834001541138\n",
      "Step: 36000  \tValid loss: 0.5175665616989136\n",
      "Step: 36100  \tTraining loss: 0.5597913265228271\n",
      "Step: 36100  \tTraining accuracy: 0.7411932349205017\n",
      "Step: 36100  \tValid loss: 0.5175634026527405\n",
      "Step: 36200  \tTraining loss: 0.5597853660583496\n",
      "Step: 36200  \tTraining accuracy: 0.7412030696868896\n",
      "Step: 36200  \tValid loss: 0.517553985118866\n",
      "Step: 36300  \tTraining loss: 0.5597796440124512\n",
      "Step: 36300  \tTraining accuracy: 0.7412128448486328\n",
      "Step: 36300  \tValid loss: 0.5175555348396301\n",
      "Step: 36400  \tTraining loss: 0.5597735643386841\n",
      "Step: 36400  \tTraining accuracy: 0.7412225604057312\n",
      "Step: 36400  \tValid loss: 0.5175594091415405\n",
      "Step: 36500  \tTraining loss: 0.559768557548523\n",
      "Step: 36500  \tTraining accuracy: 0.7412322163581848\n",
      "Step: 36500  \tValid loss: 0.517579972743988\n",
      "Step: 36600  \tTraining loss: 0.5597624182701111\n",
      "Step: 36600  \tTraining accuracy: 0.7412418127059937\n",
      "Step: 36600  \tValid loss: 0.5175658464431763\n",
      "Step: 36700  \tTraining loss: 0.5597561001777649\n",
      "Step: 36700  \tTraining accuracy: 0.7412513494491577\n",
      "Step: 36700  \tValid loss: 0.5175618529319763\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.7412609\n",
      "Precision: 0.71348315\n",
      "Recall: 0.5319372\n",
      "F1 score: 0.63320667\n",
      "AUC: 0.694289\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.741261   0.713483  0.531937  0.633207  0.694289  0.559752      0.741281   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.517543       0.741266    0.51445      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  36762.0  \n",
      "31\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_73632/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(1160, 3)\n",
      "(1160, 1)\n",
      "(624, 3)\n",
      "(624, 1)\n",
      "(507, 3)\n",
      "(507, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.5964584946632385\n",
      "Step: 100  \tTraining accuracy: 0.7491379380226135\n",
      "Step: 100  \tValid loss: 0.6109042763710022\n",
      "Step: 200  \tTraining loss: 0.4831898510456085\n",
      "Step: 200  \tTraining accuracy: 0.7638365626335144\n",
      "Step: 200  \tValid loss: 0.5149510502815247\n",
      "Step: 300  \tTraining loss: 0.40740662813186646\n",
      "Step: 300  \tTraining accuracy: 0.7819575071334839\n",
      "Step: 300  \tValid loss: 0.4444252550601959\n",
      "Step: 400  \tTraining loss: 0.35982006788253784\n",
      "Step: 400  \tTraining accuracy: 0.7952197194099426\n",
      "Step: 400  \tValid loss: 0.39747554063796997\n",
      "Step: 500  \tTraining loss: 0.33436891436576843\n",
      "Step: 500  \tTraining accuracy: 0.8057923316955566\n",
      "Step: 500  \tValid loss: 0.3725281059741974\n",
      "Step: 600  \tTraining loss: 0.3210790753364563\n",
      "Step: 600  \tTraining accuracy: 0.8140309453010559\n",
      "Step: 600  \tValid loss: 0.3596944510936737\n",
      "Step: 700  \tTraining loss: 0.31324851512908936\n",
      "Step: 700  \tTraining accuracy: 0.8210116624832153\n",
      "Step: 700  \tValid loss: 0.3520135283470154\n",
      "Step: 800  \tTraining loss: 0.3078603148460388\n",
      "Step: 800  \tTraining accuracy: 0.8262487649917603\n",
      "Step: 800  \tValid loss: 0.3464091718196869\n",
      "Step: 900  \tTraining loss: 0.30367806553840637\n",
      "Step: 900  \tTraining accuracy: 0.8311781883239746\n",
      "Step: 900  \tValid loss: 0.3416956663131714\n",
      "Step: 1000  \tTraining loss: 0.3001941442489624\n",
      "Step: 1000  \tTraining accuracy: 0.8354378342628479\n",
      "Step: 1000  \tValid loss: 0.33743584156036377\n",
      "Step: 1100  \tTraining loss: 0.29719239473342896\n",
      "Step: 1100  \tTraining accuracy: 0.8390942811965942\n",
      "Step: 1100  \tValid loss: 0.3334871828556061\n",
      "Step: 1200  \tTraining loss: 0.29458513855934143\n",
      "Step: 1200  \tTraining accuracy: 0.8425704836845398\n",
      "Step: 1200  \tValid loss: 0.32985231280326843\n",
      "Step: 1300  \tTraining loss: 0.2922044098377228\n",
      "Step: 1300  \tTraining accuracy: 0.8455256223678589\n",
      "Step: 1300  \tValid loss: 0.3265635669231415\n",
      "Step: 1400  \tTraining loss: 0.2901187241077423\n",
      "Step: 1400  \tTraining accuracy: 0.8479462265968323\n",
      "Step: 1400  \tValid loss: 0.32357245683670044\n",
      "Step: 1500  \tTraining loss: 0.28823763132095337\n",
      "Step: 1500  \tTraining accuracy: 0.8500331044197083\n",
      "Step: 1500  \tValid loss: 0.3209827244281769\n",
      "Step: 1600  \tTraining loss: 0.2864995300769806\n",
      "Step: 1600  \tTraining accuracy: 0.8518508076667786\n",
      "Step: 1600  \tValid loss: 0.3185921609401703\n",
      "Step: 1700  \tTraining loss: 0.2851918637752533\n",
      "Step: 1700  \tTraining accuracy: 0.8534482717514038\n",
      "Step: 1700  \tValid loss: 0.3166709840297699\n",
      "Step: 1800  \tTraining loss: 0.28405430912971497\n",
      "Step: 1800  \tTraining accuracy: 0.8548632264137268\n",
      "Step: 1800  \tValid loss: 0.31511178612709045\n",
      "Step: 1900  \tTraining loss: 0.2830139100551605\n",
      "Step: 1900  \tTraining accuracy: 0.8561252951622009\n",
      "Step: 1900  \tValid loss: 0.31374993920326233\n",
      "Step: 2000  \tTraining loss: 0.2820843458175659\n",
      "Step: 2000  \tTraining accuracy: 0.8572579622268677\n",
      "Step: 2000  \tValid loss: 0.31238648295402527\n",
      "Step: 2100  \tTraining loss: 0.2812211513519287\n",
      "Step: 2100  \tTraining accuracy: 0.8583652377128601\n",
      "Step: 2100  \tValid loss: 0.31107816100120544\n",
      "Step: 2200  \tTraining loss: 0.2804236114025116\n",
      "Step: 2200  \tTraining accuracy: 0.8594710826873779\n",
      "Step: 2200  \tValid loss: 0.30989113450050354\n",
      "Step: 2300  \tTraining loss: 0.27968183159828186\n",
      "Step: 2300  \tTraining accuracy: 0.8605368137359619\n",
      "Step: 2300  \tValid loss: 0.30881285667419434\n",
      "Step: 2400  \tTraining loss: 0.2789963185787201\n",
      "Step: 2400  \tTraining accuracy: 0.8615861535072327\n",
      "Step: 2400  \tValid loss: 0.30780118703842163\n",
      "Step: 2500  \tTraining loss: 0.2783689796924591\n",
      "Step: 2500  \tTraining accuracy: 0.862549901008606\n",
      "Step: 2500  \tValid loss: 0.3068602681159973\n",
      "Step: 2600  \tTraining loss: 0.2777879238128662\n",
      "Step: 2600  \tTraining accuracy: 0.8634380102157593\n",
      "Step: 2600  \tValid loss: 0.3059786856174469\n",
      "Step: 2700  \tTraining loss: 0.27723321318626404\n",
      "Step: 2700  \tTraining accuracy: 0.8642591238021851\n",
      "Step: 2700  \tValid loss: 0.30515822768211365\n",
      "Step: 2800  \tTraining loss: 0.2766968309879303\n",
      "Step: 2800  \tTraining accuracy: 0.8650522828102112\n",
      "Step: 2800  \tValid loss: 0.30439117550849915\n",
      "Step: 2900  \tTraining loss: 0.2761717736721039\n",
      "Step: 2900  \tTraining accuracy: 0.8658050894737244\n",
      "Step: 2900  \tValid loss: 0.3036755323410034\n",
      "Step: 3000  \tTraining loss: 0.27565035223960876\n",
      "Step: 3000  \tTraining accuracy: 0.8665068745613098\n",
      "Step: 3000  \tValid loss: 0.3029894530773163\n",
      "Step: 3100  \tTraining loss: 0.2751246988773346\n",
      "Step: 3100  \tTraining accuracy: 0.8671627044677734\n",
      "Step: 3100  \tValid loss: 0.30232521891593933\n",
      "Step: 3200  \tTraining loss: 0.27458521723747253\n",
      "Step: 3200  \tTraining accuracy: 0.8677768111228943\n",
      "Step: 3200  \tValid loss: 0.30167388916015625\n",
      "Step: 3300  \tTraining loss: 0.27402737736701965\n",
      "Step: 3300  \tTraining accuracy: 0.8683531880378723\n",
      "Step: 3300  \tValid loss: 0.3010919988155365\n",
      "Step: 3400  \tTraining loss: 0.2734455168247223\n",
      "Step: 3400  \tTraining accuracy: 0.8688951730728149\n",
      "Step: 3400  \tValid loss: 0.3005578815937042\n",
      "Step: 3500  \tTraining loss: 0.2728561460971832\n",
      "Step: 3500  \tTraining accuracy: 0.8695322275161743\n",
      "Step: 3500  \tValid loss: 0.3000450134277344\n",
      "Step: 3600  \tTraining loss: 0.272216260433197\n",
      "Step: 3600  \tTraining accuracy: 0.8701702356338501\n",
      "Step: 3600  \tValid loss: 0.2996082901954651\n",
      "Step: 3700  \tTraining loss: 0.2715294361114502\n",
      "Step: 3700  \tTraining accuracy: 0.8707733750343323\n",
      "Step: 3700  \tValid loss: 0.29923051595687866\n",
      "Step: 3800  \tTraining loss: 0.27081093192100525\n",
      "Step: 3800  \tTraining accuracy: 0.871344268321991\n",
      "Step: 3800  \tValid loss: 0.29889780282974243\n",
      "Step: 3900  \tTraining loss: 0.2700766623020172\n",
      "Step: 3900  \tTraining accuracy: 0.8718855381011963\n",
      "Step: 3900  \tValid loss: 0.29858872294425964\n",
      "Step: 4000  \tTraining loss: 0.26934075355529785\n",
      "Step: 4000  \tTraining accuracy: 0.8723994493484497\n",
      "Step: 4000  \tValid loss: 0.2982529401779175\n",
      "Step: 4100  \tTraining loss: 0.2686086595058441\n",
      "Step: 4100  \tTraining accuracy: 0.872887909412384\n",
      "Step: 4100  \tValid loss: 0.2979086935520172\n",
      "Step: 4200  \tTraining loss: 0.2678911089897156\n",
      "Step: 4200  \tTraining accuracy: 0.8733528852462769\n",
      "Step: 4200  \tValid loss: 0.2975850999355316\n",
      "Step: 4300  \tTraining loss: 0.26718270778656006\n",
      "Step: 4300  \tTraining accuracy: 0.8737959861755371\n",
      "Step: 4300  \tValid loss: 0.29721471667289734\n",
      "Step: 4400  \tTraining loss: 0.26649096608161926\n",
      "Step: 4400  \tTraining accuracy: 0.8742187023162842\n",
      "Step: 4400  \tValid loss: 0.29684996604919434\n",
      "Step: 4500  \tTraining loss: 0.2658241391181946\n",
      "Step: 4500  \tTraining accuracy: 0.8746224045753479\n",
      "Step: 4500  \tValid loss: 0.29652079939842224\n",
      "Step: 4600  \tTraining loss: 0.26517197489738464\n",
      "Step: 4600  \tTraining accuracy: 0.8750084042549133\n",
      "Step: 4600  \tValid loss: 0.2962559759616852\n",
      "Step: 4700  \tTraining loss: 0.26455041766166687\n",
      "Step: 4700  \tTraining accuracy: 0.8753777742385864\n",
      "Step: 4700  \tValid loss: 0.2960016429424286\n",
      "Step: 4800  \tTraining loss: 0.2639561891555786\n",
      "Step: 4800  \tTraining accuracy: 0.8757315874099731\n",
      "Step: 4800  \tValid loss: 0.295818567276001\n",
      "Step: 4900  \tTraining loss: 0.263385534286499\n",
      "Step: 4900  \tTraining accuracy: 0.8760708570480347\n",
      "Step: 4900  \tValid loss: 0.2956884503364563\n",
      "Step: 5000  \tTraining loss: 0.2628408968448639\n",
      "Step: 5000  \tTraining accuracy: 0.8763963580131531\n",
      "Step: 5000  \tValid loss: 0.2955540716648102\n",
      "Step: 5100  \tTraining loss: 0.26232337951660156\n",
      "Step: 5100  \tTraining accuracy: 0.876708984375\n",
      "Step: 5100  \tValid loss: 0.29542845487594604\n",
      "Step: 5200  \tTraining loss: 0.2618301510810852\n",
      "Step: 5200  \tTraining accuracy: 0.8770095109939575\n",
      "Step: 5200  \tValid loss: 0.295295387506485\n",
      "Step: 5300  \tTraining loss: 0.2613641917705536\n",
      "Step: 5300  \tTraining accuracy: 0.8772902488708496\n",
      "Step: 5300  \tValid loss: 0.2952120900154114\n",
      "Step: 5400  \tTraining loss: 0.2609242796897888\n",
      "Step: 5400  \tTraining accuracy: 0.8775604963302612\n",
      "Step: 5400  \tValid loss: 0.29517635703086853\n",
      "Step: 5500  \tTraining loss: 0.260510116815567\n",
      "Step: 5500  \tTraining accuracy: 0.8778208494186401\n",
      "Step: 5500  \tValid loss: 0.2951565384864807\n",
      "Step: 5600  \tTraining loss: 0.26012101769447327\n",
      "Step: 5600  \tTraining accuracy: 0.8780717849731445\n",
      "Step: 5600  \tValid loss: 0.29514360427856445\n",
      "Step: 5700  \tTraining loss: 0.25975531339645386\n",
      "Step: 5700  \tTraining accuracy: 0.8783138394355774\n",
      "Step: 5700  \tValid loss: 0.29513055086135864\n",
      "Step: 5800  \tTraining loss: 0.259412944316864\n",
      "Step: 5800  \tTraining accuracy: 0.8785474896430969\n",
      "Step: 5800  \tValid loss: 0.29517650604248047\n",
      "Step: 5900  \tTraining loss: 0.2590949535369873\n",
      "Step: 5900  \tTraining accuracy: 0.8787732124328613\n",
      "Step: 5900  \tValid loss: 0.2951852083206177\n",
      "Step: 6000  \tTraining loss: 0.25879016518592834\n",
      "Step: 6000  \tTraining accuracy: 0.8789912462234497\n",
      "Step: 6000  \tValid loss: 0.29514390230178833\n",
      "Step: 6100  \tTraining loss: 0.25851428508758545\n",
      "Step: 6100  \tTraining accuracy: 0.879202127456665\n",
      "Step: 6100  \tValid loss: 0.2951238453388214\n",
      "Step: 6200  \tTraining loss: 0.2582431733608246\n",
      "Step: 6200  \tTraining accuracy: 0.8794061541557312\n",
      "Step: 6200  \tValid loss: 0.295168936252594\n",
      "Step: 6300  \tTraining loss: 0.257997065782547\n",
      "Step: 6300  \tTraining accuracy: 0.8796036243438721\n",
      "Step: 6300  \tValid loss: 0.2951388359069824\n",
      "Step: 6400  \tTraining loss: 0.2577645778656006\n",
      "Step: 6400  \tTraining accuracy: 0.8797948956489563\n",
      "Step: 6400  \tValid loss: 0.295147180557251\n",
      "Step: 6500  \tTraining loss: 0.2575475573539734\n",
      "Step: 6500  \tTraining accuracy: 0.8799802660942078\n",
      "Step: 6500  \tValid loss: 0.29515114426612854\n",
      "Step: 6600  \tTraining loss: 0.2573409378528595\n",
      "Step: 6600  \tTraining accuracy: 0.8801599144935608\n",
      "Step: 6600  \tValid loss: 0.2952122390270233\n",
      "Step: 6700  \tTraining loss: 0.257148802280426\n",
      "Step: 6700  \tTraining accuracy: 0.8803210854530334\n",
      "Step: 6700  \tValid loss: 0.2951364815235138\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8804774\n",
      "Precision: 0.8604651\n",
      "Recall: 0.86595744\n",
      "F1 score: 0.88327855\n",
      "AUC: 0.88515264\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.880477   0.860465  0.865957  0.883279  0.885153  0.257107      0.880415   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.295044       0.880384   0.283661      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  6723.0  \n",
      "32\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_29557/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(841, 3)\n",
      "(841, 1)\n",
      "(448, 3)\n",
      "(448, 1)\n",
      "(364, 3)\n",
      "(364, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6464062333106995\n",
      "Step: 100  \tTraining accuracy: 0.6492270827293396\n",
      "Step: 100  \tValid loss: 0.6872350573539734\n",
      "Step: 200  \tTraining loss: 0.6187885403633118\n",
      "Step: 200  \tTraining accuracy: 0.6339213848114014\n",
      "Step: 200  \tValid loss: 0.6640340685844421\n",
      "Step: 300  \tTraining loss: 0.5978652834892273\n",
      "Step: 300  \tTraining accuracy: 0.6460091471672058\n",
      "Step: 300  \tValid loss: 0.64536052942276\n",
      "Step: 400  \tTraining loss: 0.5746904015541077\n",
      "Step: 400  \tTraining accuracy: 0.6539655327796936\n",
      "Step: 400  \tValid loss: 0.6195782423019409\n",
      "Step: 500  \tTraining loss: 0.5420737266540527\n",
      "Step: 500  \tTraining accuracy: 0.6638937592506409\n",
      "Step: 500  \tValid loss: 0.5827093124389648\n",
      "Step: 600  \tTraining loss: 0.48920828104019165\n",
      "Step: 600  \tTraining accuracy: 0.6769163012504578\n",
      "Step: 600  \tValid loss: 0.524217963218689\n",
      "Step: 700  \tTraining loss: 0.4578297734260559\n",
      "Step: 700  \tTraining accuracy: 0.6903987526893616\n",
      "Step: 700  \tValid loss: 0.4906754493713379\n",
      "Step: 800  \tTraining loss: 0.43460947275161743\n",
      "Step: 800  \tTraining accuracy: 0.7016596794128418\n",
      "Step: 800  \tValid loss: 0.4664154350757599\n",
      "Step: 900  \tTraining loss: 0.4169481098651886\n",
      "Step: 900  \tTraining accuracy: 0.7111980319023132\n",
      "Step: 900  \tValid loss: 0.4481557309627533\n",
      "Step: 1000  \tTraining loss: 0.4031623601913452\n",
      "Step: 1000  \tTraining accuracy: 0.7198116779327393\n",
      "Step: 1000  \tValid loss: 0.43411877751350403\n",
      "Step: 1100  \tTraining loss: 0.3921932578086853\n",
      "Step: 1100  \tTraining accuracy: 0.7275344133377075\n",
      "Step: 1100  \tValid loss: 0.42303118109703064\n",
      "Step: 1200  \tTraining loss: 0.38333776593208313\n",
      "Step: 1200  \tTraining accuracy: 0.7343356013298035\n",
      "Step: 1200  \tValid loss: 0.41412416100502014\n",
      "Step: 1300  \tTraining loss: 0.3761046826839447\n",
      "Step: 1300  \tTraining accuracy: 0.7398558855056763\n",
      "Step: 1300  \tValid loss: 0.40687131881713867\n",
      "Step: 1400  \tTraining loss: 0.3701394200325012\n",
      "Step: 1400  \tTraining accuracy: 0.7450962662696838\n",
      "Step: 1400  \tValid loss: 0.40090104937553406\n",
      "Step: 1500  \tTraining loss: 0.36517956852912903\n",
      "Step: 1500  \tTraining accuracy: 0.74978107213974\n",
      "Step: 1500  \tValid loss: 0.3959421217441559\n",
      "Step: 1600  \tTraining loss: 0.3610270619392395\n",
      "Step: 1600  \tTraining accuracy: 0.7538617849349976\n",
      "Step: 1600  \tValid loss: 0.39179283380508423\n",
      "Step: 1700  \tTraining loss: 0.3575296401977539\n",
      "Step: 1700  \tTraining accuracy: 0.7575213313102722\n",
      "Step: 1700  \tValid loss: 0.38829952478408813\n",
      "Step: 1800  \tTraining loss: 0.35456839203834534\n",
      "Step: 1800  \tTraining accuracy: 0.7606247067451477\n",
      "Step: 1800  \tValid loss: 0.38534289598464966\n",
      "Step: 1900  \tTraining loss: 0.3519391715526581\n",
      "Step: 1900  \tTraining accuracy: 0.7634907960891724\n",
      "Step: 1900  \tValid loss: 0.38307687640190125\n",
      "Step: 2000  \tTraining loss: 0.34960034489631653\n",
      "Step: 2000  \tTraining accuracy: 0.7662800550460815\n",
      "Step: 2000  \tValid loss: 0.3809506893157959\n",
      "Step: 2100  \tTraining loss: 0.34756147861480713\n",
      "Step: 2100  \tTraining accuracy: 0.7687973976135254\n",
      "Step: 2100  \tValid loss: 0.37912517786026\n",
      "Step: 2200  \tTraining loss: 0.34577515721321106\n",
      "Step: 2200  \tTraining accuracy: 0.7710243463516235\n",
      "Step: 2200  \tValid loss: 0.37755322456359863\n",
      "Step: 2300  \tTraining loss: 0.3442072570323944\n",
      "Step: 2300  \tTraining accuracy: 0.772972822189331\n",
      "Step: 2300  \tValid loss: 0.37618938088417053\n",
      "Step: 2400  \tTraining loss: 0.34282395243644714\n",
      "Step: 2400  \tTraining accuracy: 0.7747040390968323\n",
      "Step: 2400  \tValid loss: 0.37501761317253113\n",
      "Step: 2500  \tTraining loss: 0.341598778963089\n",
      "Step: 2500  \tTraining accuracy: 0.7762446403503418\n",
      "Step: 2500  \tValid loss: 0.37400057911872864\n",
      "Step: 2600  \tTraining loss: 0.3405071794986725\n",
      "Step: 2600  \tTraining accuracy: 0.777688205242157\n",
      "Step: 2600  \tValid loss: 0.3731229603290558\n",
      "Step: 2700  \tTraining loss: 0.3395299017429352\n",
      "Step: 2700  \tTraining accuracy: 0.7791368961334229\n",
      "Step: 2700  \tValid loss: 0.37236061692237854\n",
      "Step: 2800  \tTraining loss: 0.3386607766151428\n",
      "Step: 2800  \tTraining accuracy: 0.7804803252220154\n",
      "Step: 2800  \tValid loss: 0.3714312016963959\n",
      "Step: 2900  \tTraining loss: 0.3378797471523285\n",
      "Step: 2900  \tTraining accuracy: 0.7817294597625732\n",
      "Step: 2900  \tValid loss: 0.3705958425998688\n",
      "Step: 3000  \tTraining loss: 0.33717283606529236\n",
      "Step: 3000  \tTraining accuracy: 0.78289395570755\n",
      "Step: 3000  \tValid loss: 0.3698318898677826\n",
      "Step: 3100  \tTraining loss: 0.336537629365921\n",
      "Step: 3100  \tTraining accuracy: 0.784001886844635\n",
      "Step: 3100  \tValid loss: 0.36911773681640625\n",
      "Step: 3200  \tTraining loss: 0.335929811000824\n",
      "Step: 3200  \tTraining accuracy: 0.7850011587142944\n",
      "Step: 3200  \tValid loss: 0.36862173676490784\n",
      "Step: 3300  \tTraining loss: 0.33485934138298035\n",
      "Step: 3300  \tTraining accuracy: 0.7859017252922058\n",
      "Step: 3300  \tValid loss: 0.3678891360759735\n",
      "Step: 3400  \tTraining loss: 0.3339728116989136\n",
      "Step: 3400  \tTraining accuracy: 0.7868026494979858\n",
      "Step: 3400  \tValid loss: 0.3669308125972748\n",
      "Step: 3500  \tTraining loss: 0.33321645855903625\n",
      "Step: 3500  \tTraining accuracy: 0.7877039909362793\n",
      "Step: 3500  \tValid loss: 0.36591213941574097\n",
      "Step: 3600  \tTraining loss: 0.3326447606086731\n",
      "Step: 3600  \tTraining accuracy: 0.7885546088218689\n",
      "Step: 3600  \tValid loss: 0.365192711353302\n",
      "Step: 3700  \tTraining loss: 0.3322046399116516\n",
      "Step: 3700  \tTraining accuracy: 0.7893088459968567\n",
      "Step: 3700  \tValid loss: 0.3647066652774811\n",
      "Step: 3800  \tTraining loss: 0.3317793011665344\n",
      "Step: 3800  \tTraining accuracy: 0.7900390028953552\n",
      "Step: 3800  \tValid loss: 0.36442121863365173\n",
      "Step: 3900  \tTraining loss: 0.33140769600868225\n",
      "Step: 3900  \tTraining accuracy: 0.7907313108444214\n",
      "Step: 3900  \tValid loss: 0.36393824219703674\n",
      "Step: 4000  \tTraining loss: 0.3310842216014862\n",
      "Step: 4000  \tTraining accuracy: 0.7913885116577148\n",
      "Step: 4000  \tValid loss: 0.36365029215812683\n",
      "Step: 4100  \tTraining loss: 0.3307829201221466\n",
      "Step: 4100  \tTraining accuracy: 0.7920132875442505\n",
      "Step: 4100  \tValid loss: 0.3633776307106018\n",
      "Step: 4200  \tTraining loss: 0.3305102586746216\n",
      "Step: 4200  \tTraining accuracy: 0.7926079034805298\n",
      "Step: 4200  \tValid loss: 0.36314916610717773\n",
      "Step: 4300  \tTraining loss: 0.33026057481765747\n",
      "Step: 4300  \tTraining accuracy: 0.7931746244430542\n",
      "Step: 4300  \tValid loss: 0.3629884719848633\n",
      "Step: 4400  \tTraining loss: 0.33002862334251404\n",
      "Step: 4400  \tTraining accuracy: 0.793715238571167\n",
      "Step: 4400  \tValid loss: 0.3629003167152405\n",
      "Step: 4500  \tTraining loss: 0.3296302258968353\n",
      "Step: 4500  \tTraining accuracy: 0.7942315936088562\n",
      "Step: 4500  \tValid loss: 0.362252801656723\n",
      "Step: 4600  \tTraining loss: 0.32936689257621765\n",
      "Step: 4600  \tTraining accuracy: 0.794725239276886\n",
      "Step: 4600  \tValid loss: 0.36197686195373535\n",
      "Step: 4700  \tTraining loss: 0.3290800154209137\n",
      "Step: 4700  \tTraining accuracy: 0.7951976656913757\n",
      "Step: 4700  \tValid loss: 0.3616822063922882\n",
      "Step: 4800  \tTraining loss: 0.3288048803806305\n",
      "Step: 4800  \tTraining accuracy: 0.7956501841545105\n",
      "Step: 4800  \tValid loss: 0.3615398406982422\n",
      "Step: 4900  \tTraining loss: 0.32859498262405396\n",
      "Step: 4900  \tTraining accuracy: 0.7960840463638306\n",
      "Step: 4900  \tValid loss: 0.36144277453422546\n",
      "Step: 5000  \tTraining loss: 0.32841944694519043\n",
      "Step: 5000  \tTraining accuracy: 0.7965003848075867\n",
      "Step: 5000  \tValid loss: 0.36142948269844055\n",
      "Step: 5100  \tTraining loss: 0.3282642662525177\n",
      "Step: 5100  \tTraining accuracy: 0.7969002723693848\n",
      "Step: 5100  \tValid loss: 0.3613244593143463\n",
      "Step: 5200  \tTraining loss: 0.328092098236084\n",
      "Step: 5200  \tTraining accuracy: 0.7972846031188965\n",
      "Step: 5200  \tValid loss: 0.361071914434433\n",
      "Step: 5300  \tTraining loss: 0.3279561698436737\n",
      "Step: 5300  \tTraining accuracy: 0.7976542711257935\n",
      "Step: 5300  \tValid loss: 0.36102062463760376\n",
      "Step: 5400  \tTraining loss: 0.3278321325778961\n",
      "Step: 5400  \tTraining accuracy: 0.7980101704597473\n",
      "Step: 5400  \tValid loss: 0.3609667122364044\n",
      "Step: 5500  \tTraining loss: 0.32771703600883484\n",
      "Step: 5500  \tTraining accuracy: 0.7983530163764954\n",
      "Step: 5500  \tValid loss: 0.36087557673454285\n",
      "Step: 5600  \tTraining loss: 0.3276095986366272\n",
      "Step: 5600  \tTraining accuracy: 0.7986834645271301\n",
      "Step: 5600  \tValid loss: 0.3607780933380127\n",
      "Step: 5700  \tTraining loss: 0.32750964164733887\n",
      "Step: 5700  \tTraining accuracy: 0.7990022301673889\n",
      "Step: 5700  \tValid loss: 0.36068281531333923\n",
      "Step: 5800  \tTraining loss: 0.3274192810058594\n",
      "Step: 5800  \tTraining accuracy: 0.7993099093437195\n",
      "Step: 5800  \tValid loss: 0.3606725335121155\n",
      "Step: 5900  \tTraining loss: 0.32733669877052307\n",
      "Step: 5900  \tTraining accuracy: 0.7996070981025696\n",
      "Step: 5900  \tValid loss: 0.36064592003822327\n",
      "Step: 6000  \tTraining loss: 0.327260285615921\n",
      "Step: 6000  \tTraining accuracy: 0.7998942732810974\n",
      "Step: 6000  \tValid loss: 0.36060768365859985\n",
      "Step: 6100  \tTraining loss: 0.3271893262863159\n",
      "Step: 6100  \tTraining accuracy: 0.800171971321106\n",
      "Step: 6100  \tValid loss: 0.3605687916278839\n",
      "Step: 6200  \tTraining loss: 0.3271239101886749\n",
      "Step: 6200  \tTraining accuracy: 0.8004406094551086\n",
      "Step: 6200  \tValid loss: 0.36051103472709656\n",
      "Step: 6300  \tTraining loss: 0.3270643949508667\n",
      "Step: 6300  \tTraining accuracy: 0.8007006645202637\n",
      "Step: 6300  \tValid loss: 0.3604583144187927\n",
      "Step: 6400  \tTraining loss: 0.32700785994529724\n",
      "Step: 6400  \tTraining accuracy: 0.8009525537490845\n",
      "Step: 6400  \tValid loss: 0.36041536927223206\n",
      "Step: 6500  \tTraining loss: 0.32695382833480835\n",
      "Step: 6500  \tTraining accuracy: 0.8011966347694397\n",
      "Step: 6500  \tValid loss: 0.36036771535873413\n",
      "Step: 6600  \tTraining loss: 0.32690224051475525\n",
      "Step: 6600  \tTraining accuracy: 0.801433265209198\n",
      "Step: 6600  \tValid loss: 0.36032193899154663\n",
      "Step: 6700  \tTraining loss: 0.326852411031723\n",
      "Step: 6700  \tTraining accuracy: 0.8016627430915833\n",
      "Step: 6700  \tValid loss: 0.36028724908828735\n",
      "Step: 6800  \tTraining loss: 0.3268042206764221\n",
      "Step: 6800  \tTraining accuracy: 0.8018854260444641\n",
      "Step: 6800  \tValid loss: 0.36024442315101624\n",
      "Step: 6900  \tTraining loss: 0.3267573416233063\n",
      "Step: 6900  \tTraining accuracy: 0.8021016120910645\n",
      "Step: 6900  \tValid loss: 0.36020609736442566\n",
      "Step: 7000  \tTraining loss: 0.32671162486076355\n",
      "Step: 7000  \tTraining accuracy: 0.8023115992546082\n",
      "Step: 7000  \tValid loss: 0.36016878485679626\n",
      "Step: 7100  \tTraining loss: 0.32666710019111633\n",
      "Step: 7100  \tTraining accuracy: 0.8025156259536743\n",
      "Step: 7100  \tValid loss: 0.3601250648498535\n",
      "Step: 7200  \tTraining loss: 0.32662320137023926\n",
      "Step: 7200  \tTraining accuracy: 0.802713930606842\n",
      "Step: 7200  \tValid loss: 0.36008551716804504\n",
      "Step: 7300  \tTraining loss: 0.3265801668167114\n",
      "Step: 7300  \tTraining accuracy: 0.8029068112373352\n",
      "Step: 7300  \tValid loss: 0.3600504398345947\n",
      "Step: 7400  \tTraining loss: 0.3265380263328552\n",
      "Step: 7400  \tTraining accuracy: 0.8030943870544434\n",
      "Step: 7400  \tValid loss: 0.3600141108036041\n",
      "Step: 7500  \tTraining loss: 0.32649627327919006\n",
      "Step: 7500  \tTraining accuracy: 0.8032769560813904\n",
      "Step: 7500  \tValid loss: 0.3599769175052643\n",
      "Step: 7600  \tTraining loss: 0.3264552652835846\n",
      "Step: 7600  \tTraining accuracy: 0.8034546971321106\n",
      "Step: 7600  \tValid loss: 0.3599434196949005\n",
      "Step: 7700  \tTraining loss: 0.32641497254371643\n",
      "Step: 7700  \tTraining accuracy: 0.8036277890205383\n",
      "Step: 7700  \tValid loss: 0.3599050045013428\n",
      "Step: 7800  \tTraining loss: 0.32637497782707214\n",
      "Step: 7800  \tTraining accuracy: 0.8037964105606079\n",
      "Step: 7800  \tValid loss: 0.3598847985267639\n",
      "Step: 7900  \tTraining loss: 0.32633528113365173\n",
      "Step: 7900  \tTraining accuracy: 0.8039606809616089\n",
      "Step: 7900  \tValid loss: 0.35986390709877014\n",
      "Step: 8000  \tTraining loss: 0.32629600167274475\n",
      "Step: 8000  \tTraining accuracy: 0.8041208982467651\n",
      "Step: 8000  \tValid loss: 0.35984039306640625\n",
      "Step: 8100  \tTraining loss: 0.3262571096420288\n",
      "Step: 8100  \tTraining accuracy: 0.8042771220207214\n",
      "Step: 8100  \tValid loss: 0.3598182201385498\n",
      "Step: 8200  \tTraining loss: 0.3262186348438263\n",
      "Step: 8200  \tTraining accuracy: 0.8044294714927673\n",
      "Step: 8200  \tValid loss: 0.3597855865955353\n",
      "Step: 8300  \tTraining loss: 0.3261803388595581\n",
      "Step: 8300  \tTraining accuracy: 0.8045781254768372\n",
      "Step: 8300  \tValid loss: 0.3597629964351654\n",
      "Step: 8400  \tTraining loss: 0.32614222168922424\n",
      "Step: 8400  \tTraining accuracy: 0.8047232627868652\n",
      "Step: 8400  \tValid loss: 0.35974010825157166\n",
      "Step: 8500  \tTraining loss: 0.32610446214675903\n",
      "Step: 8500  \tTraining accuracy: 0.8048649430274963\n",
      "Step: 8500  \tValid loss: 0.3597129285335541\n",
      "Step: 8600  \tTraining loss: 0.32606711983680725\n",
      "Step: 8600  \tTraining accuracy: 0.8050033450126648\n",
      "Step: 8600  \tValid loss: 0.3596839904785156\n",
      "Step: 8700  \tTraining loss: 0.32602980732917786\n",
      "Step: 8700  \tTraining accuracy: 0.8051384687423706\n",
      "Step: 8700  \tValid loss: 0.3596571385860443\n",
      "Step: 8800  \tTraining loss: 0.32599279284477234\n",
      "Step: 8800  \tTraining accuracy: 0.8052705526351929\n",
      "Step: 8800  \tValid loss: 0.3596363067626953\n",
      "Step: 8900  \tTraining loss: 0.3259561061859131\n",
      "Step: 8900  \tTraining accuracy: 0.8053996562957764\n",
      "Step: 8900  \tValid loss: 0.3596067428588867\n",
      "Step: 9000  \tTraining loss: 0.325919508934021\n",
      "Step: 9000  \tTraining accuracy: 0.8055258989334106\n",
      "Step: 9000  \tValid loss: 0.3595840036869049\n",
      "Step: 9100  \tTraining loss: 0.3258834183216095\n",
      "Step: 9100  \tTraining accuracy: 0.8056493401527405\n",
      "Step: 9100  \tValid loss: 0.3595530092716217\n",
      "Step: 9200  \tTraining loss: 0.32584714889526367\n",
      "Step: 9200  \tTraining accuracy: 0.8057700395584106\n",
      "Step: 9200  \tValid loss: 0.3595300316810608\n",
      "Step: 9300  \tTraining loss: 0.3258114457130432\n",
      "Step: 9300  \tTraining accuracy: 0.8058881759643555\n",
      "Step: 9300  \tValid loss: 0.35950374603271484\n",
      "Step: 9400  \tTraining loss: 0.3257758617401123\n",
      "Step: 9400  \tTraining accuracy: 0.806003749370575\n",
      "Step: 9400  \tValid loss: 0.3594748079776764\n",
      "Step: 9500  \tTraining loss: 0.32574033737182617\n",
      "Step: 9500  \tTraining accuracy: 0.8061168789863586\n",
      "Step: 9500  \tValid loss: 0.35945209860801697\n",
      "Step: 9600  \tTraining loss: 0.32570546865463257\n",
      "Step: 9600  \tTraining accuracy: 0.8062276840209961\n",
      "Step: 9600  \tValid loss: 0.35942012071609497\n",
      "Step: 9700  \tTraining loss: 0.32567042112350464\n",
      "Step: 9700  \tTraining accuracy: 0.8063361644744873\n",
      "Step: 9700  \tValid loss: 0.35939821600914\n",
      "Step: 9800  \tTraining loss: 0.32563579082489014\n",
      "Step: 9800  \tTraining accuracy: 0.806442379951477\n",
      "Step: 9800  \tValid loss: 0.35937315225601196\n",
      "Step: 9900  \tTraining loss: 0.3256012499332428\n",
      "Step: 9900  \tTraining accuracy: 0.8065465092658997\n",
      "Step: 9900  \tValid loss: 0.35935094952583313\n",
      "Step: 10000  \tTraining loss: 0.3255669176578522\n",
      "Step: 10000  \tTraining accuracy: 0.8066484928131104\n",
      "Step: 10000  \tValid loss: 0.35932427644729614\n",
      "Step: 10100  \tTraining loss: 0.32553306221961975\n",
      "Step: 10100  \tTraining accuracy: 0.8067485094070435\n",
      "Step: 10100  \tValid loss: 0.35929781198501587\n",
      "Step: 10200  \tTraining loss: 0.32549920678138733\n",
      "Step: 10200  \tTraining accuracy: 0.8068464994430542\n",
      "Step: 10200  \tValid loss: 0.3592744767665863\n",
      "Step: 10300  \tTraining loss: 0.3254653215408325\n",
      "Step: 10300  \tTraining accuracy: 0.8069425821304321\n",
      "Step: 10300  \tValid loss: 0.35926130414009094\n",
      "Step: 10400  \tTraining loss: 0.3254319429397583\n",
      "Step: 10400  \tTraining accuracy: 0.807036817073822\n",
      "Step: 10400  \tValid loss: 0.3592381179332733\n",
      "Step: 10500  \tTraining loss: 0.32539862394332886\n",
      "Step: 10500  \tTraining accuracy: 0.8071292638778687\n",
      "Step: 10500  \tValid loss: 0.35921746492385864\n",
      "Step: 10600  \tTraining loss: 0.3253655731678009\n",
      "Step: 10600  \tTraining accuracy: 0.807219922542572\n",
      "Step: 10600  \tValid loss: 0.35920268297195435\n",
      "Step: 10700  \tTraining loss: 0.3253327012062073\n",
      "Step: 10700  \tTraining accuracy: 0.8073089122772217\n",
      "Step: 10700  \tValid loss: 0.35918399691581726\n",
      "Step: 10800  \tTraining loss: 0.32530006766319275\n",
      "Step: 10800  \tTraining accuracy: 0.8073962330818176\n",
      "Step: 10800  \tValid loss: 0.3591635227203369\n",
      "Step: 10900  \tTraining loss: 0.32526764273643494\n",
      "Step: 10900  \tTraining accuracy: 0.8074819445610046\n",
      "Step: 10900  \tValid loss: 0.3591417968273163\n",
      "Step: 11000  \tTraining loss: 0.3252353072166443\n",
      "Step: 11000  \tTraining accuracy: 0.8075661063194275\n",
      "Step: 11000  \tValid loss: 0.35912156105041504\n",
      "Step: 11100  \tTraining loss: 0.3252032697200775\n",
      "Step: 11100  \tTraining accuracy: 0.8076487183570862\n",
      "Step: 11100  \tValid loss: 0.35910314321517944\n",
      "Step: 11200  \tTraining loss: 0.32517117261886597\n",
      "Step: 11200  \tTraining accuracy: 0.8077298402786255\n",
      "Step: 11200  \tValid loss: 0.35908809304237366\n",
      "Step: 11300  \tTraining loss: 0.32513946294784546\n",
      "Step: 11300  \tTraining accuracy: 0.807809591293335\n",
      "Step: 11300  \tValid loss: 0.35907426476478577\n",
      "Step: 11400  \tTraining loss: 0.3251079022884369\n",
      "Step: 11400  \tTraining accuracy: 0.807887852191925\n",
      "Step: 11400  \tValid loss: 0.35905390977859497\n",
      "Step: 11500  \tTraining loss: 0.3250764310359955\n",
      "Step: 11500  \tTraining accuracy: 0.8079648017883301\n",
      "Step: 11500  \tValid loss: 0.35903945565223694\n",
      "Step: 11600  \tTraining loss: 0.3250451982021332\n",
      "Step: 11600  \tTraining accuracy: 0.8080403804779053\n",
      "Step: 11600  \tValid loss: 0.3590224087238312\n",
      "Step: 11700  \tTraining loss: 0.3250144124031067\n",
      "Step: 11700  \tTraining accuracy: 0.8081147074699402\n",
      "Step: 11700  \tValid loss: 0.35899999737739563\n",
      "Step: 11800  \tTraining loss: 0.3249833583831787\n",
      "Step: 11800  \tTraining accuracy: 0.80818772315979\n",
      "Step: 11800  \tValid loss: 0.3589920997619629\n",
      "Step: 11900  \tTraining loss: 0.3249525725841522\n",
      "Step: 11900  \tTraining accuracy: 0.8082595467567444\n",
      "Step: 11900  \tValid loss: 0.35898175835609436\n",
      "Step: 12000  \tTraining loss: 0.32492220401763916\n",
      "Step: 12000  \tTraining accuracy: 0.8083301186561584\n",
      "Step: 12000  \tValid loss: 0.3589451014995575\n",
      "Step: 12100  \tTraining loss: 0.32489195466041565\n",
      "Step: 12100  \tTraining accuracy: 0.8083995580673218\n",
      "Step: 12100  \tValid loss: 0.358921617269516\n",
      "Step: 12200  \tTraining loss: 0.3248620629310608\n",
      "Step: 12200  \tTraining accuracy: 0.8084678649902344\n",
      "Step: 12200  \tValid loss: 0.3588934540748596\n",
      "Step: 12300  \tTraining loss: 0.3248319923877716\n",
      "Step: 12300  \tTraining accuracy: 0.8085350394248962\n",
      "Step: 12300  \tValid loss: 0.35886409878730774\n",
      "Step: 12400  \tTraining loss: 0.3248025178909302\n",
      "Step: 12400  \tTraining accuracy: 0.8086010813713074\n",
      "Step: 12400  \tValid loss: 0.3588380813598633\n",
      "Step: 12500  \tTraining loss: 0.3247731328010559\n",
      "Step: 12500  \tTraining accuracy: 0.8086661100387573\n",
      "Step: 12500  \tValid loss: 0.3588132858276367\n",
      "Step: 12600  \tTraining loss: 0.32474377751350403\n",
      "Step: 12600  \tTraining accuracy: 0.8087301254272461\n",
      "Step: 12600  \tValid loss: 0.3587936758995056\n",
      "Step: 12700  \tTraining loss: 0.3247145414352417\n",
      "Step: 12700  \tTraining accuracy: 0.8087930679321289\n",
      "Step: 12700  \tValid loss: 0.3587814271450043\n",
      "Step: 12800  \tTraining loss: 0.32468557357788086\n",
      "Step: 12800  \tTraining accuracy: 0.8088550567626953\n",
      "Step: 12800  \tValid loss: 0.3587552607059479\n",
      "Step: 12900  \tTraining loss: 0.324657142162323\n",
      "Step: 12900  \tTraining accuracy: 0.8089160919189453\n",
      "Step: 12900  \tValid loss: 0.35872626304626465\n",
      "Step: 13000  \tTraining loss: 0.3246285319328308\n",
      "Step: 13000  \tTraining accuracy: 0.8089761734008789\n",
      "Step: 13000  \tValid loss: 0.3587034046649933\n",
      "Step: 13100  \tTraining loss: 0.324599951505661\n",
      "Step: 13100  \tTraining accuracy: 0.8090353012084961\n",
      "Step: 13100  \tValid loss: 0.35868582129478455\n",
      "Step: 13200  \tTraining loss: 0.3245715796947479\n",
      "Step: 13200  \tTraining accuracy: 0.8090935945510864\n",
      "Step: 13200  \tValid loss: 0.3586726486682892\n",
      "Step: 13300  \tTraining loss: 0.32454341650009155\n",
      "Step: 13300  \tTraining accuracy: 0.8091509938240051\n",
      "Step: 13300  \tValid loss: 0.3586549758911133\n",
      "Step: 13400  \tTraining loss: 0.3245154023170471\n",
      "Step: 13400  \tTraining accuracy: 0.8092074990272522\n",
      "Step: 13400  \tValid loss: 0.35863614082336426\n",
      "Step: 13500  \tTraining loss: 0.3244876265525818\n",
      "Step: 13500  \tTraining accuracy: 0.8092631697654724\n",
      "Step: 13500  \tValid loss: 0.35861995816230774\n",
      "Step: 13600  \tTraining loss: 0.32446032762527466\n",
      "Step: 13600  \tTraining accuracy: 0.8093180060386658\n",
      "Step: 13600  \tValid loss: 0.3585955500602722\n",
      "Step: 13700  \tTraining loss: 0.32443273067474365\n",
      "Step: 13700  \tTraining accuracy: 0.809372067451477\n",
      "Step: 13700  \tValid loss: 0.3585820496082306\n",
      "Step: 13800  \tTraining loss: 0.3244054615497589\n",
      "Step: 13800  \tTraining accuracy: 0.8094253540039062\n",
      "Step: 13800  \tValid loss: 0.3585650622844696\n",
      "Step: 13900  \tTraining loss: 0.32437819242477417\n",
      "Step: 13900  \tTraining accuracy: 0.8094778656959534\n",
      "Step: 13900  \tValid loss: 0.3585518002510071\n",
      "Step: 14000  \tTraining loss: 0.32435154914855957\n",
      "Step: 14000  \tTraining accuracy: 0.8095296025276184\n",
      "Step: 14000  \tValid loss: 0.3585270941257477\n",
      "Step: 14100  \tTraining loss: 0.3243245780467987\n",
      "Step: 14100  \tTraining accuracy: 0.8095806241035461\n",
      "Step: 14100  \tValid loss: 0.3585185706615448\n",
      "Step: 14200  \tTraining loss: 0.3242979347705841\n",
      "Step: 14200  \tTraining accuracy: 0.8096308708190918\n",
      "Step: 14200  \tValid loss: 0.3584986627101898\n",
      "Step: 14300  \tTraining loss: 0.3242712914943695\n",
      "Step: 14300  \tTraining accuracy: 0.8096804618835449\n",
      "Step: 14300  \tValid loss: 0.35848861932754517\n",
      "Step: 14400  \tTraining loss: 0.324244886636734\n",
      "Step: 14400  \tTraining accuracy: 0.8097293376922607\n",
      "Step: 14400  \tValid loss: 0.3584774434566498\n",
      "Step: 14500  \tTraining loss: 0.32421875\n",
      "Step: 14500  \tTraining accuracy: 0.809777557849884\n",
      "Step: 14500  \tValid loss: 0.3584607243537903\n",
      "Step: 14600  \tTraining loss: 0.32419300079345703\n",
      "Step: 14600  \tTraining accuracy: 0.8098251223564148\n",
      "Step: 14600  \tValid loss: 0.35843971371650696\n",
      "Step: 14700  \tTraining loss: 0.32416674494743347\n",
      "Step: 14700  \tTraining accuracy: 0.809872031211853\n",
      "Step: 14700  \tValid loss: 0.3584344983100891\n",
      "Step: 14800  \tTraining loss: 0.32414114475250244\n",
      "Step: 14800  \tTraining accuracy: 0.8099182844161987\n",
      "Step: 14800  \tValid loss: 0.3584160804748535\n",
      "Step: 14900  \tTraining loss: 0.3241153657436371\n",
      "Step: 14900  \tTraining accuracy: 0.8099639415740967\n",
      "Step: 14900  \tValid loss: 0.35840654373168945\n",
      "Step: 15000  \tTraining loss: 0.32408997416496277\n",
      "Step: 15000  \tTraining accuracy: 0.8100090026855469\n",
      "Step: 15000  \tValid loss: 0.3583954870700836\n",
      "Step: 15100  \tTraining loss: 0.3240646421909332\n",
      "Step: 15100  \tTraining accuracy: 0.8100534081459045\n",
      "Step: 15100  \tValid loss: 0.3583870530128479\n",
      "Step: 15200  \tTraining loss: 0.3240395188331604\n",
      "Step: 15200  \tTraining accuracy: 0.8100972771644592\n",
      "Step: 15200  \tValid loss: 0.3583739101886749\n",
      "Step: 15300  \tTraining loss: 0.3240145146846771\n",
      "Step: 15300  \tTraining accuracy: 0.8101405501365662\n",
      "Step: 15300  \tValid loss: 0.3583751320838928\n",
      "Step: 15400  \tTraining loss: 0.32398977875709534\n",
      "Step: 15400  \tTraining accuracy: 0.8101832270622253\n",
      "Step: 15400  \tValid loss: 0.35835668444633484\n",
      "Step: 15500  \tTraining loss: 0.3239660859107971\n",
      "Step: 15500  \tTraining accuracy: 0.8102254271507263\n",
      "Step: 15500  \tValid loss: 0.35832419991493225\n",
      "Step: 15600  \tTraining loss: 0.3239407539367676\n",
      "Step: 15600  \tTraining accuracy: 0.8102670311927795\n",
      "Step: 15600  \tValid loss: 0.3583260476589203\n",
      "Step: 15700  \tTraining loss: 0.3239165246486664\n",
      "Step: 15700  \tTraining accuracy: 0.8103080987930298\n",
      "Step: 15700  \tValid loss: 0.3583128750324249\n",
      "Step: 15800  \tTraining loss: 0.32389262318611145\n",
      "Step: 15800  \tTraining accuracy: 0.8103486895561218\n",
      "Step: 15800  \tValid loss: 0.3582928478717804\n",
      "Step: 15900  \tTraining loss: 0.3238685429096222\n",
      "Step: 15900  \tTraining accuracy: 0.8103887438774109\n",
      "Step: 15900  \tValid loss: 0.35828083753585815\n",
      "Step: 16000  \tTraining loss: 0.32384467124938965\n",
      "Step: 16000  \tTraining accuracy: 0.810428261756897\n",
      "Step: 16000  \tValid loss: 0.3582707941532135\n",
      "Step: 16100  \tTraining loss: 0.3238210082054138\n",
      "Step: 16100  \tTraining accuracy: 0.8104673027992249\n",
      "Step: 16100  \tValid loss: 0.35825422406196594\n",
      "Step: 16200  \tTraining loss: 0.3237980008125305\n",
      "Step: 16200  \tTraining accuracy: 0.8105058670043945\n",
      "Step: 16200  \tValid loss: 0.35822874307632446\n",
      "Step: 16300  \tTraining loss: 0.3237742781639099\n",
      "Step: 16300  \tTraining accuracy: 0.8105440139770508\n",
      "Step: 16300  \tValid loss: 0.3582306504249573\n",
      "Step: 16400  \tTraining loss: 0.3237513303756714\n",
      "Step: 16400  \tTraining accuracy: 0.810581624507904\n",
      "Step: 16400  \tValid loss: 0.35820576548576355\n",
      "Step: 16500  \tTraining loss: 0.3237282931804657\n",
      "Step: 16500  \tTraining accuracy: 0.8106188178062439\n",
      "Step: 16500  \tValid loss: 0.3581945598125458\n",
      "Step: 16600  \tTraining loss: 0.3237055540084839\n",
      "Step: 16600  \tTraining accuracy: 0.8106555342674255\n",
      "Step: 16600  \tValid loss: 0.3581809401512146\n",
      "Step: 16700  \tTraining loss: 0.3236827850341797\n",
      "Step: 16700  \tTraining accuracy: 0.810691773891449\n",
      "Step: 16700  \tValid loss: 0.35816311836242676\n",
      "Step: 16800  \tTraining loss: 0.32366055250167847\n",
      "Step: 16800  \tTraining accuracy: 0.8107276558876038\n",
      "Step: 16800  \tValid loss: 0.35814884305000305\n",
      "Step: 16900  \tTraining loss: 0.3236377537250519\n",
      "Step: 16900  \tTraining accuracy: 0.8107630610466003\n",
      "Step: 16900  \tValid loss: 0.35814496874809265\n",
      "Step: 17000  \tTraining loss: 0.32361552119255066\n",
      "Step: 17000  \tTraining accuracy: 0.8107980489730835\n",
      "Step: 17000  \tValid loss: 0.35813087224960327\n",
      "Step: 17100  \tTraining loss: 0.32359349727630615\n",
      "Step: 17100  \tTraining accuracy: 0.810832679271698\n",
      "Step: 17100  \tValid loss: 0.3581114113330841\n",
      "Step: 17200  \tTraining loss: 0.3235715329647064\n",
      "Step: 17200  \tTraining accuracy: 0.8108668923377991\n",
      "Step: 17200  \tValid loss: 0.3581097424030304\n",
      "Step: 17300  \tTraining loss: 0.32355016469955444\n",
      "Step: 17300  \tTraining accuracy: 0.8109006881713867\n",
      "Step: 17300  \tValid loss: 0.358078271150589\n",
      "Step: 17400  \tTraining loss: 0.323528528213501\n",
      "Step: 17400  \tTraining accuracy: 0.8109340667724609\n",
      "Step: 17400  \tValid loss: 0.3580656051635742\n",
      "Step: 17500  \tTraining loss: 0.32350704073905945\n",
      "Step: 17500  \tTraining accuracy: 0.8109670877456665\n",
      "Step: 17500  \tValid loss: 0.358055055141449\n",
      "Step: 17600  \tTraining loss: 0.32348567247390747\n",
      "Step: 17600  \tTraining accuracy: 0.8109997510910034\n",
      "Step: 17600  \tValid loss: 0.3580446243286133\n",
      "Step: 17700  \tTraining loss: 0.32346445322036743\n",
      "Step: 17700  \tTraining accuracy: 0.8110319972038269\n",
      "Step: 17700  \tValid loss: 0.3580394685268402\n",
      "Step: 17800  \tTraining loss: 0.3234436511993408\n",
      "Step: 17800  \tTraining accuracy: 0.8110639452934265\n",
      "Step: 17800  \tValid loss: 0.35801807045936584\n",
      "Step: 17900  \tTraining loss: 0.3234230577945709\n",
      "Step: 17900  \tTraining accuracy: 0.8110954761505127\n",
      "Step: 17900  \tValid loss: 0.3579985201358795\n",
      "Step: 18000  \tTraining loss: 0.32340213656425476\n",
      "Step: 18000  \tTraining accuracy: 0.811126708984375\n",
      "Step: 18000  \tValid loss: 0.3579961955547333\n",
      "Step: 18100  \tTraining loss: 0.32338133454322815\n",
      "Step: 18100  \tTraining accuracy: 0.8111575245857239\n",
      "Step: 18100  \tValid loss: 0.3579932749271393\n",
      "Step: 18200  \tTraining loss: 0.3233608901500702\n",
      "Step: 18200  \tTraining accuracy: 0.8111880421638489\n",
      "Step: 18200  \tValid loss: 0.35797902941703796\n",
      "Step: 18300  \tTraining loss: 0.32334086298942566\n",
      "Step: 18300  \tTraining accuracy: 0.81121826171875\n",
      "Step: 18300  \tValid loss: 0.35796302556991577\n",
      "Step: 18400  \tTraining loss: 0.323320597410202\n",
      "Step: 18400  \tTraining accuracy: 0.8112481236457825\n",
      "Step: 18400  \tValid loss: 0.35795754194259644\n",
      "Step: 18500  \tTraining loss: 0.3233006000518799\n",
      "Step: 18500  \tTraining accuracy: 0.8112776279449463\n",
      "Step: 18500  \tValid loss: 0.35794708132743835\n",
      "Step: 18600  \tTraining loss: 0.3232806921005249\n",
      "Step: 18600  \tTraining accuracy: 0.8113068342208862\n",
      "Step: 18600  \tValid loss: 0.35794273018836975\n",
      "Step: 18700  \tTraining loss: 0.32326099276542664\n",
      "Step: 18700  \tTraining accuracy: 0.8113357424736023\n",
      "Step: 18700  \tValid loss: 0.35793033242225647\n",
      "Step: 18800  \tTraining loss: 0.3232415020465851\n",
      "Step: 18800  \tTraining accuracy: 0.8113643527030945\n",
      "Step: 18800  \tValid loss: 0.35792049765586853\n",
      "Step: 18900  \tTraining loss: 0.3232221305370331\n",
      "Step: 18900  \tTraining accuracy: 0.811392605304718\n",
      "Step: 18900  \tValid loss: 0.3579070270061493\n",
      "Step: 19000  \tTraining loss: 0.32320281863212585\n",
      "Step: 19000  \tTraining accuracy: 0.8114206194877625\n",
      "Step: 19000  \tValid loss: 0.3578965961933136\n",
      "Step: 19100  \tTraining loss: 0.32318365573883057\n",
      "Step: 19100  \tTraining accuracy: 0.811448335647583\n",
      "Step: 19100  \tValid loss: 0.35789188742637634\n",
      "Step: 19200  \tTraining loss: 0.32316479086875916\n",
      "Step: 19200  \tTraining accuracy: 0.8114756941795349\n",
      "Step: 19200  \tValid loss: 0.35786980390548706\n",
      "Step: 19300  \tTraining loss: 0.32314619421958923\n",
      "Step: 19300  \tTraining accuracy: 0.8115028142929077\n",
      "Step: 19300  \tValid loss: 0.35785531997680664\n",
      "Step: 19400  \tTraining loss: 0.3231273293495178\n",
      "Step: 19400  \tTraining accuracy: 0.8115296959877014\n",
      "Step: 19400  \tValid loss: 0.3578520119190216\n",
      "Step: 19500  \tTraining loss: 0.32310861349105835\n",
      "Step: 19500  \tTraining accuracy: 0.8115562200546265\n",
      "Step: 19500  \tValid loss: 0.35785117745399475\n",
      "Step: 19600  \tTraining loss: 0.32309025526046753\n",
      "Step: 19600  \tTraining accuracy: 0.8115825653076172\n",
      "Step: 19600  \tValid loss: 0.35783651471138\n",
      "Step: 19700  \tTraining loss: 0.3230721950531006\n",
      "Step: 19700  \tTraining accuracy: 0.8116085529327393\n",
      "Step: 19700  \tValid loss: 0.3578202724456787\n",
      "Step: 19800  \tTraining loss: 0.32305413484573364\n",
      "Step: 19800  \tTraining accuracy: 0.811634361743927\n",
      "Step: 19800  \tValid loss: 0.3578042685985565\n",
      "Step: 19900  \tTraining loss: 0.3230358958244324\n",
      "Step: 19900  \tTraining accuracy: 0.8116598129272461\n",
      "Step: 19900  \tValid loss: 0.35780200362205505\n",
      "Step: 20000  \tTraining loss: 0.3230183720588684\n",
      "Step: 20000  \tTraining accuracy: 0.8116850852966309\n",
      "Step: 20000  \tValid loss: 0.35777997970581055\n",
      "Step: 20100  \tTraining loss: 0.32300060987472534\n",
      "Step: 20100  \tTraining accuracy: 0.8117100596427917\n",
      "Step: 20100  \tValid loss: 0.3577708899974823\n",
      "Step: 20200  \tTraining loss: 0.32298290729522705\n",
      "Step: 20200  \tTraining accuracy: 0.8117348551750183\n",
      "Step: 20200  \tValid loss: 0.35776329040527344\n",
      "Step: 20300  \tTraining loss: 0.3229653239250183\n",
      "Step: 20300  \tTraining accuracy: 0.811759352684021\n",
      "Step: 20300  \tValid loss: 0.3577556312084198\n",
      "Step: 20400  \tTraining loss: 0.322948157787323\n",
      "Step: 20400  \tTraining accuracy: 0.8117836117744446\n",
      "Step: 20400  \tValid loss: 0.3577418327331543\n",
      "Step: 20500  \tTraining loss: 0.32293081283569336\n",
      "Step: 20500  \tTraining accuracy: 0.8118076324462891\n",
      "Step: 20500  \tValid loss: 0.3577386438846588\n",
      "Step: 20600  \tTraining loss: 0.32291361689567566\n",
      "Step: 20600  \tTraining accuracy: 0.8118314146995544\n",
      "Step: 20600  \tValid loss: 0.3577291965484619\n",
      "Step: 20700  \tTraining loss: 0.322896808385849\n",
      "Step: 20700  \tTraining accuracy: 0.8118549585342407\n",
      "Step: 20700  \tValid loss: 0.35771676898002625\n",
      "Step: 20800  \tTraining loss: 0.3228798508644104\n",
      "Step: 20800  \tTraining accuracy: 0.8118783235549927\n",
      "Step: 20800  \tValid loss: 0.3577133119106293\n",
      "Step: 20900  \tTraining loss: 0.3228631317615509\n",
      "Step: 20900  \tTraining accuracy: 0.8119013905525208\n",
      "Step: 20900  \tValid loss: 0.3577093482017517\n",
      "Step: 21000  \tTraining loss: 0.32284682989120483\n",
      "Step: 21000  \tTraining accuracy: 0.8119242787361145\n",
      "Step: 21000  \tValid loss: 0.3576919734477997\n",
      "Step: 21100  \tTraining loss: 0.32283031940460205\n",
      "Step: 21100  \tTraining accuracy: 0.8119469881057739\n",
      "Step: 21100  \tValid loss: 0.3576831817626953\n",
      "Step: 21200  \tTraining loss: 0.32281389832496643\n",
      "Step: 21200  \tTraining accuracy: 0.8119694590568542\n",
      "Step: 21200  \tValid loss: 0.3576766550540924\n",
      "Step: 21300  \tTraining loss: 0.32279762625694275\n",
      "Step: 21300  \tTraining accuracy: 0.8119916915893555\n",
      "Step: 21300  \tValid loss: 0.357670396566391\n",
      "Step: 21400  \tTraining loss: 0.3227818012237549\n",
      "Step: 21400  \tTraining accuracy: 0.8120136857032776\n",
      "Step: 21400  \tValid loss: 0.3576553761959076\n",
      "Step: 21500  \tTraining loss: 0.3227657079696655\n",
      "Step: 21500  \tTraining accuracy: 0.8120355606079102\n",
      "Step: 21500  \tValid loss: 0.3576517403125763\n",
      "Step: 21600  \tTraining loss: 0.3227497637271881\n",
      "Step: 21600  \tTraining accuracy: 0.8120571970939636\n",
      "Step: 21600  \tValid loss: 0.35764768719673157\n",
      "Step: 21700  \tTraining loss: 0.32273387908935547\n",
      "Step: 21700  \tTraining accuracy: 0.812078595161438\n",
      "Step: 21700  \tValid loss: 0.3576551377773285\n",
      "Step: 21800  \tTraining loss: 0.3227183222770691\n",
      "Step: 21800  \tTraining accuracy: 0.812099814414978\n",
      "Step: 21800  \tValid loss: 0.35763460397720337\n",
      "Step: 21900  \tTraining loss: 0.32270264625549316\n",
      "Step: 21900  \tTraining accuracy: 0.8121208548545837\n",
      "Step: 21900  \tValid loss: 0.3576396107673645\n",
      "Step: 22000  \tTraining loss: 0.3226873278617859\n",
      "Step: 22000  \tTraining accuracy: 0.8121417164802551\n",
      "Step: 22000  \tValid loss: 0.3576321601867676\n",
      "Step: 22100  \tTraining loss: 0.3226718604564667\n",
      "Step: 22100  \tTraining accuracy: 0.8121623992919922\n",
      "Step: 22100  \tValid loss: 0.35763126611709595\n",
      "Step: 22200  \tTraining loss: 0.32265719771385193\n",
      "Step: 22200  \tTraining accuracy: 0.8121828436851501\n",
      "Step: 22200  \tValid loss: 0.3576047718524933\n",
      "Step: 22300  \tTraining loss: 0.3226419687271118\n",
      "Step: 22300  \tTraining accuracy: 0.8122031688690186\n",
      "Step: 22300  \tValid loss: 0.3576022684574127\n",
      "Step: 22400  \tTraining loss: 0.32262685894966125\n",
      "Step: 22400  \tTraining accuracy: 0.8122232556343079\n",
      "Step: 22400  \tValid loss: 0.3576013743877411\n",
      "Step: 22500  \tTraining loss: 0.3226119875907898\n",
      "Step: 22500  \tTraining accuracy: 0.8122431635856628\n",
      "Step: 22500  \tValid loss: 0.35759714245796204\n",
      "Step: 22600  \tTraining loss: 0.3225973844528198\n",
      "Step: 22600  \tTraining accuracy: 0.8122628927230835\n",
      "Step: 22600  \tValid loss: 0.35758668184280396\n",
      "Step: 22700  \tTraining loss: 0.3225826025009155\n",
      "Step: 22700  \tTraining accuracy: 0.8122825026512146\n",
      "Step: 22700  \tValid loss: 0.3575863838195801\n",
      "Step: 22800  \tTraining loss: 0.32256844639778137\n",
      "Step: 22800  \tTraining accuracy: 0.8123018741607666\n",
      "Step: 22800  \tValid loss: 0.3575722277164459\n",
      "Step: 22900  \tTraining loss: 0.32255399227142334\n",
      "Step: 22900  \tTraining accuracy: 0.812321126461029\n",
      "Step: 22900  \tValid loss: 0.35756638646125793\n",
      "Step: 23000  \tTraining loss: 0.3225395679473877\n",
      "Step: 23000  \tTraining accuracy: 0.8123401999473572\n",
      "Step: 23000  \tValid loss: 0.35756853222846985\n",
      "Step: 23100  \tTraining loss: 0.3225252628326416\n",
      "Step: 23100  \tTraining accuracy: 0.812359094619751\n",
      "Step: 23100  \tValid loss: 0.35757094621658325\n",
      "Step: 23200  \tTraining loss: 0.3225114047527313\n",
      "Step: 23200  \tTraining accuracy: 0.8123778104782104\n",
      "Step: 23200  \tValid loss: 0.3575552999973297\n",
      "Step: 23300  \tTraining loss: 0.3224976062774658\n",
      "Step: 23300  \tTraining accuracy: 0.8123964071273804\n",
      "Step: 23300  \tValid loss: 0.35754919052124023\n",
      "Step: 23400  \tTraining loss: 0.32248374819755554\n",
      "Step: 23400  \tTraining accuracy: 0.812414824962616\n",
      "Step: 23400  \tValid loss: 0.3575454354286194\n",
      "Step: 23500  \tTraining loss: 0.3224696218967438\n",
      "Step: 23500  \tTraining accuracy: 0.8124330639839172\n",
      "Step: 23500  \tValid loss: 0.3575548529624939\n",
      "Step: 23600  \tTraining loss: 0.3224559426307678\n",
      "Step: 23600  \tTraining accuracy: 0.8124537467956543\n",
      "Step: 23600  \tValid loss: 0.3575514256954193\n",
      "Step: 23700  \tTraining loss: 0.32244259119033813\n",
      "Step: 23700  \tTraining accuracy: 0.8124793767929077\n",
      "Step: 23700  \tValid loss: 0.3575379550457001\n",
      "Step: 23800  \tTraining loss: 0.32242923974990845\n",
      "Step: 23800  \tTraining accuracy: 0.812504768371582\n",
      "Step: 23800  \tValid loss: 0.35753488540649414\n",
      "Step: 23900  \tTraining loss: 0.32241588830947876\n",
      "Step: 23900  \tTraining accuracy: 0.812529981136322\n",
      "Step: 23900  \tValid loss: 0.3575266897678375\n",
      "Step: 24000  \tTraining loss: 0.3224024772644043\n",
      "Step: 24000  \tTraining accuracy: 0.8125549554824829\n",
      "Step: 24000  \tValid loss: 0.35752639174461365\n",
      "Step: 24100  \tTraining loss: 0.3223893940448761\n",
      "Step: 24100  \tTraining accuracy: 0.8125796914100647\n",
      "Step: 24100  \tValid loss: 0.35752302408218384\n",
      "Step: 24200  \tTraining loss: 0.3223763108253479\n",
      "Step: 24200  \tTraining accuracy: 0.8126042485237122\n",
      "Step: 24200  \tValid loss: 0.35752245783805847\n",
      "Step: 24300  \tTraining loss: 0.32236334681510925\n",
      "Step: 24300  \tTraining accuracy: 0.8126286268234253\n",
      "Step: 24300  \tValid loss: 0.3575184643268585\n",
      "Step: 24400  \tTraining loss: 0.3223506510257721\n",
      "Step: 24400  \tTraining accuracy: 0.8126527667045593\n",
      "Step: 24400  \tValid loss: 0.3575117290019989\n",
      "Step: 24500  \tTraining loss: 0.32233795523643494\n",
      "Step: 24500  \tTraining accuracy: 0.812676727771759\n",
      "Step: 24500  \tValid loss: 0.35750812292099\n",
      "Step: 24600  \tTraining loss: 0.3223251700401306\n",
      "Step: 24600  \tTraining accuracy: 0.8127005100250244\n",
      "Step: 24600  \tValid loss: 0.3575151264667511\n",
      "Step: 24700  \tTraining loss: 0.3223128318786621\n",
      "Step: 24700  \tTraining accuracy: 0.8127241134643555\n",
      "Step: 24700  \tValid loss: 0.3575012981891632\n",
      "Step: 24800  \tTraining loss: 0.3223004639148712\n",
      "Step: 24800  \tTraining accuracy: 0.8127474784851074\n",
      "Step: 24800  \tValid loss: 0.3574965298175812\n",
      "Step: 24900  \tTraining loss: 0.3222881257534027\n",
      "Step: 24900  \tTraining accuracy: 0.812770664691925\n",
      "Step: 24900  \tValid loss: 0.35749727487564087\n",
      "Step: 25000  \tTraining loss: 0.32227590680122375\n",
      "Step: 25000  \tTraining accuracy: 0.8127936720848083\n",
      "Step: 25000  \tValid loss: 0.35749801993370056\n",
      "Step: 25100  \tTraining loss: 0.3222639262676239\n",
      "Step: 25100  \tTraining accuracy: 0.8128237724304199\n",
      "Step: 25100  \tValid loss: 0.3574886918067932\n",
      "Step: 25200  \tTraining loss: 0.3222517967224121\n",
      "Step: 25200  \tTraining accuracy: 0.8128607869148254\n",
      "Step: 25200  \tValid loss: 0.35748910903930664\n",
      "Step: 25300  \tTraining loss: 0.32223978638648987\n",
      "Step: 25300  \tTraining accuracy: 0.8128975629806519\n",
      "Step: 25300  \tValid loss: 0.357486754655838\n",
      "Step: 25400  \tTraining loss: 0.32222780585289\n",
      "Step: 25400  \tTraining accuracy: 0.8129340410232544\n",
      "Step: 25400  \tValid loss: 0.35748645663261414\n",
      "Step: 25500  \tTraining loss: 0.32221630215644836\n",
      "Step: 25500  \tTraining accuracy: 0.8129702210426331\n",
      "Step: 25500  \tValid loss: 0.3574800491333008\n",
      "Step: 25600  \tTraining loss: 0.3222045600414276\n",
      "Step: 25600  \tTraining accuracy: 0.8130061030387878\n",
      "Step: 25600  \tValid loss: 0.3574821650981903\n",
      "Step: 25700  \tTraining loss: 0.3221932351589203\n",
      "Step: 25700  \tTraining accuracy: 0.8130416870117188\n",
      "Step: 25700  \tValid loss: 0.35747000575065613\n",
      "Step: 25800  \tTraining loss: 0.3221816122531891\n",
      "Step: 25800  \tTraining accuracy: 0.8130770325660706\n",
      "Step: 25800  \tValid loss: 0.3574754297733307\n",
      "Step: 25900  \tTraining loss: 0.322170227766037\n",
      "Step: 25900  \tTraining accuracy: 0.8131120800971985\n",
      "Step: 25900  \tValid loss: 0.35747048258781433\n",
      "Step: 26000  \tTraining loss: 0.3221591114997864\n",
      "Step: 26000  \tTraining accuracy: 0.8131468892097473\n",
      "Step: 26000  \tValid loss: 0.3574688732624054\n",
      "Step: 26100  \tTraining loss: 0.322147935628891\n",
      "Step: 26100  \tTraining accuracy: 0.8131814002990723\n",
      "Step: 26100  \tValid loss: 0.35746675729751587\n",
      "Step: 26200  \tTraining loss: 0.32213687896728516\n",
      "Step: 26200  \tTraining accuracy: 0.8132156729698181\n",
      "Step: 26200  \tValid loss: 0.3574655055999756\n",
      "Step: 26300  \tTraining loss: 0.3221258223056793\n",
      "Step: 26300  \tTraining accuracy: 0.8132497072219849\n",
      "Step: 26300  \tValid loss: 0.35746875405311584\n",
      "Step: 26400  \tTraining loss: 0.3221150040626526\n",
      "Step: 26400  \tTraining accuracy: 0.8132834434509277\n",
      "Step: 26400  \tValid loss: 0.3574712574481964\n",
      "Step: 26500  \tTraining loss: 0.32210421562194824\n",
      "Step: 26500  \tTraining accuracy: 0.8133169412612915\n",
      "Step: 26500  \tValid loss: 0.3574755787849426\n",
      "Step: 26600  \tTraining loss: 0.3220938444137573\n",
      "Step: 26600  \tTraining accuracy: 0.8133501410484314\n",
      "Step: 26600  \tValid loss: 0.35747241973876953\n",
      "Step: 26700  \tTraining loss: 0.3220832943916321\n",
      "Step: 26700  \tTraining accuracy: 0.813383162021637\n",
      "Step: 26700  \tValid loss: 0.3574741780757904\n",
      "Step: 26800  \tTraining loss: 0.32207298278808594\n",
      "Step: 26800  \tTraining accuracy: 0.8134158849716187\n",
      "Step: 26800  \tValid loss: 0.3574686646461487\n",
      "Step: 26900  \tTraining loss: 0.322062611579895\n",
      "Step: 26900  \tTraining accuracy: 0.8134483695030212\n",
      "Step: 26900  \tValid loss: 0.3574720025062561\n",
      "Step: 27000  \tTraining loss: 0.32205265760421753\n",
      "Step: 27000  \tTraining accuracy: 0.8134806156158447\n",
      "Step: 27000  \tValid loss: 0.3574683666229248\n",
      "Step: 27100  \tTraining loss: 0.3220424950122833\n",
      "Step: 27100  \tTraining accuracy: 0.8135126829147339\n",
      "Step: 27100  \tValid loss: 0.35747092962265015\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.81354445\n",
      "Precision: 0.79850745\n",
      "Recall: 0.72542375\n",
      "F1 score: 0.774529\n",
      "AUC: 0.81326133\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.813544   0.798507  0.725424  0.774529  0.813261  0.322034      0.813496   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.357452       0.813472   0.553894      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  27182.0  \n",
      "33\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_18076/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(928, 3)\n",
      "(928, 1)\n",
      "(512, 3)\n",
      "(512, 1)\n",
      "(416, 3)\n",
      "(416, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.6342186331748962\n",
      "Step: 100  \tTraining accuracy: 0.7209051847457886\n",
      "Step: 100  \tValid loss: 0.6341768503189087\n",
      "Step: 200  \tTraining loss: 0.5519353747367859\n",
      "Step: 200  \tTraining accuracy: 0.7255747318267822\n",
      "Step: 200  \tValid loss: 0.5594620704650879\n",
      "Step: 300  \tTraining loss: 0.5344867706298828\n",
      "Step: 300  \tTraining accuracy: 0.7275862097740173\n",
      "Step: 300  \tValid loss: 0.544023871421814\n",
      "Step: 400  \tTraining loss: 0.5276848673820496\n",
      "Step: 400  \tTraining accuracy: 0.7333743572235107\n",
      "Step: 400  \tValid loss: 0.5351439714431763\n",
      "Step: 500  \tTraining loss: 0.5220391750335693\n",
      "Step: 500  \tTraining accuracy: 0.7391043901443481\n",
      "Step: 500  \tValid loss: 0.5273606777191162\n",
      "Step: 600  \tTraining loss: 0.5171698331832886\n",
      "Step: 600  \tTraining accuracy: 0.7452977895736694\n",
      "Step: 600  \tValid loss: 0.5208461284637451\n",
      "Step: 700  \tTraining loss: 0.5128851532936096\n",
      "Step: 700  \tTraining accuracy: 0.7502486705780029\n",
      "Step: 700  \tValid loss: 0.5151885747909546\n",
      "Step: 800  \tTraining loss: 0.5085664391517639\n",
      "Step: 800  \tTraining accuracy: 0.754023015499115\n",
      "Step: 800  \tValid loss: 0.510395348072052\n",
      "Step: 900  \tTraining loss: 0.5051217675209045\n",
      "Step: 900  \tTraining accuracy: 0.7569092512130737\n",
      "Step: 900  \tValid loss: 0.5068249702453613\n",
      "Step: 1000  \tTraining loss: 0.5022351741790771\n",
      "Step: 1000  \tTraining accuracy: 0.7595281600952148\n",
      "Step: 1000  \tValid loss: 0.5041698813438416\n",
      "Step: 1100  \tTraining loss: 0.49977773427963257\n",
      "Step: 1100  \tTraining accuracy: 0.7615968585014343\n",
      "Step: 1100  \tValid loss: 0.5020986795425415\n",
      "Step: 1200  \tTraining loss: 0.4977096915245056\n",
      "Step: 1200  \tTraining accuracy: 0.7633058428764343\n",
      "Step: 1200  \tValid loss: 0.5006338357925415\n",
      "Step: 1300  \tTraining loss: 0.4957205653190613\n",
      "Step: 1300  \tTraining accuracy: 0.7647413611412048\n",
      "Step: 1300  \tValid loss: 0.4994421601295471\n",
      "Step: 1400  \tTraining loss: 0.4938124716281891\n",
      "Step: 1400  \tTraining accuracy: 0.7660041451454163\n",
      "Step: 1400  \tValid loss: 0.49831125140190125\n",
      "Step: 1500  \tTraining loss: 0.4912709891796112\n",
      "Step: 1500  \tTraining accuracy: 0.767092764377594\n",
      "Step: 1500  \tValid loss: 0.49633994698524475\n",
      "Step: 1600  \tTraining loss: 0.48968371748924255\n",
      "Step: 1600  \tTraining accuracy: 0.7681798934936523\n",
      "Step: 1600  \tValid loss: 0.49538958072662354\n",
      "Step: 1700  \tTraining loss: 0.4883151948451996\n",
      "Step: 1700  \tTraining accuracy: 0.7694292068481445\n",
      "Step: 1700  \tValid loss: 0.49504292011260986\n",
      "Step: 1800  \tTraining loss: 0.4871228039264679\n",
      "Step: 1800  \tTraining accuracy: 0.7705665230751038\n",
      "Step: 1800  \tValid loss: 0.4946678876876831\n",
      "Step: 1900  \tTraining loss: 0.48605623841285706\n",
      "Step: 1900  \tTraining accuracy: 0.7715808749198914\n",
      "Step: 1900  \tValid loss: 0.49435558915138245\n",
      "Step: 2000  \tTraining loss: 0.48508328199386597\n",
      "Step: 2000  \tTraining accuracy: 0.7724911570549011\n",
      "Step: 2000  \tValid loss: 0.49418023228645325\n",
      "Step: 2100  \tTraining loss: 0.4842146635055542\n",
      "Step: 2100  \tTraining accuracy: 0.7733126282691956\n",
      "Step: 2100  \tValid loss: 0.49404171109199524\n",
      "Step: 2200  \tTraining loss: 0.48342546820640564\n",
      "Step: 2200  \tTraining accuracy: 0.7740577459335327\n",
      "Step: 2200  \tValid loss: 0.4939948320388794\n",
      "Step: 2300  \tTraining loss: 0.482735812664032\n",
      "Step: 2300  \tTraining accuracy: 0.774784505367279\n",
      "Step: 2300  \tValid loss: 0.4939876198768616\n",
      "Step: 2400  \tTraining loss: 0.4821224808692932\n",
      "Step: 2400  \tTraining accuracy: 0.7753806114196777\n",
      "Step: 2400  \tValid loss: 0.49393266439437866\n",
      "Step: 2500  \tTraining loss: 0.481568306684494\n",
      "Step: 2500  \tTraining accuracy: 0.7759280204772949\n",
      "Step: 2500  \tValid loss: 0.49396005272865295\n",
      "Step: 2600  \tTraining loss: 0.481057733297348\n",
      "Step: 2600  \tTraining accuracy: 0.7764325737953186\n",
      "Step: 2600  \tValid loss: 0.4939061999320984\n",
      "Step: 2700  \tTraining loss: 0.4805813729763031\n",
      "Step: 2700  \tTraining accuracy: 0.776898980140686\n",
      "Step: 2700  \tValid loss: 0.4938984513282776\n",
      "Step: 2800  \tTraining loss: 0.48013028502464294\n",
      "Step: 2800  \tTraining accuracy: 0.777331531047821\n",
      "Step: 2800  \tValid loss: 0.4939075708389282\n",
      "Step: 2900  \tTraining loss: 0.47969377040863037\n",
      "Step: 2900  \tTraining accuracy: 0.7777336835861206\n",
      "Step: 2900  \tValid loss: 0.49402916431427\n",
      "Step: 3000  \tTraining loss: 0.47929224371910095\n",
      "Step: 3000  \tTraining accuracy: 0.778108537197113\n",
      "Step: 3000  \tValid loss: 0.49412161111831665\n",
      "Step: 3100  \tTraining loss: 0.47891300916671753\n",
      "Step: 3100  \tTraining accuracy: 0.7784588932991028\n",
      "Step: 3100  \tValid loss: 0.49423226714134216\n",
      "Step: 3200  \tTraining loss: 0.4785378575325012\n",
      "Step: 3200  \tTraining accuracy: 0.7787869572639465\n",
      "Step: 3200  \tValid loss: 0.49472707509994507\n",
      "Step: 3300  \tTraining loss: 0.4781796634197235\n",
      "Step: 3300  \tTraining accuracy: 0.7790948152542114\n",
      "Step: 3300  \tValid loss: 0.49456071853637695\n",
      "Step: 3400  \tTraining loss: 0.47786593437194824\n",
      "Step: 3400  \tTraining accuracy: 0.7793843150138855\n",
      "Step: 3400  \tValid loss: 0.4948015511035919\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.77965707\n",
      "Precision: 0.82302773\n",
      "Recall: 0.779798\n",
      "F1 score: 0.758629\n",
      "AUC: 0.79405606\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.779657   0.823028  0.779798  0.758629  0.794056  0.477581      0.779459   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0   0.49382       0.779461   0.471638      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  3497.0  \n",
      "34\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_16723/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(986, 3)\n",
      "(986, 1)\n",
      "(544, 3)\n",
      "(544, 1)\n",
      "(442, 3)\n",
      "(442, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.42301568388938904\n",
      "Step: 100  \tTraining accuracy: 0.8316429853439331\n",
      "Step: 100  \tValid loss: 0.47548243403434753\n",
      "Step: 200  \tTraining loss: 0.4086352288722992\n",
      "Step: 200  \tTraining accuracy: 0.8272481560707092\n",
      "Step: 200  \tValid loss: 0.4681565463542938\n",
      "Step: 300  \tTraining loss: 0.39529120922088623\n",
      "Step: 300  \tTraining accuracy: 0.8271805047988892\n",
      "Step: 300  \tValid loss: 0.45572352409362793\n",
      "Step: 400  \tTraining loss: 0.3827185332775116\n",
      "Step: 400  \tTraining accuracy: 0.8287453055381775\n",
      "Step: 400  \tValid loss: 0.4446384608745575\n",
      "Step: 500  \tTraining loss: 0.3721393942832947\n",
      "Step: 500  \tTraining accuracy: 0.8300653696060181\n",
      "Step: 500  \tValid loss: 0.4354742169380188\n",
      "Step: 600  \tTraining loss: 0.3636947572231293\n",
      "Step: 600  \tTraining accuracy: 0.8321040272712708\n",
      "Step: 600  \tValid loss: 0.4279986023902893\n",
      "Step: 700  \tTraining loss: 0.35705360770225525\n",
      "Step: 700  \tTraining accuracy: 0.8345295786857605\n",
      "Step: 700  \tValid loss: 0.4219512939453125\n",
      "Step: 800  \tTraining loss: 0.35184141993522644\n",
      "Step: 800  \tTraining accuracy: 0.8363759517669678\n",
      "Step: 800  \tValid loss: 0.41713765263557434\n",
      "Step: 900  \tTraining loss: 0.34775856137275696\n",
      "Step: 900  \tTraining accuracy: 0.8375492095947266\n",
      "Step: 900  \tValid loss: 0.4133898615837097\n",
      "Step: 1000  \tTraining loss: 0.34457868337631226\n",
      "Step: 1000  \tTraining accuracy: 0.8384221196174622\n",
      "Step: 1000  \tValid loss: 0.41054919362068176\n",
      "Step: 1100  \tTraining loss: 0.3421250879764557\n",
      "Step: 1100  \tTraining accuracy: 0.8389355540275574\n",
      "Step: 1100  \tValid loss: 0.4084612727165222\n",
      "Step: 1200  \tTraining loss: 0.3402540385723114\n",
      "Step: 1200  \tTraining accuracy: 0.8391392827033997\n",
      "Step: 1200  \tValid loss: 0.4069800078868866\n",
      "Step: 1300  \tTraining loss: 0.3388441503047943\n",
      "Step: 1300  \tTraining accuracy: 0.8394725918769836\n",
      "Step: 1300  \tValid loss: 0.40597131848335266\n",
      "Step: 1400  \tTraining loss: 0.33779191970825195\n",
      "Step: 1400  \tTraining accuracy: 0.8397941589355469\n",
      "Step: 1400  \tValid loss: 0.40531647205352783\n",
      "Step: 1500  \tTraining loss: 0.3370119631290436\n",
      "Step: 1500  \tTraining accuracy: 0.8400713205337524\n",
      "Step: 1500  \tValid loss: 0.40491485595703125\n",
      "Step: 1600  \tTraining loss: 0.33643394708633423\n",
      "Step: 1600  \tTraining accuracy: 0.8403127789497375\n",
      "Step: 1600  \tValid loss: 0.40468525886535645\n",
      "Step: 1700  \tTraining loss: 0.3360019028186798\n",
      "Step: 1700  \tTraining accuracy: 0.8405249118804932\n",
      "Step: 1700  \tValid loss: 0.40456461906433105\n",
      "Step: 1800  \tTraining loss: 0.33567267656326294\n",
      "Step: 1800  \tTraining accuracy: 0.84071284532547\n",
      "Step: 1800  \tValid loss: 0.4045066237449646\n",
      "Step: 1900  \tTraining loss: 0.3354138433933258\n",
      "Step: 1900  \tTraining accuracy: 0.8408804535865784\n",
      "Step: 1900  \tValid loss: 0.40447840094566345\n",
      "Step: 2000  \tTraining loss: 0.33520105481147766\n",
      "Step: 2000  \tTraining accuracy: 0.8410308361053467\n",
      "Step: 2000  \tValid loss: 0.4044581949710846\n",
      "Step: 2100  \tTraining loss: 0.335016667842865\n",
      "Step: 2100  \tTraining accuracy: 0.8410923480987549\n",
      "Step: 2100  \tValid loss: 0.4044327437877655\n",
      "Step: 2200  \tTraining loss: 0.334848016500473\n",
      "Step: 2200  \tTraining accuracy: 0.8410773873329163\n",
      "Step: 2200  \tValid loss: 0.40439411997795105\n",
      "Step: 2300  \tTraining loss: 0.33468616008758545\n",
      "Step: 2300  \tTraining accuracy: 0.8410637974739075\n",
      "Step: 2300  \tValid loss: 0.40433937311172485\n",
      "Step: 2400  \tTraining loss: 0.33452489972114563\n",
      "Step: 2400  \tTraining accuracy: 0.8410513401031494\n",
      "Step: 2400  \tValid loss: 0.4042676091194153\n",
      "Step: 2500  \tTraining loss: 0.3343605399131775\n",
      "Step: 2500  \tTraining accuracy: 0.8410398364067078\n",
      "Step: 2500  \tValid loss: 0.4041803479194641\n",
      "Step: 2600  \tTraining loss: 0.33419105410575867\n",
      "Step: 2600  \tTraining accuracy: 0.8410292863845825\n",
      "Step: 2600  \tValid loss: 0.4040796160697937\n",
      "Step: 2700  \tTraining loss: 0.33401569724082947\n",
      "Step: 2700  \tTraining accuracy: 0.8410195708274841\n",
      "Step: 2700  \tValid loss: 0.40396836400032043\n",
      "Step: 2800  \tTraining loss: 0.333834707736969\n",
      "Step: 2800  \tTraining accuracy: 0.8409736156463623\n",
      "Step: 2800  \tValid loss: 0.4038490056991577\n",
      "Step: 2900  \tTraining loss: 0.33364856243133545\n",
      "Step: 2900  \tTraining accuracy: 0.8409309387207031\n",
      "Step: 2900  \tValid loss: 0.40372365713119507\n",
      "Step: 3000  \tTraining loss: 0.33345797657966614\n",
      "Step: 3000  \tTraining accuracy: 0.8408911228179932\n",
      "Step: 3000  \tValid loss: 0.40359383821487427\n",
      "Step: 3100  \tTraining loss: 0.3332631289958954\n",
      "Step: 3100  \tTraining accuracy: 0.8408539295196533\n",
      "Step: 3100  \tValid loss: 0.4034613072872162\n",
      "Step: 3200  \tTraining loss: 0.3330647647380829\n",
      "Step: 3200  \tTraining accuracy: 0.8408190608024597\n",
      "Step: 3200  \tValid loss: 0.40332770347595215\n",
      "Step: 3300  \tTraining loss: 0.33286330103874207\n",
      "Step: 3300  \tTraining accuracy: 0.8407863974571228\n",
      "Step: 3300  \tValid loss: 0.40319377183914185\n",
      "Step: 3400  \tTraining loss: 0.3326592743396759\n",
      "Step: 3400  \tTraining accuracy: 0.8407556414604187\n",
      "Step: 3400  \tValid loss: 0.4030608534812927\n",
      "Step: 3500  \tTraining loss: 0.3324532210826874\n",
      "Step: 3500  \tTraining accuracy: 0.8407266736030579\n",
      "Step: 3500  \tValid loss: 0.4029301404953003\n",
      "Step: 3600  \tTraining loss: 0.3322453498840332\n",
      "Step: 3600  \tTraining accuracy: 0.8407136797904968\n",
      "Step: 3600  \tValid loss: 0.4028022289276123\n",
      "Step: 3700  \tTraining loss: 0.33203670382499695\n",
      "Step: 3700  \tTraining accuracy: 0.8407013416290283\n",
      "Step: 3700  \tValid loss: 0.40267813205718994\n",
      "Step: 3800  \tTraining loss: 0.33182746171951294\n",
      "Step: 3800  \tTraining accuracy: 0.8406896591186523\n",
      "Step: 3800  \tValid loss: 0.4025583267211914\n",
      "Step: 3900  \tTraining loss: 0.3316188156604767\n",
      "Step: 3900  \tTraining accuracy: 0.8406785726547241\n",
      "Step: 3900  \tValid loss: 0.40244314074516296\n",
      "Step: 4000  \tTraining loss: 0.33141061663627625\n",
      "Step: 4000  \tTraining accuracy: 0.8406680822372437\n",
      "Step: 4000  \tValid loss: 0.4023329019546509\n",
      "Step: 4100  \tTraining loss: 0.331203818321228\n",
      "Step: 4100  \tTraining accuracy: 0.8406581282615662\n",
      "Step: 4100  \tValid loss: 0.4022276699542999\n",
      "Step: 4200  \tTraining loss: 0.3309985101222992\n",
      "Step: 4200  \tTraining accuracy: 0.8406485915184021\n",
      "Step: 4200  \tValid loss: 0.4021275043487549\n",
      "Step: 4300  \tTraining loss: 0.33079537749290466\n",
      "Step: 4300  \tTraining accuracy: 0.8406395316123962\n",
      "Step: 4300  \tValid loss: 0.4020322263240814\n",
      "Step: 4400  \tTraining loss: 0.33059415221214294\n",
      "Step: 4400  \tTraining accuracy: 0.8406308889389038\n",
      "Step: 4400  \tValid loss: 0.40194135904312134\n",
      "Step: 4500  \tTraining loss: 0.3303954601287842\n",
      "Step: 4500  \tTraining accuracy: 0.8405998349189758\n",
      "Step: 4500  \tValid loss: 0.40185463428497314\n",
      "Step: 4600  \tTraining loss: 0.33019933104515076\n",
      "Step: 4600  \tTraining accuracy: 0.8405701518058777\n",
      "Step: 4600  \tValid loss: 0.4017713665962219\n",
      "Step: 4700  \tTraining loss: 0.33000534772872925\n",
      "Step: 4700  \tTraining accuracy: 0.8405417799949646\n",
      "Step: 4700  \tValid loss: 0.401691198348999\n",
      "Step: 4800  \tTraining loss: 0.3298136293888092\n",
      "Step: 4800  \tTraining accuracy: 0.840514600276947\n",
      "Step: 4800  \tValid loss: 0.40161383152008057\n",
      "Step: 4900  \tTraining loss: 0.32962384819984436\n",
      "Step: 4900  \tTraining accuracy: 0.8405198454856873\n",
      "Step: 4900  \tValid loss: 0.401538610458374\n",
      "Step: 5000  \tTraining loss: 0.3294362425804138\n",
      "Step: 5000  \tTraining accuracy: 0.8405249118804932\n",
      "Step: 5000  \tValid loss: 0.40146541595458984\n",
      "Step: 5100  \tTraining loss: 0.3292500376701355\n",
      "Step: 5100  \tTraining accuracy: 0.8405297994613647\n",
      "Step: 5100  \tValid loss: 0.4013940095901489\n",
      "Step: 5200  \tTraining loss: 0.3290654122829437\n",
      "Step: 5200  \tTraining accuracy: 0.8405344486236572\n",
      "Step: 5200  \tValid loss: 0.40132424235343933\n",
      "Step: 5300  \tTraining loss: 0.3288818895816803\n",
      "Step: 5300  \tTraining accuracy: 0.8405389785766602\n",
      "Step: 5300  \tValid loss: 0.40125593543052673\n",
      "Step: 5400  \tTraining loss: 0.32869967818260193\n",
      "Step: 5400  \tTraining accuracy: 0.8405433297157288\n",
      "Step: 5400  \tValid loss: 0.4011894166469574\n",
      "Step: 5500  \tTraining loss: 0.32851865887641907\n",
      "Step: 5500  \tTraining accuracy: 0.840547502040863\n",
      "Step: 5500  \tValid loss: 0.401124507188797\n",
      "Step: 5600  \tTraining loss: 0.32833829522132874\n",
      "Step: 5600  \tTraining accuracy: 0.840551495552063\n",
      "Step: 5600  \tValid loss: 0.401061475276947\n",
      "Step: 5700  \tTraining loss: 0.3281588554382324\n",
      "Step: 5700  \tTraining accuracy: 0.8405553698539734\n",
      "Step: 5700  \tValid loss: 0.4010000228881836\n",
      "Step: 5800  \tTraining loss: 0.3279801309108734\n",
      "Step: 5800  \tTraining accuracy: 0.8405591249465942\n",
      "Step: 5800  \tValid loss: 0.4009401798248291\n",
      "Step: 5900  \tTraining loss: 0.32780221104621887\n",
      "Step: 5900  \tTraining accuracy: 0.8405627608299255\n",
      "Step: 5900  \tValid loss: 0.40088212490081787\n",
      "Step: 6000  \tTraining loss: 0.3276251256465912\n",
      "Step: 6000  \tTraining accuracy: 0.8405662178993225\n",
      "Step: 6000  \tValid loss: 0.4008260667324066\n",
      "Step: 6100  \tTraining loss: 0.32744890451431274\n",
      "Step: 6100  \tTraining accuracy: 0.8405696153640747\n",
      "Step: 6100  \tValid loss: 0.400771826505661\n",
      "Step: 6200  \tTraining loss: 0.32727378606796265\n",
      "Step: 6200  \tTraining accuracy: 0.8405728936195374\n",
      "Step: 6200  \tValid loss: 0.4007202684879303\n",
      "Step: 6300  \tTraining loss: 0.3270998001098633\n",
      "Step: 6300  \tTraining accuracy: 0.8405760526657104\n",
      "Step: 6300  \tValid loss: 0.4006713628768921\n",
      "Step: 6400  \tTraining loss: 0.3269275724887848\n",
      "Step: 6400  \tTraining accuracy: 0.840563178062439\n",
      "Step: 6400  \tValid loss: 0.40062564611434937\n",
      "Step: 6500  \tTraining loss: 0.3267571032047272\n",
      "Step: 6500  \tTraining accuracy: 0.8405506610870361\n",
      "Step: 6500  \tValid loss: 0.4005835950374603\n",
      "Step: 6600  \tTraining loss: 0.326588898897171\n",
      "Step: 6600  \tTraining accuracy: 0.8405385613441467\n",
      "Step: 6600  \tValid loss: 0.4005453884601593\n",
      "Step: 6700  \tTraining loss: 0.32642340660095215\n",
      "Step: 6700  \tTraining accuracy: 0.8405267596244812\n",
      "Step: 6700  \tValid loss: 0.4005119502544403\n",
      "Step: 6800  \tTraining loss: 0.3262611925601959\n",
      "Step: 6800  \tTraining accuracy: 0.8405153751373291\n",
      "Step: 6800  \tValid loss: 0.4004829525947571\n",
      "Step: 6900  \tTraining loss: 0.32610243558883667\n",
      "Step: 6900  \tTraining accuracy: 0.8404820561408997\n",
      "Step: 6900  \tValid loss: 0.40045928955078125\n",
      "Step: 7000  \tTraining loss: 0.3259475529193878\n",
      "Step: 7000  \tTraining accuracy: 0.8404497504234314\n",
      "Step: 7000  \tValid loss: 0.40044066309928894\n",
      "Step: 7100  \tTraining loss: 0.325796902179718\n",
      "Step: 7100  \tTraining accuracy: 0.8404183387756348\n",
      "Step: 7100  \tValid loss: 0.40042737126350403\n",
      "Step: 7200  \tTraining loss: 0.3256511092185974\n",
      "Step: 7200  \tTraining accuracy: 0.8403878211975098\n",
      "Step: 7200  \tValid loss: 0.4004195034503937\n",
      "Step: 7300  \tTraining loss: 0.3255102336406708\n",
      "Step: 7300  \tTraining accuracy: 0.8403581380844116\n",
      "Step: 7300  \tValid loss: 0.40041759610176086\n",
      "Step: 7400  \tTraining loss: 0.32537418603897095\n",
      "Step: 7400  \tTraining accuracy: 0.8403292298316956\n",
      "Step: 7400  \tValid loss: 0.4004226624965668\n",
      "Step: 7500  \tTraining loss: 0.32524293661117554\n",
      "Step: 7500  \tTraining accuracy: 0.8403011560440063\n",
      "Step: 7500  \tValid loss: 0.40042999386787415\n",
      "Step: 7600  \tTraining loss: 0.3251168727874756\n",
      "Step: 7600  \tTraining accuracy: 0.8402737379074097\n",
      "Step: 7600  \tValid loss: 0.40044504404067993\n",
      "Step: 7700  \tTraining loss: 0.32499590516090393\n",
      "Step: 7700  \tTraining accuracy: 0.8402470946311951\n",
      "Step: 7700  \tValid loss: 0.4004645049571991\n",
      "Step: 7800  \tTraining loss: 0.32487979531288147\n",
      "Step: 7800  \tTraining accuracy: 0.8402211666107178\n",
      "Step: 7800  \tValid loss: 0.4004879295825958\n",
      "Step: 7900  \tTraining loss: 0.3247688412666321\n",
      "Step: 7900  \tTraining accuracy: 0.8401958346366882\n",
      "Step: 7900  \tValid loss: 0.400515615940094\n",
      "Step: 8000  \tTraining loss: 0.3246627449989319\n",
      "Step: 8000  \tTraining accuracy: 0.840171217918396\n",
      "Step: 8000  \tValid loss: 0.4005463123321533\n",
      "Step: 8100  \tTraining loss: 0.3245614767074585\n",
      "Step: 8100  \tTraining accuracy: 0.8401471376419067\n",
      "Step: 8100  \tValid loss: 0.40058067440986633\n",
      "Step: 8200  \tTraining loss: 0.32446470856666565\n",
      "Step: 8200  \tTraining accuracy: 0.84012371301651\n",
      "Step: 8200  \tValid loss: 0.4006173610687256\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8401008\n",
      "Precision: 0.88\n",
      "Recall: 0.96585363\n",
      "F1 score: 0.8791753\n",
      "AUC: 0.6576256\n",
      "   accuracy  precision    recall  f1_score       auc     loss  accuracy_val  \\\n",
      "0  0.840101       0.88  0.965854  0.879175  0.657626  0.32438      0.839967   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.400416       0.840004   0.459062      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  8290.0  \n",
      "35\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_5714/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(4031, 3)\n",
      "(4031, 1)\n",
      "(2224, 3)\n",
      "(2224, 1)\n",
      "(1807, 3)\n",
      "(1807, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.556518018245697\n",
      "Step: 100  \tTraining accuracy: 0.870751678943634\n",
      "Step: 100  \tValid loss: 0.5574180483818054\n",
      "Step: 200  \tTraining loss: 0.37118545174598694\n",
      "Step: 200  \tTraining accuracy: 0.8668651580810547\n",
      "Step: 200  \tValid loss: 0.3757796585559845\n",
      "Step: 300  \tTraining loss: 0.3101937770843506\n",
      "Step: 300  \tTraining accuracy: 0.866087794303894\n",
      "Step: 300  \tValid loss: 0.3183519244194031\n",
      "Step: 400  \tTraining loss: 0.2925388813018799\n",
      "Step: 400  \tTraining accuracy: 0.8657546639442444\n",
      "Step: 400  \tValid loss: 0.3020056486129761\n",
      "Step: 500  \tTraining loss: 0.28303462266921997\n",
      "Step: 500  \tTraining accuracy: 0.8655695915222168\n",
      "Step: 500  \tValid loss: 0.29263052344322205\n",
      "Step: 600  \tTraining loss: 0.27518320083618164\n",
      "Step: 600  \tTraining accuracy: 0.8660833239555359\n",
      "Step: 600  \tValid loss: 0.28458210825920105\n",
      "Step: 700  \tTraining loss: 0.26784178614616394\n",
      "Step: 700  \tTraining accuracy: 0.8671640753746033\n",
      "Step: 700  \tValid loss: 0.2769226133823395\n",
      "Step: 800  \tTraining loss: 0.26086127758026123\n",
      "Step: 800  \tTraining accuracy: 0.8683039546012878\n",
      "Step: 800  \tValid loss: 0.26962795853614807\n",
      "Step: 900  \tTraining loss: 0.254315584897995\n",
      "Step: 900  \tTraining accuracy: 0.8702701330184937\n",
      "Step: 900  \tValid loss: 0.2627575993537903\n",
      "Step: 1000  \tTraining loss: 0.2484290897846222\n",
      "Step: 1000  \tTraining accuracy: 0.8726057410240173\n",
      "Step: 1000  \tValid loss: 0.25647011399269104\n",
      "Step: 1100  \tTraining loss: 0.24344448745250702\n",
      "Step: 1100  \tTraining accuracy: 0.8749335408210754\n",
      "Step: 1100  \tValid loss: 0.25103604793548584\n",
      "Step: 1200  \tTraining loss: 0.23947195708751678\n",
      "Step: 1200  \tTraining accuracy: 0.8772879838943481\n",
      "Step: 1200  \tValid loss: 0.24658092856407166\n",
      "Step: 1300  \tTraining loss: 0.2365480661392212\n",
      "Step: 1300  \tTraining accuracy: 0.8793947100639343\n",
      "Step: 1300  \tValid loss: 0.24319934844970703\n",
      "Step: 1400  \tTraining loss: 0.23455213010311127\n",
      "Step: 1400  \tTraining accuracy: 0.8814006447792053\n",
      "Step: 1400  \tValid loss: 0.24080263078212738\n",
      "Step: 1500  \tTraining loss: 0.23327942192554474\n",
      "Step: 1500  \tTraining accuracy: 0.883129894733429\n",
      "Step: 1500  \tValid loss: 0.23921392858028412\n",
      "Step: 1600  \tTraining loss: 0.2325069159269333\n",
      "Step: 1600  \tTraining accuracy: 0.8846440315246582\n",
      "Step: 1600  \tValid loss: 0.23819494247436523\n",
      "Step: 1700  \tTraining loss: 0.23204687237739563\n",
      "Step: 1700  \tTraining accuracy: 0.8860498070716858\n",
      "Step: 1700  \tValid loss: 0.23755969107151031\n",
      "Step: 1800  \tTraining loss: 0.23176494240760803\n",
      "Step: 1800  \tTraining accuracy: 0.8872948884963989\n",
      "Step: 1800  \tValid loss: 0.23716096580028534\n",
      "Step: 1900  \tTraining loss: 0.23152536153793335\n",
      "Step: 1900  \tTraining accuracy: 0.8883785605430603\n",
      "Step: 1900  \tValid loss: 0.23681025207042694\n",
      "Step: 2000  \tTraining loss: 0.23115317523479462\n",
      "Step: 2000  \tTraining accuracy: 0.8892747759819031\n",
      "Step: 2000  \tValid loss: 0.2360668033361435\n",
      "Step: 2100  \tTraining loss: 0.2303716093301773\n",
      "Step: 2100  \tTraining accuracy: 0.8900835514068604\n",
      "Step: 2100  \tValid loss: 0.23461316525936127\n",
      "Step: 2200  \tTraining loss: 0.22839416563510895\n",
      "Step: 2200  \tTraining accuracy: 0.8908863067626953\n",
      "Step: 2200  \tValid loss: 0.23332376778125763\n",
      "Step: 2300  \tTraining loss: 0.22780932486057281\n",
      "Step: 2300  \tTraining accuracy: 0.8916838765144348\n",
      "Step: 2300  \tValid loss: 0.23290258646011353\n",
      "Step: 2400  \tTraining loss: 0.22720161080360413\n",
      "Step: 2400  \tTraining accuracy: 0.8923977613449097\n",
      "Step: 2400  \tValid loss: 0.2325790524482727\n",
      "Step: 2500  \tTraining loss: 0.22642338275909424\n",
      "Step: 2500  \tTraining accuracy: 0.8930178880691528\n",
      "Step: 2500  \tValid loss: 0.23198208212852478\n",
      "Step: 2600  \tTraining loss: 0.22599177062511444\n",
      "Step: 2600  \tTraining accuracy: 0.8935893774032593\n",
      "Step: 2600  \tValid loss: 0.23171745240688324\n",
      "Step: 2700  \tTraining loss: 0.22566471993923187\n",
      "Step: 2700  \tTraining accuracy: 0.8941177725791931\n",
      "Step: 2700  \tValid loss: 0.23150752484798431\n",
      "Step: 2800  \tTraining loss: 0.22536322474479675\n",
      "Step: 2800  \tTraining accuracy: 0.8946077227592468\n",
      "Step: 2800  \tValid loss: 0.2313392460346222\n",
      "Step: 2900  \tTraining loss: 0.22507892549037933\n",
      "Step: 2900  \tTraining accuracy: 0.8950632810592651\n",
      "Step: 2900  \tValid loss: 0.231188103556633\n",
      "Step: 3000  \tTraining loss: 0.22480951249599457\n",
      "Step: 3000  \tTraining accuracy: 0.8954879641532898\n",
      "Step: 3000  \tValid loss: 0.23104199767112732\n",
      "Step: 3100  \tTraining loss: 0.22455666959285736\n",
      "Step: 3100  \tTraining accuracy: 0.8958847522735596\n",
      "Step: 3100  \tValid loss: 0.23088034987449646\n",
      "Step: 3200  \tTraining loss: 0.22431161999702454\n",
      "Step: 3200  \tTraining accuracy: 0.8962563872337341\n",
      "Step: 3200  \tValid loss: 0.23069466650485992\n",
      "Step: 3300  \tTraining loss: 0.22406719624996185\n",
      "Step: 3300  \tTraining accuracy: 0.8966051340103149\n",
      "Step: 3300  \tValid loss: 0.23046691715717316\n",
      "Step: 3400  \tTraining loss: 0.2238432765007019\n",
      "Step: 3400  \tTraining accuracy: 0.8969330787658691\n",
      "Step: 3400  \tValid loss: 0.23025444149971008\n",
      "Step: 3500  \tTraining loss: 0.22364120185375214\n",
      "Step: 3500  \tTraining accuracy: 0.89724200963974\n",
      "Step: 3500  \tValid loss: 0.23004844784736633\n",
      "Step: 3600  \tTraining loss: 0.22345826029777527\n",
      "Step: 3600  \tTraining accuracy: 0.8975335359573364\n",
      "Step: 3600  \tValid loss: 0.22985577583312988\n",
      "Step: 3700  \tTraining loss: 0.22329483926296234\n",
      "Step: 3700  \tTraining accuracy: 0.8978090882301331\n",
      "Step: 3700  \tValid loss: 0.2296755164861679\n",
      "Step: 3800  \tTraining loss: 0.22314755618572235\n",
      "Step: 3800  \tTraining accuracy: 0.8980732560157776\n",
      "Step: 3800  \tValid loss: 0.2295129895210266\n",
      "Step: 3900  \tTraining loss: 0.2230159342288971\n",
      "Step: 3900  \tTraining accuracy: 0.8983720541000366\n",
      "Step: 3900  \tValid loss: 0.22936254739761353\n",
      "Step: 4000  \tTraining loss: 0.22289729118347168\n",
      "Step: 4000  \tTraining accuracy: 0.8986556529998779\n",
      "Step: 4000  \tValid loss: 0.2292255312204361\n",
      "Step: 4100  \tTraining loss: 0.2227906733751297\n",
      "Step: 4100  \tTraining accuracy: 0.8989253044128418\n",
      "Step: 4100  \tValid loss: 0.22909875214099884\n",
      "Step: 4200  \tTraining loss: 0.22265231609344482\n",
      "Step: 4200  \tTraining accuracy: 0.8991819620132446\n",
      "Step: 4200  \tValid loss: 0.22895902395248413\n",
      "Step: 4300  \tTraining loss: 0.22249573469161987\n",
      "Step: 4300  \tTraining accuracy: 0.8994265198707581\n",
      "Step: 4300  \tValid loss: 0.22883890569210052\n",
      "Step: 4400  \tTraining loss: 0.22235597670078278\n",
      "Step: 4400  \tTraining accuracy: 0.8996598124504089\n",
      "Step: 4400  \tValid loss: 0.2287292629480362\n",
      "Step: 4500  \tTraining loss: 0.22221116721630096\n",
      "Step: 4500  \tTraining accuracy: 0.8998826742172241\n",
      "Step: 4500  \tValid loss: 0.22863151133060455\n",
      "Step: 4600  \tTraining loss: 0.22208891808986664\n",
      "Step: 4600  \tTraining accuracy: 0.9000957012176514\n",
      "Step: 4600  \tValid loss: 0.22850877046585083\n",
      "Step: 4700  \tTraining loss: 0.2220378965139389\n",
      "Step: 4700  \tTraining accuracy: 0.9002995491027832\n",
      "Step: 4700  \tValid loss: 0.22838164865970612\n",
      "Step: 4800  \tTraining loss: 0.22199654579162598\n",
      "Step: 4800  \tTraining accuracy: 0.9004948735237122\n",
      "Step: 4800  \tValid loss: 0.22829441726207733\n",
      "Step: 4900  \tTraining loss: 0.22195804119110107\n",
      "Step: 4900  \tTraining accuracy: 0.9006820917129517\n",
      "Step: 4900  \tValid loss: 0.22821955382823944\n",
      "Step: 5000  \tTraining loss: 0.22192160785198212\n",
      "Step: 5000  \tTraining accuracy: 0.9008617401123047\n",
      "Step: 5000  \tValid loss: 0.2281506210565567\n",
      "Step: 5100  \tTraining loss: 0.22188785672187805\n",
      "Step: 5100  \tTraining accuracy: 0.9010342955589294\n",
      "Step: 5100  \tValid loss: 0.22808696329593658\n",
      "Step: 5200  \tTraining loss: 0.22184216976165771\n",
      "Step: 5200  \tTraining accuracy: 0.9012001752853394\n",
      "Step: 5200  \tValid loss: 0.22797685861587524\n",
      "Step: 5300  \tTraining loss: 0.22180922329425812\n",
      "Step: 5300  \tTraining accuracy: 0.9013596773147583\n",
      "Step: 5300  \tValid loss: 0.2278866320848465\n",
      "Step: 5400  \tTraining loss: 0.2217789590358734\n",
      "Step: 5400  \tTraining accuracy: 0.9015132784843445\n",
      "Step: 5400  \tValid loss: 0.22782835364341736\n",
      "Step: 5500  \tTraining loss: 0.22175045311450958\n",
      "Step: 5500  \tTraining accuracy: 0.901661217212677\n",
      "Step: 5500  \tValid loss: 0.22778025269508362\n",
      "Step: 5600  \tTraining loss: 0.22172287106513977\n",
      "Step: 5600  \tTraining accuracy: 0.9018037915229797\n",
      "Step: 5600  \tValid loss: 0.22774019837379456\n",
      "Step: 5700  \tTraining loss: 0.22169707715511322\n",
      "Step: 5700  \tTraining accuracy: 0.9019413590431213\n",
      "Step: 5700  \tValid loss: 0.2277054786682129\n",
      "Step: 5800  \tTraining loss: 0.22167222201824188\n",
      "Step: 5800  \tTraining accuracy: 0.9020741581916809\n",
      "Step: 5800  \tValid loss: 0.22767147421836853\n",
      "Step: 5900  \tTraining loss: 0.22164833545684814\n",
      "Step: 5900  \tTraining accuracy: 0.9022023677825928\n",
      "Step: 5900  \tValid loss: 0.22763432562351227\n",
      "Step: 6000  \tTraining loss: 0.22162461280822754\n",
      "Step: 6000  \tTraining accuracy: 0.9023262858390808\n",
      "Step: 6000  \tValid loss: 0.22760441899299622\n",
      "Step: 6100  \tTraining loss: 0.22160238027572632\n",
      "Step: 6100  \tTraining accuracy: 0.9024461507797241\n",
      "Step: 6100  \tValid loss: 0.22757625579833984\n",
      "Step: 6200  \tTraining loss: 0.22157993912696838\n",
      "Step: 6200  \tTraining accuracy: 0.9025620818138123\n",
      "Step: 6200  \tValid loss: 0.2275523841381073\n",
      "Step: 6300  \tTraining loss: 0.22155867516994476\n",
      "Step: 6300  \tTraining accuracy: 0.9026742577552795\n",
      "Step: 6300  \tValid loss: 0.2275286614894867\n",
      "Step: 6400  \tTraining loss: 0.22153757512569427\n",
      "Step: 6400  \tTraining accuracy: 0.9027829766273499\n",
      "Step: 6400  \tValid loss: 0.22750671207904816\n",
      "Step: 6500  \tTraining loss: 0.221516951918602\n",
      "Step: 6500  \tTraining accuracy: 0.902888298034668\n",
      "Step: 6500  \tValid loss: 0.22748661041259766\n",
      "Step: 6600  \tTraining loss: 0.2214965522289276\n",
      "Step: 6600  \tTraining accuracy: 0.9029904007911682\n",
      "Step: 6600  \tValid loss: 0.22746756672859192\n",
      "Step: 6700  \tTraining loss: 0.2214769721031189\n",
      "Step: 6700  \tTraining accuracy: 0.9030894041061401\n",
      "Step: 6700  \tValid loss: 0.22744855284690857\n",
      "Step: 6800  \tTraining loss: 0.2214573174715042\n",
      "Step: 6800  \tTraining accuracy: 0.9031854867935181\n",
      "Step: 6800  \tValid loss: 0.2274307906627655\n",
      "Step: 6900  \tTraining loss: 0.22143782675266266\n",
      "Step: 6900  \tTraining accuracy: 0.9032787680625916\n",
      "Step: 6900  \tValid loss: 0.22741279006004333\n",
      "Step: 7000  \tTraining loss: 0.2214190512895584\n",
      "Step: 7000  \tTraining accuracy: 0.9033693671226501\n",
      "Step: 7000  \tValid loss: 0.22739580273628235\n",
      "Step: 7100  \tTraining loss: 0.22140032052993774\n",
      "Step: 7100  \tTraining accuracy: 0.9034574031829834\n",
      "Step: 7100  \tValid loss: 0.22737887501716614\n",
      "Step: 7200  \tTraining loss: 0.22138196229934692\n",
      "Step: 7200  \tTraining accuracy: 0.9035429954528809\n",
      "Step: 7200  \tValid loss: 0.2273624986410141\n",
      "Step: 7300  \tTraining loss: 0.22136352956295013\n",
      "Step: 7300  \tTraining accuracy: 0.9036262035369873\n",
      "Step: 7300  \tValid loss: 0.227346733212471\n",
      "Step: 7400  \tTraining loss: 0.2213457226753235\n",
      "Step: 7400  \tTraining accuracy: 0.9037071466445923\n",
      "Step: 7400  \tValid loss: 0.22733069956302643\n",
      "Step: 7500  \tTraining loss: 0.22132807970046997\n",
      "Step: 7500  \tTraining accuracy: 0.9037859439849854\n",
      "Step: 7500  \tValid loss: 0.22731493413448334\n",
      "Step: 7600  \tTraining loss: 0.22131092846393585\n",
      "Step: 7600  \tTraining accuracy: 0.9038625955581665\n",
      "Step: 7600  \tValid loss: 0.22730034589767456\n",
      "Step: 7700  \tTraining loss: 0.22116053104400635\n",
      "Step: 7700  \tTraining accuracy: 0.9039372801780701\n",
      "Step: 7700  \tValid loss: 0.22704079747200012\n",
      "Step: 7800  \tTraining loss: 0.22112004458904266\n",
      "Step: 7800  \tTraining accuracy: 0.9040100574493408\n",
      "Step: 7800  \tValid loss: 0.22685961425304413\n",
      "Step: 7900  \tTraining loss: 0.22109641134738922\n",
      "Step: 7900  \tTraining accuracy: 0.9040809273719788\n",
      "Step: 7900  \tValid loss: 0.22680020332336426\n",
      "Step: 8000  \tTraining loss: 0.22107666730880737\n",
      "Step: 8000  \tTraining accuracy: 0.9041500687599182\n",
      "Step: 8000  \tValid loss: 0.2267596423625946\n",
      "Step: 8100  \tTraining loss: 0.22105726599693298\n",
      "Step: 8100  \tTraining accuracy: 0.9042174816131592\n",
      "Step: 8100  \tValid loss: 0.22673332691192627\n",
      "Step: 8200  \tTraining loss: 0.22103911638259888\n",
      "Step: 8200  \tTraining accuracy: 0.9042832255363464\n",
      "Step: 8200  \tValid loss: 0.22671309113502502\n",
      "Step: 8300  \tTraining loss: 0.22102096676826477\n",
      "Step: 8300  \tTraining accuracy: 0.9043473601341248\n",
      "Step: 8300  \tValid loss: 0.22670051455497742\n",
      "Step: 8400  \tTraining loss: 0.22100332379341125\n",
      "Step: 8400  \tTraining accuracy: 0.9044100046157837\n",
      "Step: 8400  \tValid loss: 0.22668997943401337\n",
      "Step: 8500  \tTraining loss: 0.22098682820796967\n",
      "Step: 8500  \tTraining accuracy: 0.9044710993766785\n",
      "Step: 8500  \tValid loss: 0.22668032348155975\n",
      "Step: 8600  \tTraining loss: 0.22096988558769226\n",
      "Step: 8600  \tTraining accuracy: 0.9045308232307434\n",
      "Step: 8600  \tValid loss: 0.226671501994133\n",
      "Step: 8700  \tTraining loss: 0.22095419466495514\n",
      "Step: 8700  \tTraining accuracy: 0.9045891165733337\n",
      "Step: 8700  \tValid loss: 0.22666315734386444\n",
      "Step: 8800  \tTraining loss: 0.22093871235847473\n",
      "Step: 8800  \tTraining accuracy: 0.9046461582183838\n",
      "Step: 8800  \tValid loss: 0.2266547679901123\n",
      "Step: 8900  \tTraining loss: 0.22092345356941223\n",
      "Step: 8900  \tTraining accuracy: 0.904701828956604\n",
      "Step: 8900  \tValid loss: 0.22664567828178406\n",
      "Step: 9000  \tTraining loss: 0.22090816497802734\n",
      "Step: 9000  \tTraining accuracy: 0.9047563076019287\n",
      "Step: 9000  \tValid loss: 0.22664086520671844\n",
      "Step: 9100  \tTraining loss: 0.22089357674121857\n",
      "Step: 9100  \tTraining accuracy: 0.9048095345497131\n",
      "Step: 9100  \tValid loss: 0.22663582861423492\n",
      "Step: 9200  \tTraining loss: 0.22087955474853516\n",
      "Step: 9200  \tTraining accuracy: 0.9048616290092468\n",
      "Step: 9200  \tValid loss: 0.22662808001041412\n",
      "Step: 9300  \tTraining loss: 0.22086548805236816\n",
      "Step: 9300  \tTraining accuracy: 0.9049125909805298\n",
      "Step: 9300  \tValid loss: 0.22661659121513367\n",
      "Step: 9400  \tTraining loss: 0.22085151076316833\n",
      "Step: 9400  \tTraining accuracy: 0.9049624800682068\n",
      "Step: 9400  \tValid loss: 0.2266054004430771\n",
      "Step: 9500  \tTraining loss: 0.2208380401134491\n",
      "Step: 9500  \tTraining accuracy: 0.9050112962722778\n",
      "Step: 9500  \tValid loss: 0.22659185528755188\n",
      "Step: 9600  \tTraining loss: 0.22082482278347015\n",
      "Step: 9600  \tTraining accuracy: 0.9050590991973877\n",
      "Step: 9600  \tValid loss: 0.2265779674053192\n",
      "Step: 9700  \tTraining loss: 0.22081126272678375\n",
      "Step: 9700  \tTraining accuracy: 0.9051058888435364\n",
      "Step: 9700  \tValid loss: 0.22656333446502686\n",
      "Step: 9800  \tTraining loss: 0.220798522233963\n",
      "Step: 9800  \tTraining accuracy: 0.9051517248153687\n",
      "Step: 9800  \tValid loss: 0.22654932737350464\n",
      "Step: 9900  \tTraining loss: 0.22078511118888855\n",
      "Step: 9900  \tTraining accuracy: 0.9051966667175293\n",
      "Step: 9900  \tValid loss: 0.22653448581695557\n",
      "Step: 10000  \tTraining loss: 0.22077368199825287\n",
      "Step: 10000  \tTraining accuracy: 0.9052406549453735\n",
      "Step: 10000  \tValid loss: 0.2265148162841797\n",
      "Step: 10100  \tTraining loss: 0.22076265513896942\n",
      "Step: 10100  \tTraining accuracy: 0.9052838087081909\n",
      "Step: 10100  \tValid loss: 0.22649729251861572\n",
      "Step: 10200  \tTraining loss: 0.22075210511684418\n",
      "Step: 10200  \tTraining accuracy: 0.9053260684013367\n",
      "Step: 10200  \tValid loss: 0.22647824883460999\n",
      "Step: 10300  \tTraining loss: 0.22074224054813385\n",
      "Step: 10300  \tTraining accuracy: 0.9053675532341003\n",
      "Step: 10300  \tValid loss: 0.22646164894104004\n",
      "Step: 10400  \tTraining loss: 0.2207321971654892\n",
      "Step: 10400  \tTraining accuracy: 0.9054082036018372\n",
      "Step: 10400  \tValid loss: 0.22644644975662231\n",
      "Step: 10500  \tTraining loss: 0.22072267532348633\n",
      "Step: 10500  \tTraining accuracy: 0.9054480791091919\n",
      "Step: 10500  \tValid loss: 0.22642841935157776\n",
      "Step: 10600  \tTraining loss: 0.22071290016174316\n",
      "Step: 10600  \tTraining accuracy: 0.9054872393608093\n",
      "Step: 10600  \tValid loss: 0.22641414403915405\n",
      "Step: 10700  \tTraining loss: 0.22069938480854034\n",
      "Step: 10700  \tTraining accuracy: 0.9055256247520447\n",
      "Step: 10700  \tValid loss: 0.22638610005378723\n",
      "Step: 10800  \tTraining loss: 0.22068384289741516\n",
      "Step: 10800  \tTraining accuracy: 0.9055632948875427\n",
      "Step: 10800  \tValid loss: 0.22638539969921112\n",
      "Step: 10900  \tTraining loss: 0.22067029774188995\n",
      "Step: 10900  \tTraining accuracy: 0.9056002497673035\n",
      "Step: 10900  \tValid loss: 0.2263960838317871\n",
      "Step: 11000  \tTraining loss: 0.22065593302249908\n",
      "Step: 11000  \tTraining accuracy: 0.9056365489959717\n",
      "Step: 11000  \tValid loss: 0.22640052437782288\n",
      "Step: 11100  \tTraining loss: 0.22064217925071716\n",
      "Step: 11100  \tTraining accuracy: 0.9056721925735474\n",
      "Step: 11100  \tValid loss: 0.226399227976799\n",
      "Step: 11200  \tTraining loss: 0.22062863409519196\n",
      "Step: 11200  \tTraining accuracy: 0.9057072401046753\n",
      "Step: 11200  \tValid loss: 0.22639374434947968\n",
      "Step: 11300  \tTraining loss: 0.22061443328857422\n",
      "Step: 11300  \tTraining accuracy: 0.9057416319847107\n",
      "Step: 11300  \tValid loss: 0.22638565301895142\n",
      "Step: 11400  \tTraining loss: 0.22060060501098633\n",
      "Step: 11400  \tTraining accuracy: 0.9057753682136536\n",
      "Step: 11400  \tValid loss: 0.22637411952018738\n",
      "Step: 11500  \tTraining loss: 0.22058537602424622\n",
      "Step: 11500  \tTraining accuracy: 0.9058085680007935\n",
      "Step: 11500  \tValid loss: 0.22637434303760529\n",
      "Step: 11600  \tTraining loss: 0.2205704301595688\n",
      "Step: 11600  \tTraining accuracy: 0.9058412313461304\n",
      "Step: 11600  \tValid loss: 0.2263789027929306\n",
      "Step: 11700  \tTraining loss: 0.22055496275424957\n",
      "Step: 11700  \tTraining accuracy: 0.9058732390403748\n",
      "Step: 11700  \tValid loss: 0.22637976706027985\n",
      "Step: 11800  \tTraining loss: 0.22053930163383484\n",
      "Step: 11800  \tTraining accuracy: 0.9059047698974609\n",
      "Step: 11800  \tValid loss: 0.2263811230659485\n",
      "Step: 11900  \tTraining loss: 0.22052402794361115\n",
      "Step: 11900  \tTraining accuracy: 0.9059357643127441\n",
      "Step: 11900  \tValid loss: 0.22637026011943817\n",
      "Step: 12000  \tTraining loss: 0.2205037921667099\n",
      "Step: 12000  \tTraining accuracy: 0.9059662222862244\n",
      "Step: 12000  \tValid loss: 0.22632035613059998\n",
      "Step: 12100  \tTraining loss: 0.22048155963420868\n",
      "Step: 12100  \tTraining accuracy: 0.9059961438179016\n",
      "Step: 12100  \tValid loss: 0.22627809643745422\n",
      "Step: 12200  \tTraining loss: 0.22046513855457306\n",
      "Step: 12200  \tTraining accuracy: 0.9060256481170654\n",
      "Step: 12200  \tValid loss: 0.2262764722108841\n",
      "Step: 12300  \tTraining loss: 0.22045297920703888\n",
      "Step: 12300  \tTraining accuracy: 0.9060546159744263\n",
      "Step: 12300  \tValid loss: 0.22627530992031097\n",
      "Step: 12400  \tTraining loss: 0.22044172883033752\n",
      "Step: 12400  \tTraining accuracy: 0.9060831069946289\n",
      "Step: 12400  \tValid loss: 0.2262570858001709\n",
      "Step: 12500  \tTraining loss: 0.2204308807849884\n",
      "Step: 12500  \tTraining accuracy: 0.9061111807823181\n",
      "Step: 12500  \tValid loss: 0.22623594105243683\n",
      "Step: 12600  \tTraining loss: 0.22042080760002136\n",
      "Step: 12600  \tTraining accuracy: 0.9061387777328491\n",
      "Step: 12600  \tValid loss: 0.22620894014835358\n",
      "Step: 12700  \tTraining loss: 0.22041133046150208\n",
      "Step: 12700  \tTraining accuracy: 0.9061659574508667\n",
      "Step: 12700  \tValid loss: 0.22619377076625824\n",
      "Step: 12800  \tTraining loss: 0.2204028218984604\n",
      "Step: 12800  \tTraining accuracy: 0.9061927199363708\n",
      "Step: 12800  \tValid loss: 0.2261749804019928\n",
      "Step: 12900  \tTraining loss: 0.2203947752714157\n",
      "Step: 12900  \tTraining accuracy: 0.9062190055847168\n",
      "Step: 12900  \tValid loss: 0.22615621984004974\n",
      "Step: 13000  \tTraining loss: 0.22038634121418\n",
      "Step: 13000  \tTraining accuracy: 0.9062449336051941\n",
      "Step: 13000  \tValid loss: 0.22614271938800812\n",
      "Step: 13100  \tTraining loss: 0.2203778773546219\n",
      "Step: 13100  \tTraining accuracy: 0.906270444393158\n",
      "Step: 13100  \tValid loss: 0.2261304259300232\n",
      "Step: 13200  \tTraining loss: 0.2203696072101593\n",
      "Step: 13200  \tTraining accuracy: 0.9062955975532532\n",
      "Step: 13200  \tValid loss: 0.22611889243125916\n",
      "Step: 13300  \tTraining loss: 0.22036100924015045\n",
      "Step: 13300  \tTraining accuracy: 0.906320333480835\n",
      "Step: 13300  \tValid loss: 0.2261085957288742\n",
      "Step: 13400  \tTraining loss: 0.22035247087478638\n",
      "Step: 13400  \tTraining accuracy: 0.9063447713851929\n",
      "Step: 13400  \tValid loss: 0.22609788179397583\n",
      "Step: 13500  \tTraining loss: 0.2203444391489029\n",
      "Step: 13500  \tTraining accuracy: 0.9063687920570374\n",
      "Step: 13500  \tValid loss: 0.22608615458011627\n",
      "Step: 13600  \tTraining loss: 0.2203359454870224\n",
      "Step: 13600  \tTraining accuracy: 0.9063924551010132\n",
      "Step: 13600  \tValid loss: 0.22607778012752533\n",
      "Step: 13700  \tTraining loss: 0.2203272581100464\n",
      "Step: 13700  \tTraining accuracy: 0.9064157605171204\n",
      "Step: 13700  \tValid loss: 0.22606700658798218\n",
      "Step: 13800  \tTraining loss: 0.22031888365745544\n",
      "Step: 13800  \tTraining accuracy: 0.9064387083053589\n",
      "Step: 13800  \tValid loss: 0.22605696320533752\n",
      "Step: 13900  \tTraining loss: 0.22031019628047943\n",
      "Step: 13900  \tTraining accuracy: 0.9064613580703735\n",
      "Step: 13900  \tValid loss: 0.22604677081108093\n",
      "Step: 14000  \tTraining loss: 0.22030167281627655\n",
      "Step: 14000  \tTraining accuracy: 0.9064837098121643\n",
      "Step: 14000  \tValid loss: 0.2260380983352661\n",
      "Step: 14100  \tTraining loss: 0.220292866230011\n",
      "Step: 14100  \tTraining accuracy: 0.9065057039260864\n",
      "Step: 14100  \tValid loss: 0.22602654993534088\n",
      "Step: 14200  \tTraining loss: 0.22028447687625885\n",
      "Step: 14200  \tTraining accuracy: 0.9065274000167847\n",
      "Step: 14200  \tValid loss: 0.22601789236068726\n",
      "Step: 14300  \tTraining loss: 0.22027575969696045\n",
      "Step: 14300  \tTraining accuracy: 0.906548798084259\n",
      "Step: 14300  \tValid loss: 0.22600750625133514\n",
      "Step: 14400  \tTraining loss: 0.2202671319246292\n",
      "Step: 14400  \tTraining accuracy: 0.9065698981285095\n",
      "Step: 14400  \tValid loss: 0.22599905729293823\n",
      "Step: 14500  \tTraining loss: 0.22025853395462036\n",
      "Step: 14500  \tTraining accuracy: 0.9065907001495361\n",
      "Step: 14500  \tValid loss: 0.22598755359649658\n",
      "Step: 14600  \tTraining loss: 0.2202499806880951\n",
      "Step: 14600  \tTraining accuracy: 0.9066112041473389\n",
      "Step: 14600  \tValid loss: 0.22597721219062805\n",
      "Step: 14700  \tTraining loss: 0.22024141252040863\n",
      "Step: 14700  \tTraining accuracy: 0.9066314697265625\n",
      "Step: 14700  \tValid loss: 0.22596395015716553\n",
      "Step: 14800  \tTraining loss: 0.2202330082654953\n",
      "Step: 14800  \tTraining accuracy: 0.9066514372825623\n",
      "Step: 14800  \tValid loss: 0.22594532370567322\n",
      "Step: 14900  \tTraining loss: 0.22022517025470734\n",
      "Step: 14900  \tTraining accuracy: 0.9066711068153381\n",
      "Step: 14900  \tValid loss: 0.22592563927173615\n",
      "Step: 15000  \tTraining loss: 0.2202175110578537\n",
      "Step: 15000  \tTraining accuracy: 0.9066905379295349\n",
      "Step: 15000  \tValid loss: 0.22590778768062592\n",
      "Step: 15100  \tTraining loss: 0.22020982205867767\n",
      "Step: 15100  \tTraining accuracy: 0.9067097306251526\n",
      "Step: 15100  \tValid loss: 0.2258896380662918\n",
      "Step: 15200  \tTraining loss: 0.22020259499549866\n",
      "Step: 15200  \tTraining accuracy: 0.9067286252975464\n",
      "Step: 15200  \tValid loss: 0.22587129473686218\n",
      "Step: 15300  \tTraining loss: 0.22019539773464203\n",
      "Step: 15300  \tTraining accuracy: 0.9067472815513611\n",
      "Step: 15300  \tValid loss: 0.2258499264717102\n",
      "Step: 15400  \tTraining loss: 0.22018861770629883\n",
      "Step: 15400  \tTraining accuracy: 0.9067656993865967\n",
      "Step: 15400  \tValid loss: 0.22583040595054626\n",
      "Step: 15500  \tTraining loss: 0.22018231451511383\n",
      "Step: 15500  \tTraining accuracy: 0.906783938407898\n",
      "Step: 15500  \tValid loss: 0.22581161558628082\n",
      "Step: 15600  \tTraining loss: 0.22017648816108704\n",
      "Step: 15600  \tTraining accuracy: 0.9068018794059753\n",
      "Step: 15600  \tValid loss: 0.22579142451286316\n",
      "Step: 15700  \tTraining loss: 0.22017072141170502\n",
      "Step: 15700  \tTraining accuracy: 0.9068195819854736\n",
      "Step: 15700  \tValid loss: 0.22577252984046936\n",
      "Step: 15800  \tTraining loss: 0.2201654464006424\n",
      "Step: 15800  \tTraining accuracy: 0.9068371057510376\n",
      "Step: 15800  \tValid loss: 0.22575336694717407\n",
      "Step: 15900  \tTraining loss: 0.22016039490699768\n",
      "Step: 15900  \tTraining accuracy: 0.9068543910980225\n",
      "Step: 15900  \tValid loss: 0.225735142827034\n",
      "Step: 16000  \tTraining loss: 0.22015482187271118\n",
      "Step: 16000  \tTraining accuracy: 0.9068714380264282\n",
      "Step: 16000  \tValid loss: 0.2257179617881775\n",
      "Step: 16100  \tTraining loss: 0.22014985978603363\n",
      "Step: 16100  \tTraining accuracy: 0.9068883061408997\n",
      "Step: 16100  \tValid loss: 0.22570253908634186\n",
      "Step: 16200  \tTraining loss: 0.22014470398426056\n",
      "Step: 16200  \tTraining accuracy: 0.906904935836792\n",
      "Step: 16200  \tValid loss: 0.2256864607334137\n",
      "Step: 16300  \tTraining loss: 0.22013986110687256\n",
      "Step: 16300  \tTraining accuracy: 0.90692138671875\n",
      "Step: 16300  \tValid loss: 0.22566987574100494\n",
      "Step: 16400  \tTraining loss: 0.2201346904039383\n",
      "Step: 16400  \tTraining accuracy: 0.9069375991821289\n",
      "Step: 16400  \tValid loss: 0.2256559133529663\n",
      "Step: 16500  \tTraining loss: 0.2201303243637085\n",
      "Step: 16500  \tTraining accuracy: 0.9069536328315735\n",
      "Step: 16500  \tValid loss: 0.22564099729061127\n",
      "Step: 16600  \tTraining loss: 0.22012536227703094\n",
      "Step: 16600  \tTraining accuracy: 0.9069694876670837\n",
      "Step: 16600  \tValid loss: 0.22562618553638458\n",
      "Step: 16700  \tTraining loss: 0.22012071311473846\n",
      "Step: 16700  \tTraining accuracy: 0.9069851040840149\n",
      "Step: 16700  \tValid loss: 0.22561341524124146\n",
      "Step: 16800  \tTraining loss: 0.22011642158031464\n",
      "Step: 16800  \tTraining accuracy: 0.9070006012916565\n",
      "Step: 16800  \tValid loss: 0.22559984028339386\n",
      "Step: 16900  \tTraining loss: 0.22011150419712067\n",
      "Step: 16900  \tTraining accuracy: 0.907015860080719\n",
      "Step: 16900  \tValid loss: 0.22558675706386566\n",
      "Step: 17000  \tTraining loss: 0.22010718286037445\n",
      "Step: 17000  \tTraining accuracy: 0.9070309996604919\n",
      "Step: 17000  \tValid loss: 0.22557374835014343\n",
      "Step: 17100  \tTraining loss: 0.22010311484336853\n",
      "Step: 17100  \tTraining accuracy: 0.9070459008216858\n",
      "Step: 17100  \tValid loss: 0.2255614846944809\n",
      "Step: 17200  \tTraining loss: 0.2200988382101059\n",
      "Step: 17200  \tTraining accuracy: 0.9070606827735901\n",
      "Step: 17200  \tValid loss: 0.22554847598075867\n",
      "Step: 17300  \tTraining loss: 0.2200944572687149\n",
      "Step: 17300  \tTraining accuracy: 0.9070752263069153\n",
      "Step: 17300  \tValid loss: 0.22553828358650208\n",
      "Step: 17400  \tTraining loss: 0.22009044885635376\n",
      "Step: 17400  \tTraining accuracy: 0.9070896506309509\n",
      "Step: 17400  \tValid loss: 0.22552669048309326\n",
      "Step: 17500  \tTraining loss: 0.2200864851474762\n",
      "Step: 17500  \tTraining accuracy: 0.9071038961410522\n",
      "Step: 17500  \tValid loss: 0.22551539540290833\n",
      "Step: 17600  \tTraining loss: 0.22008223831653595\n",
      "Step: 17600  \tTraining accuracy: 0.9071179628372192\n",
      "Step: 17600  \tValid loss: 0.2255047857761383\n",
      "Step: 17700  \tTraining loss: 0.2200784683227539\n",
      "Step: 17700  \tTraining accuracy: 0.9071319103240967\n",
      "Step: 17700  \tValid loss: 0.22549447417259216\n",
      "Step: 17800  \tTraining loss: 0.2200746387243271\n",
      "Step: 17800  \tTraining accuracy: 0.9071456789970398\n",
      "Step: 17800  \tValid loss: 0.22548481822013855\n",
      "Step: 17900  \tTraining loss: 0.22007109224796295\n",
      "Step: 17900  \tTraining accuracy: 0.9071592688560486\n",
      "Step: 17900  \tValid loss: 0.2254745215177536\n",
      "Step: 18000  \tTraining loss: 0.2200671285390854\n",
      "Step: 18000  \tTraining accuracy: 0.9071727395057678\n",
      "Step: 18000  \tValid loss: 0.22546520829200745\n",
      "Step: 18100  \tTraining loss: 0.22006343305110931\n",
      "Step: 18100  \tTraining accuracy: 0.9071860909461975\n",
      "Step: 18100  \tValid loss: 0.22545568645000458\n",
      "Step: 18200  \tTraining loss: 0.2200596183538437\n",
      "Step: 18200  \tTraining accuracy: 0.9071992039680481\n",
      "Step: 18200  \tValid loss: 0.22544696927070618\n",
      "Step: 18300  \tTraining loss: 0.22005592286586761\n",
      "Step: 18300  \tTraining accuracy: 0.9072122573852539\n",
      "Step: 18300  \tValid loss: 0.22543831169605255\n",
      "Step: 18400  \tTraining loss: 0.22005246579647064\n",
      "Step: 18400  \tTraining accuracy: 0.9072251319885254\n",
      "Step: 18400  \tValid loss: 0.22542990744113922\n",
      "Step: 18500  \tTraining loss: 0.22004921734333038\n",
      "Step: 18500  \tTraining accuracy: 0.9072378873825073\n",
      "Step: 18500  \tValid loss: 0.22542142868041992\n",
      "Step: 18600  \tTraining loss: 0.22004570066928864\n",
      "Step: 18600  \tTraining accuracy: 0.9072504639625549\n",
      "Step: 18600  \tValid loss: 0.2254130095243454\n",
      "Step: 18700  \tTraining loss: 0.22004178166389465\n",
      "Step: 18700  \tTraining accuracy: 0.907262921333313\n",
      "Step: 18700  \tValid loss: 0.22540587186813354\n",
      "Step: 18800  \tTraining loss: 0.22003893554210663\n",
      "Step: 18800  \tTraining accuracy: 0.9072752594947815\n",
      "Step: 18800  \tValid loss: 0.22539815306663513\n",
      "Step: 18900  \tTraining loss: 0.220035120844841\n",
      "Step: 18900  \tTraining accuracy: 0.9072874784469604\n",
      "Step: 18900  \tValid loss: 0.22539080679416656\n",
      "Step: 19000  \tTraining loss: 0.2200319468975067\n",
      "Step: 19000  \tTraining accuracy: 0.9072995781898499\n",
      "Step: 19000  \tValid loss: 0.22538374364376068\n",
      "Step: 19100  \tTraining loss: 0.22002875804901123\n",
      "Step: 19100  \tTraining accuracy: 0.9073114991188049\n",
      "Step: 19100  \tValid loss: 0.22537687420845032\n",
      "Step: 19200  \tTraining loss: 0.22002553939819336\n",
      "Step: 19200  \tTraining accuracy: 0.9073233604431152\n",
      "Step: 19200  \tValid loss: 0.2253699004650116\n",
      "Step: 19300  \tTraining loss: 0.2200223207473755\n",
      "Step: 19300  \tTraining accuracy: 0.9073350429534912\n",
      "Step: 19300  \tValid loss: 0.22536338865756989\n",
      "Step: 19400  \tTraining loss: 0.22001913189888\n",
      "Step: 19400  \tTraining accuracy: 0.9073466062545776\n",
      "Step: 19400  \tValid loss: 0.22535695135593414\n",
      "Step: 19500  \tTraining loss: 0.2200157791376114\n",
      "Step: 19500  \tTraining accuracy: 0.9073580503463745\n",
      "Step: 19500  \tValid loss: 0.2253509908914566\n",
      "Step: 19600  \tTraining loss: 0.22001296281814575\n",
      "Step: 19600  \tTraining accuracy: 0.9073694348335266\n",
      "Step: 19600  \tValid loss: 0.22534388303756714\n",
      "Step: 19700  \tTraining loss: 0.2200099676847458\n",
      "Step: 19700  \tTraining accuracy: 0.9073806405067444\n",
      "Step: 19700  \tValid loss: 0.22533899545669556\n",
      "Step: 19800  \tTraining loss: 0.22000646591186523\n",
      "Step: 19800  \tTraining accuracy: 0.9073917865753174\n",
      "Step: 19800  \tValid loss: 0.22533318400382996\n",
      "Step: 19900  \tTraining loss: 0.22000351548194885\n",
      "Step: 19900  \tTraining accuracy: 0.907402753829956\n",
      "Step: 19900  \tValid loss: 0.2253279983997345\n",
      "Step: 20000  \tTraining loss: 0.22000069916248322\n",
      "Step: 20000  \tTraining accuracy: 0.90741366147995\n",
      "Step: 20000  \tValid loss: 0.2253224104642868\n",
      "Step: 20100  \tTraining loss: 0.21999777853488922\n",
      "Step: 20100  \tTraining accuracy: 0.9074244499206543\n",
      "Step: 20100  \tValid loss: 0.22531621158123016\n",
      "Step: 20200  \tTraining loss: 0.21999439597129822\n",
      "Step: 20200  \tTraining accuracy: 0.9074351191520691\n",
      "Step: 20200  \tValid loss: 0.22531212866306305\n",
      "Step: 20300  \tTraining loss: 0.21999134123325348\n",
      "Step: 20300  \tTraining accuracy: 0.9074456691741943\n",
      "Step: 20300  \tValid loss: 0.22530990839004517\n",
      "Step: 20400  \tTraining loss: 0.21998846530914307\n",
      "Step: 20400  \tTraining accuracy: 0.9074561595916748\n",
      "Step: 20400  \tValid loss: 0.22530469298362732\n",
      "Step: 20500  \tTraining loss: 0.219985693693161\n",
      "Step: 20500  \tTraining accuracy: 0.9074665307998657\n",
      "Step: 20500  \tValid loss: 0.22530123591423035\n",
      "Step: 20600  \tTraining loss: 0.2199828028678894\n",
      "Step: 20600  \tTraining accuracy: 0.9074767827987671\n",
      "Step: 20600  \tValid loss: 0.22529709339141846\n",
      "Step: 20700  \tTraining loss: 0.2199796736240387\n",
      "Step: 20700  \tTraining accuracy: 0.9074869751930237\n",
      "Step: 20700  \tValid loss: 0.22529298067092896\n",
      "Step: 20800  \tTraining loss: 0.21997730433940887\n",
      "Step: 20800  \tTraining accuracy: 0.9074970483779907\n",
      "Step: 20800  \tValid loss: 0.22528989613056183\n",
      "Step: 20900  \tTraining loss: 0.21997442841529846\n",
      "Step: 20900  \tTraining accuracy: 0.9075070023536682\n",
      "Step: 20900  \tValid loss: 0.22528652846813202\n",
      "Step: 21000  \tTraining loss: 0.2199712097644806\n",
      "Step: 21000  \tTraining accuracy: 0.9075168371200562\n",
      "Step: 21000  \tValid loss: 0.22528328001499176\n",
      "Step: 21100  \tTraining loss: 0.21996867656707764\n",
      "Step: 21100  \tTraining accuracy: 0.9075266718864441\n",
      "Step: 21100  \tValid loss: 0.22527948021888733\n",
      "Step: 21200  \tTraining loss: 0.21996566653251648\n",
      "Step: 21200  \tTraining accuracy: 0.9075363278388977\n",
      "Step: 21200  \tValid loss: 0.22527636587619781\n",
      "Step: 21300  \tTraining loss: 0.2199629843235016\n",
      "Step: 21300  \tTraining accuracy: 0.9075459241867065\n",
      "Step: 21300  \tValid loss: 0.22527411580085754\n",
      "Step: 21400  \tTraining loss: 0.2199603021144867\n",
      "Step: 21400  \tTraining accuracy: 0.9075554609298706\n",
      "Step: 21400  \tValid loss: 0.2252698689699173\n",
      "Step: 21500  \tTraining loss: 0.21995709836483002\n",
      "Step: 21500  \tTraining accuracy: 0.9075648784637451\n",
      "Step: 21500  \tValid loss: 0.22526699304580688\n",
      "Step: 21600  \tTraining loss: 0.2199544608592987\n",
      "Step: 21600  \tTraining accuracy: 0.9075741767883301\n",
      "Step: 21600  \tValid loss: 0.22526365518569946\n",
      "Step: 21700  \tTraining loss: 0.2199515998363495\n",
      "Step: 21700  \tTraining accuracy: 0.9075834155082703\n",
      "Step: 21700  \tValid loss: 0.22526048123836517\n",
      "Step: 21800  \tTraining loss: 0.21994900703430176\n",
      "Step: 21800  \tTraining accuracy: 0.9075925946235657\n",
      "Step: 21800  \tValid loss: 0.22525817155838013\n",
      "Step: 21900  \tTraining loss: 0.21994620561599731\n",
      "Step: 21900  \tTraining accuracy: 0.9076016545295715\n",
      "Step: 21900  \tValid loss: 0.22525499761104584\n",
      "Step: 22000  \tTraining loss: 0.21994322538375854\n",
      "Step: 22000  \tTraining accuracy: 0.9076106548309326\n",
      "Step: 22000  \tValid loss: 0.2252524048089981\n",
      "Step: 22100  \tTraining loss: 0.21994063258171082\n",
      "Step: 22100  \tTraining accuracy: 0.9076195955276489\n",
      "Step: 22100  \tValid loss: 0.2252492904663086\n",
      "Step: 22200  \tTraining loss: 0.21993793547153473\n",
      "Step: 22200  \tTraining accuracy: 0.9076284170150757\n",
      "Step: 22200  \tValid loss: 0.22524651885032654\n",
      "Step: 22300  \tTraining loss: 0.21993482112884521\n",
      "Step: 22300  \tTraining accuracy: 0.9076371788978577\n",
      "Step: 22300  \tValid loss: 0.22524386644363403\n",
      "Step: 22400  \tTraining loss: 0.21993213891983032\n",
      "Step: 22400  \tTraining accuracy: 0.9076458215713501\n",
      "Step: 22400  \tValid loss: 0.22524157166481018\n",
      "Step: 22500  \tTraining loss: 0.2199295312166214\n",
      "Step: 22500  \tTraining accuracy: 0.9076544046401978\n",
      "Step: 22500  \tValid loss: 0.22523869574069977\n",
      "Step: 22600  \tTraining loss: 0.2199268788099289\n",
      "Step: 22600  \tTraining accuracy: 0.9076629281044006\n",
      "Step: 22600  \tValid loss: 0.22523565590381622\n",
      "Step: 22700  \tTraining loss: 0.21992401778697968\n",
      "Step: 22700  \tTraining accuracy: 0.9076713919639587\n",
      "Step: 22700  \tValid loss: 0.22523392736911774\n",
      "Step: 22800  \tTraining loss: 0.21992093324661255\n",
      "Step: 22800  \tTraining accuracy: 0.9076797962188721\n",
      "Step: 22800  \tValid loss: 0.2252315729856491\n",
      "Step: 22900  \tTraining loss: 0.2199183851480484\n",
      "Step: 22900  \tTraining accuracy: 0.9076880812644958\n",
      "Step: 22900  \tValid loss: 0.2252282351255417\n",
      "Step: 23000  \tTraining loss: 0.2199154794216156\n",
      "Step: 23000  \tTraining accuracy: 0.9076963067054749\n",
      "Step: 23000  \tValid loss: 0.2252260446548462\n",
      "Step: 23100  \tTraining loss: 0.21991273760795593\n",
      "Step: 23100  \tTraining accuracy: 0.9077044725418091\n",
      "Step: 23100  \tValid loss: 0.2252228856086731\n",
      "Step: 23200  \tTraining loss: 0.2199098765850067\n",
      "Step: 23200  \tTraining accuracy: 0.9077125191688538\n",
      "Step: 23200  \tValid loss: 0.22522148489952087\n",
      "Step: 23300  \tTraining loss: 0.21990753710269928\n",
      "Step: 23300  \tTraining accuracy: 0.9077205657958984\n",
      "Step: 23300  \tValid loss: 0.22521834075450897\n",
      "Step: 23400  \tTraining loss: 0.2199045717716217\n",
      "Step: 23400  \tTraining accuracy: 0.9077284932136536\n",
      "Step: 23400  \tValid loss: 0.225216343998909\n",
      "Step: 23500  \tTraining loss: 0.21990184485912323\n",
      "Step: 23500  \tTraining accuracy: 0.9077363610267639\n",
      "Step: 23500  \tValid loss: 0.22521457076072693\n",
      "Step: 23600  \tTraining loss: 0.2198990285396576\n",
      "Step: 23600  \tTraining accuracy: 0.9077441692352295\n",
      "Step: 23600  \tValid loss: 0.22521252930164337\n",
      "Step: 23700  \tTraining loss: 0.21989630162715912\n",
      "Step: 23700  \tTraining accuracy: 0.9077519178390503\n",
      "Step: 23700  \tValid loss: 0.22521063685417175\n",
      "Step: 23800  \tTraining loss: 0.21989350020885468\n",
      "Step: 23800  \tTraining accuracy: 0.9077596068382263\n",
      "Step: 23800  \tValid loss: 0.22520770132541656\n",
      "Step: 23900  \tTraining loss: 0.21989081799983978\n",
      "Step: 23900  \tTraining accuracy: 0.9077672362327576\n",
      "Step: 23900  \tValid loss: 0.2252056747674942\n",
      "Step: 24000  \tTraining loss: 0.21988803148269653\n",
      "Step: 24000  \tTraining accuracy: 0.9077747464179993\n",
      "Step: 24000  \tValid loss: 0.22520317137241364\n",
      "Step: 24100  \tTraining loss: 0.21988525986671448\n",
      "Step: 24100  \tTraining accuracy: 0.907782256603241\n",
      "Step: 24100  \tValid loss: 0.2252010852098465\n",
      "Step: 24200  \tTraining loss: 0.21988269686698914\n",
      "Step: 24200  \tTraining accuracy: 0.9077897071838379\n",
      "Step: 24200  \tValid loss: 0.22519885003566742\n",
      "Step: 24300  \tTraining loss: 0.21987953782081604\n",
      "Step: 24300  \tTraining accuracy: 0.9077970385551453\n",
      "Step: 24300  \tValid loss: 0.22519731521606445\n",
      "Step: 24400  \tTraining loss: 0.2198769897222519\n",
      "Step: 24400  \tTraining accuracy: 0.9078043699264526\n",
      "Step: 24400  \tValid loss: 0.2251947522163391\n",
      "Step: 24500  \tTraining loss: 0.21987414360046387\n",
      "Step: 24500  \tTraining accuracy: 0.9078115820884705\n",
      "Step: 24500  \tValid loss: 0.22519277036190033\n",
      "Step: 24600  \tTraining loss: 0.2198714017868042\n",
      "Step: 24600  \tTraining accuracy: 0.9078187942504883\n",
      "Step: 24600  \tValid loss: 0.2251913696527481\n",
      "Step: 24700  \tTraining loss: 0.21986842155456543\n",
      "Step: 24700  \tTraining accuracy: 0.9078258872032166\n",
      "Step: 24700  \tValid loss: 0.2251889854669571\n",
      "Step: 24800  \tTraining loss: 0.21986554563045502\n",
      "Step: 24800  \tTraining accuracy: 0.9078329801559448\n",
      "Step: 24800  \tValid loss: 0.22518770396709442\n",
      "Step: 24900  \tTraining loss: 0.21986307203769684\n",
      "Step: 24900  \tTraining accuracy: 0.9078400135040283\n",
      "Step: 24900  \tValid loss: 0.22518564760684967\n",
      "Step: 25000  \tTraining loss: 0.21986009180545807\n",
      "Step: 25000  \tTraining accuracy: 0.9078469276428223\n",
      "Step: 25000  \tValid loss: 0.2251838594675064\n",
      "Step: 25100  \tTraining loss: 0.21985724568367004\n",
      "Step: 25100  \tTraining accuracy: 0.9078538417816162\n",
      "Step: 25100  \tValid loss: 0.22518134117126465\n",
      "Step: 25200  \tTraining loss: 0.2198544144630432\n",
      "Step: 25200  \tTraining accuracy: 0.9078606963157654\n",
      "Step: 25200  \tValid loss: 0.22517912089824677\n",
      "Step: 25300  \tTraining loss: 0.21985168755054474\n",
      "Step: 25300  \tTraining accuracy: 0.9078674912452698\n",
      "Step: 25300  \tValid loss: 0.22517801821231842\n",
      "Step: 25400  \tTraining loss: 0.21984882652759552\n",
      "Step: 25400  \tTraining accuracy: 0.9078742265701294\n",
      "Step: 25400  \tValid loss: 0.22517602145671844\n",
      "Step: 25500  \tTraining loss: 0.21984584629535675\n",
      "Step: 25500  \tTraining accuracy: 0.9078809022903442\n",
      "Step: 25500  \tValid loss: 0.22517386078834534\n",
      "Step: 25600  \tTraining loss: 0.2198430299758911\n",
      "Step: 25600  \tTraining accuracy: 0.9078875780105591\n",
      "Step: 25600  \tValid loss: 0.22517195343971252\n",
      "Step: 25700  \tTraining loss: 0.2198398858308792\n",
      "Step: 25700  \tTraining accuracy: 0.9078941345214844\n",
      "Step: 25700  \tValid loss: 0.22516952455043793\n",
      "Step: 25800  \tTraining loss: 0.21983715891838074\n",
      "Step: 25800  \tTraining accuracy: 0.9079006910324097\n",
      "Step: 25800  \tValid loss: 0.22516770660877228\n",
      "Step: 25900  \tTraining loss: 0.21983422338962555\n",
      "Step: 25900  \tTraining accuracy: 0.9079071283340454\n",
      "Step: 25900  \tValid loss: 0.2251662313938141\n",
      "Step: 26000  \tTraining loss: 0.2198314219713211\n",
      "Step: 26000  \tTraining accuracy: 0.9079135656356812\n",
      "Step: 26000  \tValid loss: 0.22516408562660217\n",
      "Step: 26100  \tTraining loss: 0.21982847154140472\n",
      "Step: 26100  \tTraining accuracy: 0.9079199433326721\n",
      "Step: 26100  \tValid loss: 0.22516170144081116\n",
      "Step: 26200  \tTraining loss: 0.21982568502426147\n",
      "Step: 26200  \tTraining accuracy: 0.9079262614250183\n",
      "Step: 26200  \tValid loss: 0.22515924274921417\n",
      "Step: 26300  \tTraining loss: 0.21982264518737793\n",
      "Step: 26300  \tTraining accuracy: 0.9079325795173645\n",
      "Step: 26300  \tValid loss: 0.22515739500522614\n",
      "Step: 26400  \tTraining loss: 0.21981953084468842\n",
      "Step: 26400  \tTraining accuracy: 0.9079387784004211\n",
      "Step: 26400  \tValid loss: 0.2251528650522232\n",
      "Step: 26500  \tTraining loss: 0.21981684863567352\n",
      "Step: 26500  \tTraining accuracy: 0.9079449772834778\n",
      "Step: 26500  \tValid loss: 0.2251501828432083\n",
      "Step: 26600  \tTraining loss: 0.21981382369995117\n",
      "Step: 26600  \tTraining accuracy: 0.9079511165618896\n",
      "Step: 26600  \tValid loss: 0.2251478135585785\n",
      "Step: 26700  \tTraining loss: 0.21981045603752136\n",
      "Step: 26700  \tTraining accuracy: 0.9079572558403015\n",
      "Step: 26700  \tValid loss: 0.22514496743679047\n",
      "Step: 26800  \tTraining loss: 0.21980734169483185\n",
      "Step: 26800  \tTraining accuracy: 0.9079632759094238\n",
      "Step: 26800  \tValid loss: 0.2251427322626114\n",
      "Step: 26900  \tTraining loss: 0.21980446577072144\n",
      "Step: 26900  \tTraining accuracy: 0.9079692959785461\n",
      "Step: 26900  \tValid loss: 0.22514085471630096\n",
      "Step: 27000  \tTraining loss: 0.21980129182338715\n",
      "Step: 27000  \tTraining accuracy: 0.9079752564430237\n",
      "Step: 27000  \tValid loss: 0.22513693571090698\n",
      "Step: 27100  \tTraining loss: 0.21979841589927673\n",
      "Step: 27100  \tTraining accuracy: 0.9079811573028564\n",
      "Step: 27100  \tValid loss: 0.22513426840305328\n",
      "Step: 27200  \tTraining loss: 0.21979527175426483\n",
      "Step: 27200  \tTraining accuracy: 0.9079870581626892\n",
      "Step: 27200  \tValid loss: 0.22513143718242645\n",
      "Step: 27300  \tTraining loss: 0.21979226171970367\n",
      "Step: 27300  \tTraining accuracy: 0.9079928994178772\n",
      "Step: 27300  \tValid loss: 0.2251283824443817\n",
      "Step: 27400  \tTraining loss: 0.2197895050048828\n",
      "Step: 27400  \tTraining accuracy: 0.9079986810684204\n",
      "Step: 27400  \tValid loss: 0.22512571513652802\n",
      "Step: 27500  \tTraining loss: 0.2197861224412918\n",
      "Step: 27500  \tTraining accuracy: 0.9080044031143188\n",
      "Step: 27500  \tValid loss: 0.2251233011484146\n",
      "Step: 27600  \tTraining loss: 0.2197829931974411\n",
      "Step: 27600  \tTraining accuracy: 0.9080101251602173\n",
      "Step: 27600  \tValid loss: 0.22512014210224152\n",
      "Step: 27700  \tTraining loss: 0.2197800874710083\n",
      "Step: 27700  \tTraining accuracy: 0.908015787601471\n",
      "Step: 27700  \tValid loss: 0.22511711716651917\n",
      "Step: 27800  \tTraining loss: 0.21977698802947998\n",
      "Step: 27800  \tTraining accuracy: 0.9080213904380798\n",
      "Step: 27800  \tValid loss: 0.2251150757074356\n",
      "Step: 27900  \tTraining loss: 0.21977373957633972\n",
      "Step: 27900  \tTraining accuracy: 0.9080269932746887\n",
      "Step: 27900  \tValid loss: 0.2251114547252655\n",
      "Step: 28000  \tTraining loss: 0.21977059543132782\n",
      "Step: 28000  \tTraining accuracy: 0.9080325365066528\n",
      "Step: 28000  \tValid loss: 0.22510869801044464\n",
      "Step: 28100  \tTraining loss: 0.21976739168167114\n",
      "Step: 28100  \tTraining accuracy: 0.9080380201339722\n",
      "Step: 28100  \tValid loss: 0.2251061499118805\n",
      "Step: 28200  \tTraining loss: 0.21976454555988312\n",
      "Step: 28200  \tTraining accuracy: 0.9080435037612915\n",
      "Step: 28200  \tValid loss: 0.2251032590866089\n",
      "Step: 28300  \tTraining loss: 0.21976152062416077\n",
      "Step: 28300  \tTraining accuracy: 0.9080489277839661\n",
      "Step: 28300  \tValid loss: 0.2251003384590149\n",
      "Step: 28400  \tTraining loss: 0.21975865960121155\n",
      "Step: 28400  \tTraining accuracy: 0.9080542922019958\n",
      "Step: 28400  \tValid loss: 0.22509662806987762\n",
      "Step: 28500  \tTraining loss: 0.21975532174110413\n",
      "Step: 28500  \tTraining accuracy: 0.9080596566200256\n",
      "Step: 28500  \tValid loss: 0.22509442269802094\n",
      "Step: 28600  \tTraining loss: 0.21975214779376984\n",
      "Step: 28600  \tTraining accuracy: 0.9080649614334106\n",
      "Step: 28600  \tValid loss: 0.22509093582630157\n",
      "Step: 28700  \tTraining loss: 0.21974928677082062\n",
      "Step: 28700  \tTraining accuracy: 0.9080702066421509\n",
      "Step: 28700  \tValid loss: 0.22508828341960907\n",
      "Step: 28800  \tTraining loss: 0.2197456806898117\n",
      "Step: 28800  \tTraining accuracy: 0.9080754518508911\n",
      "Step: 28800  \tValid loss: 0.22508522868156433\n",
      "Step: 28900  \tTraining loss: 0.21974292397499084\n",
      "Step: 28900  \tTraining accuracy: 0.9080806374549866\n",
      "Step: 28900  \tValid loss: 0.22508221864700317\n",
      "Step: 29000  \tTraining loss: 0.219739630818367\n",
      "Step: 29000  \tTraining accuracy: 0.908085823059082\n",
      "Step: 29000  \tValid loss: 0.225078284740448\n",
      "Step: 29100  \tTraining loss: 0.21973656117916107\n",
      "Step: 29100  \tTraining accuracy: 0.9080909490585327\n",
      "Step: 29100  \tValid loss: 0.22507570683956146\n",
      "Step: 29200  \tTraining loss: 0.21973355114459991\n",
      "Step: 29200  \tTraining accuracy: 0.9080960750579834\n",
      "Step: 29200  \tValid loss: 0.22507283091545105\n",
      "Step: 29300  \tTraining loss: 0.21973049640655518\n",
      "Step: 29300  \tTraining accuracy: 0.9081010818481445\n",
      "Step: 29300  \tValid loss: 0.2250695824623108\n",
      "Step: 29400  \tTraining loss: 0.21972709894180298\n",
      "Step: 29400  \tTraining accuracy: 0.9081061482429504\n",
      "Step: 29400  \tValid loss: 0.22506612539291382\n",
      "Step: 29500  \tTraining loss: 0.21972396969795227\n",
      "Step: 29500  \tTraining accuracy: 0.9081110954284668\n",
      "Step: 29500  \tValid loss: 0.22506344318389893\n",
      "Step: 29600  \tTraining loss: 0.2197207510471344\n",
      "Step: 29600  \tTraining accuracy: 0.9081161022186279\n",
      "Step: 29600  \tValid loss: 0.2250603586435318\n",
      "Step: 29700  \tTraining loss: 0.21971780061721802\n",
      "Step: 29700  \tTraining accuracy: 0.9081209897994995\n",
      "Step: 29700  \tValid loss: 0.22505706548690796\n",
      "Step: 29800  \tTraining loss: 0.21971464157104492\n",
      "Step: 29800  \tTraining accuracy: 0.9081258773803711\n",
      "Step: 29800  \tValid loss: 0.2250533550977707\n",
      "Step: 29900  \tTraining loss: 0.21971146762371063\n",
      "Step: 29900  \tTraining accuracy: 0.9081307649612427\n",
      "Step: 29900  \tValid loss: 0.2250506430864334\n",
      "Step: 30000  \tTraining loss: 0.21970847249031067\n",
      "Step: 30000  \tTraining accuracy: 0.9081355929374695\n",
      "Step: 30000  \tValid loss: 0.22504732012748718\n",
      "Step: 30100  \tTraining loss: 0.2197052389383316\n",
      "Step: 30100  \tTraining accuracy: 0.9081403613090515\n",
      "Step: 30100  \tValid loss: 0.22504419088363647\n",
      "Step: 30200  \tTraining loss: 0.21970196068286896\n",
      "Step: 30200  \tTraining accuracy: 0.9081451296806335\n",
      "Step: 30200  \tValid loss: 0.22504056990146637\n",
      "Step: 30300  \tTraining loss: 0.21969914436340332\n",
      "Step: 30300  \tTraining accuracy: 0.9081498384475708\n",
      "Step: 30300  \tValid loss: 0.22503724694252014\n",
      "Step: 30400  \tTraining loss: 0.21969570219516754\n",
      "Step: 30400  \tTraining accuracy: 0.9081545472145081\n",
      "Step: 30400  \tValid loss: 0.2250341922044754\n",
      "Step: 30500  \tTraining loss: 0.21969282627105713\n",
      "Step: 30500  \tTraining accuracy: 0.9081591963768005\n",
      "Step: 30500  \tValid loss: 0.22503060102462769\n",
      "Step: 30600  \tTraining loss: 0.2196895182132721\n",
      "Step: 30600  \tTraining accuracy: 0.908163845539093\n",
      "Step: 30600  \tValid loss: 0.2250276356935501\n",
      "Step: 30700  \tTraining loss: 0.21968641877174377\n",
      "Step: 30700  \tTraining accuracy: 0.9081684350967407\n",
      "Step: 30700  \tValid loss: 0.22502385079860687\n",
      "Step: 30800  \tTraining loss: 0.21968303620815277\n",
      "Step: 30800  \tTraining accuracy: 0.9081730246543884\n",
      "Step: 30800  \tValid loss: 0.22502070665359497\n",
      "Step: 30900  \tTraining loss: 0.21968010067939758\n",
      "Step: 30900  \tTraining accuracy: 0.9081776142120361\n",
      "Step: 30900  \tValid loss: 0.2250170111656189\n",
      "Step: 31000  \tTraining loss: 0.2196768969297409\n",
      "Step: 31000  \tTraining accuracy: 0.9081820845603943\n",
      "Step: 31000  \tValid loss: 0.22501376271247864\n",
      "Step: 31100  \tTraining loss: 0.21967388689517975\n",
      "Step: 31100  \tTraining accuracy: 0.9081866145133972\n",
      "Step: 31100  \tValid loss: 0.22501036524772644\n",
      "Step: 31200  \tTraining loss: 0.21967077255249023\n",
      "Step: 31200  \tTraining accuracy: 0.9081910252571106\n",
      "Step: 31200  \tValid loss: 0.2250073403120041\n",
      "Step: 31300  \tTraining loss: 0.21966765820980072\n",
      "Step: 31300  \tTraining accuracy: 0.9081954956054688\n",
      "Step: 31300  \tValid loss: 0.22500358521938324\n",
      "Step: 31400  \tTraining loss: 0.21966446936130524\n",
      "Step: 31400  \tTraining accuracy: 0.9081999063491821\n",
      "Step: 31400  \tValid loss: 0.22500070929527283\n",
      "Step: 31500  \tTraining loss: 0.2196614295244217\n",
      "Step: 31500  \tTraining accuracy: 0.9082042574882507\n",
      "Step: 31500  \tValid loss: 0.22499720752239227\n",
      "Step: 31600  \tTraining loss: 0.21965838968753815\n",
      "Step: 31600  \tTraining accuracy: 0.9082086086273193\n",
      "Step: 31600  \tValid loss: 0.22499366104602814\n",
      "Step: 31700  \tTraining loss: 0.21965523064136505\n",
      "Step: 31700  \tTraining accuracy: 0.9082129001617432\n",
      "Step: 31700  \tValid loss: 0.2249900996685028\n",
      "Step: 31800  \tTraining loss: 0.21965214610099792\n",
      "Step: 31800  \tTraining accuracy: 0.9082172513008118\n",
      "Step: 31800  \tValid loss: 0.2249870002269745\n",
      "Step: 31900  \tTraining loss: 0.21964894235134125\n",
      "Step: 31900  \tTraining accuracy: 0.9082214832305908\n",
      "Step: 31900  \tValid loss: 0.22498396039009094\n",
      "Step: 32000  \tTraining loss: 0.21964611113071442\n",
      "Step: 32000  \tTraining accuracy: 0.9082257151603699\n",
      "Step: 32000  \tValid loss: 0.22498036921024323\n",
      "Step: 32100  \tTraining loss: 0.2196427434682846\n",
      "Step: 32100  \tTraining accuracy: 0.9082299470901489\n",
      "Step: 32100  \tValid loss: 0.2249770164489746\n",
      "Step: 32200  \tTraining loss: 0.21963967382907867\n",
      "Step: 32200  \tTraining accuracy: 0.9082341194152832\n",
      "Step: 32200  \tValid loss: 0.2249738574028015\n",
      "Step: 32300  \tTraining loss: 0.21963687241077423\n",
      "Step: 32300  \tTraining accuracy: 0.9082382917404175\n",
      "Step: 32300  \tValid loss: 0.2249709516763687\n",
      "Step: 32400  \tTraining loss: 0.2196335643529892\n",
      "Step: 32400  \tTraining accuracy: 0.908242404460907\n",
      "Step: 32400  \tValid loss: 0.22496762871742249\n",
      "Step: 32500  \tTraining loss: 0.21963082253932953\n",
      "Step: 32500  \tTraining accuracy: 0.9082465171813965\n",
      "Step: 32500  \tValid loss: 0.2249641716480255\n",
      "Step: 32600  \tTraining loss: 0.21962732076644897\n",
      "Step: 32600  \tTraining accuracy: 0.908250629901886\n",
      "Step: 32600  \tValid loss: 0.2249612659215927\n",
      "Step: 32700  \tTraining loss: 0.21962448954582214\n",
      "Step: 32700  \tTraining accuracy: 0.9082546830177307\n",
      "Step: 32700  \tValid loss: 0.2249576449394226\n",
      "Step: 32800  \tTraining loss: 0.2196216881275177\n",
      "Step: 32800  \tTraining accuracy: 0.9082586765289307\n",
      "Step: 32800  \tValid loss: 0.22495469450950623\n",
      "Step: 32900  \tTraining loss: 0.21961842477321625\n",
      "Step: 32900  \tTraining accuracy: 0.9082627296447754\n",
      "Step: 32900  \tValid loss: 0.22495175898075104\n",
      "Step: 33000  \tTraining loss: 0.21961544454097748\n",
      "Step: 33000  \tTraining accuracy: 0.9082667231559753\n",
      "Step: 33000  \tValid loss: 0.22494903206825256\n",
      "Step: 33100  \tTraining loss: 0.21961233019828796\n",
      "Step: 33100  \tTraining accuracy: 0.9082706570625305\n",
      "Step: 33100  \tValid loss: 0.22494573891162872\n",
      "Step: 33200  \tTraining loss: 0.21960948407649994\n",
      "Step: 33200  \tTraining accuracy: 0.9082745909690857\n",
      "Step: 33200  \tValid loss: 0.22494225203990936\n",
      "Step: 33300  \tTraining loss: 0.21960628032684326\n",
      "Step: 33300  \tTraining accuracy: 0.9082785248756409\n",
      "Step: 33300  \tValid loss: 0.22493961453437805\n",
      "Step: 33400  \tTraining loss: 0.21960341930389404\n",
      "Step: 33400  \tTraining accuracy: 0.9082823991775513\n",
      "Step: 33400  \tValid loss: 0.22493718564510345\n",
      "Step: 33500  \tTraining loss: 0.2196006178855896\n",
      "Step: 33500  \tTraining accuracy: 0.9082862734794617\n",
      "Step: 33500  \tValid loss: 0.22493356466293335\n",
      "Step: 33600  \tTraining loss: 0.21959777176380157\n",
      "Step: 33600  \tTraining accuracy: 0.9082900881767273\n",
      "Step: 33600  \tValid loss: 0.22493082284927368\n",
      "Step: 33700  \tTraining loss: 0.2195945680141449\n",
      "Step: 33700  \tTraining accuracy: 0.9082939028739929\n",
      "Step: 33700  \tValid loss: 0.22492781281471252\n",
      "Step: 33800  \tTraining loss: 0.21959158778190613\n",
      "Step: 33800  \tTraining accuracy: 0.9082977175712585\n",
      "Step: 33800  \tValid loss: 0.22492536902427673\n",
      "Step: 33900  \tTraining loss: 0.21958866715431213\n",
      "Step: 33900  \tTraining accuracy: 0.9083015322685242\n",
      "Step: 33900  \tValid loss: 0.22492240369319916\n",
      "Step: 34000  \tTraining loss: 0.21958568692207336\n",
      "Step: 34000  \tTraining accuracy: 0.908305287361145\n",
      "Step: 34000  \tValid loss: 0.22491957247257233\n",
      "Step: 34100  \tTraining loss: 0.21958287060260773\n",
      "Step: 34100  \tTraining accuracy: 0.9083089828491211\n",
      "Step: 34100  \tValid loss: 0.22491759061813354\n",
      "Step: 34200  \tTraining loss: 0.2195798009634018\n",
      "Step: 34200  \tTraining accuracy: 0.9083126783370972\n",
      "Step: 34200  \tValid loss: 0.22491417825222015\n",
      "Step: 34300  \tTraining loss: 0.21957719326019287\n",
      "Step: 34300  \tTraining accuracy: 0.9083163738250732\n",
      "Step: 34300  \tValid loss: 0.2249116152524948\n",
      "Step: 34400  \tTraining loss: 0.21957404911518097\n",
      "Step: 34400  \tTraining accuracy: 0.9083200693130493\n",
      "Step: 34400  \tValid loss: 0.22490893304347992\n",
      "Step: 34500  \tTraining loss: 0.2195713073015213\n",
      "Step: 34500  \tTraining accuracy: 0.9083237051963806\n",
      "Step: 34500  \tValid loss: 0.22490663826465607\n",
      "Step: 34600  \tTraining loss: 0.21956808865070343\n",
      "Step: 34600  \tTraining accuracy: 0.9083273410797119\n",
      "Step: 34600  \tValid loss: 0.224903866648674\n",
      "Step: 34700  \tTraining loss: 0.21956539154052734\n",
      "Step: 34700  \tTraining accuracy: 0.9083309173583984\n",
      "Step: 34700  \tValid loss: 0.22490115463733673\n",
      "Step: 34800  \tTraining loss: 0.2195625603199005\n",
      "Step: 34800  \tTraining accuracy: 0.908334493637085\n",
      "Step: 34800  \tValid loss: 0.2248988300561905\n",
      "Step: 34900  \tTraining loss: 0.21955984830856323\n",
      "Step: 34900  \tTraining accuracy: 0.9083380699157715\n",
      "Step: 34900  \tValid loss: 0.22489707171916962\n",
      "Step: 35000  \tTraining loss: 0.2195570021867752\n",
      "Step: 35000  \tTraining accuracy: 0.9083415865898132\n",
      "Step: 35000  \tValid loss: 0.22489424049854279\n",
      "Step: 35100  \tTraining loss: 0.21955393254756927\n",
      "Step: 35100  \tTraining accuracy: 0.908345103263855\n",
      "Step: 35100  \tValid loss: 0.22489230334758759\n",
      "Step: 35200  \tTraining loss: 0.2195514291524887\n",
      "Step: 35200  \tTraining accuracy: 0.9083486199378967\n",
      "Step: 35200  \tValid loss: 0.22488977015018463\n",
      "Step: 35300  \tTraining loss: 0.21954841911792755\n",
      "Step: 35300  \tTraining accuracy: 0.9083521366119385\n",
      "Step: 35300  \tValid loss: 0.22488726675510406\n",
      "Step: 35400  \tTraining loss: 0.21954558789730072\n",
      "Step: 35400  \tTraining accuracy: 0.9083555936813354\n",
      "Step: 35400  \tValid loss: 0.224885493516922\n",
      "Step: 35500  \tTraining loss: 0.21954317390918732\n",
      "Step: 35500  \tTraining accuracy: 0.9083589911460876\n",
      "Step: 35500  \tValid loss: 0.2248826026916504\n",
      "Step: 35600  \tTraining loss: 0.2195400893688202\n",
      "Step: 35600  \tTraining accuracy: 0.9083624482154846\n",
      "Step: 35600  \tValid loss: 0.2248816341161728\n",
      "Step: 35700  \tTraining loss: 0.21953725814819336\n",
      "Step: 35700  \tTraining accuracy: 0.9083658456802368\n",
      "Step: 35700  \tValid loss: 0.22487908601760864\n",
      "Step: 35800  \tTraining loss: 0.21953442692756653\n",
      "Step: 35800  \tTraining accuracy: 0.908369243144989\n",
      "Step: 35800  \tValid loss: 0.2248769849538803\n",
      "Step: 35900  \tTraining loss: 0.21953190863132477\n",
      "Step: 35900  \tTraining accuracy: 0.9083725810050964\n",
      "Step: 35900  \tValid loss: 0.22487489879131317\n",
      "Step: 36000  \tTraining loss: 0.21952906250953674\n",
      "Step: 36000  \tTraining accuracy: 0.9083759188652039\n",
      "Step: 36000  \tValid loss: 0.2248740792274475\n",
      "Step: 36100  \tTraining loss: 0.21952611207962036\n",
      "Step: 36100  \tTraining accuracy: 0.9083792567253113\n",
      "Step: 36100  \tValid loss: 0.22487211227416992\n",
      "Step: 36200  \tTraining loss: 0.21952344477176666\n",
      "Step: 36200  \tTraining accuracy: 0.9083825945854187\n",
      "Step: 36200  \tValid loss: 0.2248704731464386\n",
      "Step: 36300  \tTraining loss: 0.2195209264755249\n",
      "Step: 36300  \tTraining accuracy: 0.9083858728408813\n",
      "Step: 36300  \tValid loss: 0.2248692810535431\n",
      "Step: 36400  \tTraining loss: 0.21951821446418762\n",
      "Step: 36400  \tTraining accuracy: 0.908389151096344\n",
      "Step: 36400  \tValid loss: 0.22486646473407745\n",
      "Step: 36500  \tTraining loss: 0.21951517462730408\n",
      "Step: 36500  \tTraining accuracy: 0.9083924293518066\n",
      "Step: 36500  \tValid loss: 0.22486256062984467\n",
      "Step: 36600  \tTraining loss: 0.21951301395893097\n",
      "Step: 36600  \tTraining accuracy: 0.9083956480026245\n",
      "Step: 36600  \tValid loss: 0.2248636931180954\n",
      "Step: 36700  \tTraining loss: 0.21951009333133698\n",
      "Step: 36700  \tTraining accuracy: 0.9083988666534424\n",
      "Step: 36700  \tValid loss: 0.22486211359500885\n",
      "Step: 36800  \tTraining loss: 0.21950724720954895\n",
      "Step: 36800  \tTraining accuracy: 0.9084020853042603\n",
      "Step: 36800  \tValid loss: 0.22485984861850739\n",
      "Step: 36900  \tTraining loss: 0.21950511634349823\n",
      "Step: 36900  \tTraining accuracy: 0.9084052443504333\n",
      "Step: 36900  \tValid loss: 0.224856898188591\n",
      "Step: 37000  \tTraining loss: 0.219502255320549\n",
      "Step: 37000  \tTraining accuracy: 0.9084084033966064\n",
      "Step: 37000  \tValid loss: 0.22485926747322083\n",
      "Step: 37100  \tTraining loss: 0.21949899196624756\n",
      "Step: 37100  \tTraining accuracy: 0.9084115624427795\n",
      "Step: 37100  \tValid loss: 0.22485549747943878\n",
      "Step: 37200  \tTraining loss: 0.21949678659439087\n",
      "Step: 37200  \tTraining accuracy: 0.9084147214889526\n",
      "Step: 37200  \tValid loss: 0.22485128045082092\n",
      "Step: 37300  \tTraining loss: 0.21949435770511627\n",
      "Step: 37300  \tTraining accuracy: 0.908417820930481\n",
      "Step: 37300  \tValid loss: 0.22485148906707764\n",
      "Step: 37400  \tTraining loss: 0.21949146687984467\n",
      "Step: 37400  \tTraining accuracy: 0.9084209203720093\n",
      "Step: 37400  \tValid loss: 0.22485098242759705\n",
      "Step: 37500  \tTraining loss: 0.21948882937431335\n",
      "Step: 37500  \tTraining accuracy: 0.9084240198135376\n",
      "Step: 37500  \tValid loss: 0.22484751045703888\n",
      "Step: 37600  \tTraining loss: 0.21948617696762085\n",
      "Step: 37600  \tTraining accuracy: 0.9084270596504211\n",
      "Step: 37600  \tValid loss: 0.2248462736606598\n",
      "Step: 37700  \tTraining loss: 0.21948395669460297\n",
      "Step: 37700  \tTraining accuracy: 0.9084300994873047\n",
      "Step: 37700  \tValid loss: 0.22484585642814636\n",
      "Step: 37800  \tTraining loss: 0.21948136389255524\n",
      "Step: 37800  \tTraining accuracy: 0.9084331393241882\n",
      "Step: 37800  \tValid loss: 0.22484251856803894\n",
      "Step: 37900  \tTraining loss: 0.21947850286960602\n",
      "Step: 37900  \tTraining accuracy: 0.9084361791610718\n",
      "Step: 37900  \tValid loss: 0.2248397171497345\n",
      "Step: 38000  \tTraining loss: 0.21947596967220306\n",
      "Step: 38000  \tTraining accuracy: 0.9084391593933105\n",
      "Step: 38000  \tValid loss: 0.224839448928833\n",
      "Step: 38100  \tTraining loss: 0.21947383880615234\n",
      "Step: 38100  \tTraining accuracy: 0.9084421396255493\n",
      "Step: 38100  \tValid loss: 0.22483879327774048\n",
      "Step: 38200  \tTraining loss: 0.21947091817855835\n",
      "Step: 38200  \tTraining accuracy: 0.9084451198577881\n",
      "Step: 38200  \tValid loss: 0.2248360812664032\n",
      "Step: 38300  \tTraining loss: 0.21946865320205688\n",
      "Step: 38300  \tTraining accuracy: 0.9084481000900269\n",
      "Step: 38300  \tValid loss: 0.224836066365242\n",
      "Step: 38400  \tTraining loss: 0.2194659560918808\n",
      "Step: 38400  \tTraining accuracy: 0.9084510207176208\n",
      "Step: 38400  \tValid loss: 0.22483278810977936\n",
      "Step: 38500  \tTraining loss: 0.21946343779563904\n",
      "Step: 38500  \tTraining accuracy: 0.9084539413452148\n",
      "Step: 38500  \tValid loss: 0.22482921183109283\n",
      "Step: 38600  \tTraining loss: 0.21946075558662415\n",
      "Step: 38600  \tTraining accuracy: 0.9084568619728088\n",
      "Step: 38600  \tValid loss: 0.22482766211032867\n",
      "Step: 38700  \tTraining loss: 0.21945813298225403\n",
      "Step: 38700  \tTraining accuracy: 0.9084597826004028\n",
      "Step: 38700  \tValid loss: 0.22482439875602722\n",
      "Step: 38800  \tTraining loss: 0.21945607662200928\n",
      "Step: 38800  \tTraining accuracy: 0.908462643623352\n",
      "Step: 38800  \tValid loss: 0.2248227447271347\n",
      "Step: 38900  \tTraining loss: 0.21945346891880035\n",
      "Step: 38900  \tTraining accuracy: 0.9084655046463013\n",
      "Step: 38900  \tValid loss: 0.22482110559940338\n",
      "Step: 39000  \tTraining loss: 0.21945111453533173\n",
      "Step: 39000  \tTraining accuracy: 0.9084683656692505\n",
      "Step: 39000  \tValid loss: 0.22481675446033478\n",
      "Step: 39100  \tTraining loss: 0.21944868564605713\n",
      "Step: 39100  \tTraining accuracy: 0.9084711670875549\n",
      "Step: 39100  \tValid loss: 0.22481727600097656\n",
      "Step: 39200  \tTraining loss: 0.2194465547800064\n",
      "Step: 39200  \tTraining accuracy: 0.9084740281105042\n",
      "Step: 39200  \tValid loss: 0.2248120754957199\n",
      "Step: 39300  \tTraining loss: 0.21944405138492584\n",
      "Step: 39300  \tTraining accuracy: 0.9084768295288086\n",
      "Step: 39300  \tValid loss: 0.224811390042305\n",
      "Step: 39400  \tTraining loss: 0.21944166719913483\n",
      "Step: 39400  \tTraining accuracy: 0.908479630947113\n",
      "Step: 39400  \tValid loss: 0.22480995953083038\n",
      "Step: 39500  \tTraining loss: 0.2194393128156662\n",
      "Step: 39500  \tTraining accuracy: 0.9084823727607727\n",
      "Step: 39500  \tValid loss: 0.22480538487434387\n",
      "Step: 39600  \tTraining loss: 0.21943733096122742\n",
      "Step: 39600  \tTraining accuracy: 0.9084851741790771\n",
      "Step: 39600  \tValid loss: 0.22480784356594086\n",
      "Step: 39700  \tTraining loss: 0.21943451464176178\n",
      "Step: 39700  \tTraining accuracy: 0.9084879159927368\n",
      "Step: 39700  \tValid loss: 0.2248024195432663\n",
      "Step: 39800  \tTraining loss: 0.21943235397338867\n",
      "Step: 39800  \tTraining accuracy: 0.9084906578063965\n",
      "Step: 39800  \tValid loss: 0.2248009592294693\n",
      "Step: 39900  \tTraining loss: 0.21942995488643646\n",
      "Step: 39900  \tTraining accuracy: 0.9084933400154114\n",
      "Step: 39900  \tValid loss: 0.22480028867721558\n",
      "Step: 40000  \tTraining loss: 0.21942783892154694\n",
      "Step: 40000  \tTraining accuracy: 0.908496081829071\n",
      "Step: 40000  \tValid loss: 0.22479651868343353\n",
      "Step: 40100  \tTraining loss: 0.21942560374736786\n",
      "Step: 40100  \tTraining accuracy: 0.9084987640380859\n",
      "Step: 40100  \tValid loss: 0.22479411959648132\n",
      "Step: 40200  \tTraining loss: 0.21942348778247833\n",
      "Step: 40200  \tTraining accuracy: 0.9085014462471008\n",
      "Step: 40200  \tValid loss: 0.2247934490442276\n",
      "Step: 40300  \tTraining loss: 0.2194211184978485\n",
      "Step: 40300  \tTraining accuracy: 0.9085041284561157\n",
      "Step: 40300  \tValid loss: 0.22478701174259186\n",
      "Step: 40400  \tTraining loss: 0.21941886842250824\n",
      "Step: 40400  \tTraining accuracy: 0.9085067510604858\n",
      "Step: 40400  \tValid loss: 0.2247852236032486\n",
      "Step: 40500  \tTraining loss: 0.21941663324832916\n",
      "Step: 40500  \tTraining accuracy: 0.9085094332695007\n",
      "Step: 40500  \tValid loss: 0.22478145360946655\n",
      "Step: 40600  \tTraining loss: 0.21941420435905457\n",
      "Step: 40600  \tTraining accuracy: 0.9085120558738708\n",
      "Step: 40600  \tValid loss: 0.22478023171424866\n",
      "Step: 40700  \tTraining loss: 0.2194124013185501\n",
      "Step: 40700  \tTraining accuracy: 0.908514678478241\n",
      "Step: 40700  \tValid loss: 0.22477829456329346\n",
      "Step: 40800  \tTraining loss: 0.21941009163856506\n",
      "Step: 40800  \tTraining accuracy: 0.9085173010826111\n",
      "Step: 40800  \tValid loss: 0.2247781753540039\n",
      "Step: 40900  \tTraining loss: 0.21940772235393524\n",
      "Step: 40900  \tTraining accuracy: 0.9085198640823364\n",
      "Step: 40900  \tValid loss: 0.224773570895195\n",
      "Step: 41000  \tTraining loss: 0.21940582990646362\n",
      "Step: 41000  \tTraining accuracy: 0.9085224270820618\n",
      "Step: 41000  \tValid loss: 0.22477208077907562\n",
      "Step: 41100  \tTraining loss: 0.219403475522995\n",
      "Step: 41100  \tTraining accuracy: 0.9085249900817871\n",
      "Step: 41100  \tValid loss: 0.22476904094219208\n",
      "Step: 41200  \tTraining loss: 0.21940135955810547\n",
      "Step: 41200  \tTraining accuracy: 0.9085275530815125\n",
      "Step: 41200  \tValid loss: 0.22476723790168762\n",
      "Step: 41300  \tTraining loss: 0.21939939260482788\n",
      "Step: 41300  \tTraining accuracy: 0.9085301160812378\n",
      "Step: 41300  \tValid loss: 0.22476361691951752\n",
      "Step: 41400  \tTraining loss: 0.2193969041109085\n",
      "Step: 41400  \tTraining accuracy: 0.9085326194763184\n",
      "Step: 41400  \tValid loss: 0.2247616946697235\n",
      "Step: 41500  \tTraining loss: 0.21939517557621002\n",
      "Step: 41500  \tTraining accuracy: 0.9085351228713989\n",
      "Step: 41500  \tValid loss: 0.22476039826869965\n",
      "Step: 41600  \tTraining loss: 0.2193928211927414\n",
      "Step: 41600  \tTraining accuracy: 0.9085376262664795\n",
      "Step: 41600  \tValid loss: 0.22475625574588776\n",
      "Step: 41700  \tTraining loss: 0.21939082443714142\n",
      "Step: 41700  \tTraining accuracy: 0.9085401296615601\n",
      "Step: 41700  \tValid loss: 0.22475220263004303\n",
      "Step: 41800  \tTraining loss: 0.2193887084722519\n",
      "Step: 41800  \tTraining accuracy: 0.9085426330566406\n",
      "Step: 41800  \tValid loss: 0.22475089132785797\n",
      "Step: 41900  \tTraining loss: 0.219386488199234\n",
      "Step: 41900  \tTraining accuracy: 0.9085450768470764\n",
      "Step: 41900  \tValid loss: 0.22474731504917145\n",
      "Step: 42000  \tTraining loss: 0.2193845808506012\n",
      "Step: 42000  \tTraining accuracy: 0.908547580242157\n",
      "Step: 42000  \tValid loss: 0.22474543750286102\n",
      "Step: 42100  \tTraining loss: 0.21938234567642212\n",
      "Step: 42100  \tTraining accuracy: 0.9085500240325928\n",
      "Step: 42100  \tValid loss: 0.22474153339862823\n",
      "Step: 42200  \tTraining loss: 0.21938031911849976\n",
      "Step: 42200  \tTraining accuracy: 0.9085524082183838\n",
      "Step: 42200  \tValid loss: 0.22473914921283722\n",
      "Step: 42300  \tTraining loss: 0.21937833726406097\n",
      "Step: 42300  \tTraining accuracy: 0.9085548520088196\n",
      "Step: 42300  \tValid loss: 0.2247357815504074\n",
      "Step: 42400  \tTraining loss: 0.21937628090381622\n",
      "Step: 42400  \tTraining accuracy: 0.9085572361946106\n",
      "Step: 42400  \tValid loss: 0.22473448514938354\n",
      "Step: 42500  \tTraining loss: 0.21937419474124908\n",
      "Step: 42500  \tTraining accuracy: 0.9085596799850464\n",
      "Step: 42500  \tValid loss: 0.2247340977191925\n",
      "Step: 42600  \tTraining loss: 0.21937234699726105\n",
      "Step: 42600  \tTraining accuracy: 0.9085620641708374\n",
      "Step: 42600  \tValid loss: 0.22473515570163727\n",
      "Step: 42700  \tTraining loss: 0.21936999261379242\n",
      "Step: 42700  \tTraining accuracy: 0.9085644483566284\n",
      "Step: 42700  \tValid loss: 0.22473210096359253\n",
      "Step: 42800  \tTraining loss: 0.21936823427677155\n",
      "Step: 42800  \tTraining accuracy: 0.9085667729377747\n",
      "Step: 42800  \tValid loss: 0.22472858428955078\n",
      "Step: 42900  \tTraining loss: 0.21936607360839844\n",
      "Step: 42900  \tTraining accuracy: 0.9085691571235657\n",
      "Step: 42900  \tValid loss: 0.22472916543483734\n",
      "Step: 43000  \tTraining loss: 0.21936433017253876\n",
      "Step: 43000  \tTraining accuracy: 0.9085714817047119\n",
      "Step: 43000  \tValid loss: 0.22472582757472992\n",
      "Step: 43100  \tTraining loss: 0.2193620800971985\n",
      "Step: 43100  \tTraining accuracy: 0.9085738062858582\n",
      "Step: 43100  \tValid loss: 0.22472144663333893\n",
      "Step: 43200  \tTraining loss: 0.21936003863811493\n",
      "Step: 43200  \tTraining accuracy: 0.908577024936676\n",
      "Step: 43200  \tValid loss: 0.22472456097602844\n",
      "Step: 43300  \tTraining loss: 0.21935828030109406\n",
      "Step: 43300  \tTraining accuracy: 0.9085822105407715\n",
      "Step: 43300  \tValid loss: 0.22472138702869415\n",
      "Step: 43400  \tTraining loss: 0.2193560153245926\n",
      "Step: 43400  \tTraining accuracy: 0.9085873365402222\n",
      "Step: 43400  \tValid loss: 0.22472067177295685\n",
      "Step: 43500  \tTraining loss: 0.21935413777828217\n",
      "Step: 43500  \tTraining accuracy: 0.9085924625396729\n",
      "Step: 43500  \tValid loss: 0.22471916675567627\n",
      "Step: 43600  \tTraining loss: 0.21935251355171204\n",
      "Step: 43600  \tTraining accuracy: 0.9085975885391235\n",
      "Step: 43600  \tValid loss: 0.22471730411052704\n",
      "Step: 43700  \tTraining loss: 0.21935029327869415\n",
      "Step: 43700  \tTraining accuracy: 0.9086026549339294\n",
      "Step: 43700  \tValid loss: 0.2247127890586853\n",
      "Step: 43800  \tTraining loss: 0.21934853494167328\n",
      "Step: 43800  \tTraining accuracy: 0.9086077213287354\n",
      "Step: 43800  \tValid loss: 0.2247147113084793\n",
      "Step: 43900  \tTraining loss: 0.21934635937213898\n",
      "Step: 43900  \tTraining accuracy: 0.9086127281188965\n",
      "Step: 43900  \tValid loss: 0.22470934689044952\n",
      "Step: 44000  \tTraining loss: 0.21934443712234497\n",
      "Step: 44000  \tTraining accuracy: 0.9086177945137024\n",
      "Step: 44000  \tValid loss: 0.22470547258853912\n",
      "Step: 44100  \tTraining loss: 0.21934247016906738\n",
      "Step: 44100  \tTraining accuracy: 0.9086227416992188\n",
      "Step: 44100  \tValid loss: 0.22470729053020477\n",
      "Step: 44200  \tTraining loss: 0.21934086084365845\n",
      "Step: 44200  \tTraining accuracy: 0.9086277484893799\n",
      "Step: 44200  \tValid loss: 0.22470664978027344\n",
      "Step: 44300  \tTraining loss: 0.2193388193845749\n",
      "Step: 44300  \tTraining accuracy: 0.9086326956748962\n",
      "Step: 44300  \tValid loss: 0.22470112144947052\n",
      "Step: 44400  \tTraining loss: 0.2193370908498764\n",
      "Step: 44400  \tTraining accuracy: 0.9086375832557678\n",
      "Step: 44400  \tValid loss: 0.2246989607810974\n",
      "Step: 44500  \tTraining loss: 0.21933528780937195\n",
      "Step: 44500  \tTraining accuracy: 0.9086424708366394\n",
      "Step: 44500  \tValid loss: 0.2246999442577362\n",
      "Step: 44600  \tTraining loss: 0.2193332016468048\n",
      "Step: 44600  \tTraining accuracy: 0.908647358417511\n",
      "Step: 44600  \tValid loss: 0.22470027208328247\n",
      "Step: 44700  \tTraining loss: 0.21933139860630035\n",
      "Step: 44700  \tTraining accuracy: 0.9086522459983826\n",
      "Step: 44700  \tValid loss: 0.2246958166360855\n",
      "Step: 44800  \tTraining loss: 0.2193296104669571\n",
      "Step: 44800  \tTraining accuracy: 0.9086570739746094\n",
      "Step: 44800  \tValid loss: 0.2246926873922348\n",
      "Step: 44900  \tTraining loss: 0.21932744979858398\n",
      "Step: 44900  \tTraining accuracy: 0.9086619019508362\n",
      "Step: 44900  \tValid loss: 0.22469554841518402\n",
      "Step: 45000  \tTraining loss: 0.2193259596824646\n",
      "Step: 45000  \tTraining accuracy: 0.9086666703224182\n",
      "Step: 45000  \tValid loss: 0.22469355165958405\n",
      "Step: 45100  \tTraining loss: 0.21932394802570343\n",
      "Step: 45100  \tTraining accuracy: 0.9086714386940002\n",
      "Step: 45100  \tValid loss: 0.22468864917755127\n",
      "Step: 45200  \tTraining loss: 0.21932220458984375\n",
      "Step: 45200  \tTraining accuracy: 0.9086762070655823\n",
      "Step: 45200  \tValid loss: 0.22468797862529755\n",
      "Step: 45300  \tTraining loss: 0.219320610165596\n",
      "Step: 45300  \tTraining accuracy: 0.9086809158325195\n",
      "Step: 45300  \tValid loss: 0.22468604147434235\n",
      "Step: 45400  \tTraining loss: 0.21931865811347961\n",
      "Step: 45400  \tTraining accuracy: 0.9086856245994568\n",
      "Step: 45400  \tValid loss: 0.22468626499176025\n",
      "Step: 45500  \tTraining loss: 0.2193170040845871\n",
      "Step: 45500  \tTraining accuracy: 0.908690333366394\n",
      "Step: 45500  \tValid loss: 0.22468127310276031\n",
      "Step: 45600  \tTraining loss: 0.21931517124176025\n",
      "Step: 45600  \tTraining accuracy: 0.9086949825286865\n",
      "Step: 45600  \tValid loss: 0.22468119859695435\n",
      "Step: 45700  \tTraining loss: 0.21931332349777222\n",
      "Step: 45700  \tTraining accuracy: 0.908699631690979\n",
      "Step: 45700  \tValid loss: 0.22468051314353943\n",
      "Step: 45800  \tTraining loss: 0.21931160986423492\n",
      "Step: 45800  \tTraining accuracy: 0.9087042808532715\n",
      "Step: 45800  \tValid loss: 0.22467832267284393\n",
      "Step: 45900  \tTraining loss: 0.21930988132953644\n",
      "Step: 45900  \tTraining accuracy: 0.9087088704109192\n",
      "Step: 45900  \tValid loss: 0.22467705607414246\n",
      "Step: 46000  \tTraining loss: 0.21930816769599915\n",
      "Step: 46000  \tTraining accuracy: 0.9087134599685669\n",
      "Step: 46000  \tValid loss: 0.22467604279518127\n",
      "Step: 46100  \tTraining loss: 0.21930639445781708\n",
      "Step: 46100  \tTraining accuracy: 0.9087180495262146\n",
      "Step: 46100  \tValid loss: 0.22467352449893951\n",
      "Step: 46200  \tTraining loss: 0.21930445730686188\n",
      "Step: 46200  \tTraining accuracy: 0.9087225794792175\n",
      "Step: 46200  \tValid loss: 0.2246718853712082\n",
      "Step: 46300  \tTraining loss: 0.21930259466171265\n",
      "Step: 46300  \tTraining accuracy: 0.9087271094322205\n",
      "Step: 46300  \tValid loss: 0.2246701866388321\n",
      "Step: 46400  \tTraining loss: 0.21930064260959625\n",
      "Step: 46400  \tTraining accuracy: 0.9087315797805786\n",
      "Step: 46400  \tValid loss: 0.22466900944709778\n",
      "Step: 46500  \tTraining loss: 0.2192995697259903\n",
      "Step: 46500  \tTraining accuracy: 0.9087361097335815\n",
      "Step: 46500  \tValid loss: 0.22466826438903809\n",
      "Step: 46600  \tTraining loss: 0.2192975878715515\n",
      "Step: 46600  \tTraining accuracy: 0.9087405800819397\n",
      "Step: 46600  \tValid loss: 0.22466737031936646\n",
      "Step: 46700  \tTraining loss: 0.2192961573600769\n",
      "Step: 46700  \tTraining accuracy: 0.9087449908256531\n",
      "Step: 46700  \tValid loss: 0.22466535866260529\n",
      "Step: 46800  \tTraining loss: 0.21929433941841125\n",
      "Step: 46800  \tTraining accuracy: 0.9087494611740112\n",
      "Step: 46800  \tValid loss: 0.22466500103473663\n",
      "Step: 46900  \tTraining loss: 0.2192922830581665\n",
      "Step: 46900  \tTraining accuracy: 0.9087538719177246\n",
      "Step: 46900  \tValid loss: 0.2246648520231247\n",
      "Step: 47000  \tTraining loss: 0.2192910611629486\n",
      "Step: 47000  \tTraining accuracy: 0.9087582230567932\n",
      "Step: 47000  \tValid loss: 0.22466471791267395\n",
      "Step: 47100  \tTraining loss: 0.21928928792476654\n",
      "Step: 47100  \tTraining accuracy: 0.9087626338005066\n",
      "Step: 47100  \tValid loss: 0.22466233372688293\n",
      "Step: 47200  \tTraining loss: 0.21928763389587402\n",
      "Step: 47200  \tTraining accuracy: 0.9087669849395752\n",
      "Step: 47200  \tValid loss: 0.22466117143630981\n",
      "Step: 47300  \tTraining loss: 0.21928595006465912\n",
      "Step: 47300  \tTraining accuracy: 0.9087713360786438\n",
      "Step: 47300  \tValid loss: 0.22466163337230682\n",
      "Step: 47400  \tTraining loss: 0.21928423643112183\n",
      "Step: 47400  \tTraining accuracy: 0.9087756276130676\n",
      "Step: 47400  \tValid loss: 0.2246602326631546\n",
      "Step: 47500  \tTraining loss: 0.21928267180919647\n",
      "Step: 47500  \tTraining accuracy: 0.9087799191474915\n",
      "Step: 47500  \tValid loss: 0.2246609628200531\n",
      "Step: 47600  \tTraining loss: 0.2192809283733368\n",
      "Step: 47600  \tTraining accuracy: 0.9087842106819153\n",
      "Step: 47600  \tValid loss: 0.22465939819812775\n",
      "Step: 47700  \tTraining loss: 0.219279482960701\n",
      "Step: 47700  \tTraining accuracy: 0.9087885022163391\n",
      "Step: 47700  \tValid loss: 0.22465932369232178\n",
      "Step: 47800  \tTraining loss: 0.21927803754806519\n",
      "Step: 47800  \tTraining accuracy: 0.9087927341461182\n",
      "Step: 47800  \tValid loss: 0.22465817630290985\n",
      "Step: 47900  \tTraining loss: 0.21927616000175476\n",
      "Step: 47900  \tTraining accuracy: 0.9087969660758972\n",
      "Step: 47900  \tValid loss: 0.22465795278549194\n",
      "Step: 48000  \tTraining loss: 0.21927468478679657\n",
      "Step: 48000  \tTraining accuracy: 0.9088011384010315\n",
      "Step: 48000  \tValid loss: 0.22465673089027405\n",
      "Step: 48100  \tTraining loss: 0.21927309036254883\n",
      "Step: 48100  \tTraining accuracy: 0.9088053703308105\n",
      "Step: 48100  \tValid loss: 0.22465695440769196\n",
      "Step: 48200  \tTraining loss: 0.21927157044410706\n",
      "Step: 48200  \tTraining accuracy: 0.9088095426559448\n",
      "Step: 48200  \tValid loss: 0.22465641796588898\n",
      "Step: 48300  \tTraining loss: 0.21926993131637573\n",
      "Step: 48300  \tTraining accuracy: 0.9088137149810791\n",
      "Step: 48300  \tValid loss: 0.22465495765209198\n",
      "Step: 48400  \tTraining loss: 0.21926818788051605\n",
      "Step: 48400  \tTraining accuracy: 0.9088178277015686\n",
      "Step: 48400  \tValid loss: 0.2246558666229248\n",
      "Step: 48500  \tTraining loss: 0.2192665934562683\n",
      "Step: 48500  \tTraining accuracy: 0.9088219404220581\n",
      "Step: 48500  \tValid loss: 0.2246568650007248\n",
      "Step: 48600  \tTraining loss: 0.2192651480436325\n",
      "Step: 48600  \tTraining accuracy: 0.9088260531425476\n",
      "Step: 48600  \tValid loss: 0.2246544361114502\n",
      "Step: 48700  \tTraining loss: 0.2192637026309967\n",
      "Step: 48700  \tTraining accuracy: 0.9088301658630371\n",
      "Step: 48700  \tValid loss: 0.22465485334396362\n",
      "Step: 48800  \tTraining loss: 0.21926207840442657\n",
      "Step: 48800  \tTraining accuracy: 0.9088342189788818\n",
      "Step: 48800  \tValid loss: 0.2246554046869278\n",
      "Step: 48900  \tTraining loss: 0.21926061809062958\n",
      "Step: 48900  \tTraining accuracy: 0.9088382720947266\n",
      "Step: 48900  \tValid loss: 0.22465580701828003\n",
      "Step: 49000  \tTraining loss: 0.21925881505012512\n",
      "Step: 49000  \tTraining accuracy: 0.9088423252105713\n",
      "Step: 49000  \tValid loss: 0.22465504705905914\n",
      "Step: 49100  \tTraining loss: 0.21925733983516693\n",
      "Step: 49100  \tTraining accuracy: 0.908846378326416\n",
      "Step: 49100  \tValid loss: 0.22465479373931885\n",
      "Step: 49200  \tTraining loss: 0.2192562222480774\n",
      "Step: 49200  \tTraining accuracy: 0.908850371837616\n",
      "Step: 49200  \tValid loss: 0.22465264797210693\n",
      "Step: 49300  \tTraining loss: 0.219254732131958\n",
      "Step: 49300  \tTraining accuracy: 0.9088543653488159\n",
      "Step: 49300  \tValid loss: 0.22465182840824127\n",
      "Step: 49400  \tTraining loss: 0.21925321221351624\n",
      "Step: 49400  \tTraining accuracy: 0.9088582992553711\n",
      "Step: 49400  \tValid loss: 0.22465470433235168\n",
      "Step: 49500  \tTraining loss: 0.2192515730857849\n",
      "Step: 49500  \tTraining accuracy: 0.908862292766571\n",
      "Step: 49500  \tValid loss: 0.22465355694293976\n",
      "Step: 49600  \tTraining loss: 0.21925008296966553\n",
      "Step: 49600  \tTraining accuracy: 0.9088662266731262\n",
      "Step: 49600  \tValid loss: 0.22465358674526215\n",
      "Step: 49700  \tTraining loss: 0.2192486822605133\n",
      "Step: 49700  \tTraining accuracy: 0.9088701605796814\n",
      "Step: 49700  \tValid loss: 0.2246522158384323\n",
      "Step: 49800  \tTraining loss: 0.21924729645252228\n",
      "Step: 49800  \tTraining accuracy: 0.9088740944862366\n",
      "Step: 49800  \tValid loss: 0.22465434670448303\n",
      "Step: 49900  \tTraining loss: 0.2192457765340805\n",
      "Step: 49900  \tTraining accuracy: 0.908877968788147\n",
      "Step: 49900  \tValid loss: 0.22465388476848602\n",
      "Step: 50000  \tTraining loss: 0.21924430131912231\n",
      "Step: 50000  \tTraining accuracy: 0.9088818430900574\n",
      "Step: 50000  \tValid loss: 0.2246541678905487\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.9088857\n",
      "Precision: 0.8722628\n",
      "Recall: 0.90702087\n",
      "F1 score: 0.9266407\n",
      "AUC: 0.91065323\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.908886   0.872263  0.907021  0.926641  0.910653  0.219244      0.908883   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.224651       0.908881    0.24778      8.0          0.001   50000.0   \n",
      "\n",
      "     steps  \n",
      "0  49999.0  \n",
      "36\n",
      "return as is\n",
      "/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/subject_num_108107/OddEvenPlays/RandomizedPlays1/RandomizeTrials\n",
      "(1421, 3)\n",
      "(1421, 1)\n",
      "(784, 3)\n",
      "(784, 1)\n",
      "(637, 3)\n",
      "(637, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/Users/ritwik7/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 100  \tTraining loss: 0.4825937747955322\n",
      "Step: 100  \tTraining accuracy: 0.8332160711288452\n",
      "Step: 100  \tValid loss: 0.4921784996986389\n",
      "Step: 200  \tTraining loss: 0.4605080187320709\n",
      "Step: 200  \tTraining accuracy: 0.8400187492370605\n",
      "Step: 200  \tValid loss: 0.4726478159427643\n",
      "Step: 300  \tTraining loss: 0.4488756060600281\n",
      "Step: 300  \tTraining accuracy: 0.8413792848587036\n",
      "Step: 300  \tValid loss: 0.4586648643016815\n",
      "Step: 400  \tTraining loss: 0.43960514664649963\n",
      "Step: 400  \tTraining accuracy: 0.8419623970985413\n",
      "Step: 400  \tValid loss: 0.447162389755249\n",
      "Step: 500  \tTraining loss: 0.4309469163417816\n",
      "Step: 500  \tTraining accuracy: 0.8422863483428955\n",
      "Step: 500  \tValid loss: 0.4365220367908478\n",
      "Step: 600  \tTraining loss: 0.41991204023361206\n",
      "Step: 600  \tTraining accuracy: 0.8424924612045288\n",
      "Step: 600  \tValid loss: 0.423277884721756\n",
      "Step: 700  \tTraining loss: 0.4065781533718109\n",
      "Step: 700  \tTraining accuracy: 0.8426352143287659\n",
      "Step: 700  \tValid loss: 0.4081227481365204\n",
      "Step: 800  \tTraining loss: 0.3941798508167267\n",
      "Step: 800  \tTraining accuracy: 0.8427398800849915\n",
      "Step: 800  \tValid loss: 0.39423155784606934\n",
      "Step: 900  \tTraining loss: 0.38223153352737427\n",
      "Step: 900  \tTraining accuracy: 0.84281986951828\n",
      "Step: 900  \tValid loss: 0.38054025173187256\n",
      "Step: 1000  \tTraining loss: 0.3719028830528259\n",
      "Step: 1000  \tTraining accuracy: 0.8428830504417419\n",
      "Step: 1000  \tValid loss: 0.36870041489601135\n",
      "Step: 1100  \tTraining loss: 0.3634175658226013\n",
      "Step: 1100  \tTraining accuracy: 0.8429341912269592\n",
      "Step: 1100  \tValid loss: 0.35835206508636475\n",
      "Step: 1200  \tTraining loss: 0.3565974533557892\n",
      "Step: 1200  \tTraining accuracy: 0.842976450920105\n",
      "Step: 1200  \tValid loss: 0.3496653139591217\n",
      "Step: 1300  \tTraining loss: 0.35124775767326355\n",
      "Step: 1300  \tTraining accuracy: 0.8430119752883911\n",
      "Step: 1300  \tValid loss: 0.342662513256073\n",
      "Step: 1400  \tTraining loss: 0.3470712900161743\n",
      "Step: 1400  \tTraining accuracy: 0.8431724905967712\n",
      "Step: 1400  \tValid loss: 0.33712777495384216\n",
      "Step: 1500  \tTraining loss: 0.3438524603843689\n",
      "Step: 1500  \tTraining accuracy: 0.8438447713851929\n",
      "Step: 1500  \tValid loss: 0.33269017934799194\n",
      "Step: 1600  \tTraining loss: 0.3413673937320709\n",
      "Step: 1600  \tTraining accuracy: 0.8444757461547852\n",
      "Step: 1600  \tValid loss: 0.3291555643081665\n",
      "Step: 1700  \tTraining loss: 0.33938533067703247\n",
      "Step: 1700  \tTraining accuracy: 0.845200777053833\n",
      "Step: 1700  \tValid loss: 0.3263006806373596\n",
      "Step: 1800  \tTraining loss: 0.3377210199832916\n",
      "Step: 1800  \tTraining accuracy: 0.8458631038665771\n",
      "Step: 1800  \tValid loss: 0.32396963238716125\n",
      "Step: 1900  \tTraining loss: 0.33626431226730347\n",
      "Step: 1900  \tTraining accuracy: 0.8464537858963013\n",
      "Step: 1900  \tValid loss: 0.32202377915382385\n",
      "Step: 2000  \tTraining loss: 0.3349529802799225\n",
      "Step: 2000  \tTraining accuracy: 0.8469839096069336\n",
      "Step: 2000  \tValid loss: 0.32014939188957214\n",
      "Step: 2100  \tTraining loss: 0.3337673246860504\n",
      "Step: 2100  \tTraining accuracy: 0.8474622964859009\n",
      "Step: 2100  \tValid loss: 0.31853294372558594\n",
      "Step: 2200  \tTraining loss: 0.3327158987522125\n",
      "Step: 2200  \tTraining accuracy: 0.847896158695221\n",
      "Step: 2200  \tValid loss: 0.31701281666755676\n",
      "Step: 2300  \tTraining loss: 0.3318159580230713\n",
      "Step: 2300  \tTraining accuracy: 0.8482915163040161\n",
      "Step: 2300  \tValid loss: 0.31575414538383484\n",
      "Step: 2400  \tTraining loss: 0.33107468485832214\n",
      "Step: 2400  \tTraining accuracy: 0.8486531972885132\n",
      "Step: 2400  \tValid loss: 0.31469398736953735\n",
      "Step: 2500  \tTraining loss: 0.3304886519908905\n",
      "Step: 2500  \tTraining accuracy: 0.8489853143692017\n",
      "Step: 2500  \tValid loss: 0.313821405172348\n",
      "Step: 2600  \tTraining loss: 0.3300388753414154\n",
      "Step: 2600  \tTraining accuracy: 0.8492914438247681\n",
      "Step: 2600  \tValid loss: 0.31312188506126404\n",
      "Step: 2700  \tTraining loss: 0.3296961486339569\n",
      "Step: 2700  \tTraining accuracy: 0.8495744466781616\n",
      "Step: 2700  \tValid loss: 0.3125667870044708\n",
      "Step: 2800  \tTraining loss: 0.3293881118297577\n",
      "Step: 2800  \tTraining accuracy: 0.8498368859291077\n",
      "Step: 2800  \tValid loss: 0.3121117055416107\n",
      "Step: 2900  \tTraining loss: 0.3291652500629425\n",
      "Step: 2900  \tTraining accuracy: 0.8500808477401733\n",
      "Step: 2900  \tValid loss: 0.3117142915725708\n",
      "Step: 3000  \tTraining loss: 0.3289697766304016\n",
      "Step: 3000  \tTraining accuracy: 0.850308358669281\n",
      "Step: 3000  \tValid loss: 0.31142354011535645\n",
      "Step: 3100  \tTraining loss: 0.3287891149520874\n",
      "Step: 3100  \tTraining accuracy: 0.8505208492279053\n",
      "Step: 3100  \tValid loss: 0.31118807196617126\n",
      "Step: 3200  \tTraining loss: 0.3286173641681671\n",
      "Step: 3200  \tTraining accuracy: 0.8507199287414551\n",
      "Step: 3200  \tValid loss: 0.3109906315803528\n",
      "Step: 3300  \tTraining loss: 0.32845339179039\n",
      "Step: 3300  \tTraining accuracy: 0.8509067296981812\n",
      "Step: 3300  \tValid loss: 0.3108258843421936\n",
      "Step: 3400  \tTraining loss: 0.32829731702804565\n",
      "Step: 3400  \tTraining accuracy: 0.8510823845863342\n",
      "Step: 3400  \tValid loss: 0.3106847107410431\n",
      "Step: 3500  \tTraining loss: 0.32814568281173706\n",
      "Step: 3500  \tTraining accuracy: 0.8512478470802307\n",
      "Step: 3500  \tValid loss: 0.31056272983551025\n",
      "Step: 3600  \tTraining loss: 0.3279932737350464\n",
      "Step: 3600  \tTraining accuracy: 0.8514040112495422\n",
      "Step: 3600  \tValid loss: 0.31044772267341614\n",
      "Step: 3700  \tTraining loss: 0.32785844802856445\n",
      "Step: 3700  \tTraining accuracy: 0.8515515923500061\n",
      "Step: 3700  \tValid loss: 0.31035298109054565\n",
      "Step: 3800  \tTraining loss: 0.3277300298213959\n",
      "Step: 3800  \tTraining accuracy: 0.8516913056373596\n",
      "Step: 3800  \tValid loss: 0.31026601791381836\n",
      "Step: 3900  \tTraining loss: 0.32760560512542725\n",
      "Step: 3900  \tTraining accuracy: 0.8518237471580505\n",
      "Step: 3900  \tValid loss: 0.310189425945282\n",
      "Step: 4000  \tTraining loss: 0.3274840712547302\n",
      "Step: 4000  \tTraining accuracy: 0.8519495129585266\n",
      "Step: 4000  \tValid loss: 0.3101157248020172\n",
      "Step: 4100  \tTraining loss: 0.3273634612560272\n",
      "Step: 4100  \tTraining accuracy: 0.852069079875946\n",
      "Step: 4100  \tValid loss: 0.31004253029823303\n",
      "Step: 4200  \tTraining loss: 0.3272436261177063\n",
      "Step: 4200  \tTraining accuracy: 0.8521828055381775\n",
      "Step: 4200  \tValid loss: 0.3099747598171234\n",
      "Step: 4300  \tTraining loss: 0.3271234929561615\n",
      "Step: 4300  \tTraining accuracy: 0.8522912859916687\n",
      "Step: 4300  \tValid loss: 0.3099118769168854\n",
      "Step: 4400  \tTraining loss: 0.32700517773628235\n",
      "Step: 4400  \tTraining accuracy: 0.852394700050354\n",
      "Step: 4400  \tValid loss: 0.30985644459724426\n",
      "Step: 4500  \tTraining loss: 0.32688984274864197\n",
      "Step: 4500  \tTraining accuracy: 0.8524935245513916\n",
      "Step: 4500  \tValid loss: 0.30981460213661194\n",
      "Step: 4600  \tTraining loss: 0.32678237557411194\n",
      "Step: 4600  \tTraining accuracy: 0.8525879383087158\n",
      "Step: 4600  \tValid loss: 0.30985766649246216\n",
      "Step: 4700  \tTraining loss: 0.326686829328537\n",
      "Step: 4700  \tTraining accuracy: 0.8526783585548401\n",
      "Step: 4700  \tValid loss: 0.309919148683548\n",
      "Step: 4800  \tTraining loss: 0.32659924030303955\n",
      "Step: 4800  \tTraining accuracy: 0.8527575135231018\n",
      "Step: 4800  \tValid loss: 0.30993667244911194\n",
      "Step: 4900  \tTraining loss: 0.3265153765678406\n",
      "Step: 4900  \tTraining accuracy: 0.8528769612312317\n",
      "Step: 4900  \tValid loss: 0.30990296602249146\n",
      "Step: 5000  \tTraining loss: 0.32641929388046265\n",
      "Step: 5000  \tTraining accuracy: 0.8529915809631348\n",
      "Step: 5000  \tValid loss: 0.30977049469947815\n",
      "Step: 5100  \tTraining loss: 0.3263424336910248\n",
      "Step: 5100  \tTraining accuracy: 0.8531016111373901\n",
      "Step: 5100  \tValid loss: 0.30981457233428955\n",
      "Step: 5200  \tTraining loss: 0.32627376914024353\n",
      "Step: 5200  \tTraining accuracy: 0.8532074093818665\n",
      "Step: 5200  \tValid loss: 0.3099045753479004\n",
      "Step: 5300  \tTraining loss: 0.3262113332748413\n",
      "Step: 5300  \tTraining accuracy: 0.8533092141151428\n",
      "Step: 5300  \tValid loss: 0.30991053581237793\n",
      "Step: 5400  \tTraining loss: 0.3261546194553375\n",
      "Step: 5400  \tTraining accuracy: 0.8534071445465088\n",
      "Step: 5400  \tValid loss: 0.30998241901397705\n",
      "Step: 5500  \tTraining loss: 0.3261033892631531\n",
      "Step: 5500  \tTraining accuracy: 0.853501558303833\n",
      "Step: 5500  \tValid loss: 0.31000426411628723\n",
      "Step: 5600  \tTraining loss: 0.3260585367679596\n",
      "Step: 5600  \tTraining accuracy: 0.8535925149917603\n",
      "Step: 5600  \tValid loss: 0.31004399061203003\n",
      "Step: 5700  \tTraining loss: 0.32601478695869446\n",
      "Step: 5700  \tTraining accuracy: 0.8536802530288696\n",
      "Step: 5700  \tValid loss: 0.31009966135025024\n",
      "Step: 5800  \tTraining loss: 0.32597652077674866\n",
      "Step: 5800  \tTraining accuracy: 0.8537649512290955\n",
      "Step: 5800  \tValid loss: 0.3101232349872589\n",
      "Step: 5900  \tTraining loss: 0.3259398341178894\n",
      "Step: 5900  \tTraining accuracy: 0.8538467288017273\n",
      "Step: 5900  \tValid loss: 0.3101429343223572\n",
      "Early stopping!\n",
      "\n",
      "Evaluation  on training set\n",
      "Accuracy: 0.8539258\n",
      "Precision: 0.8671698\n",
      "Recall: 0.9704392\n",
      "F1 score: 0.9019145\n",
      "AUC: 0.61391157\n",
      "   accuracy  precision    recall  f1_score       auc      loss  accuracy_val  \\\n",
      "0  0.853926    0.86717  0.970439  0.901914  0.613912  0.325908      0.853946   \n",
      "\n",
      "   loss_val  accuracy_test  loss_test  neurons  learning_rate  n_epochs  \\\n",
      "0  0.309748       0.853967   0.257383      8.0          0.001   50000.0   \n",
      "\n",
      "    steps  \n",
      "0  5996.0  \n"
     ]
    }
   ],
   "source": [
    "neurons = 8\n",
    "hist_flag=0\n",
    "randomize_trials_flag=True\n",
    "\n",
    "\n",
    "for num, subj_file_path in enumerate(subj_files_list):\n",
    "    print(num)\n",
    "\n",
    "# for subj_file_path in [subj_files_list[0]]:\n",
    "\n",
    "    file_path  =\"/Users/ritwik7/Dropbox (Personal)/Postdoc_UCL/DATA/rlab_incomplete_rewardSWB_code/by_RN/appdata/\"+ subj_file_path\n",
    "                \n",
    "#     file_path = file_path + \"/OddEvenPlays\"\n",
    "    file_path = file_path + \"/OddEvenPlays/RandomizedPlays1\"\n",
    "\n",
    "\n",
    "\n",
    "    train_data_df= pd.read_csv(file_path+\"/train_data.csv\")\n",
    "    test_data_df = pd.read_csv(file_path+\"/test_data.csv\")\n",
    "    val_data_df = pd.read_csv(file_path+\"/val_data.csv\")\n",
    "    \n",
    "    \n",
    "    ####### IMPORTANT ALTERATION\n",
    "    train_data_df = add_kback_features(train_data_df)\n",
    "    val_data_df = add_kback_features(val_data_df)\n",
    "    test_data_df = pd.read_csv(file_path+\"/test_data_new.csv\") ## otherwise testdata df would have NaNs for 1 back\n",
    "\n",
    "    if randomize_trials_flag==True:\n",
    "        file_path = file_path + \"/RandomizeTrials\"\n",
    "\n",
    "        #### FOR GENERATING RANDOMIZATIONS\n",
    "#         train_locs, test_locs, val_locs = generate_randomizations(train_data_df,test_data_df,val_data_df)\n",
    "\n",
    "        train_data_random_df, test_data_random_df,val_data_random_df, train_locs, test_locs, val_locs  = get_shuffled_data()\n",
    "\n",
    "        train_data_random_df, test_data_random_df,val_data_random_df = restore_input_feature(randomize_trials_flag,hist_flag)\n",
    "        \n",
    "        train_data_df , test_data_df, val_data_df = train_data_random_df, test_data_random_df, val_data_random_df\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    print(file_path)\n",
    "    \n",
    "    \n",
    "    train_X, train_y, test_X, test_y,val_X,val_y = data_split_odd_even(train_data_df,test_data_df,val_data_df)\n",
    "\n",
    "    pretraining = False; \n",
    "    metric_out_df, prob_train, prob_test, prob_val = train_RNN(neurons,train_X,train_y,test_X,test_y,val_X,val_y)\n",
    "    \n",
    "    print(metric_out_df)\n",
    "    \n",
    "   \n",
    " \n",
    "    prob_train_df = pd.DataFrame(prob_train.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_test_df = pd.DataFrame(prob_test.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "    prob_val_df = pd.DataFrame(prob_val.reshape(-1,2),columns = {'action_0','action_1'})\n",
    "\n",
    "    if hist_flag==0:\n",
    "        metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currO_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(file_path + \"/prob_train_currO_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(file_path + \"/prob_test_currO_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_val_df.to_csv(file_path + \"/prob_val_currO_neurons=\"+str(neurons)+\".csv\")\n",
    "    \n",
    "    \n",
    "    elif hist_flag==1:\n",
    "        metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currOprevC_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(file_path + \"/prob_train_currOprevC_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(file_path + \"/prob_test_currOprevC_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_val_df.to_csv(file_path + \"/prob_val_currOprevC_neurons=\"+str(neurons)+\".csv\")\n",
    "        \n",
    "        \n",
    "    elif hist_flag==2:\n",
    "        metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currOprevR_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(file_path + \"/prob_train_currOprevR_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(file_path + \"/prob_test_currOprevR_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_val_df.to_csv(file_path + \"/prob_val_currOprevR_neurons=\"+str(neurons)+\".csv\")\n",
    "\n",
    "    elif hist_flag==3:\n",
    "        metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currOprevRC_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(file_path + \"/prob_train_currOprevRC_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(file_path + \"/prob_test_currOprevRC_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_val_df.to_csv(file_path + \"/prob_val_currOprevRC_neurons=\"+str(neurons)+\".csv\")\n",
    "# ################################\n",
    "    elif hist_flag==4:\n",
    "        metric_out_df.to_csv(file_path+\"/LSTM_updated_Crossval_currprev_opts_metricsneurons=\"+str(neurons)+\".csv\")\n",
    "        prob_train_df.to_csv(file_path + \"/prob_train_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_test_df.to_csv(file_path + \"/prob_test_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "        prob_val_df.to_csv(file_path + \"/prob_val_currentprevopts_neurons=\"+str(neurons)+\".csv\")\n",
    "# #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_out = randomize_trials(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_locs.iloc[0:5,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_random_df[[\"PrevChoice\",\"PrevOutcome\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_out.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_random_df[[\"PrevChoice\",\"PrevOutcome\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
